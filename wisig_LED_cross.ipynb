{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb24e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集发射机数量： 150 具体为： ['1-1', '1-10', '1-11', '1-12', '1-14', '1-15', '1-16', '1-18', '1-19', '1-2', '1-8', '10-1', '10-10', '10-11', '10-17', '10-4', '10-7', '11-1', '11-10', '11-17', '11-19', '11-20', '11-4', '11-7', '12-1', '12-19', '12-20', '12-7', '13-14', '13-18', '13-19', '13-20', '13-3', '13-7', '14-10', '14-11', '14-12', '14-13', '14-14', '14-20', '14-7', '14-8', '14-9', '15-1', '15-19', '15-6', '16-1', '16-16', '16-19', '16-20', '16-5', '17-10', '17-11', '18-1', '18-10', '18-11', '18-12', '18-13', '18-14', '18-15', '18-16', '18-17', '18-2', '18-20', '18-4', '18-5', '18-7', '18-8', '18-9', '19-1', '19-10', '19-11', '19-12', '19-13', '19-14', '19-19', '19-2', '19-20', '19-3', '19-4', '19-6', '19-7', '19-8', '19-9', '2-1', '2-12', '2-13', '2-14', '2-15', '2-16', '2-17', '2-19', '2-20', '2-3', '2-4', '2-5', '2-6', '2-7', '2-8', '20-1', '20-12', '20-14', '20-15', '20-16', '20-18', '20-19', '20-20', '20-3', '20-4', '20-5', '20-7', '20-8', '3-1', '3-13', '3-18', '3-19', '3-2', '3-20', '3-8', '4-1', '4-10', '4-11', '5-1', '5-16', '5-20', '5-5', '6-1', '6-15', '6-6', '7-10', '7-11', '7-12', '7-13', '7-14', '7-20', '7-7', '7-8', '7-9', '8-1', '8-13', '8-14', '8-18', '8-20', '8-3', '8-7', '8-8', '9-1', '9-14', '9-20', '9-7']\n",
      "数据集接收机数量： 18 具体为： ['1-1', '1-19', '1-20', '13-7', '14-7', '18-19', '18-2', '19-1', '19-2', '2-1', '20-1', '20-19', '3-19', '7-14', '7-7', '8-14', '8-7', '8-8']\n",
      "数据集采集天数： 4 具体为： ['2021_03_01', '2021_03_08', '2021_03_15', '2021_03_23']\n",
      "150 18\n",
      "✅ 总样本数: 536064, 训练集: 402048, 测试集: 134016\n",
      "X_train shape: (402048, 240, 2)\n",
      "y_train shape: (402048,)\n",
      "X_test  shape: (134016, 240, 2)\n",
      "y_test  shape: (134016,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZY\\.conda\\envs\\MW-RFF\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 1/200 | TrainAcc=3.45% | ValAcc=4.14% | TrainLoss=4.6977 | ValLoss=4.6715 | AvgGrad=0.0823\n",
      "Epoch 2/200 | TrainAcc=7.46% | ValAcc=3.66% | TrainLoss=4.4545 | ValLoss=5.1885 | AvgGrad=0.1603\n",
      "Epoch 3/200 | TrainAcc=11.32% | ValAcc=7.47% | TrainLoss=4.2307 | ValLoss=4.5571 | AvgGrad=0.2348\n",
      "Epoch 4/200 | TrainAcc=14.17% | ValAcc=8.17% | TrainLoss=4.0610 | ValLoss=4.8306 | AvgGrad=0.2829\n",
      "Epoch 5/200 | TrainAcc=16.36% | ValAcc=3.93% | TrainLoss=3.9276 | ValLoss=6.4030 | AvgGrad=0.3149\n",
      "Epoch 6/200 | TrainAcc=18.25% | ValAcc=5.69% | TrainLoss=3.8220 | ValLoss=7.1926 | AvgGrad=0.3365\n",
      "Epoch 7/200 | TrainAcc=19.63% | ValAcc=7.79% | TrainLoss=3.7294 | ValLoss=5.2930 | AvgGrad=0.3542\n",
      "Epoch 8/200 | TrainAcc=21.06% | ValAcc=11.62% | TrainLoss=3.6474 | ValLoss=4.4682 | AvgGrad=0.3688\n",
      "Epoch 9/200 | TrainAcc=22.38% | ValAcc=8.44% | TrainLoss=3.5699 | ValLoss=4.7256 | AvgGrad=0.3816\n",
      "Epoch 10/200 | TrainAcc=23.51% | ValAcc=13.90% | TrainLoss=3.4999 | ValLoss=4.4098 | AvgGrad=0.3948\n",
      "Epoch 11/200 | TrainAcc=26.10% | ValAcc=19.53% | TrainLoss=3.3646 | ValLoss=3.6916 | AvgGrad=0.4173\n",
      "Epoch 12/200 | TrainAcc=26.81% | ValAcc=9.12% | TrainLoss=3.3221 | ValLoss=4.9593 | AvgGrad=0.4385\n",
      "Epoch 13/200 | TrainAcc=27.43% | ValAcc=19.06% | TrainLoss=3.2865 | ValLoss=3.7626 | AvgGrad=0.4526\n",
      "Epoch 14/200 | TrainAcc=27.95% | ValAcc=23.76% | TrainLoss=3.2551 | ValLoss=3.4649 | AvgGrad=0.4657\n",
      "Epoch 15/200 | TrainAcc=28.51% | ValAcc=13.49% | TrainLoss=3.2229 | ValLoss=4.1676 | AvgGrad=0.4770\n",
      "Epoch 16/200 | TrainAcc=29.15% | ValAcc=9.08% | TrainLoss=3.1939 | ValLoss=5.2015 | AvgGrad=0.4867\n",
      "Epoch 17/200 | TrainAcc=29.63% | ValAcc=20.85% | TrainLoss=3.1650 | ValLoss=3.6724 | AvgGrad=0.4950\n",
      "Epoch 18/200 | TrainAcc=30.07% | ValAcc=25.24% | TrainLoss=3.1386 | ValLoss=3.3831 | AvgGrad=0.5038\n",
      "Epoch 19/200 | TrainAcc=30.63% | ValAcc=21.82% | TrainLoss=3.1128 | ValLoss=3.5538 | AvgGrad=0.5121\n",
      "Epoch 20/200 | TrainAcc=31.08% | ValAcc=24.53% | TrainLoss=3.0867 | ValLoss=3.3952 | AvgGrad=0.5189\n",
      "Epoch 21/200 | TrainAcc=32.63% | ValAcc=25.86% | TrainLoss=3.0139 | ValLoss=3.3352 | AvgGrad=0.5276\n",
      "Epoch 22/200 | TrainAcc=32.93% | ValAcc=22.28% | TrainLoss=2.9962 | ValLoss=3.5343 | AvgGrad=0.5400\n",
      "Epoch 23/200 | TrainAcc=33.18% | ValAcc=27.00% | TrainLoss=2.9820 | ValLoss=3.2790 | AvgGrad=0.5472\n",
      "Epoch 24/200 | TrainAcc=33.50% | ValAcc=23.91% | TrainLoss=2.9684 | ValLoss=3.4654 | AvgGrad=0.5538\n",
      "Epoch 25/200 | TrainAcc=33.74% | ValAcc=27.11% | TrainLoss=2.9565 | ValLoss=3.2650 | AvgGrad=0.5606\n",
      "Epoch 26/200 | TrainAcc=33.94% | ValAcc=16.50% | TrainLoss=2.9432 | ValLoss=4.1636 | AvgGrad=0.5668\n",
      "Epoch 27/200 | TrainAcc=34.12% | ValAcc=19.99% | TrainLoss=2.9305 | ValLoss=3.7551 | AvgGrad=0.5730\n",
      "Epoch 28/200 | TrainAcc=34.48% | ValAcc=20.82% | TrainLoss=2.9181 | ValLoss=3.6932 | AvgGrad=0.5785\n",
      "Epoch 29/200 | TrainAcc=34.73% | ValAcc=23.79% | TrainLoss=2.9063 | ValLoss=3.4594 | AvgGrad=0.5838\n",
      "Epoch 30/200 | TrainAcc=34.87% | ValAcc=21.31% | TrainLoss=2.8949 | ValLoss=3.6416 | AvgGrad=0.5885\n",
      "Epoch 31/200 | TrainAcc=35.78% | ValAcc=31.34% | TrainLoss=2.8548 | ValLoss=3.0427 | AvgGrad=0.5922\n",
      "Epoch 32/200 | TrainAcc=35.92% | ValAcc=32.18% | TrainLoss=2.8470 | ValLoss=3.0164 | AvgGrad=0.5998\n",
      "Epoch 33/200 | TrainAcc=36.08% | ValAcc=30.63% | TrainLoss=2.8405 | ValLoss=3.0784 | AvgGrad=0.6048\n",
      "Epoch 34/200 | TrainAcc=36.10% | ValAcc=32.90% | TrainLoss=2.8343 | ValLoss=2.9770 | AvgGrad=0.6086\n",
      "Epoch 35/200 | TrainAcc=36.33% | ValAcc=32.59% | TrainLoss=2.8277 | ValLoss=2.9824 | AvgGrad=0.6128\n",
      "Epoch 36/200 | TrainAcc=36.39% | ValAcc=27.98% | TrainLoss=2.8216 | ValLoss=3.2186 | AvgGrad=0.6164\n",
      "Epoch 37/200 | TrainAcc=36.50% | ValAcc=29.39% | TrainLoss=2.8160 | ValLoss=3.1373 | AvgGrad=0.6211\n",
      "Epoch 38/200 | TrainAcc=36.63% | ValAcc=33.45% | TrainLoss=2.8100 | ValLoss=2.9545 | AvgGrad=0.6242\n",
      "Epoch 39/200 | TrainAcc=36.66% | ValAcc=26.69% | TrainLoss=2.8050 | ValLoss=3.2532 | AvgGrad=0.6279\n",
      "Epoch 40/200 | TrainAcc=36.86% | ValAcc=32.42% | TrainLoss=2.7987 | ValLoss=2.9837 | AvgGrad=0.6309\n",
      "Epoch 41/200 | TrainAcc=37.35% | ValAcc=32.86% | TrainLoss=2.7769 | ValLoss=2.9700 | AvgGrad=0.6316\n",
      "Epoch 42/200 | TrainAcc=37.40% | ValAcc=32.93% | TrainLoss=2.7729 | ValLoss=2.9669 | AvgGrad=0.6359\n",
      "Epoch 43/200 | TrainAcc=37.50% | ValAcc=35.01% | TrainLoss=2.7694 | ValLoss=2.8803 | AvgGrad=0.6389\n",
      "Epoch 44/200 | TrainAcc=37.50% | ValAcc=34.61% | TrainLoss=2.7665 | ValLoss=2.8944 | AvgGrad=0.6405\n",
      "Epoch 45/200 | TrainAcc=37.54% | ValAcc=34.91% | TrainLoss=2.7639 | ValLoss=2.8814 | AvgGrad=0.6433\n",
      "Epoch 46/200 | TrainAcc=37.57% | ValAcc=34.85% | TrainLoss=2.7612 | ValLoss=2.8786 | AvgGrad=0.6462\n",
      "Epoch 47/200 | TrainAcc=37.66% | ValAcc=34.59% | TrainLoss=2.7580 | ValLoss=2.8870 | AvgGrad=0.6479\n",
      "Epoch 48/200 | TrainAcc=37.75% | ValAcc=32.99% | TrainLoss=2.7542 | ValLoss=2.9524 | AvgGrad=0.6496\n",
      "Epoch 49/200 | TrainAcc=37.81% | ValAcc=34.98% | TrainLoss=2.7518 | ValLoss=2.8845 | AvgGrad=0.6519\n",
      "Epoch 50/200 | TrainAcc=37.83% | ValAcc=32.94% | TrainLoss=2.7493 | ValLoss=2.9566 | AvgGrad=0.6537\n",
      "Epoch 51/200 | TrainAcc=38.04% | ValAcc=35.87% | TrainLoss=2.7385 | ValLoss=2.8395 | AvgGrad=0.6542\n",
      "Epoch 52/200 | TrainAcc=38.14% | ValAcc=35.82% | TrainLoss=2.7355 | ValLoss=2.8451 | AvgGrad=0.6558\n",
      "Epoch 53/200 | TrainAcc=38.12% | ValAcc=35.99% | TrainLoss=2.7342 | ValLoss=2.8373 | AvgGrad=0.6576\n",
      "Epoch 54/200 | TrainAcc=38.19% | ValAcc=35.76% | TrainLoss=2.7334 | ValLoss=2.8386 | AvgGrad=0.6590\n",
      "Epoch 55/200 | TrainAcc=38.16% | ValAcc=36.03% | TrainLoss=2.7308 | ValLoss=2.8374 | AvgGrad=0.6598\n",
      "Epoch 56/200 | TrainAcc=38.22% | ValAcc=35.60% | TrainLoss=2.7305 | ValLoss=2.8524 | AvgGrad=0.6615\n",
      "Epoch 57/200 | TrainAcc=38.28% | ValAcc=35.81% | TrainLoss=2.7288 | ValLoss=2.8444 | AvgGrad=0.6622\n",
      "Epoch 58/200 | TrainAcc=38.27% | ValAcc=35.80% | TrainLoss=2.7267 | ValLoss=2.8391 | AvgGrad=0.6646\n",
      "Epoch 59/200 | TrainAcc=38.31% | ValAcc=35.08% | TrainLoss=2.7259 | ValLoss=2.8638 | AvgGrad=0.6648\n",
      "Epoch 60/200 | TrainAcc=38.31% | ValAcc=36.11% | TrainLoss=2.7238 | ValLoss=2.8288 | AvgGrad=0.6660\n",
      "Epoch 61/200 | TrainAcc=38.55% | ValAcc=36.25% | TrainLoss=2.7182 | ValLoss=2.8235 | AvgGrad=0.6666\n",
      "Epoch 62/200 | TrainAcc=38.55% | ValAcc=36.23% | TrainLoss=2.7174 | ValLoss=2.8263 | AvgGrad=0.6671\n",
      "Epoch 63/200 | TrainAcc=38.57% | ValAcc=36.22% | TrainLoss=2.7160 | ValLoss=2.8237 | AvgGrad=0.6667\n",
      "Epoch 64/200 | TrainAcc=38.49% | ValAcc=36.24% | TrainLoss=2.7151 | ValLoss=2.8219 | AvgGrad=0.6685\n",
      "Epoch 65/200 | TrainAcc=38.59% | ValAcc=36.24% | TrainLoss=2.7149 | ValLoss=2.8208 | AvgGrad=0.6695\n",
      "Epoch 66/200 | TrainAcc=38.55% | ValAcc=36.24% | TrainLoss=2.7147 | ValLoss=2.8205 | AvgGrad=0.6702\n",
      "Epoch 67/200 | TrainAcc=38.66% | ValAcc=36.29% | TrainLoss=2.7131 | ValLoss=2.8168 | AvgGrad=0.6702\n",
      "Epoch 68/200 | TrainAcc=38.64% | ValAcc=36.20% | TrainLoss=2.7122 | ValLoss=2.8221 | AvgGrad=0.6712\n",
      "Epoch 69/200 | TrainAcc=38.64% | ValAcc=36.10% | TrainLoss=2.7119 | ValLoss=2.8215 | AvgGrad=0.6712\n",
      "Epoch 70/200 | TrainAcc=38.65% | ValAcc=36.21% | TrainLoss=2.7111 | ValLoss=2.8238 | AvgGrad=0.6718\n",
      "Epoch 71/200 | TrainAcc=38.68% | ValAcc=36.38% | TrainLoss=2.7070 | ValLoss=2.8152 | AvgGrad=0.6708\n",
      "Epoch 72/200 | TrainAcc=38.70% | ValAcc=36.31% | TrainLoss=2.7077 | ValLoss=2.8144 | AvgGrad=0.6727\n",
      "Epoch 73/200 | TrainAcc=38.72% | ValAcc=36.42% | TrainLoss=2.7065 | ValLoss=2.8157 | AvgGrad=0.6723\n",
      "Epoch 74/200 | TrainAcc=38.79% | ValAcc=36.37% | TrainLoss=2.7064 | ValLoss=2.8160 | AvgGrad=0.6726\n",
      "Epoch 75/200 | TrainAcc=38.78% | ValAcc=36.35% | TrainLoss=2.7053 | ValLoss=2.8147 | AvgGrad=0.6734\n",
      "Epoch 76/200 | TrainAcc=38.73% | ValAcc=36.33% | TrainLoss=2.7055 | ValLoss=2.8156 | AvgGrad=0.6742\n",
      "Epoch 77/200 | TrainAcc=38.75% | ValAcc=36.38% | TrainLoss=2.7052 | ValLoss=2.8134 | AvgGrad=0.6744\n",
      "Epoch 78/200 | TrainAcc=38.73% | ValAcc=36.48% | TrainLoss=2.7050 | ValLoss=2.8115 | AvgGrad=0.6745\n",
      "Epoch 79/200 | TrainAcc=38.76% | ValAcc=36.44% | TrainLoss=2.7050 | ValLoss=2.8127 | AvgGrad=0.6751\n",
      "Epoch 80/200 | TrainAcc=38.76% | ValAcc=36.40% | TrainLoss=2.7044 | ValLoss=2.8127 | AvgGrad=0.6752\n",
      "Epoch 81/200 | TrainAcc=38.87% | ValAcc=36.44% | TrainLoss=2.7026 | ValLoss=2.8133 | AvgGrad=0.6754\n",
      "Epoch 82/200 | TrainAcc=38.92% | ValAcc=36.42% | TrainLoss=2.7015 | ValLoss=2.8111 | AvgGrad=0.6748\n",
      "Epoch 83/200 | TrainAcc=38.77% | ValAcc=36.41% | TrainLoss=2.7024 | ValLoss=2.8146 | AvgGrad=0.6763\n",
      "Epoch 84/200 | TrainAcc=38.84% | ValAcc=36.38% | TrainLoss=2.7023 | ValLoss=2.8114 | AvgGrad=0.6755\n",
      "Epoch 85/200 | TrainAcc=38.86% | ValAcc=36.43% | TrainLoss=2.7012 | ValLoss=2.8093 | AvgGrad=0.6767\n",
      "Epoch 86/200 | TrainAcc=38.85% | ValAcc=36.45% | TrainLoss=2.7013 | ValLoss=2.8103 | AvgGrad=0.6759\n",
      "Early stopping.\n",
      "Fold 1 Test Accuracy: 36.16%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 1/200 | TrainAcc=3.52% | ValAcc=1.87% | TrainLoss=4.6887 | ValLoss=5.4232 | AvgGrad=0.0838\n",
      "Epoch 2/200 | TrainAcc=8.07% | ValAcc=2.07% | TrainLoss=4.4218 | ValLoss=5.9373 | AvgGrad=0.1691\n",
      "Epoch 3/200 | TrainAcc=11.81% | ValAcc=5.86% | TrainLoss=4.2023 | ValLoss=4.8957 | AvgGrad=0.2425\n",
      "Epoch 4/200 | TrainAcc=14.47% | ValAcc=6.48% | TrainLoss=4.0429 | ValLoss=5.1095 | AvgGrad=0.2843\n",
      "Epoch 5/200 | TrainAcc=16.53% | ValAcc=6.05% | TrainLoss=3.9166 | ValLoss=5.1500 | AvgGrad=0.3120\n",
      "Epoch 6/200 | TrainAcc=18.34% | ValAcc=11.33% | TrainLoss=3.8090 | ValLoss=4.3158 | AvgGrad=0.3323\n",
      "Epoch 7/200 | TrainAcc=19.88% | ValAcc=8.53% | TrainLoss=3.7197 | ValLoss=5.4744 | AvgGrad=0.3489\n",
      "Epoch 8/200 | TrainAcc=21.22% | ValAcc=8.48% | TrainLoss=3.6398 | ValLoss=5.2102 | AvgGrad=0.3626\n",
      "Epoch 9/200 | TrainAcc=22.42% | ValAcc=8.38% | TrainLoss=3.5675 | ValLoss=5.3084 | AvgGrad=0.3746\n",
      "Epoch 10/200 | TrainAcc=23.57% | ValAcc=6.90% | TrainLoss=3.5014 | ValLoss=5.9994 | AvgGrad=0.3857\n",
      "Epoch 11/200 | TrainAcc=25.95% | ValAcc=16.87% | TrainLoss=3.3723 | ValLoss=3.9213 | AvgGrad=0.4074\n",
      "Epoch 12/200 | TrainAcc=26.72% | ValAcc=15.23% | TrainLoss=3.3309 | ValLoss=4.0710 | AvgGrad=0.4304\n",
      "Epoch 13/200 | TrainAcc=27.31% | ValAcc=7.84% | TrainLoss=3.2969 | ValLoss=6.2441 | AvgGrad=0.4455\n",
      "Epoch 14/200 | TrainAcc=27.89% | ValAcc=19.50% | TrainLoss=3.2663 | ValLoss=3.7979 | AvgGrad=0.4566\n",
      "Epoch 15/200 | TrainAcc=28.37% | ValAcc=7.36% | TrainLoss=3.2376 | ValLoss=6.0977 | AvgGrad=0.4659\n",
      "Epoch 16/200 | TrainAcc=28.95% | ValAcc=16.59% | TrainLoss=3.2082 | ValLoss=3.9793 | AvgGrad=0.4750\n",
      "Epoch 17/200 | TrainAcc=29.35% | ValAcc=16.16% | TrainLoss=3.1816 | ValLoss=3.9833 | AvgGrad=0.4833\n",
      "Epoch 18/200 | TrainAcc=29.90% | ValAcc=15.46% | TrainLoss=3.1565 | ValLoss=4.1543 | AvgGrad=0.4905\n",
      "Epoch 19/200 | TrainAcc=30.32% | ValAcc=15.02% | TrainLoss=3.1308 | ValLoss=4.1810 | AvgGrad=0.4966\n",
      "Epoch 20/200 | TrainAcc=30.76% | ValAcc=13.43% | TrainLoss=3.1071 | ValLoss=4.4018 | AvgGrad=0.5028\n",
      "Epoch 21/200 | TrainAcc=32.25% | ValAcc=23.47% | TrainLoss=3.0357 | ValLoss=3.4904 | AvgGrad=0.5120\n",
      "Epoch 22/200 | TrainAcc=32.52% | ValAcc=14.11% | TrainLoss=3.0197 | ValLoss=4.3082 | AvgGrad=0.5233\n",
      "Epoch 23/200 | TrainAcc=32.79% | ValAcc=21.46% | TrainLoss=3.0066 | ValLoss=3.6735 | AvgGrad=0.5306\n",
      "Epoch 24/200 | TrainAcc=32.96% | ValAcc=18.05% | TrainLoss=2.9928 | ValLoss=3.9554 | AvgGrad=0.5376\n",
      "Epoch 25/200 | TrainAcc=33.27% | ValAcc=22.46% | TrainLoss=2.9809 | ValLoss=3.5285 | AvgGrad=0.5444\n",
      "Epoch 26/200 | TrainAcc=33.46% | ValAcc=25.84% | TrainLoss=2.9689 | ValLoss=3.3608 | AvgGrad=0.5493\n",
      "Epoch 27/200 | TrainAcc=33.75% | ValAcc=19.60% | TrainLoss=2.9565 | ValLoss=3.7849 | AvgGrad=0.5554\n",
      "Epoch 28/200 | TrainAcc=33.96% | ValAcc=21.70% | TrainLoss=2.9445 | ValLoss=3.5949 | AvgGrad=0.5604\n",
      "Epoch 29/200 | TrainAcc=34.23% | ValAcc=25.96% | TrainLoss=2.9330 | ValLoss=3.3354 | AvgGrad=0.5653\n",
      "Epoch 30/200 | TrainAcc=34.34% | ValAcc=20.80% | TrainLoss=2.9229 | ValLoss=3.6765 | AvgGrad=0.5706\n",
      "Epoch 31/200 | TrainAcc=35.22% | ValAcc=29.54% | TrainLoss=2.8834 | ValLoss=3.1297 | AvgGrad=0.5747\n",
      "Epoch 32/200 | TrainAcc=35.36% | ValAcc=32.32% | TrainLoss=2.8746 | ValLoss=3.0203 | AvgGrad=0.5808\n",
      "Epoch 33/200 | TrainAcc=35.47% | ValAcc=32.52% | TrainLoss=2.8688 | ValLoss=3.0087 | AvgGrad=0.5860\n",
      "Epoch 34/200 | TrainAcc=35.61% | ValAcc=31.34% | TrainLoss=2.8627 | ValLoss=3.0531 | AvgGrad=0.5905\n",
      "Epoch 35/200 | TrainAcc=35.72% | ValAcc=31.74% | TrainLoss=2.8571 | ValLoss=3.0368 | AvgGrad=0.5946\n",
      "Epoch 36/200 | TrainAcc=35.89% | ValAcc=28.66% | TrainLoss=2.8512 | ValLoss=3.1906 | AvgGrad=0.5981\n",
      "Epoch 37/200 | TrainAcc=35.90% | ValAcc=27.60% | TrainLoss=2.8446 | ValLoss=3.2644 | AvgGrad=0.6015\n",
      "Epoch 38/200 | TrainAcc=36.01% | ValAcc=32.49% | TrainLoss=2.8396 | ValLoss=3.0008 | AvgGrad=0.6063\n",
      "Epoch 39/200 | TrainAcc=36.17% | ValAcc=29.08% | TrainLoss=2.8333 | ValLoss=3.1632 | AvgGrad=0.6078\n",
      "Epoch 40/200 | TrainAcc=36.26% | ValAcc=28.73% | TrainLoss=2.8284 | ValLoss=3.1931 | AvgGrad=0.6130\n",
      "Epoch 41/200 | TrainAcc=36.75% | ValAcc=33.78% | TrainLoss=2.8078 | ValLoss=2.9475 | AvgGrad=0.6134\n",
      "Epoch 42/200 | TrainAcc=36.79% | ValAcc=34.32% | TrainLoss=2.8025 | ValLoss=2.9235 | AvgGrad=0.6174\n",
      "Epoch 43/200 | TrainAcc=36.86% | ValAcc=33.85% | TrainLoss=2.7998 | ValLoss=2.9415 | AvgGrad=0.6205\n",
      "Epoch 44/200 | TrainAcc=36.91% | ValAcc=34.14% | TrainLoss=2.7968 | ValLoss=2.9290 | AvgGrad=0.6228\n",
      "Epoch 45/200 | TrainAcc=37.01% | ValAcc=33.53% | TrainLoss=2.7937 | ValLoss=2.9531 | AvgGrad=0.6258\n",
      "Epoch 46/200 | TrainAcc=37.01% | ValAcc=33.83% | TrainLoss=2.7913 | ValLoss=2.9421 | AvgGrad=0.6277\n",
      "Epoch 47/200 | TrainAcc=37.08% | ValAcc=33.66% | TrainLoss=2.7879 | ValLoss=2.9485 | AvgGrad=0.6297\n",
      "Epoch 48/200 | TrainAcc=37.13% | ValAcc=32.73% | TrainLoss=2.7851 | ValLoss=2.9830 | AvgGrad=0.6316\n",
      "Epoch 49/200 | TrainAcc=37.25% | ValAcc=31.73% | TrainLoss=2.7820 | ValLoss=3.0274 | AvgGrad=0.6344\n",
      "Epoch 50/200 | TrainAcc=37.29% | ValAcc=33.51% | TrainLoss=2.7790 | ValLoss=2.9529 | AvgGrad=0.6364\n",
      "Early stopping.\n",
      "Fold 2 Test Accuracy: 33.56%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 1/200 | TrainAcc=3.44% | ValAcc=4.08% | TrainLoss=4.6945 | ValLoss=4.7491 | AvgGrad=0.0829\n",
      "Epoch 2/200 | TrainAcc=7.78% | ValAcc=3.98% | TrainLoss=4.4310 | ValLoss=4.8520 | AvgGrad=0.1659\n",
      "Epoch 3/200 | TrainAcc=11.80% | ValAcc=5.05% | TrainLoss=4.2044 | ValLoss=5.0393 | AvgGrad=0.2411\n",
      "Epoch 4/200 | TrainAcc=14.50% | ValAcc=4.24% | TrainLoss=4.0387 | ValLoss=6.9808 | AvgGrad=0.2867\n",
      "Epoch 5/200 | TrainAcc=16.63% | ValAcc=6.76% | TrainLoss=3.9127 | ValLoss=5.1354 | AvgGrad=0.3151\n",
      "Epoch 6/200 | TrainAcc=18.36% | ValAcc=5.94% | TrainLoss=3.8107 | ValLoss=4.9594 | AvgGrad=0.3352\n",
      "Epoch 7/200 | TrainAcc=19.78% | ValAcc=7.50% | TrainLoss=3.7253 | ValLoss=5.9855 | AvgGrad=0.3510\n",
      "Epoch 8/200 | TrainAcc=21.18% | ValAcc=4.81% | TrainLoss=3.6474 | ValLoss=6.1995 | AvgGrad=0.3662\n",
      "Epoch 9/200 | TrainAcc=22.37% | ValAcc=11.59% | TrainLoss=3.5767 | ValLoss=4.8552 | AvgGrad=0.3788\n",
      "Epoch 10/200 | TrainAcc=23.32% | ValAcc=2.52% | TrainLoss=3.5118 | ValLoss=12.2494 | AvgGrad=0.3909\n",
      "Epoch 11/200 | TrainAcc=25.84% | ValAcc=11.89% | TrainLoss=3.3836 | ValLoss=4.3633 | AvgGrad=0.4128\n",
      "Epoch 12/200 | TrainAcc=26.52% | ValAcc=11.45% | TrainLoss=3.3419 | ValLoss=4.4736 | AvgGrad=0.4348\n",
      "Epoch 13/200 | TrainAcc=27.18% | ValAcc=11.82% | TrainLoss=3.3089 | ValLoss=4.7284 | AvgGrad=0.4484\n",
      "Epoch 14/200 | TrainAcc=27.74% | ValAcc=13.09% | TrainLoss=3.2778 | ValLoss=4.3715 | AvgGrad=0.4598\n",
      "Epoch 15/200 | TrainAcc=28.20% | ValAcc=15.47% | TrainLoss=3.2474 | ValLoss=4.1312 | AvgGrad=0.4717\n",
      "Epoch 16/200 | TrainAcc=28.73% | ValAcc=11.75% | TrainLoss=3.2203 | ValLoss=4.4867 | AvgGrad=0.4812\n",
      "Epoch 17/200 | TrainAcc=29.18% | ValAcc=19.16% | TrainLoss=3.1930 | ValLoss=3.7514 | AvgGrad=0.4909\n",
      "Epoch 18/200 | TrainAcc=29.70% | ValAcc=17.91% | TrainLoss=3.1672 | ValLoss=3.8656 | AvgGrad=0.4988\n",
      "Epoch 19/200 | TrainAcc=30.12% | ValAcc=18.67% | TrainLoss=3.1411 | ValLoss=3.8613 | AvgGrad=0.5069\n",
      "Epoch 20/200 | TrainAcc=30.62% | ValAcc=15.42% | TrainLoss=3.1159 | ValLoss=4.0053 | AvgGrad=0.5145\n",
      "Epoch 21/200 | TrainAcc=32.16% | ValAcc=26.50% | TrainLoss=3.0457 | ValLoss=3.3130 | AvgGrad=0.5246\n",
      "Epoch 22/200 | TrainAcc=32.31% | ValAcc=21.36% | TrainLoss=3.0301 | ValLoss=3.5718 | AvgGrad=0.5363\n",
      "Epoch 23/200 | TrainAcc=32.72% | ValAcc=23.18% | TrainLoss=3.0158 | ValLoss=3.4916 | AvgGrad=0.5449\n",
      "Epoch 24/200 | TrainAcc=32.93% | ValAcc=14.60% | TrainLoss=3.0018 | ValLoss=4.3194 | AvgGrad=0.5524\n",
      "Epoch 25/200 | TrainAcc=33.04% | ValAcc=21.34% | TrainLoss=2.9904 | ValLoss=3.5950 | AvgGrad=0.5597\n",
      "Epoch 26/200 | TrainAcc=33.38% | ValAcc=21.18% | TrainLoss=2.9777 | ValLoss=3.6144 | AvgGrad=0.5665\n",
      "Epoch 27/200 | TrainAcc=33.55% | ValAcc=22.42% | TrainLoss=2.9660 | ValLoss=3.5359 | AvgGrad=0.5718\n",
      "Epoch 28/200 | TrainAcc=33.78% | ValAcc=18.55% | TrainLoss=2.9538 | ValLoss=3.8392 | AvgGrad=0.5782\n",
      "Epoch 29/200 | TrainAcc=33.98% | ValAcc=26.91% | TrainLoss=2.9433 | ValLoss=3.2641 | AvgGrad=0.5844\n",
      "Epoch 30/200 | TrainAcc=34.18% | ValAcc=24.75% | TrainLoss=2.9315 | ValLoss=3.4391 | AvgGrad=0.5901\n",
      "Epoch 31/200 | TrainAcc=35.06% | ValAcc=29.92% | TrainLoss=2.8926 | ValLoss=3.1256 | AvgGrad=0.5933\n",
      "Epoch 32/200 | TrainAcc=35.19% | ValAcc=27.28% | TrainLoss=2.8837 | ValLoss=3.2428 | AvgGrad=0.6008\n",
      "Epoch 33/200 | TrainAcc=35.39% | ValAcc=29.48% | TrainLoss=2.8780 | ValLoss=3.1442 | AvgGrad=0.6068\n",
      "Epoch 34/200 | TrainAcc=35.42% | ValAcc=24.72% | TrainLoss=2.8727 | ValLoss=3.4335 | AvgGrad=0.6108\n",
      "Epoch 35/200 | TrainAcc=35.54% | ValAcc=27.12% | TrainLoss=2.8660 | ValLoss=3.2779 | AvgGrad=0.6158\n",
      "Epoch 36/200 | TrainAcc=35.66% | ValAcc=30.90% | TrainLoss=2.8615 | ValLoss=3.0637 | AvgGrad=0.6193\n",
      "Epoch 37/200 | TrainAcc=35.80% | ValAcc=29.87% | TrainLoss=2.8555 | ValLoss=3.1291 | AvgGrad=0.6235\n",
      "Epoch 38/200 | TrainAcc=35.87% | ValAcc=29.43% | TrainLoss=2.8494 | ValLoss=3.1457 | AvgGrad=0.6270\n",
      "Epoch 39/200 | TrainAcc=35.97% | ValAcc=31.05% | TrainLoss=2.8442 | ValLoss=3.0601 | AvgGrad=0.6303\n",
      "Epoch 40/200 | TrainAcc=36.11% | ValAcc=26.08% | TrainLoss=2.8384 | ValLoss=3.3147 | AvgGrad=0.6336\n",
      "Epoch 41/200 | TrainAcc=36.60% | ValAcc=33.13% | TrainLoss=2.8174 | ValLoss=2.9726 | AvgGrad=0.6359\n",
      "Epoch 42/200 | TrainAcc=36.73% | ValAcc=33.56% | TrainLoss=2.8130 | ValLoss=2.9489 | AvgGrad=0.6386\n",
      "Epoch 43/200 | TrainAcc=36.68% | ValAcc=32.98% | TrainLoss=2.8111 | ValLoss=2.9765 | AvgGrad=0.6422\n",
      "Epoch 44/200 | TrainAcc=36.78% | ValAcc=32.69% | TrainLoss=2.8075 | ValLoss=2.9943 | AvgGrad=0.6450\n",
      "Epoch 45/200 | TrainAcc=36.83% | ValAcc=32.74% | TrainLoss=2.8051 | ValLoss=2.9768 | AvgGrad=0.6469\n",
      "Epoch 46/200 | TrainAcc=36.86% | ValAcc=34.45% | TrainLoss=2.8016 | ValLoss=2.9094 | AvgGrad=0.6503\n",
      "Epoch 47/200 | TrainAcc=36.94% | ValAcc=33.95% | TrainLoss=2.7990 | ValLoss=2.9335 | AvgGrad=0.6516\n",
      "Epoch 48/200 | TrainAcc=36.98% | ValAcc=34.22% | TrainLoss=2.7969 | ValLoss=2.9166 | AvgGrad=0.6551\n",
      "Epoch 49/200 | TrainAcc=37.04% | ValAcc=31.34% | TrainLoss=2.7928 | ValLoss=3.0502 | AvgGrad=0.6564\n",
      "Epoch 50/200 | TrainAcc=37.12% | ValAcc=33.41% | TrainLoss=2.7906 | ValLoss=2.9473 | AvgGrad=0.6586\n",
      "Epoch 51/200 | TrainAcc=37.34% | ValAcc=35.04% | TrainLoss=2.7804 | ValLoss=2.8825 | AvgGrad=0.6589\n",
      "Epoch 52/200 | TrainAcc=37.32% | ValAcc=35.22% | TrainLoss=2.7781 | ValLoss=2.8779 | AvgGrad=0.6609\n",
      "Epoch 53/200 | TrainAcc=37.41% | ValAcc=34.15% | TrainLoss=2.7767 | ValLoss=2.9179 | AvgGrad=0.6618\n",
      "Epoch 54/200 | TrainAcc=37.44% | ValAcc=34.75% | TrainLoss=2.7750 | ValLoss=2.8878 | AvgGrad=0.6640\n",
      "Epoch 55/200 | TrainAcc=37.45% | ValAcc=34.77% | TrainLoss=2.7736 | ValLoss=2.8878 | AvgGrad=0.6655\n",
      "Epoch 56/200 | TrainAcc=37.53% | ValAcc=34.74% | TrainLoss=2.7723 | ValLoss=2.8932 | AvgGrad=0.6663\n",
      "Epoch 57/200 | TrainAcc=37.48% | ValAcc=34.97% | TrainLoss=2.7714 | ValLoss=2.8822 | AvgGrad=0.6677\n",
      "Epoch 58/200 | TrainAcc=37.56% | ValAcc=34.99% | TrainLoss=2.7703 | ValLoss=2.8847 | AvgGrad=0.6690\n",
      "Epoch 59/200 | TrainAcc=37.57% | ValAcc=35.31% | TrainLoss=2.7680 | ValLoss=2.8702 | AvgGrad=0.6711\n",
      "Epoch 60/200 | TrainAcc=37.61% | ValAcc=34.95% | TrainLoss=2.7668 | ValLoss=2.8795 | AvgGrad=0.6719\n",
      "Epoch 61/200 | TrainAcc=37.76% | ValAcc=35.40% | TrainLoss=2.7603 | ValLoss=2.8670 | AvgGrad=0.6715\n",
      "Epoch 62/200 | TrainAcc=37.78% | ValAcc=35.07% | TrainLoss=2.7600 | ValLoss=2.8805 | AvgGrad=0.6728\n",
      "Epoch 63/200 | TrainAcc=37.79% | ValAcc=35.35% | TrainLoss=2.7588 | ValLoss=2.8662 | AvgGrad=0.6740\n",
      "Epoch 64/200 | TrainAcc=37.76% | ValAcc=35.46% | TrainLoss=2.7579 | ValLoss=2.8617 | AvgGrad=0.6737\n",
      "Epoch 65/200 | TrainAcc=37.76% | ValAcc=35.42% | TrainLoss=2.7575 | ValLoss=2.8663 | AvgGrad=0.6748\n",
      "Epoch 66/200 | TrainAcc=37.81% | ValAcc=35.39% | TrainLoss=2.7569 | ValLoss=2.8632 | AvgGrad=0.6753\n",
      "Epoch 67/200 | TrainAcc=37.80% | ValAcc=35.49% | TrainLoss=2.7556 | ValLoss=2.8632 | AvgGrad=0.6759\n",
      "Epoch 68/200 | TrainAcc=37.83% | ValAcc=35.33% | TrainLoss=2.7554 | ValLoss=2.8663 | AvgGrad=0.6771\n",
      "Epoch 69/200 | TrainAcc=37.83% | ValAcc=35.51% | TrainLoss=2.7547 | ValLoss=2.8595 | AvgGrad=0.6778\n",
      "Epoch 70/200 | TrainAcc=37.84% | ValAcc=35.49% | TrainLoss=2.7542 | ValLoss=2.8590 | AvgGrad=0.6784\n",
      "Epoch 71/200 | TrainAcc=37.93% | ValAcc=35.61% | TrainLoss=2.7511 | ValLoss=2.8562 | AvgGrad=0.6791\n",
      "Epoch 72/200 | TrainAcc=37.92% | ValAcc=35.57% | TrainLoss=2.7499 | ValLoss=2.8553 | AvgGrad=0.6788\n",
      "Epoch 73/200 | TrainAcc=37.94% | ValAcc=35.60% | TrainLoss=2.7503 | ValLoss=2.8543 | AvgGrad=0.6791\n",
      "Epoch 74/200 | TrainAcc=38.01% | ValAcc=35.56% | TrainLoss=2.7504 | ValLoss=2.8537 | AvgGrad=0.6798\n",
      "Epoch 75/200 | TrainAcc=37.93% | ValAcc=35.53% | TrainLoss=2.7495 | ValLoss=2.8563 | AvgGrad=0.6802\n",
      "Epoch 76/200 | TrainAcc=37.93% | ValAcc=35.61% | TrainLoss=2.7490 | ValLoss=2.8547 | AvgGrad=0.6797\n",
      "Epoch 77/200 | TrainAcc=37.94% | ValAcc=35.60% | TrainLoss=2.7491 | ValLoss=2.8534 | AvgGrad=0.6807\n",
      "Epoch 78/200 | TrainAcc=37.99% | ValAcc=35.64% | TrainLoss=2.7487 | ValLoss=2.8528 | AvgGrad=0.6809\n",
      "Epoch 79/200 | TrainAcc=37.97% | ValAcc=35.67% | TrainLoss=2.7485 | ValLoss=2.8535 | AvgGrad=0.6814\n",
      "Epoch 80/200 | TrainAcc=38.01% | ValAcc=35.57% | TrainLoss=2.7472 | ValLoss=2.8535 | AvgGrad=0.6818\n",
      "Epoch 81/200 | TrainAcc=38.02% | ValAcc=35.68% | TrainLoss=2.7462 | ValLoss=2.8516 | AvgGrad=0.6809\n",
      "Epoch 82/200 | TrainAcc=38.05% | ValAcc=35.69% | TrainLoss=2.7454 | ValLoss=2.8514 | AvgGrad=0.6822\n",
      "Epoch 83/200 | TrainAcc=37.97% | ValAcc=35.67% | TrainLoss=2.7454 | ValLoss=2.8510 | AvgGrad=0.6815\n",
      "Epoch 84/200 | TrainAcc=38.06% | ValAcc=35.71% | TrainLoss=2.7452 | ValLoss=2.8513 | AvgGrad=0.6829\n",
      "Epoch 85/200 | TrainAcc=38.02% | ValAcc=35.65% | TrainLoss=2.7455 | ValLoss=2.8507 | AvgGrad=0.6825\n",
      "Epoch 86/200 | TrainAcc=38.03% | ValAcc=35.66% | TrainLoss=2.7450 | ValLoss=2.8516 | AvgGrad=0.6824\n",
      "Epoch 87/200 | TrainAcc=37.98% | ValAcc=35.72% | TrainLoss=2.7453 | ValLoss=2.8507 | AvgGrad=0.6830\n",
      "Epoch 88/200 | TrainAcc=37.99% | ValAcc=35.69% | TrainLoss=2.7454 | ValLoss=2.8509 | AvgGrad=0.6823\n",
      "Epoch 89/200 | TrainAcc=38.03% | ValAcc=35.70% | TrainLoss=2.7442 | ValLoss=2.8518 | AvgGrad=0.6833\n",
      "Epoch 90/200 | TrainAcc=38.05% | ValAcc=35.67% | TrainLoss=2.7443 | ValLoss=2.8513 | AvgGrad=0.6828\n",
      "Epoch 91/200 | TrainAcc=38.09% | ValAcc=35.69% | TrainLoss=2.7435 | ValLoss=2.8514 | AvgGrad=0.6833\n",
      "Epoch 92/200 | TrainAcc=38.08% | ValAcc=35.66% | TrainLoss=2.7437 | ValLoss=2.8496 | AvgGrad=0.6838\n",
      "Early stopping.\n",
      "Fold 3 Test Accuracy: 35.24%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 1/200 | TrainAcc=3.51% | ValAcc=4.81% | TrainLoss=4.6948 | ValLoss=4.6222 | AvgGrad=0.0817\n",
      "Epoch 2/200 | TrainAcc=7.54% | ValAcc=5.38% | TrainLoss=4.4475 | ValLoss=4.7602 | AvgGrad=0.1581\n",
      "Epoch 3/200 | TrainAcc=11.50% | ValAcc=7.89% | TrainLoss=4.2166 | ValLoss=4.4783 | AvgGrad=0.2353\n",
      "Epoch 4/200 | TrainAcc=14.42% | ValAcc=5.82% | TrainLoss=4.0437 | ValLoss=5.1149 | AvgGrad=0.2840\n",
      "Epoch 5/200 | TrainAcc=16.61% | ValAcc=11.79% | TrainLoss=3.9114 | ValLoss=4.2298 | AvgGrad=0.3171\n",
      "Epoch 6/200 | TrainAcc=18.49% | ValAcc=9.51% | TrainLoss=3.8011 | ValLoss=4.6878 | AvgGrad=0.3426\n",
      "Epoch 7/200 | TrainAcc=20.11% | ValAcc=3.87% | TrainLoss=3.7073 | ValLoss=6.8627 | AvgGrad=0.3631\n",
      "Epoch 8/200 | TrainAcc=21.54% | ValAcc=7.59% | TrainLoss=3.6246 | ValLoss=5.3261 | AvgGrad=0.3784\n",
      "Epoch 9/200 | TrainAcc=22.79% | ValAcc=4.87% | TrainLoss=3.5486 | ValLoss=6.5838 | AvgGrad=0.3911\n",
      "Epoch 10/200 | TrainAcc=24.01% | ValAcc=9.90% | TrainLoss=3.4763 | ValLoss=4.8077 | AvgGrad=0.4020\n",
      "Epoch 11/200 | TrainAcc=26.53% | ValAcc=22.15% | TrainLoss=3.3451 | ValLoss=3.5536 | AvgGrad=0.4229\n",
      "Epoch 12/200 | TrainAcc=27.31% | ValAcc=20.52% | TrainLoss=3.3019 | ValLoss=3.6640 | AvgGrad=0.4447\n",
      "Epoch 13/200 | TrainAcc=27.87% | ValAcc=19.99% | TrainLoss=3.2677 | ValLoss=3.6800 | AvgGrad=0.4593\n",
      "Epoch 14/200 | TrainAcc=28.40% | ValAcc=15.78% | TrainLoss=3.2351 | ValLoss=4.0721 | AvgGrad=0.4717\n",
      "Epoch 15/200 | TrainAcc=29.02% | ValAcc=16.47% | TrainLoss=3.2033 | ValLoss=3.9521 | AvgGrad=0.4827\n",
      "Epoch 16/200 | TrainAcc=29.46% | ValAcc=3.32% | TrainLoss=3.1741 | ValLoss=8.6351 | AvgGrad=0.4938\n",
      "Epoch 17/200 | TrainAcc=30.01% | ValAcc=24.38% | TrainLoss=3.1451 | ValLoss=3.4111 | AvgGrad=0.5020\n",
      "Epoch 18/200 | TrainAcc=30.43% | ValAcc=19.90% | TrainLoss=3.1193 | ValLoss=3.6921 | AvgGrad=0.5112\n",
      "Epoch 19/200 | TrainAcc=30.97% | ValAcc=19.13% | TrainLoss=3.0923 | ValLoss=3.8370 | AvgGrad=0.5193\n",
      "Epoch 20/200 | TrainAcc=31.55% | ValAcc=21.65% | TrainLoss=3.0658 | ValLoss=3.6009 | AvgGrad=0.5272\n",
      "Epoch 21/200 | TrainAcc=33.02% | ValAcc=26.03% | TrainLoss=2.9949 | ValLoss=3.3285 | AvgGrad=0.5364\n",
      "Epoch 22/200 | TrainAcc=33.24% | ValAcc=24.24% | TrainLoss=2.9781 | ValLoss=3.4492 | AvgGrad=0.5479\n",
      "Epoch 23/200 | TrainAcc=33.40% | ValAcc=26.41% | TrainLoss=2.9651 | ValLoss=3.3015 | AvgGrad=0.5574\n",
      "Epoch 24/200 | TrainAcc=33.81% | ValAcc=29.08% | TrainLoss=2.9507 | ValLoss=3.1557 | AvgGrad=0.5648\n",
      "Epoch 25/200 | TrainAcc=34.00% | ValAcc=27.10% | TrainLoss=2.9378 | ValLoss=3.2613 | AvgGrad=0.5708\n",
      "Epoch 26/200 | TrainAcc=34.22% | ValAcc=22.81% | TrainLoss=2.9261 | ValLoss=3.5511 | AvgGrad=0.5781\n",
      "Epoch 27/200 | TrainAcc=34.46% | ValAcc=29.21% | TrainLoss=2.9125 | ValLoss=3.1495 | AvgGrad=0.5841\n",
      "Epoch 28/200 | TrainAcc=34.75% | ValAcc=23.39% | TrainLoss=2.9009 | ValLoss=3.4797 | AvgGrad=0.5903\n",
      "Epoch 29/200 | TrainAcc=34.98% | ValAcc=27.26% | TrainLoss=2.8896 | ValLoss=3.2476 | AvgGrad=0.5953\n",
      "Epoch 30/200 | TrainAcc=35.20% | ValAcc=31.05% | TrainLoss=2.8777 | ValLoss=3.0793 | AvgGrad=0.6008\n",
      "Epoch 31/200 | TrainAcc=36.00% | ValAcc=30.10% | TrainLoss=2.8385 | ValLoss=3.1015 | AvgGrad=0.6041\n",
      "Epoch 32/200 | TrainAcc=36.16% | ValAcc=30.54% | TrainLoss=2.8302 | ValLoss=3.0774 | AvgGrad=0.6100\n",
      "Epoch 33/200 | TrainAcc=36.24% | ValAcc=32.35% | TrainLoss=2.8245 | ValLoss=2.9968 | AvgGrad=0.6164\n",
      "Epoch 34/200 | TrainAcc=36.40% | ValAcc=33.09% | TrainLoss=2.8181 | ValLoss=2.9705 | AvgGrad=0.6204\n",
      "Epoch 35/200 | TrainAcc=36.55% | ValAcc=28.83% | TrainLoss=2.8117 | ValLoss=3.1520 | AvgGrad=0.6250\n",
      "Epoch 36/200 | TrainAcc=36.63% | ValAcc=29.90% | TrainLoss=2.8055 | ValLoss=3.1204 | AvgGrad=0.6289\n",
      "Epoch 37/200 | TrainAcc=36.73% | ValAcc=32.62% | TrainLoss=2.7997 | ValLoss=2.9794 | AvgGrad=0.6318\n",
      "Epoch 38/200 | TrainAcc=36.85% | ValAcc=29.49% | TrainLoss=2.7945 | ValLoss=3.1320 | AvgGrad=0.6362\n",
      "Epoch 39/200 | TrainAcc=37.00% | ValAcc=34.02% | TrainLoss=2.7885 | ValLoss=2.9313 | AvgGrad=0.6407\n",
      "Epoch 40/200 | TrainAcc=37.06% | ValAcc=32.31% | TrainLoss=2.7835 | ValLoss=2.9946 | AvgGrad=0.6431\n",
      "Epoch 41/200 | TrainAcc=37.54% | ValAcc=33.72% | TrainLoss=2.7618 | ValLoss=2.9232 | AvgGrad=0.6443\n",
      "Epoch 42/200 | TrainAcc=37.62% | ValAcc=34.19% | TrainLoss=2.7578 | ValLoss=2.9154 | AvgGrad=0.6483\n",
      "Epoch 43/200 | TrainAcc=37.72% | ValAcc=34.57% | TrainLoss=2.7550 | ValLoss=2.8878 | AvgGrad=0.6507\n",
      "Epoch 44/200 | TrainAcc=37.76% | ValAcc=35.18% | TrainLoss=2.7514 | ValLoss=2.8760 | AvgGrad=0.6529\n",
      "Epoch 45/200 | TrainAcc=37.77% | ValAcc=35.37% | TrainLoss=2.7490 | ValLoss=2.8624 | AvgGrad=0.6560\n",
      "Epoch 46/200 | TrainAcc=37.92% | ValAcc=35.12% | TrainLoss=2.7455 | ValLoss=2.8685 | AvgGrad=0.6587\n",
      "Epoch 47/200 | TrainAcc=37.92% | ValAcc=35.27% | TrainLoss=2.7426 | ValLoss=2.8654 | AvgGrad=0.6608\n",
      "Epoch 48/200 | TrainAcc=37.96% | ValAcc=34.72% | TrainLoss=2.7399 | ValLoss=2.8839 | AvgGrad=0.6627\n",
      "Epoch 49/200 | TrainAcc=38.03% | ValAcc=35.28% | TrainLoss=2.7367 | ValLoss=2.8644 | AvgGrad=0.6642\n",
      "Epoch 50/200 | TrainAcc=38.10% | ValAcc=35.27% | TrainLoss=2.7345 | ValLoss=2.8679 | AvgGrad=0.6672\n",
      "Epoch 51/200 | TrainAcc=38.29% | ValAcc=35.76% | TrainLoss=2.7228 | ValLoss=2.8397 | AvgGrad=0.6669\n",
      "Epoch 52/200 | TrainAcc=38.36% | ValAcc=35.89% | TrainLoss=2.7207 | ValLoss=2.8349 | AvgGrad=0.6684\n",
      "Epoch 53/200 | TrainAcc=38.38% | ValAcc=35.84% | TrainLoss=2.7189 | ValLoss=2.8399 | AvgGrad=0.6713\n",
      "Epoch 54/200 | TrainAcc=38.44% | ValAcc=36.16% | TrainLoss=2.7173 | ValLoss=2.8271 | AvgGrad=0.6721\n",
      "Epoch 55/200 | TrainAcc=38.44% | ValAcc=35.84% | TrainLoss=2.7159 | ValLoss=2.8363 | AvgGrad=0.6730\n",
      "Epoch 56/200 | TrainAcc=38.49% | ValAcc=35.88% | TrainLoss=2.7144 | ValLoss=2.8320 | AvgGrad=0.6746\n",
      "Epoch 57/200 | TrainAcc=38.55% | ValAcc=35.86% | TrainLoss=2.7132 | ValLoss=2.8361 | AvgGrad=0.6757\n",
      "Epoch 58/200 | TrainAcc=38.52% | ValAcc=35.99% | TrainLoss=2.7122 | ValLoss=2.8288 | AvgGrad=0.6770\n",
      "Epoch 59/200 | TrainAcc=38.53% | ValAcc=36.37% | TrainLoss=2.7110 | ValLoss=2.8208 | AvgGrad=0.6785\n",
      "Epoch 60/200 | TrainAcc=38.64% | ValAcc=36.08% | TrainLoss=2.7098 | ValLoss=2.8216 | AvgGrad=0.6797\n",
      "Epoch 61/200 | TrainAcc=38.76% | ValAcc=36.31% | TrainLoss=2.7028 | ValLoss=2.8139 | AvgGrad=0.6794\n",
      "Epoch 62/200 | TrainAcc=38.73% | ValAcc=36.25% | TrainLoss=2.7021 | ValLoss=2.8190 | AvgGrad=0.6798\n",
      "Epoch 63/200 | TrainAcc=38.78% | ValAcc=36.42% | TrainLoss=2.7014 | ValLoss=2.8107 | AvgGrad=0.6810\n",
      "Epoch 64/200 | TrainAcc=38.79% | ValAcc=36.38% | TrainLoss=2.7007 | ValLoss=2.8142 | AvgGrad=0.6806\n",
      "Epoch 65/200 | TrainAcc=38.79% | ValAcc=36.46% | TrainLoss=2.7001 | ValLoss=2.8104 | AvgGrad=0.6820\n",
      "Epoch 66/200 | TrainAcc=38.71% | ValAcc=36.46% | TrainLoss=2.7001 | ValLoss=2.8096 | AvgGrad=0.6830\n",
      "Epoch 67/200 | TrainAcc=38.82% | ValAcc=36.46% | TrainLoss=2.6989 | ValLoss=2.8123 | AvgGrad=0.6833\n",
      "Epoch 68/200 | TrainAcc=38.85% | ValAcc=36.37% | TrainLoss=2.6973 | ValLoss=2.8100 | AvgGrad=0.6842\n",
      "Epoch 69/200 | TrainAcc=38.80% | ValAcc=36.52% | TrainLoss=2.6972 | ValLoss=2.8117 | AvgGrad=0.6844\n",
      "Epoch 70/200 | TrainAcc=38.82% | ValAcc=36.39% | TrainLoss=2.6967 | ValLoss=2.8150 | AvgGrad=0.6863\n",
      "Epoch 71/200 | TrainAcc=38.88% | ValAcc=36.59% | TrainLoss=2.6935 | ValLoss=2.8043 | AvgGrad=0.6851\n",
      "Epoch 72/200 | TrainAcc=38.92% | ValAcc=36.54% | TrainLoss=2.6928 | ValLoss=2.8057 | AvgGrad=0.6856\n",
      "Epoch 73/200 | TrainAcc=38.90% | ValAcc=36.58% | TrainLoss=2.6925 | ValLoss=2.8056 | AvgGrad=0.6865\n",
      "Epoch 74/200 | TrainAcc=39.00% | ValAcc=36.58% | TrainLoss=2.6917 | ValLoss=2.8058 | AvgGrad=0.6867\n",
      "Epoch 75/200 | TrainAcc=39.02% | ValAcc=36.62% | TrainLoss=2.6912 | ValLoss=2.8043 | AvgGrad=0.6871\n",
      "Epoch 76/200 | TrainAcc=38.95% | ValAcc=36.65% | TrainLoss=2.6913 | ValLoss=2.8033 | AvgGrad=0.6880\n",
      "Epoch 77/200 | TrainAcc=39.01% | ValAcc=36.62% | TrainLoss=2.6911 | ValLoss=2.8009 | AvgGrad=0.6873\n",
      "Epoch 78/200 | TrainAcc=38.96% | ValAcc=36.66% | TrainLoss=2.6907 | ValLoss=2.8015 | AvgGrad=0.6882\n",
      "Epoch 79/200 | TrainAcc=39.03% | ValAcc=36.62% | TrainLoss=2.6904 | ValLoss=2.8023 | AvgGrad=0.6889\n",
      "Epoch 80/200 | TrainAcc=38.97% | ValAcc=36.68% | TrainLoss=2.6896 | ValLoss=2.8049 | AvgGrad=0.6891\n",
      "Epoch 81/200 | TrainAcc=38.99% | ValAcc=36.73% | TrainLoss=2.6878 | ValLoss=2.8005 | AvgGrad=0.6882\n",
      "Epoch 82/200 | TrainAcc=39.03% | ValAcc=36.71% | TrainLoss=2.6880 | ValLoss=2.8015 | AvgGrad=0.6889\n",
      "Epoch 83/200 | TrainAcc=39.06% | ValAcc=36.65% | TrainLoss=2.6876 | ValLoss=2.8013 | AvgGrad=0.6888\n",
      "Epoch 84/200 | TrainAcc=39.09% | ValAcc=36.59% | TrainLoss=2.6869 | ValLoss=2.8003 | AvgGrad=0.6891\n",
      "Epoch 85/200 | TrainAcc=39.03% | ValAcc=36.58% | TrainLoss=2.6867 | ValLoss=2.8009 | AvgGrad=0.6897\n",
      "Epoch 86/200 | TrainAcc=39.03% | ValAcc=36.64% | TrainLoss=2.6876 | ValLoss=2.8007 | AvgGrad=0.6905\n",
      "Epoch 87/200 | TrainAcc=39.08% | ValAcc=36.67% | TrainLoss=2.6870 | ValLoss=2.8001 | AvgGrad=0.6903\n",
      "Epoch 88/200 | TrainAcc=39.05% | ValAcc=36.75% | TrainLoss=2.6871 | ValLoss=2.8009 | AvgGrad=0.6903\n",
      "Epoch 89/200 | TrainAcc=39.06% | ValAcc=36.67% | TrainLoss=2.6866 | ValLoss=2.8003 | AvgGrad=0.6900\n",
      "Epoch 90/200 | TrainAcc=39.05% | ValAcc=36.74% | TrainLoss=2.6863 | ValLoss=2.8009 | AvgGrad=0.6906\n",
      "Epoch 91/200 | TrainAcc=39.13% | ValAcc=36.73% | TrainLoss=2.6858 | ValLoss=2.8003 | AvgGrad=0.6902\n",
      "Epoch 92/200 | TrainAcc=39.08% | ValAcc=36.73% | TrainLoss=2.6859 | ValLoss=2.8010 | AvgGrad=0.6901\n",
      "Epoch 93/200 | TrainAcc=39.12% | ValAcc=36.71% | TrainLoss=2.6861 | ValLoss=2.7989 | AvgGrad=0.6909\n",
      "Epoch 94/200 | TrainAcc=39.11% | ValAcc=36.66% | TrainLoss=2.6856 | ValLoss=2.7990 | AvgGrad=0.6903\n",
      "Epoch 95/200 | TrainAcc=39.11% | ValAcc=36.73% | TrainLoss=2.6856 | ValLoss=2.8011 | AvgGrad=0.6908\n",
      "Epoch 96/200 | TrainAcc=39.10% | ValAcc=36.62% | TrainLoss=2.6844 | ValLoss=2.8003 | AvgGrad=0.6904\n",
      "Early stopping.\n",
      "Fold 4 Test Accuracy: 36.62%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 1/200 | TrainAcc=3.49% | ValAcc=3.92% | TrainLoss=4.6948 | ValLoss=4.7223 | AvgGrad=0.0825\n",
      "Epoch 2/200 | TrainAcc=7.67% | ValAcc=3.94% | TrainLoss=4.4418 | ValLoss=5.1388 | AvgGrad=0.1654\n",
      "Epoch 3/200 | TrainAcc=11.72% | ValAcc=3.32% | TrainLoss=4.2111 | ValLoss=7.9047 | AvgGrad=0.2423\n",
      "Epoch 4/200 | TrainAcc=14.66% | ValAcc=10.84% | TrainLoss=4.0394 | ValLoss=4.2582 | AvgGrad=0.2890\n",
      "Epoch 5/200 | TrainAcc=16.84% | ValAcc=4.55% | TrainLoss=3.9068 | ValLoss=6.3317 | AvgGrad=0.3212\n",
      "Epoch 6/200 | TrainAcc=18.53% | ValAcc=15.15% | TrainLoss=3.7997 | ValLoss=3.9991 | AvgGrad=0.3446\n",
      "Epoch 7/200 | TrainAcc=20.18% | ValAcc=11.93% | TrainLoss=3.7040 | ValLoss=4.2737 | AvgGrad=0.3634\n",
      "Epoch 8/200 | TrainAcc=21.62% | ValAcc=4.94% | TrainLoss=3.6211 | ValLoss=7.0838 | AvgGrad=0.3785\n",
      "Epoch 9/200 | TrainAcc=22.84% | ValAcc=6.01% | TrainLoss=3.5470 | ValLoss=5.4640 | AvgGrad=0.3895\n",
      "Epoch 10/200 | TrainAcc=23.98% | ValAcc=9.95% | TrainLoss=3.4793 | ValLoss=4.8492 | AvgGrad=0.3981\n",
      "Epoch 11/200 | TrainAcc=26.45% | ValAcc=17.38% | TrainLoss=3.3479 | ValLoss=3.8693 | AvgGrad=0.4185\n",
      "Epoch 12/200 | TrainAcc=27.11% | ValAcc=16.02% | TrainLoss=3.3075 | ValLoss=3.9796 | AvgGrad=0.4395\n",
      "Epoch 13/200 | TrainAcc=27.80% | ValAcc=12.51% | TrainLoss=3.2734 | ValLoss=4.4170 | AvgGrad=0.4540\n",
      "Epoch 14/200 | TrainAcc=28.30% | ValAcc=22.23% | TrainLoss=3.2411 | ValLoss=3.5658 | AvgGrad=0.4660\n",
      "Epoch 15/200 | TrainAcc=28.81% | ValAcc=14.46% | TrainLoss=3.2111 | ValLoss=4.1951 | AvgGrad=0.4762\n",
      "Epoch 16/200 | TrainAcc=29.41% | ValAcc=11.24% | TrainLoss=3.1828 | ValLoss=4.5808 | AvgGrad=0.4869\n",
      "Epoch 17/200 | TrainAcc=29.83% | ValAcc=16.09% | TrainLoss=3.1561 | ValLoss=3.9689 | AvgGrad=0.4948\n",
      "Epoch 18/200 | TrainAcc=30.39% | ValAcc=12.10% | TrainLoss=3.1291 | ValLoss=4.6388 | AvgGrad=0.5033\n",
      "Epoch 19/200 | TrainAcc=30.88% | ValAcc=19.22% | TrainLoss=3.1025 | ValLoss=3.7599 | AvgGrad=0.5111\n",
      "Epoch 20/200 | TrainAcc=31.23% | ValAcc=7.92% | TrainLoss=3.0787 | ValLoss=5.2830 | AvgGrad=0.5184\n",
      "Epoch 21/200 | TrainAcc=32.75% | ValAcc=29.04% | TrainLoss=3.0084 | ValLoss=3.1722 | AvgGrad=0.5270\n",
      "Epoch 22/200 | TrainAcc=33.03% | ValAcc=29.67% | TrainLoss=2.9911 | ValLoss=3.1435 | AvgGrad=0.5384\n",
      "Epoch 23/200 | TrainAcc=33.36% | ValAcc=27.98% | TrainLoss=2.9770 | ValLoss=3.2202 | AvgGrad=0.5474\n",
      "Epoch 24/200 | TrainAcc=33.58% | ValAcc=27.09% | TrainLoss=2.9642 | ValLoss=3.2718 | AvgGrad=0.5539\n",
      "Epoch 25/200 | TrainAcc=33.80% | ValAcc=28.60% | TrainLoss=2.9508 | ValLoss=3.1914 | AvgGrad=0.5608\n",
      "Epoch 26/200 | TrainAcc=33.98% | ValAcc=24.07% | TrainLoss=2.9398 | ValLoss=3.4356 | AvgGrad=0.5668\n",
      "Epoch 27/200 | TrainAcc=34.26% | ValAcc=21.30% | TrainLoss=2.9274 | ValLoss=3.6263 | AvgGrad=0.5727\n",
      "Epoch 28/200 | TrainAcc=34.44% | ValAcc=27.42% | TrainLoss=2.9164 | ValLoss=3.2347 | AvgGrad=0.5790\n",
      "Epoch 29/200 | TrainAcc=34.68% | ValAcc=29.07% | TrainLoss=2.9047 | ValLoss=3.1432 | AvgGrad=0.5844\n",
      "Epoch 30/200 | TrainAcc=34.93% | ValAcc=27.12% | TrainLoss=2.8917 | ValLoss=3.2439 | AvgGrad=0.5908\n",
      "Early stopping.\n",
      "Fold 5 Test Accuracy: 27.16%\n",
      "\n",
      "=== Overall Summary ===\n",
      "Val Acc: 33.87 ± 3.55\n",
      "Test Acc: 33.75 ± 3.45\n",
      "\n",
      "All results saved in ./training_results\\2025-11-24_21-53-32_wisig_LED_SNR20dB_fd266_classes_150_CNN\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from  data_utilities import *\n",
    "import cv2  # OpenCV 用于调整图像大小和颜色处理\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gc  # 引入垃圾回收模块\n",
    "from tqdm.auto import tqdm  # 自动适配环境 导入tqdm进度条库\n",
    "from collections import defaultdict\n",
    "\n",
    "dataset_name = 'ManyTx'\n",
    "dataset_path='../ManyTx.pkl/'\n",
    "\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path,dataset_name)\n",
    "\n",
    "print(\"数据集发射机数量：\",len(compact_dataset['tx_list']),\"具体为：\",compact_dataset['tx_list'])\n",
    "print(\"数据集接收机数量：\",len(compact_dataset['rx_list']),\"具体为：\",compact_dataset['rx_list'])\n",
    "print(\"数据集采集天数：\",len(compact_dataset['capture_date_list']),\"具体为：\",compact_dataset['capture_date_list'])\n",
    "\n",
    "\n",
    "tx_list = compact_dataset['tx_list']\n",
    "rx_list = compact_dataset['rx_list']\n",
    "equalized = 0\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "\n",
    "\n",
    "n_tx = len(tx_list)\n",
    "n_rx = len(rx_list)\n",
    "print(n_tx,n_rx)\n",
    "\n",
    "\n",
    "# 参数设置\n",
    "max_sig = None          # 每个 TX-RX-日期最多使用的信号数\n",
    "block_size = 240        # 每个 block 的信号数\n",
    "y = 5                  # 拼接时每组多少条信号\n",
    "test_ratio = 0.25        # 测试集比例\n",
    "\n",
    "# 调用函数\n",
    "X_train, y_train, X_test, y_test = preprocess_dataset_cross_IQ_blocks_all_mix_random(\n",
    "    compact_dataset=compact_dataset,\n",
    "    tx_list=tx_list,\n",
    "    max_sig=max_sig,\n",
    "    equalized=equalized,\n",
    "    block_size=block_size,\n",
    "    y=y,\n",
    "    test_ratio=test_ratio,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test  shape:\", X_test.shape) \n",
    "print(\"y_test  shape:\", y_test.shape)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "torch._dynamo.disable()\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import pywt\n",
    "\n",
    "# ====================== 参数设置 ======================\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 数据处理参数\n",
    "USE_LOG = True\n",
    "WAVELET = 'db6'\n",
    "WAVELET_LEVEL = 6\n",
    "FS = 20e6\n",
    "FC = 2.4e9\n",
    "SNR_DB = 20          # None 或具体数值\n",
    "VELOCITY_KMH = 120     # None 或具体数值\n",
    "ADD_NOISE = True\n",
    "ADD_DOPPLER = True\n",
    "\n",
    "# 训练参数\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "N_SPLITS = 5\n",
    "PATIENCE = 8\n",
    "\n",
    "# 保存路径\n",
    "SAVE_ROOT = \"./training_results\"\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "# ====================== 数据处理函数 ======================\n",
    "def compute_doppler_shift(v_kmh, fc_hz):\n",
    "    if not v_kmh: return 0\n",
    "    c = 3e8\n",
    "    v_mps = v_kmh / 3.6\n",
    "    return fc_hz * v_mps / c\n",
    "\n",
    "def add_complex_awgn(signal, snr_db):\n",
    "    \"\"\"\n",
    "    为复数信号添加AWGN噪声\n",
    "    \"\"\"\n",
    "    # 计算信号功率\n",
    "    signal_power = np.mean(np.abs(signal) ** 2)\n",
    "    \n",
    "    # 计算噪声功率\n",
    "    snr_linear = 10 ** (snr_db / 10)\n",
    "    noise_power = signal_power / snr_linear\n",
    "    \n",
    "    # 生成复数噪声（实部和虚部独立，各占一半功率）\n",
    "    noise_std = np.sqrt(noise_power / 2)\n",
    "    noise_real = np.random.normal(0, noise_std, signal.shape)\n",
    "    noise_imag = np.random.normal(0, noise_std, signal.shape)\n",
    "    noise = noise_real + 1j * noise_imag\n",
    "    \n",
    "    return signal + noise\n",
    "\n",
    "def apply_doppler_shift(signal, fd_hz, fs_hz):\n",
    "    if fd_hz is None or fd_hz == 0:\n",
    "        return signal\n",
    "    t = np.arange(len(signal)) / fs_hz\n",
    "    return signal * np.exp(1j * 2 * np.pi * fd_hz * t)\n",
    "\n",
    "def process_signal_led_rff(sig_complex, use_log=False, wavelet='db6', level=6):\n",
    "    amp = np.abs(sig_complex)   # 幅度谱\n",
    "\n",
    "    # --------------------\n",
    "    # 1. 可选 log 处理\n",
    "    # --------------------\n",
    "    if use_log:\n",
    "        amp = np.log(amp + 1e-8)\n",
    "\n",
    "    # --------------------\n",
    "    # 2. 小波分解 + 最高频置零 + 重构\n",
    "    # --------------------\n",
    "    coeffs = pywt.wavedec(amp, wavelet, level=level)\n",
    "\n",
    "    # 置零最高频（最高 detail 层）\n",
    "    coeffs[-1] = np.zeros_like(coeffs[-1])\n",
    "\n",
    "    rec = pywt.waverec(coeffs, wavelet)\n",
    "    rec = rec[:len(amp)]\n",
    "\n",
    "    # --------------------\n",
    "    # 3. 归一化\n",
    "    # --------------------\n",
    "    mu, sigma = rec.mean(), rec.std()\n",
    "    if sigma < 1e-8:\n",
    "        feat = (rec - mu).astype(np.float32)\n",
    "    else:\n",
    "        feat = ((rec - mu) / (sigma + 1e-8)).astype(np.float32)\n",
    "\n",
    "    return feat\n",
    "\n",
    "\n",
    "def preprocess_iq_dataset_led_rff(data_real_imag, snr_db=SNR_DB, velocity_kmh=VELOCITY_KMH,\n",
    "                                  fc_hz=FC, fs_hz=FS, use_log=USE_LOG, wavelet=WAVELET,\n",
    "                                  level=WAVELET_LEVEL, add_noise=ADD_NOISE, add_doppler=ADD_DOPPLER):\n",
    "    num_samples, sig_len, _ = data_real_imag.shape\n",
    "    processed_feats = []\n",
    "\n",
    "    data_complex = data_real_imag[...,0] + 1j*data_real_imag[...,1]\n",
    "    fd_hz = compute_doppler_shift(velocity_kmh, fc_hz) if add_doppler else None\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        sig = data_complex[i]\n",
    "        if add_noise: sig = add_complex_awgn(sig, snr_db)\n",
    "        if add_doppler: sig = apply_doppler_shift(sig, fd_hz, fs_hz)\n",
    "        feat = process_signal_led_rff(sig, use_log=use_log, wavelet=wavelet, level=level)\n",
    "        processed_feats.append(feat)\n",
    "\n",
    "    processed_feats = np.stack(processed_feats, axis=0)\n",
    "    return torch.tensor(processed_feats, dtype=torch.float32)[:, None, :]\n",
    "\n",
    "\n",
    "# ====================== InceptionTime 模型 ======================\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        bottleneck_channels = max(1, out_channels // 4)\n",
    "        self.bottleneck = nn.Conv1d(in_channels, bottleneck_channels, kernel_size=1, bias=False)\n",
    "        self.conv1 = nn.Conv1d(bottleneck_channels, out_channels, kernel_size=10, padding=5)\n",
    "        self.conv2 = nn.Conv1d(bottleneck_channels, out_channels, kernel_size=20, padding=10)\n",
    "        self.conv3 = nn.Conv1d(bottleneck_channels, out_channels, kernel_size=40, padding=20)\n",
    "        self.maxpool = nn.MaxPool1d(3, stride=1, padding=1)\n",
    "        self.convpool = nn.Conv1d(bottleneck_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(4*out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x_b = self.bottleneck(x)\n",
    "        c1 = self.conv1(x_b)\n",
    "        c2 = self.conv2(x_b)\n",
    "        c3 = self.conv3(x_b)\n",
    "        c4 = self.convpool(self.maxpool(x_b))\n",
    "        min_len = min(c1.shape[-1], c2.shape[-1], c3.shape[-1], c4.shape[-1])\n",
    "        c1=c1[...,:min_len]; c2=c2[...,:min_len]; c3=c3[...,:min_len]; c4=c4[...,:min_len]\n",
    "        out = torch.cat([c1,c2,c3,c4], dim=1)\n",
    "        return self.relu(self.bn(out))\n",
    "\n",
    "class InceptionTime(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, channels=32):\n",
    "        super().__init__()\n",
    "        self.b1 = InceptionBlock(in_channels, channels)\n",
    "        self.b2 = InceptionBlock(4*channels, channels)\n",
    "        self.b3 = InceptionBlock(4*channels, channels)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(4*channels, num_classes)\n",
    "    def forward(self, x):\n",
    "        if x.shape[-1] % 2 == 1: x = x[...,:-1]\n",
    "        x = self.b1(x); x = self.b2(x); x = self.b3(x)\n",
    "        x = self.gap(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ====================== 工具函数 ======================\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            _, p = torch.max(out, 1)\n",
    "            correct += (p == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "            all_preds.extend(p.cpu().numpy())\n",
    "    acc = 100.0 * correct / total\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "    return acc, cm\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, fold, save_folder, dataset_type='Test'):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{dataset_type} Confusion Matrix Fold{fold}')\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.savefig(os.path.join(save_folder,f'{dataset_type.lower()}_cm_fold{fold}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_curves(train_losses, val_losses, train_acc, val_acc, fold, save_folder):\n",
    "    plt.figure(); plt.plot(train_losses,label='Train Loss'); plt.plot(val_losses,label='Val Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title(f'Fold {fold} Loss'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_folder,f'loss_fold{fold}.png')); plt.close()\n",
    "    plt.figure(); plt.plot(train_acc,label='Train Acc'); plt.plot(val_acc,label='Val Acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)'); plt.title(f'Fold {fold} Accuracy'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_folder,f'acc_fold{fold}.png')); plt.close()\n",
    "\n",
    "# ====================== KFold 训练（带参数保存 + 混淆矩阵 + 曲线 + 平均梯度范数） ======================\n",
    "def train_kfold(X_train, y_train, X_test, y_test, num_classes, device=DEVICE, \n",
    "                     snr_db=SNR_DB, velocity_kmh=VELOCITY_KMH, fc_hz=FC, fs_hz=FS,\n",
    "                     wavelet=WAVELET, wavelet_level=WAVELET_LEVEL,\n",
    "                     batch_size=BATCH_SIZE, epochs=EPOCHS, lr=LR, weight_decay=WEIGHT_DECAY,\n",
    "                     n_splits=N_SPLITS, patience=PATIENCE):\n",
    "\n",
    "    fd_hz = compute_doppler_shift(velocity_kmh, fc_hz)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    script_name='wisig_LED'\n",
    "    save_dir = f\"{timestamp}_{script_name}_SNR{snr_db}dB_fd{int(fd_hz)}_classes_{num_classes}_CNN\"\n",
    "    save_folder = os.path.join(SAVE_ROOT, save_dir)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    results_file = os.path.join(save_folder,\"results.txt\")\n",
    "\n",
    "    # 保存实验参数\n",
    "    with open(results_file,'w') as f:\n",
    "        f.write(\"=== Experiment Parameters ===\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"Device: {device}\\n\")\n",
    "        f.write(f\"SNR_dB: {snr_db}\\nDoppler_fd: {fd_hz:.2f} Hz\\nFS: {fs_hz}\\nFC: {fc_hz}\\n\")\n",
    "        f.write(f\"Wavelet: {wavelet}, Level: {wavelet_level}\\n\")\n",
    "        f.write(f\"Batch size: {batch_size}, Epochs: {epochs}, LR: {lr}, Weight decay: {weight_decay}\\n\")\n",
    "        f.write(f\"K-Fold: {n_splits}, Patience: {patience}, Num classes: {num_classes}\\n\")\n",
    "        f.write(\"============================\\n\\n\")\n",
    "\n",
    "    full_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    indices = np.arange(len(full_dataset))\n",
    "\n",
    "    val_scores, test_scores = [], []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{n_splits} ===\")\n",
    "        tr_sub, va_sub = Subset(full_dataset, tr_idx), Subset(full_dataset, va_idx)\n",
    "        tr_loader = DataLoader(tr_sub, batch_size=batch_size, shuffle=True)\n",
    "        va_loader = DataLoader(va_sub, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = InceptionTime(num_classes=num_classes, in_channels=X_train.shape[1]).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        best_val, best_wts, patience_cnt = 0.0, None, 0\n",
    "        train_losses, val_losses, train_acc_list, val_acc_list, avg_grad_list = [], [], [], [], []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train(); running_loss, correct, total = 0.0,0,0\n",
    "            total_grad = 0.0; count_grad = 0\n",
    "            for xb, yb in tr_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                _, p = torch.max(out,1)\n",
    "                correct += (p==yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "                # 平均梯度范数\n",
    "                grad_norms = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]\n",
    "                if grad_norms:\n",
    "                    total_grad += np.mean(grad_norms)\n",
    "                    count_grad += 1\n",
    "            avg_grad = total_grad / max(count_grad,1)\n",
    "            avg_grad_list.append(avg_grad)\n",
    "\n",
    "            train_loss = running_loss / len(tr_loader)\n",
    "            train_acc = 100.0*correct/total\n",
    "            train_losses.append(train_loss)\n",
    "            train_acc_list.append(train_acc)\n",
    "\n",
    "            # Validation\n",
    "            model.eval(); vloss,vcorrect,vtotal=0.0,0,0\n",
    "            with torch.no_grad():\n",
    "                all_labels, all_preds = [], []\n",
    "                for xb,yb in va_loader:\n",
    "                    xb,yb = xb.to(device), yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    loss = criterion(out,yb)\n",
    "                    vloss += loss.item()\n",
    "                    _,p = torch.max(out,1)\n",
    "                    vcorrect += (p==yb).sum().item()\n",
    "                    vtotal += yb.size(0)\n",
    "                    all_labels.extend(yb.cpu().numpy())\n",
    "                    all_preds.extend(p.cpu().numpy())\n",
    "            val_loss = vloss / len(va_loader)\n",
    "            val_acc = 100.0*vcorrect/vtotal\n",
    "            val_losses.append(val_loss)\n",
    "            val_acc_list.append(val_acc)\n",
    "            val_cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "            np.save(os.path.join(save_folder,f'val_cm_fold{fold+1}.npy'), val_cm)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | TrainAcc={train_acc:.2f}% | ValAcc={val_acc:.2f}% | \"\n",
    "                  f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | AvgGrad={avg_grad:.4f}\")\n",
    "            with open(results_file,'a') as f:\n",
    "                f.write(f\"Fold{fold+1} Epoch{epoch+1} | TrainAcc={train_acc:.2f}% | ValAcc={val_acc:.2f}% | \"\n",
    "                        f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | AvgGrad={avg_grad:.4f}\\n\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_acc > best_val + 0.01:\n",
    "                best_val = val_acc\n",
    "                best_wts = model.state_dict()\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "                if patience_cnt >= patience:\n",
    "                    print(\"Early stopping.\")\n",
    "                    break\n",
    "            scheduler.step()\n",
    "\n",
    "        if best_wts is not None:\n",
    "            model.load_state_dict(best_wts)\n",
    "\n",
    "        # Train/Val confusion matrices\n",
    "        train_acc, train_cm = evaluate_model(model, tr_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder,f'train_cm_fold{fold+1}.npy'), train_cm)\n",
    "        plot_confusion_matrix(train_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Train')\n",
    "\n",
    "        val_acc, val_cm = evaluate_model(model, va_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder,f'val_cm_fold{fold+1}.npy'), val_cm)\n",
    "        plot_confusion_matrix(val_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Val')\n",
    "\n",
    "        # Test evaluation\n",
    "        test_acc, test_cm = evaluate_model(model, test_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder,f'test_cm_fold{fold+1}.npy'), test_cm)\n",
    "        plot_confusion_matrix(test_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Test')\n",
    "        with open(results_file,'a') as f:\n",
    "            f.write(f\"Fold{fold+1} TestAcc={test_acc:.2f}%\\n\")\n",
    "        print(f\"Fold {fold+1} Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "        # 绘制训练曲线\n",
    "        plot_curves(train_losses, val_losses, train_acc_list, val_acc_list, fold+1, save_folder)\n",
    "\n",
    "        # 保存模型\n",
    "        torch.save(model.state_dict(), os.path.join(save_folder,f'model_fold{fold+1}.pth'))\n",
    "\n",
    "        val_scores.append(val_acc)\n",
    "        test_scores.append(test_acc)\n",
    "\n",
    "    # 总结\n",
    "    print(\"\\n=== Overall Summary ===\")\n",
    "    print(f\"Val Acc: {np.mean(val_scores):.2f} ± {np.std(val_scores):.2f}\")\n",
    "    print(f\"Test Acc: {np.mean(test_scores):.2f} ± {np.std(test_scores):.2f}\")\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(f\"\\n=== Overall Summary ===\\nVal Acc: {np.mean(val_scores):.2f} ± {np.std(val_scores):.2f}\\nTest Acc: {np.mean(test_scores):.2f} ± {np.std(test_scores):.2f}\\n\")\n",
    "    \n",
    "    print(f\"\\nAll results saved in {save_folder}\")\n",
    "    return save_folder\n",
    "\n",
    "\n",
    "# ====================== 使用示例 ======================\n",
    "# 假设 IQ 数据 shape=[num_samples, length, 2]，y=[num_samples]\n",
    "X_train_proc = preprocess_iq_dataset_led_rff(X_train)\n",
    "X_test_proc  = preprocess_iq_dataset_led_rff(X_test)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_torch  = torch.tensor(y_test, dtype=torch.long)\n",
    "num_classes = len(np.unique(y_train_torch))\n",
    "save_folder = train_kfold(X_train_proc, y_train_torch, X_test_proc, y_test_torch, num_classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
