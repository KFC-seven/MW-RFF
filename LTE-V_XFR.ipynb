{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6646d3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================== 当前实验 SNR=-5 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 789, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 789\n",
      "[INFO] 训练 block 数: 591, 测试 block 数: 198\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.1723, Train Acc=15.08%, Val Loss=2.2716, Val Acc=16.65%, Grad Norm=5.7018\n",
      "Fold 1, Epoch 2: Train Loss=1.5168, Train Acc=45.85%, Val Loss=0.8694, Val Acc=69.52%, Grad Norm=5.0797\n",
      "Fold 1, Epoch 3: Train Loss=0.9295, Train Acc=68.19%, Val Loss=0.7020, Val Acc=75.82%, Grad Norm=4.6968\n",
      "Fold 1, Epoch 4: Train Loss=0.7655, Train Acc=74.02%, Val Loss=0.6232, Val Acc=78.67%, Grad Norm=4.4067\n",
      "Fold 1, Epoch 5: Train Loss=0.6787, Train Acc=77.09%, Val Loss=0.5760, Val Acc=80.60%, Grad Norm=4.2891\n",
      "Fold 1, Epoch 6: Train Loss=0.6216, Train Acc=79.18%, Val Loss=0.5479, Val Acc=81.34%, Grad Norm=4.2302\n",
      "Fold 1, Epoch 7: Train Loss=0.5765, Train Acc=80.56%, Val Loss=0.5408, Val Acc=81.77%, Grad Norm=4.2123\n",
      "Fold 1, Epoch 8: Train Loss=0.5402, Train Acc=81.91%, Val Loss=0.5161, Val Acc=82.42%, Grad Norm=4.2390\n",
      "Fold 1, Epoch 9: Train Loss=0.5096, Train Acc=82.99%, Val Loss=0.5020, Val Acc=83.02%, Grad Norm=4.2837\n",
      "Fold 1, Epoch 10: Train Loss=0.4818, Train Acc=83.88%, Val Loss=0.5055, Val Acc=82.91%, Grad Norm=4.3749\n",
      "Fold 1, Epoch 11: Train Loss=0.4250, Train Acc=85.76%, Val Loss=0.4816, Val Acc=83.57%, Grad Norm=4.4394\n",
      "Fold 1, Epoch 12: Train Loss=0.4022, Train Acc=86.55%, Val Loss=0.4721, Val Acc=84.08%, Grad Norm=4.6512\n",
      "Fold 1, Epoch 13: Train Loss=0.3825, Train Acc=87.17%, Val Loss=0.4737, Val Acc=83.85%, Grad Norm=4.8159\n",
      "Fold 1, Epoch 14: Train Loss=0.3679, Train Acc=87.65%, Val Loss=0.4650, Val Acc=84.33%, Grad Norm=5.0070\n",
      "Fold 1, Epoch 15: Train Loss=0.3536, Train Acc=88.11%, Val Loss=0.4728, Val Acc=84.30%, Grad Norm=5.1543\n",
      "Fold 1, Epoch 16: Train Loss=0.3394, Train Acc=88.52%, Val Loss=0.4751, Val Acc=83.97%, Grad Norm=5.3102\n",
      "Fold 1, Epoch 17: Train Loss=0.3298, Train Acc=89.00%, Val Loss=0.4604, Val Acc=84.64%, Grad Norm=5.4466\n",
      "Fold 1, Epoch 18: Train Loss=0.3156, Train Acc=89.48%, Val Loss=0.4825, Val Acc=84.07%, Grad Norm=5.5913\n",
      "Fold 1, Epoch 19: Train Loss=0.3048, Train Acc=89.80%, Val Loss=0.4681, Val Acc=84.57%, Grad Norm=5.7505\n",
      "Fold 1, Epoch 20: Train Loss=0.2915, Train Acc=90.32%, Val Loss=0.4768, Val Acc=84.35%, Grad Norm=5.8414\n",
      "Fold 1, Epoch 21: Train Loss=0.2619, Train Acc=91.28%, Val Loss=0.4701, Val Acc=84.73%, Grad Norm=5.8569\n",
      "Fold 1, Epoch 22: Train Loss=0.2458, Train Acc=91.79%, Val Loss=0.4758, Val Acc=84.69%, Grad Norm=6.0312\n",
      "Fold 1, Epoch 23: Train Loss=0.2376, Train Acc=92.08%, Val Loss=0.4772, Val Acc=84.84%, Grad Norm=6.2035\n",
      "Fold 1, Epoch 24: Train Loss=0.2265, Train Acc=92.41%, Val Loss=0.4836, Val Acc=84.58%, Grad Norm=6.3509\n",
      "Fold 1, Epoch 25: Train Loss=0.2177, Train Acc=92.69%, Val Loss=0.4902, Val Acc=84.53%, Grad Norm=6.4601\n",
      "Fold 1, Epoch 26: Train Loss=0.2097, Train Acc=93.00%, Val Loss=0.4940, Val Acc=84.58%, Grad Norm=6.6281\n",
      "Fold 1, Epoch 27: Train Loss=0.2051, Train Acc=93.17%, Val Loss=0.5014, Val Acc=84.44%, Grad Norm=6.7961\n",
      "Fold 1, Epoch 28: Train Loss=0.1957, Train Acc=93.51%, Val Loss=0.4994, Val Acc=84.51%, Grad Norm=6.8700\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=83.15%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.1644, Train Acc=15.42%, Val Loss=2.0881, Val Acc=22.03%, Grad Norm=5.8582\n",
      "Fold 2, Epoch 2: Train Loss=1.4402, Train Acc=49.08%, Val Loss=1.0638, Val Acc=63.15%, Grad Norm=4.7993\n",
      "Fold 2, Epoch 3: Train Loss=0.9872, Train Acc=66.32%, Val Loss=0.8098, Val Acc=72.10%, Grad Norm=4.5128\n",
      "Fold 2, Epoch 4: Train Loss=0.8028, Train Acc=72.72%, Val Loss=0.6710, Val Acc=76.96%, Grad Norm=4.4216\n",
      "Fold 2, Epoch 5: Train Loss=0.6916, Train Acc=76.76%, Val Loss=0.6301, Val Acc=78.41%, Grad Norm=4.3348\n",
      "Fold 2, Epoch 6: Train Loss=0.6154, Train Acc=79.43%, Val Loss=0.5933, Val Acc=79.81%, Grad Norm=4.2781\n",
      "Fold 2, Epoch 7: Train Loss=0.5661, Train Acc=81.02%, Val Loss=0.5713, Val Acc=80.42%, Grad Norm=4.2494\n",
      "Fold 2, Epoch 8: Train Loss=0.5292, Train Acc=82.31%, Val Loss=0.5393, Val Acc=81.87%, Grad Norm=4.2837\n",
      "Fold 2, Epoch 9: Train Loss=0.4954, Train Acc=83.40%, Val Loss=0.5333, Val Acc=81.86%, Grad Norm=4.3042\n",
      "Fold 2, Epoch 10: Train Loss=0.4699, Train Acc=84.34%, Val Loss=0.5175, Val Acc=82.41%, Grad Norm=4.3602\n",
      "Fold 2, Epoch 11: Train Loss=0.4181, Train Acc=86.14%, Val Loss=0.4993, Val Acc=83.05%, Grad Norm=4.3757\n",
      "Fold 2, Epoch 12: Train Loss=0.3912, Train Acc=87.02%, Val Loss=0.4997, Val Acc=83.19%, Grad Norm=4.5676\n",
      "Fold 2, Epoch 13: Train Loss=0.3778, Train Acc=87.41%, Val Loss=0.4954, Val Acc=83.33%, Grad Norm=4.7243\n",
      "Fold 2, Epoch 14: Train Loss=0.3620, Train Acc=88.00%, Val Loss=0.5030, Val Acc=83.23%, Grad Norm=4.9266\n",
      "Fold 2, Epoch 15: Train Loss=0.3520, Train Acc=88.29%, Val Loss=0.4950, Val Acc=83.54%, Grad Norm=5.0744\n",
      "Fold 2, Epoch 16: Train Loss=0.3362, Train Acc=88.78%, Val Loss=0.5003, Val Acc=83.32%, Grad Norm=5.2280\n",
      "Fold 2, Epoch 17: Train Loss=0.3234, Train Acc=89.25%, Val Loss=0.4966, Val Acc=83.50%, Grad Norm=5.3656\n",
      "Fold 2, Epoch 18: Train Loss=0.3136, Train Acc=89.58%, Val Loss=0.5000, Val Acc=83.51%, Grad Norm=5.5185\n",
      "Fold 2, Epoch 19: Train Loss=0.3025, Train Acc=89.89%, Val Loss=0.5051, Val Acc=83.34%, Grad Norm=5.6565\n",
      "Fold 2, Epoch 20: Train Loss=0.2900, Train Acc=90.24%, Val Loss=0.5014, Val Acc=83.52%, Grad Norm=5.7969\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=83.34%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.1795, Train Acc=14.76%, Val Loss=2.2348, Val Acc=17.91%, Grad Norm=5.5166\n",
      "Fold 3, Epoch 2: Train Loss=1.5379, Train Acc=44.85%, Val Loss=1.0150, Val Acc=64.99%, Grad Norm=4.8124\n",
      "Fold 3, Epoch 3: Train Loss=1.0089, Train Acc=65.62%, Val Loss=0.7783, Val Acc=73.39%, Grad Norm=4.5109\n",
      "Fold 3, Epoch 4: Train Loss=0.8164, Train Acc=72.50%, Val Loss=0.6503, Val Acc=78.14%, Grad Norm=4.4582\n",
      "Fold 3, Epoch 5: Train Loss=0.6999, Train Acc=76.45%, Val Loss=0.5774, Val Acc=80.49%, Grad Norm=4.3827\n",
      "Fold 3, Epoch 6: Train Loss=0.6267, Train Acc=78.98%, Val Loss=0.5450, Val Acc=81.52%, Grad Norm=4.3179\n",
      "Fold 3, Epoch 7: Train Loss=0.5743, Train Acc=80.84%, Val Loss=0.5277, Val Acc=82.03%, Grad Norm=4.2855\n",
      "Fold 3, Epoch 8: Train Loss=0.5379, Train Acc=81.95%, Val Loss=0.5271, Val Acc=82.10%, Grad Norm=4.2935\n",
      "Fold 3, Epoch 9: Train Loss=0.5060, Train Acc=83.16%, Val Loss=0.5375, Val Acc=81.75%, Grad Norm=4.3047\n",
      "Fold 3, Epoch 10: Train Loss=0.4797, Train Acc=83.99%, Val Loss=0.5065, Val Acc=82.98%, Grad Norm=4.3540\n",
      "Fold 3, Epoch 11: Train Loss=0.4219, Train Acc=85.92%, Val Loss=0.4854, Val Acc=83.58%, Grad Norm=4.3907\n",
      "Fold 3, Epoch 12: Train Loss=0.4006, Train Acc=86.72%, Val Loss=0.4809, Val Acc=83.87%, Grad Norm=4.5900\n",
      "Fold 3, Epoch 13: Train Loss=0.3825, Train Acc=87.15%, Val Loss=0.4793, Val Acc=84.01%, Grad Norm=4.7812\n",
      "Fold 3, Epoch 14: Train Loss=0.3676, Train Acc=87.72%, Val Loss=0.4782, Val Acc=84.03%, Grad Norm=4.9474\n",
      "Fold 3, Epoch 15: Train Loss=0.3541, Train Acc=88.15%, Val Loss=0.4818, Val Acc=83.94%, Grad Norm=5.1024\n",
      "Fold 3, Epoch 16: Train Loss=0.3427, Train Acc=88.52%, Val Loss=0.4808, Val Acc=83.99%, Grad Norm=5.2670\n",
      "Fold 3, Epoch 17: Train Loss=0.3303, Train Acc=88.97%, Val Loss=0.4963, Val Acc=83.62%, Grad Norm=5.4318\n",
      "Fold 3, Epoch 18: Train Loss=0.3187, Train Acc=89.32%, Val Loss=0.4900, Val Acc=84.02%, Grad Norm=5.5695\n",
      "Fold 3, Epoch 19: Train Loss=0.3073, Train Acc=89.68%, Val Loss=0.4914, Val Acc=83.93%, Grad Norm=5.7323\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=83.17%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1763, Train Acc=14.60%, Val Loss=2.1185, Val Acc=20.68%, Grad Norm=5.9069\n",
      "Fold 4, Epoch 2: Train Loss=1.4925, Train Acc=46.91%, Val Loss=0.9798, Val Acc=65.90%, Grad Norm=4.8993\n",
      "Fold 4, Epoch 3: Train Loss=0.9323, Train Acc=68.29%, Val Loss=0.7373, Val Acc=74.79%, Grad Norm=4.7412\n",
      "Fold 4, Epoch 4: Train Loss=0.7559, Train Acc=74.59%, Val Loss=0.6727, Val Acc=76.82%, Grad Norm=4.4390\n",
      "Fold 4, Epoch 5: Train Loss=0.6659, Train Acc=77.63%, Val Loss=0.6183, Val Acc=78.78%, Grad Norm=4.2826\n",
      "Fold 4, Epoch 6: Train Loss=0.6051, Train Acc=79.67%, Val Loss=0.6037, Val Acc=79.26%, Grad Norm=4.2121\n",
      "Fold 4, Epoch 7: Train Loss=0.5631, Train Acc=81.20%, Val Loss=0.5776, Val Acc=80.33%, Grad Norm=4.1885\n",
      "Fold 4, Epoch 8: Train Loss=0.5259, Train Acc=82.48%, Val Loss=0.5639, Val Acc=80.74%, Grad Norm=4.2352\n",
      "Fold 4, Epoch 9: Train Loss=0.4923, Train Acc=83.63%, Val Loss=0.5589, Val Acc=80.94%, Grad Norm=4.2951\n",
      "Fold 4, Epoch 10: Train Loss=0.4672, Train Acc=84.41%, Val Loss=0.5483, Val Acc=81.30%, Grad Norm=4.3514\n",
      "Fold 4, Epoch 11: Train Loss=0.4160, Train Acc=86.19%, Val Loss=0.5193, Val Acc=82.34%, Grad Norm=4.3977\n",
      "Fold 4, Epoch 12: Train Loss=0.3920, Train Acc=86.92%, Val Loss=0.5105, Val Acc=82.67%, Grad Norm=4.5854\n",
      "Fold 4, Epoch 13: Train Loss=0.3727, Train Acc=87.61%, Val Loss=0.5149, Val Acc=82.65%, Grad Norm=4.7979\n",
      "Fold 4, Epoch 14: Train Loss=0.3577, Train Acc=88.09%, Val Loss=0.5172, Val Acc=82.64%, Grad Norm=4.9539\n",
      "Fold 4, Epoch 15: Train Loss=0.3444, Train Acc=88.50%, Val Loss=0.5130, Val Acc=82.52%, Grad Norm=5.1111\n",
      "Fold 4, Epoch 16: Train Loss=0.3307, Train Acc=88.92%, Val Loss=0.5079, Val Acc=82.86%, Grad Norm=5.2727\n",
      "Fold 4, Epoch 17: Train Loss=0.3209, Train Acc=89.25%, Val Loss=0.5213, Val Acc=82.62%, Grad Norm=5.4095\n",
      "Fold 4, Epoch 18: Train Loss=0.3088, Train Acc=89.63%, Val Loss=0.5232, Val Acc=82.71%, Grad Norm=5.5405\n",
      "Fold 4, Epoch 19: Train Loss=0.2991, Train Acc=89.98%, Val Loss=0.5193, Val Acc=82.85%, Grad Norm=5.7048\n",
      "Fold 4, Epoch 20: Train Loss=0.2881, Train Acc=90.33%, Val Loss=0.5260, Val Acc=82.80%, Grad Norm=5.8370\n",
      "Fold 4, Epoch 21: Train Loss=0.2529, Train Acc=91.54%, Val Loss=0.5183, Val Acc=82.95%, Grad Norm=5.7424\n",
      "Fold 4, Epoch 22: Train Loss=0.2411, Train Acc=91.91%, Val Loss=0.5204, Val Acc=83.29%, Grad Norm=5.9586\n",
      "Fold 4, Epoch 23: Train Loss=0.2267, Train Acc=92.50%, Val Loss=0.5363, Val Acc=82.74%, Grad Norm=6.0660\n",
      "Fold 4, Epoch 24: Train Loss=0.2206, Train Acc=92.62%, Val Loss=0.5314, Val Acc=82.97%, Grad Norm=6.2684\n",
      "Fold 4, Epoch 25: Train Loss=0.2102, Train Acc=93.02%, Val Loss=0.5402, Val Acc=82.96%, Grad Norm=6.4166\n",
      "Fold 4, Epoch 26: Train Loss=0.2059, Train Acc=93.09%, Val Loss=0.5347, Val Acc=83.01%, Grad Norm=6.6060\n",
      "Fold 4, Epoch 27: Train Loss=0.1977, Train Acc=93.41%, Val Loss=0.5477, Val Acc=82.87%, Grad Norm=6.6520\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=83.27%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.1683, Train Acc=15.45%, Val Loss=2.4201, Val Acc=13.74%, Grad Norm=5.6897\n",
      "Fold 5, Epoch 2: Train Loss=1.5332, Train Acc=44.92%, Val Loss=0.8836, Val Acc=70.26%, Grad Norm=5.0725\n",
      "Fold 5, Epoch 3: Train Loss=0.9115, Train Acc=68.75%, Val Loss=0.7021, Val Acc=76.27%, Grad Norm=4.7642\n",
      "Fold 5, Epoch 4: Train Loss=0.7485, Train Acc=74.58%, Val Loss=0.6374, Val Acc=78.61%, Grad Norm=4.4600\n",
      "Fold 5, Epoch 5: Train Loss=0.6671, Train Acc=77.60%, Val Loss=0.6052, Val Acc=79.63%, Grad Norm=4.2846\n",
      "Fold 5, Epoch 6: Train Loss=0.6096, Train Acc=79.54%, Val Loss=0.5691, Val Acc=80.88%, Grad Norm=4.2364\n",
      "Fold 5, Epoch 7: Train Loss=0.5642, Train Acc=81.08%, Val Loss=0.5518, Val Acc=81.34%, Grad Norm=4.2051\n",
      "Fold 5, Epoch 8: Train Loss=0.5284, Train Acc=82.40%, Val Loss=0.5257, Val Acc=82.38%, Grad Norm=4.2456\n",
      "Fold 5, Epoch 9: Train Loss=0.4969, Train Acc=83.46%, Val Loss=0.5165, Val Acc=82.55%, Grad Norm=4.2848\n",
      "Fold 5, Epoch 10: Train Loss=0.4693, Train Acc=84.31%, Val Loss=0.5024, Val Acc=83.23%, Grad Norm=4.3379\n",
      "Fold 5, Epoch 11: Train Loss=0.4151, Train Acc=86.14%, Val Loss=0.4939, Val Acc=83.60%, Grad Norm=4.3825\n",
      "Fold 5, Epoch 12: Train Loss=0.3907, Train Acc=87.04%, Val Loss=0.4970, Val Acc=83.39%, Grad Norm=4.5954\n",
      "Fold 5, Epoch 13: Train Loss=0.3745, Train Acc=87.60%, Val Loss=0.4893, Val Acc=83.76%, Grad Norm=4.7618\n",
      "Fold 5, Epoch 14: Train Loss=0.3603, Train Acc=88.03%, Val Loss=0.4864, Val Acc=83.85%, Grad Norm=4.9229\n",
      "Fold 5, Epoch 15: Train Loss=0.3475, Train Acc=88.45%, Val Loss=0.4953, Val Acc=83.60%, Grad Norm=5.1162\n",
      "Fold 5, Epoch 16: Train Loss=0.3321, Train Acc=88.92%, Val Loss=0.4794, Val Acc=84.14%, Grad Norm=5.2736\n",
      "Fold 5, Epoch 17: Train Loss=0.3229, Train Acc=89.24%, Val Loss=0.4883, Val Acc=83.89%, Grad Norm=5.4407\n",
      "Fold 5, Epoch 18: Train Loss=0.3099, Train Acc=89.66%, Val Loss=0.4949, Val Acc=83.77%, Grad Norm=5.5276\n",
      "Fold 5, Epoch 19: Train Loss=0.2992, Train Acc=90.00%, Val Loss=0.5149, Val Acc=83.47%, Grad Norm=5.6769\n",
      "Fold 5, Epoch 20: Train Loss=0.2911, Train Acc=90.31%, Val Loss=0.4995, Val Acc=83.78%, Grad Norm=5.8056\n",
      "Fold 5, Epoch 21: Train Loss=0.2569, Train Acc=91.40%, Val Loss=0.4908, Val Acc=84.23%, Grad Norm=5.7851\n",
      "Fold 5, Epoch 22: Train Loss=0.2445, Train Acc=91.85%, Val Loss=0.5011, Val Acc=84.17%, Grad Norm=5.9784\n",
      "Fold 5, Epoch 23: Train Loss=0.2325, Train Acc=92.26%, Val Loss=0.4990, Val Acc=84.25%, Grad Norm=6.1497\n",
      "Fold 5, Epoch 24: Train Loss=0.2224, Train Acc=92.58%, Val Loss=0.5087, Val Acc=84.21%, Grad Norm=6.2649\n",
      "Fold 5, Epoch 25: Train Loss=0.2167, Train Acc=92.79%, Val Loss=0.5159, Val Acc=83.95%, Grad Norm=6.4708\n",
      "Fold 5, Epoch 26: Train Loss=0.2075, Train Acc=93.07%, Val Loss=0.5086, Val Acc=84.13%, Grad Norm=6.5667\n",
      "Fold 5, Epoch 27: Train Loss=0.2012, Train Acc=93.27%, Val Loss=0.5276, Val Acc=83.81%, Grad Norm=6.7194\n",
      "Fold 5, Epoch 28: Train Loss=0.1923, Train Acc=93.64%, Val Loss=0.5318, Val Acc=83.77%, Grad Norm=6.7664\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=83.47%\n",
      "\n",
      "SNR  -5 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-26_09-20-42_LTE-V_XFR_SNR-5dB_fd655_classes_9_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-10 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 789, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 789\n",
      "[INFO] 训练 block 数: 591, 测试 block 数: 198\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2188, Train Acc=11.37%, Val Loss=2.3394, Val Acc=10.92%, Grad Norm=6.3143\n",
      "Fold 1, Epoch 2: Train Loss=2.2038, Train Acc=12.32%, Val Loss=2.2703, Val Acc=12.60%, Grad Norm=4.2978\n",
      "Fold 1, Epoch 3: Train Loss=2.1383, Train Acc=17.62%, Val Loss=2.0840, Val Acc=22.54%, Grad Norm=3.0970\n",
      "Fold 1, Epoch 4: Train Loss=1.9816, Train Acc=26.87%, Val Loss=1.8160, Val Acc=35.21%, Grad Norm=3.2700\n",
      "Fold 1, Epoch 5: Train Loss=1.8150, Train Acc=35.02%, Val Loss=1.7079, Val Acc=40.05%, Grad Norm=3.5155\n",
      "Fold 1, Epoch 6: Train Loss=1.7344, Train Acc=38.74%, Val Loss=1.6626, Val Acc=41.39%, Grad Norm=3.5048\n",
      "Fold 1, Epoch 7: Train Loss=1.6811, Train Acc=40.94%, Val Loss=1.6233, Val Acc=42.82%, Grad Norm=3.5709\n",
      "Fold 1, Epoch 8: Train Loss=1.6426, Train Acc=42.60%, Val Loss=1.6215, Val Acc=42.76%, Grad Norm=3.6295\n",
      "Fold 1, Epoch 9: Train Loss=1.6053, Train Acc=43.79%, Val Loss=1.6071, Val Acc=43.29%, Grad Norm=3.7868\n",
      "Fold 1, Epoch 10: Train Loss=1.5744, Train Acc=45.16%, Val Loss=1.5787, Val Acc=44.68%, Grad Norm=3.9314\n",
      "Fold 1, Epoch 11: Train Loss=1.5120, Train Acc=47.59%, Val Loss=1.5440, Val Acc=46.01%, Grad Norm=4.2116\n",
      "Fold 1, Epoch 12: Train Loss=1.4863, Train Acc=48.53%, Val Loss=1.5404, Val Acc=46.08%, Grad Norm=4.5282\n",
      "Fold 1, Epoch 13: Train Loss=1.4639, Train Acc=49.37%, Val Loss=1.5237, Val Acc=46.74%, Grad Norm=4.8262\n",
      "Fold 1, Epoch 14: Train Loss=1.4448, Train Acc=50.29%, Val Loss=1.5342, Val Acc=46.54%, Grad Norm=5.0963\n",
      "Fold 1, Epoch 15: Train Loss=1.4240, Train Acc=50.89%, Val Loss=1.5396, Val Acc=46.39%, Grad Norm=5.3825\n",
      "Fold 1, Epoch 16: Train Loss=1.4015, Train Acc=51.73%, Val Loss=1.5333, Val Acc=46.37%, Grad Norm=5.6742\n",
      "Fold 1, Epoch 17: Train Loss=1.3853, Train Acc=52.36%, Val Loss=1.5433, Val Acc=46.36%, Grad Norm=5.9728\n",
      "Fold 1, Epoch 18: Train Loss=1.3634, Train Acc=53.04%, Val Loss=1.5392, Val Acc=46.49%, Grad Norm=6.2604\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=45.19%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2196, Train Acc=11.31%, Val Loss=2.2738, Val Acc=13.74%, Grad Norm=6.2587\n",
      "Fold 2, Epoch 2: Train Loss=2.2022, Train Acc=12.46%, Val Loss=2.2380, Val Acc=13.05%, Grad Norm=4.2909\n",
      "Fold 2, Epoch 3: Train Loss=2.1286, Train Acc=18.63%, Val Loss=2.0627, Val Acc=21.82%, Grad Norm=3.0884\n",
      "Fold 2, Epoch 4: Train Loss=1.9686, Train Acc=27.93%, Val Loss=1.8545, Val Acc=32.75%, Grad Norm=3.2723\n",
      "Fold 2, Epoch 5: Train Loss=1.8064, Train Acc=35.93%, Val Loss=1.7470, Val Acc=37.26%, Grad Norm=3.5184\n",
      "Fold 2, Epoch 6: Train Loss=1.7200, Train Acc=39.47%, Val Loss=1.6957, Val Acc=39.16%, Grad Norm=3.5400\n",
      "Fold 2, Epoch 7: Train Loss=1.6697, Train Acc=41.28%, Val Loss=1.6745, Val Acc=40.45%, Grad Norm=3.5657\n",
      "Fold 2, Epoch 8: Train Loss=1.6313, Train Acc=43.08%, Val Loss=1.6615, Val Acc=40.74%, Grad Norm=3.6572\n",
      "Fold 2, Epoch 9: Train Loss=1.5926, Train Acc=44.65%, Val Loss=1.6293, Val Acc=42.48%, Grad Norm=3.8121\n",
      "Fold 2, Epoch 10: Train Loss=1.5629, Train Acc=45.71%, Val Loss=1.6066, Val Acc=43.27%, Grad Norm=3.9383\n",
      "Fold 2, Epoch 11: Train Loss=1.5005, Train Acc=48.04%, Val Loss=1.5885, Val Acc=43.86%, Grad Norm=4.2370\n",
      "Fold 2, Epoch 12: Train Loss=1.4717, Train Acc=49.15%, Val Loss=1.5810, Val Acc=44.35%, Grad Norm=4.5666\n",
      "Fold 2, Epoch 13: Train Loss=1.4471, Train Acc=50.17%, Val Loss=1.5765, Val Acc=44.39%, Grad Norm=4.8601\n",
      "Fold 2, Epoch 14: Train Loss=1.4282, Train Acc=50.86%, Val Loss=1.5769, Val Acc=44.44%, Grad Norm=5.1632\n",
      "Fold 2, Epoch 15: Train Loss=1.4087, Train Acc=51.63%, Val Loss=1.5782, Val Acc=44.69%, Grad Norm=5.4510\n",
      "Fold 2, Epoch 16: Train Loss=1.3892, Train Acc=52.13%, Val Loss=1.5720, Val Acc=44.68%, Grad Norm=5.7245\n",
      "Fold 2, Epoch 17: Train Loss=1.3672, Train Acc=52.92%, Val Loss=1.5665, Val Acc=45.12%, Grad Norm=6.0236\n",
      "Fold 2, Epoch 18: Train Loss=1.3494, Train Acc=53.70%, Val Loss=1.5643, Val Acc=45.17%, Grad Norm=6.3307\n",
      "Fold 2, Epoch 19: Train Loss=1.3283, Train Acc=54.45%, Val Loss=1.5737, Val Acc=45.08%, Grad Norm=6.6728\n",
      "Fold 2, Epoch 20: Train Loss=1.3099, Train Acc=54.97%, Val Loss=1.5835, Val Acc=45.00%, Grad Norm=6.9649\n",
      "Fold 2, Epoch 21: Train Loss=1.2596, Train Acc=56.81%, Val Loss=1.5878, Val Acc=45.21%, Grad Norm=7.3818\n",
      "Fold 2, Epoch 22: Train Loss=1.2351, Train Acc=57.78%, Val Loss=1.5829, Val Acc=45.37%, Grad Norm=7.9062\n",
      "Fold 2, Epoch 23: Train Loss=1.2123, Train Acc=58.51%, Val Loss=1.5831, Val Acc=45.47%, Grad Norm=8.3802\n",
      "Fold 2, Epoch 24: Train Loss=1.1937, Train Acc=59.21%, Val Loss=1.6068, Val Acc=45.01%, Grad Norm=8.8636\n",
      "Fold 2, Epoch 25: Train Loss=1.1752, Train Acc=59.79%, Val Loss=1.6086, Val Acc=45.17%, Grad Norm=9.3388\n",
      "Fold 2, Epoch 26: Train Loss=1.1587, Train Acc=60.52%, Val Loss=1.6119, Val Acc=45.01%, Grad Norm=9.7713\n",
      "Fold 2, Epoch 27: Train Loss=1.1429, Train Acc=60.93%, Val Loss=1.6398, Val Acc=44.42%, Grad Norm=10.2258\n",
      "Fold 2, Epoch 28: Train Loss=1.1238, Train Acc=61.68%, Val Loss=1.6317, Val Acc=44.65%, Grad Norm=10.7061\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=44.95%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2185, Train Acc=11.47%, Val Loss=2.3438, Val Acc=9.32%, Grad Norm=6.2998\n",
      "Fold 3, Epoch 2: Train Loss=2.1906, Train Acc=13.46%, Val Loss=2.1723, Val Acc=15.79%, Grad Norm=4.0557\n",
      "Fold 3, Epoch 3: Train Loss=2.0722, Train Acc=21.95%, Val Loss=1.9529, Val Acc=28.22%, Grad Norm=3.1243\n",
      "Fold 3, Epoch 4: Train Loss=1.9167, Train Acc=30.50%, Val Loss=1.8515, Val Acc=33.49%, Grad Norm=3.0481\n",
      "Fold 3, Epoch 5: Train Loss=1.8516, Train Acc=33.50%, Val Loss=1.8332, Val Acc=34.38%, Grad Norm=2.8690\n",
      "Fold 3, Epoch 6: Train Loss=1.8027, Train Acc=35.68%, Val Loss=1.7527, Val Acc=37.68%, Grad Norm=2.9538\n",
      "Fold 3, Epoch 7: Train Loss=1.7356, Train Acc=38.51%, Val Loss=1.6811, Val Acc=40.70%, Grad Norm=3.3125\n",
      "Fold 3, Epoch 8: Train Loss=1.6685, Train Acc=41.49%, Val Loss=1.6550, Val Acc=41.81%, Grad Norm=3.5564\n",
      "Fold 3, Epoch 9: Train Loss=1.6186, Train Acc=43.34%, Val Loss=1.6188, Val Acc=43.19%, Grad Norm=3.7380\n",
      "Fold 3, Epoch 10: Train Loss=1.5780, Train Acc=45.15%, Val Loss=1.5927, Val Acc=44.35%, Grad Norm=3.9055\n",
      "Fold 3, Epoch 11: Train Loss=1.5154, Train Acc=47.52%, Val Loss=1.5667, Val Acc=45.19%, Grad Norm=4.1868\n",
      "Fold 3, Epoch 12: Train Loss=1.4839, Train Acc=48.74%, Val Loss=1.5700, Val Acc=45.32%, Grad Norm=4.5185\n",
      "Fold 3, Epoch 13: Train Loss=1.4631, Train Acc=49.46%, Val Loss=1.5696, Val Acc=45.15%, Grad Norm=4.8026\n",
      "Fold 3, Epoch 14: Train Loss=1.4413, Train Acc=50.52%, Val Loss=1.5524, Val Acc=45.85%, Grad Norm=5.0810\n",
      "Fold 3, Epoch 15: Train Loss=1.4231, Train Acc=50.97%, Val Loss=1.5568, Val Acc=45.76%, Grad Norm=5.3642\n",
      "Fold 3, Epoch 16: Train Loss=1.3992, Train Acc=51.84%, Val Loss=1.5617, Val Acc=45.74%, Grad Norm=5.6669\n",
      "Fold 3, Epoch 17: Train Loss=1.3820, Train Acc=52.51%, Val Loss=1.5617, Val Acc=45.82%, Grad Norm=5.9459\n",
      "Fold 3, Epoch 18: Train Loss=1.3637, Train Acc=53.24%, Val Loss=1.5589, Val Acc=45.82%, Grad Norm=6.2427\n",
      "Fold 3, Epoch 19: Train Loss=1.3462, Train Acc=53.83%, Val Loss=1.5752, Val Acc=45.40%, Grad Norm=6.5433\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=45.57%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2194, Train Acc=11.43%, Val Loss=2.3639, Val Acc=11.86%, Grad Norm=6.3025\n",
      "Fold 4, Epoch 2: Train Loss=2.2065, Train Acc=11.99%, Val Loss=2.3280, Val Acc=11.69%, Grad Norm=4.3906\n",
      "Fold 4, Epoch 3: Train Loss=2.1478, Train Acc=16.68%, Val Loss=2.0625, Val Acc=22.43%, Grad Norm=3.2248\n",
      "Fold 4, Epoch 4: Train Loss=1.9487, Train Acc=28.99%, Val Loss=1.8716, Val Acc=32.13%, Grad Norm=3.3538\n",
      "Fold 4, Epoch 5: Train Loss=1.8139, Train Acc=35.46%, Val Loss=1.7879, Val Acc=35.96%, Grad Norm=3.4104\n",
      "Fold 4, Epoch 6: Train Loss=1.7318, Train Acc=38.96%, Val Loss=1.7205, Val Acc=38.88%, Grad Norm=3.4901\n",
      "Fold 4, Epoch 7: Train Loss=1.6704, Train Acc=41.40%, Val Loss=1.6810, Val Acc=40.41%, Grad Norm=3.5918\n",
      "Fold 4, Epoch 8: Train Loss=1.6239, Train Acc=43.34%, Val Loss=1.6854, Val Acc=40.40%, Grad Norm=3.7156\n",
      "Fold 4, Epoch 9: Train Loss=1.5857, Train Acc=44.74%, Val Loss=1.6532, Val Acc=41.56%, Grad Norm=3.8305\n",
      "Fold 4, Epoch 10: Train Loss=1.5512, Train Acc=46.06%, Val Loss=1.6456, Val Acc=42.00%, Grad Norm=3.9694\n",
      "Fold 4, Epoch 11: Train Loss=1.4922, Train Acc=48.44%, Val Loss=1.6157, Val Acc=43.21%, Grad Norm=4.2611\n",
      "Fold 4, Epoch 12: Train Loss=1.4617, Train Acc=49.46%, Val Loss=1.6118, Val Acc=43.44%, Grad Norm=4.5940\n",
      "Fold 4, Epoch 13: Train Loss=1.4415, Train Acc=50.26%, Val Loss=1.6058, Val Acc=43.55%, Grad Norm=4.8566\n",
      "Fold 4, Epoch 14: Train Loss=1.4200, Train Acc=50.95%, Val Loss=1.5988, Val Acc=43.74%, Grad Norm=5.1573\n",
      "Fold 4, Epoch 15: Train Loss=1.3960, Train Acc=52.00%, Val Loss=1.5976, Val Acc=44.01%, Grad Norm=5.4644\n",
      "Fold 4, Epoch 16: Train Loss=1.3777, Train Acc=52.60%, Val Loss=1.6051, Val Acc=43.69%, Grad Norm=5.7530\n",
      "Fold 4, Epoch 17: Train Loss=1.3579, Train Acc=53.41%, Val Loss=1.6025, Val Acc=44.05%, Grad Norm=6.0479\n",
      "Fold 4, Epoch 18: Train Loss=1.3382, Train Acc=53.90%, Val Loss=1.6022, Val Acc=44.30%, Grad Norm=6.3431\n",
      "Fold 4, Epoch 19: Train Loss=1.3190, Train Acc=54.77%, Val Loss=1.6093, Val Acc=44.08%, Grad Norm=6.6775\n",
      "Fold 4, Epoch 20: Train Loss=1.3017, Train Acc=55.39%, Val Loss=1.6178, Val Acc=43.95%, Grad Norm=6.9871\n",
      "Fold 4, Epoch 21: Train Loss=1.2479, Train Acc=57.24%, Val Loss=1.6106, Val Acc=44.57%, Grad Norm=7.3608\n",
      "Fold 4, Epoch 22: Train Loss=1.2236, Train Acc=58.22%, Val Loss=1.6195, Val Acc=44.41%, Grad Norm=7.9108\n",
      "Fold 4, Epoch 23: Train Loss=1.2058, Train Acc=58.68%, Val Loss=1.6344, Val Acc=44.27%, Grad Norm=8.3914\n",
      "Fold 4, Epoch 24: Train Loss=1.1859, Train Acc=59.39%, Val Loss=1.6430, Val Acc=44.07%, Grad Norm=8.8196\n",
      "Fold 4, Epoch 25: Train Loss=1.1632, Train Acc=60.47%, Val Loss=1.6574, Val Acc=43.65%, Grad Norm=9.2689\n",
      "Fold 4, Epoch 26: Train Loss=1.1460, Train Acc=60.85%, Val Loss=1.6651, Val Acc=43.90%, Grad Norm=9.7981\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=45.31%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2195, Train Acc=11.34%, Val Loss=2.3129, Val Acc=9.33%, Grad Norm=6.3464\n",
      "Fold 5, Epoch 2: Train Loss=2.2011, Train Acc=12.55%, Val Loss=2.3350, Val Acc=10.74%, Grad Norm=4.2914\n",
      "Fold 5, Epoch 3: Train Loss=2.1043, Train Acc=19.68%, Val Loss=2.0025, Val Acc=26.72%, Grad Norm=3.0831\n",
      "Fold 5, Epoch 4: Train Loss=1.9438, Train Acc=28.93%, Val Loss=1.8542, Val Acc=33.80%, Grad Norm=3.0947\n",
      "Fold 5, Epoch 5: Train Loss=1.8597, Train Acc=33.08%, Val Loss=1.8211, Val Acc=35.35%, Grad Norm=2.8910\n",
      "Fold 5, Epoch 6: Train Loss=1.8103, Train Acc=35.33%, Val Loss=1.7778, Val Acc=36.96%, Grad Norm=2.9495\n",
      "Fold 5, Epoch 7: Train Loss=1.7481, Train Acc=37.99%, Val Loss=1.7166, Val Acc=39.84%, Grad Norm=3.2583\n",
      "Fold 5, Epoch 8: Train Loss=1.6759, Train Acc=41.13%, Val Loss=1.6391, Val Acc=42.76%, Grad Norm=3.5726\n",
      "Fold 5, Epoch 9: Train Loss=1.6172, Train Acc=43.37%, Val Loss=1.5936, Val Acc=44.40%, Grad Norm=3.7922\n",
      "Fold 5, Epoch 10: Train Loss=1.5751, Train Acc=45.23%, Val Loss=1.5852, Val Acc=44.84%, Grad Norm=3.9498\n",
      "Fold 5, Epoch 11: Train Loss=1.5155, Train Acc=47.51%, Val Loss=1.5669, Val Acc=45.24%, Grad Norm=4.1924\n",
      "Fold 5, Epoch 12: Train Loss=1.4841, Train Acc=48.65%, Val Loss=1.5478, Val Acc=46.29%, Grad Norm=4.4996\n",
      "Fold 5, Epoch 13: Train Loss=1.4618, Train Acc=49.63%, Val Loss=1.5642, Val Acc=45.95%, Grad Norm=4.8005\n",
      "Fold 5, Epoch 14: Train Loss=1.4405, Train Acc=50.28%, Val Loss=1.5496, Val Acc=46.38%, Grad Norm=5.0754\n",
      "Fold 5, Epoch 15: Train Loss=1.4225, Train Acc=50.88%, Val Loss=1.5428, Val Acc=46.70%, Grad Norm=5.3645\n",
      "Fold 5, Epoch 16: Train Loss=1.4032, Train Acc=51.66%, Val Loss=1.5516, Val Acc=46.35%, Grad Norm=5.6506\n",
      "Fold 5, Epoch 17: Train Loss=1.3833, Train Acc=52.47%, Val Loss=1.5553, Val Acc=46.53%, Grad Norm=5.9330\n",
      "Fold 5, Epoch 18: Train Loss=1.3634, Train Acc=53.17%, Val Loss=1.5508, Val Acc=46.53%, Grad Norm=6.2268\n",
      "Fold 5, Epoch 19: Train Loss=1.3453, Train Acc=53.77%, Val Loss=1.5504, Val Acc=46.70%, Grad Norm=6.5226\n",
      "Fold 5, Epoch 20: Train Loss=1.3284, Train Acc=54.60%, Val Loss=1.5554, Val Acc=46.46%, Grad Norm=6.8438\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=45.48%\n",
      "\n",
      "SNR -10 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-26_10-00-30_LTE-V_XFR_SNR-10dB_fd655_classes_9_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-15 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 789, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 789\n",
      "[INFO] 训练 block 数: 591, 测试 block 数: 198\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2203, Train Acc=11.46%, Val Loss=2.2104, Val Acc=12.14%, Grad Norm=6.3700\n",
      "Fold 1, Epoch 2: Train Loss=2.2102, Train Acc=11.33%, Val Loss=2.2389, Val Acc=10.08%, Grad Norm=4.4837\n",
      "Fold 1, Epoch 3: Train Loss=2.2038, Train Acc=11.39%, Val Loss=2.2328, Val Acc=10.90%, Grad Norm=2.9837\n",
      "Fold 1, Epoch 4: Train Loss=2.2005, Train Acc=11.67%, Val Loss=2.2126, Val Acc=10.65%, Grad Norm=2.2718\n",
      "Fold 1, Epoch 5: Train Loss=2.1967, Train Acc=12.35%, Val Loss=2.2102, Val Acc=11.30%, Grad Norm=1.8117\n",
      "Fold 1, Epoch 6: Train Loss=2.1874, Train Acc=13.75%, Val Loss=2.1967, Val Acc=13.14%, Grad Norm=1.4936\n",
      "Fold 1, Epoch 7: Train Loss=2.1716, Train Acc=15.73%, Val Loss=2.1775, Val Acc=16.34%, Grad Norm=1.4478\n",
      "Fold 1, Epoch 8: Train Loss=2.1530, Train Acc=17.50%, Val Loss=2.1489, Val Acc=18.22%, Grad Norm=1.5122\n",
      "Fold 1, Epoch 9: Train Loss=2.1397, Train Acc=18.78%, Val Loss=2.1574, Val Acc=17.62%, Grad Norm=1.5756\n",
      "Fold 1, Epoch 10: Train Loss=2.1284, Train Acc=19.56%, Val Loss=2.1488, Val Acc=18.39%, Grad Norm=1.6785\n",
      "Fold 1, Epoch 11: Train Loss=2.1078, Train Acc=20.95%, Val Loss=2.1266, Val Acc=20.20%, Grad Norm=1.9562\n",
      "Fold 1, Epoch 12: Train Loss=2.0942, Train Acc=21.72%, Val Loss=2.1296, Val Acc=19.86%, Grad Norm=2.2660\n",
      "Fold 1, Epoch 13: Train Loss=2.0841, Train Acc=22.43%, Val Loss=2.1249, Val Acc=20.40%, Grad Norm=2.5167\n",
      "Fold 1, Epoch 14: Train Loss=2.0758, Train Acc=22.84%, Val Loss=2.1204, Val Acc=20.66%, Grad Norm=2.7625\n",
      "Fold 1, Epoch 15: Train Loss=2.0655, Train Acc=23.38%, Val Loss=2.1181, Val Acc=20.57%, Grad Norm=3.0357\n",
      "Fold 1, Epoch 16: Train Loss=2.0556, Train Acc=24.05%, Val Loss=2.1260, Val Acc=20.28%, Grad Norm=3.3121\n",
      "Fold 1, Epoch 17: Train Loss=2.0454, Train Acc=24.62%, Val Loss=2.1306, Val Acc=20.30%, Grad Norm=3.6126\n",
      "Fold 1, Epoch 18: Train Loss=2.0361, Train Acc=25.24%, Val Loss=2.1259, Val Acc=20.56%, Grad Norm=3.9188\n",
      "Fold 1, Epoch 19: Train Loss=2.0261, Train Acc=25.55%, Val Loss=2.1314, Val Acc=20.03%, Grad Norm=4.2068\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=20.26%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2212, Train Acc=11.17%, Val Loss=2.2381, Val Acc=11.56%, Grad Norm=6.2557\n",
      "Fold 2, Epoch 2: Train Loss=2.2107, Train Acc=11.18%, Val Loss=2.2176, Val Acc=10.77%, Grad Norm=4.3277\n",
      "Fold 2, Epoch 3: Train Loss=2.2038, Train Acc=11.41%, Val Loss=2.2297, Val Acc=9.33%, Grad Norm=2.8942\n",
      "Fold 2, Epoch 4: Train Loss=2.1998, Train Acc=12.00%, Val Loss=2.2108, Val Acc=11.39%, Grad Norm=2.2667\n",
      "Fold 2, Epoch 5: Train Loss=2.1905, Train Acc=13.46%, Val Loss=2.1990, Val Acc=12.29%, Grad Norm=1.7637\n",
      "Fold 2, Epoch 6: Train Loss=2.1764, Train Acc=15.30%, Val Loss=2.1903, Val Acc=13.61%, Grad Norm=1.5041\n",
      "Fold 2, Epoch 7: Train Loss=2.1610, Train Acc=16.94%, Val Loss=2.1730, Val Acc=15.40%, Grad Norm=1.4316\n",
      "Fold 2, Epoch 8: Train Loss=2.1502, Train Acc=18.01%, Val Loss=2.1685, Val Acc=15.79%, Grad Norm=1.4155\n",
      "Fold 2, Epoch 9: Train Loss=2.1419, Train Acc=18.71%, Val Loss=2.1649, Val Acc=16.37%, Grad Norm=1.4290\n",
      "Fold 2, Epoch 10: Train Loss=2.1340, Train Acc=19.31%, Val Loss=2.1556, Val Acc=16.72%, Grad Norm=1.4823\n",
      "Fold 2, Epoch 11: Train Loss=2.1175, Train Acc=20.46%, Val Loss=2.1503, Val Acc=17.64%, Grad Norm=1.7214\n",
      "Fold 2, Epoch 12: Train Loss=2.1076, Train Acc=21.08%, Val Loss=2.1535, Val Acc=17.47%, Grad Norm=2.0011\n",
      "Fold 2, Epoch 13: Train Loss=2.0978, Train Acc=21.69%, Val Loss=2.1410, Val Acc=18.18%, Grad Norm=2.2634\n",
      "Fold 2, Epoch 14: Train Loss=2.0904, Train Acc=22.17%, Val Loss=2.1372, Val Acc=18.56%, Grad Norm=2.5273\n",
      "Fold 2, Epoch 15: Train Loss=2.0796, Train Acc=22.93%, Val Loss=2.1349, Val Acc=18.64%, Grad Norm=2.8118\n",
      "Fold 2, Epoch 16: Train Loss=2.0719, Train Acc=23.26%, Val Loss=2.1339, Val Acc=19.02%, Grad Norm=3.0541\n",
      "Fold 2, Epoch 17: Train Loss=2.0620, Train Acc=23.84%, Val Loss=2.1363, Val Acc=19.13%, Grad Norm=3.3717\n",
      "Fold 2, Epoch 18: Train Loss=2.0511, Train Acc=24.29%, Val Loss=2.1330, Val Acc=19.51%, Grad Norm=3.6956\n",
      "Fold 2, Epoch 19: Train Loss=2.0389, Train Acc=25.07%, Val Loss=2.1322, Val Acc=19.51%, Grad Norm=4.0393\n",
      "Fold 2, Epoch 20: Train Loss=2.0266, Train Acc=25.70%, Val Loss=2.1309, Val Acc=19.69%, Grad Norm=4.3897\n",
      "Fold 2, Epoch 21: Train Loss=1.9966, Train Acc=27.26%, Val Loss=2.1395, Val Acc=19.42%, Grad Norm=4.9227\n",
      "Fold 2, Epoch 22: Train Loss=1.9795, Train Acc=28.00%, Val Loss=2.1406, Val Acc=19.87%, Grad Norm=5.5762\n",
      "Fold 2, Epoch 23: Train Loss=1.9662, Train Acc=28.64%, Val Loss=2.1451, Val Acc=19.76%, Grad Norm=6.1729\n",
      "Fold 2, Epoch 24: Train Loss=1.9496, Train Acc=29.50%, Val Loss=2.1529, Val Acc=19.71%, Grad Norm=6.8010\n",
      "Fold 2, Epoch 25: Train Loss=1.9351, Train Acc=30.20%, Val Loss=2.1524, Val Acc=19.67%, Grad Norm=7.3610\n",
      "Fold 2, Epoch 26: Train Loss=1.9205, Train Acc=30.93%, Val Loss=2.1655, Val Acc=19.51%, Grad Norm=8.0005\n",
      "Fold 2, Epoch 27: Train Loss=1.9040, Train Acc=31.79%, Val Loss=2.1743, Val Acc=19.64%, Grad Norm=8.6716\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=20.12%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2203, Train Acc=11.18%, Val Loss=2.2329, Val Acc=10.34%, Grad Norm=6.3263\n",
      "Fold 3, Epoch 2: Train Loss=2.2108, Train Acc=11.33%, Val Loss=2.2039, Val Acc=13.19%, Grad Norm=4.4899\n",
      "Fold 3, Epoch 3: Train Loss=2.2043, Train Acc=11.41%, Val Loss=2.2240, Val Acc=12.53%, Grad Norm=2.9604\n",
      "Fold 3, Epoch 4: Train Loss=2.2009, Train Acc=11.68%, Val Loss=2.2081, Val Acc=12.62%, Grad Norm=2.2767\n",
      "Fold 3, Epoch 5: Train Loss=2.1962, Train Acc=12.43%, Val Loss=2.2023, Val Acc=13.04%, Grad Norm=1.8101\n",
      "Fold 3, Epoch 6: Train Loss=2.1839, Train Acc=14.43%, Val Loss=2.1906, Val Acc=13.93%, Grad Norm=1.5075\n",
      "Fold 3, Epoch 7: Train Loss=2.1684, Train Acc=16.16%, Val Loss=2.1733, Val Acc=14.83%, Grad Norm=1.4230\n",
      "Fold 3, Epoch 8: Train Loss=2.1554, Train Acc=17.47%, Val Loss=2.1691, Val Acc=15.48%, Grad Norm=1.3899\n",
      "Fold 3, Epoch 9: Train Loss=2.1448, Train Acc=18.34%, Val Loss=2.1659, Val Acc=16.94%, Grad Norm=1.4329\n",
      "Fold 3, Epoch 10: Train Loss=2.1359, Train Acc=19.08%, Val Loss=2.1481, Val Acc=18.04%, Grad Norm=1.5224\n",
      "Fold 3, Epoch 11: Train Loss=2.1159, Train Acc=20.47%, Val Loss=2.1391, Val Acc=18.88%, Grad Norm=1.8124\n",
      "Fold 3, Epoch 12: Train Loss=2.1032, Train Acc=21.25%, Val Loss=2.1314, Val Acc=19.49%, Grad Norm=2.1320\n",
      "Fold 3, Epoch 13: Train Loss=2.0940, Train Acc=21.76%, Val Loss=2.1313, Val Acc=19.20%, Grad Norm=2.3786\n",
      "Fold 3, Epoch 14: Train Loss=2.0841, Train Acc=22.61%, Val Loss=2.1282, Val Acc=19.81%, Grad Norm=2.6469\n",
      "Fold 3, Epoch 15: Train Loss=2.0740, Train Acc=23.01%, Val Loss=2.1281, Val Acc=19.56%, Grad Norm=2.8891\n",
      "Fold 3, Epoch 16: Train Loss=2.0637, Train Acc=23.67%, Val Loss=2.1306, Val Acc=19.46%, Grad Norm=3.1800\n",
      "Fold 3, Epoch 17: Train Loss=2.0533, Train Acc=24.24%, Val Loss=2.1249, Val Acc=20.12%, Grad Norm=3.4805\n",
      "Fold 3, Epoch 18: Train Loss=2.0437, Train Acc=24.79%, Val Loss=2.1382, Val Acc=19.47%, Grad Norm=3.7846\n",
      "Fold 3, Epoch 19: Train Loss=2.0336, Train Acc=25.26%, Val Loss=2.1329, Val Acc=19.78%, Grad Norm=4.0920\n",
      "Fold 3, Epoch 20: Train Loss=2.0227, Train Acc=25.80%, Val Loss=2.1318, Val Acc=19.95%, Grad Norm=4.4222\n",
      "Fold 3, Epoch 21: Train Loss=1.9902, Train Acc=27.61%, Val Loss=2.1309, Val Acc=20.17%, Grad Norm=5.0056\n",
      "Fold 3, Epoch 22: Train Loss=1.9721, Train Acc=28.47%, Val Loss=2.1423, Val Acc=20.13%, Grad Norm=5.6658\n",
      "Fold 3, Epoch 23: Train Loss=1.9587, Train Acc=28.98%, Val Loss=2.1435, Val Acc=20.04%, Grad Norm=6.2437\n",
      "Fold 3, Epoch 24: Train Loss=1.9424, Train Acc=29.78%, Val Loss=2.1515, Val Acc=20.19%, Grad Norm=6.8363\n",
      "Fold 3, Epoch 25: Train Loss=1.9282, Train Acc=30.45%, Val Loss=2.1577, Val Acc=20.19%, Grad Norm=7.5140\n",
      "Fold 3, Epoch 26: Train Loss=1.9098, Train Acc=31.25%, Val Loss=2.1620, Val Acc=19.97%, Grad Norm=8.1952\n",
      "Fold 3, Epoch 27: Train Loss=1.8955, Train Acc=32.04%, Val Loss=2.1813, Val Acc=19.63%, Grad Norm=8.7921\n",
      "Fold 3, Epoch 28: Train Loss=1.8782, Train Acc=32.75%, Val Loss=2.1873, Val Acc=19.67%, Grad Norm=9.5108\n",
      "Fold 3, Epoch 29: Train Loss=1.8613, Train Acc=33.46%, Val Loss=2.1995, Val Acc=19.49%, Grad Norm=10.2602\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=19.35%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2206, Train Acc=11.18%, Val Loss=2.2127, Val Acc=11.35%, Grad Norm=6.3482\n",
      "Fold 4, Epoch 2: Train Loss=2.2109, Train Acc=11.29%, Val Loss=2.2222, Val Acc=11.96%, Grad Norm=4.4383\n",
      "Fold 4, Epoch 3: Train Loss=2.2039, Train Acc=11.51%, Val Loss=2.2208, Val Acc=10.30%, Grad Norm=2.9201\n",
      "Fold 4, Epoch 4: Train Loss=2.1998, Train Acc=11.89%, Val Loss=2.2361, Val Acc=11.78%, Grad Norm=2.2447\n",
      "Fold 4, Epoch 5: Train Loss=2.1896, Train Acc=13.52%, Val Loss=2.1940, Val Acc=13.83%, Grad Norm=1.7701\n",
      "Fold 4, Epoch 6: Train Loss=2.1731, Train Acc=15.46%, Val Loss=2.1678, Val Acc=16.10%, Grad Norm=1.5550\n",
      "Fold 4, Epoch 7: Train Loss=2.1570, Train Acc=17.20%, Val Loss=2.1607, Val Acc=16.46%, Grad Norm=1.4756\n",
      "Fold 4, Epoch 8: Train Loss=2.1462, Train Acc=18.25%, Val Loss=2.1545, Val Acc=17.20%, Grad Norm=1.4452\n",
      "Fold 4, Epoch 9: Train Loss=2.1369, Train Acc=18.96%, Val Loss=2.1544, Val Acc=17.23%, Grad Norm=1.5048\n",
      "Fold 4, Epoch 10: Train Loss=2.1277, Train Acc=19.89%, Val Loss=2.1433, Val Acc=18.27%, Grad Norm=1.5896\n",
      "Fold 4, Epoch 11: Train Loss=2.1068, Train Acc=21.25%, Val Loss=2.1412, Val Acc=18.46%, Grad Norm=1.8645\n",
      "Fold 4, Epoch 12: Train Loss=2.0969, Train Acc=21.69%, Val Loss=2.1388, Val Acc=18.71%, Grad Norm=2.1540\n",
      "Fold 4, Epoch 13: Train Loss=2.0866, Train Acc=22.33%, Val Loss=2.1413, Val Acc=18.77%, Grad Norm=2.4248\n",
      "Fold 4, Epoch 14: Train Loss=2.0773, Train Acc=22.81%, Val Loss=2.1361, Val Acc=19.11%, Grad Norm=2.6883\n",
      "Fold 4, Epoch 15: Train Loss=2.0686, Train Acc=23.32%, Val Loss=2.1368, Val Acc=19.11%, Grad Norm=2.9367\n",
      "Fold 4, Epoch 16: Train Loss=2.0579, Train Acc=24.06%, Val Loss=2.1369, Val Acc=19.65%, Grad Norm=3.2468\n",
      "Fold 4, Epoch 17: Train Loss=2.0484, Train Acc=24.49%, Val Loss=2.1344, Val Acc=19.48%, Grad Norm=3.5331\n",
      "Fold 4, Epoch 18: Train Loss=2.0372, Train Acc=25.19%, Val Loss=2.1377, Val Acc=19.49%, Grad Norm=3.8502\n",
      "Fold 4, Epoch 19: Train Loss=2.0270, Train Acc=25.75%, Val Loss=2.1403, Val Acc=19.48%, Grad Norm=4.1748\n",
      "Fold 4, Epoch 20: Train Loss=2.0135, Train Acc=26.48%, Val Loss=2.1427, Val Acc=19.56%, Grad Norm=4.5363\n",
      "Fold 4, Epoch 21: Train Loss=1.9831, Train Acc=28.04%, Val Loss=2.1462, Val Acc=19.77%, Grad Norm=5.0692\n",
      "Fold 4, Epoch 22: Train Loss=1.9644, Train Acc=28.84%, Val Loss=2.1534, Val Acc=19.83%, Grad Norm=5.7582\n",
      "Fold 4, Epoch 23: Train Loss=1.9499, Train Acc=29.57%, Val Loss=2.1592, Val Acc=19.62%, Grad Norm=6.3517\n",
      "Fold 4, Epoch 24: Train Loss=1.9330, Train Acc=30.29%, Val Loss=2.1722, Val Acc=19.28%, Grad Norm=6.9636\n",
      "Fold 4, Epoch 25: Train Loss=1.9197, Train Acc=30.95%, Val Loss=2.1780, Val Acc=19.18%, Grad Norm=7.5985\n",
      "Fold 4, Epoch 26: Train Loss=1.9014, Train Acc=31.65%, Val Loss=2.1866, Val Acc=19.30%, Grad Norm=8.2641\n",
      "Fold 4, Epoch 27: Train Loss=1.8849, Train Acc=32.44%, Val Loss=2.1979, Val Acc=18.99%, Grad Norm=8.9505\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=19.68%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2204, Train Acc=11.13%, Val Loss=2.2579, Val Acc=8.44%, Grad Norm=6.3449\n",
      "Fold 5, Epoch 2: Train Loss=2.2110, Train Acc=11.31%, Val Loss=2.2151, Val Acc=10.57%, Grad Norm=4.5140\n",
      "Fold 5, Epoch 3: Train Loss=2.2048, Train Acc=11.63%, Val Loss=2.2279, Val Acc=8.48%, Grad Norm=2.9549\n",
      "Fold 5, Epoch 4: Train Loss=2.2011, Train Acc=11.78%, Val Loss=2.2220, Val Acc=10.13%, Grad Norm=2.2644\n",
      "Fold 5, Epoch 5: Train Loss=2.1965, Train Acc=12.68%, Val Loss=2.2029, Val Acc=11.47%, Grad Norm=1.7984\n",
      "Fold 5, Epoch 6: Train Loss=2.1868, Train Acc=14.17%, Val Loss=2.1888, Val Acc=13.54%, Grad Norm=1.5371\n",
      "Fold 5, Epoch 7: Train Loss=2.1696, Train Acc=16.02%, Val Loss=2.1643, Val Acc=16.99%, Grad Norm=1.5060\n",
      "Fold 5, Epoch 8: Train Loss=2.1513, Train Acc=17.83%, Val Loss=2.1547, Val Acc=17.13%, Grad Norm=1.5490\n",
      "Fold 5, Epoch 9: Train Loss=2.1386, Train Acc=18.66%, Val Loss=2.1624, Val Acc=16.89%, Grad Norm=1.5779\n",
      "Fold 5, Epoch 10: Train Loss=2.1289, Train Acc=19.58%, Val Loss=2.1429, Val Acc=18.55%, Grad Norm=1.6571\n",
      "Fold 5, Epoch 11: Train Loss=2.1056, Train Acc=21.26%, Val Loss=2.1297, Val Acc=19.38%, Grad Norm=1.9699\n",
      "Fold 5, Epoch 12: Train Loss=2.0927, Train Acc=21.92%, Val Loss=2.1406, Val Acc=18.89%, Grad Norm=2.2960\n",
      "Fold 5, Epoch 13: Train Loss=2.0841, Train Acc=22.45%, Val Loss=2.1276, Val Acc=19.58%, Grad Norm=2.5405\n",
      "Fold 5, Epoch 14: Train Loss=2.0754, Train Acc=22.95%, Val Loss=2.1273, Val Acc=19.56%, Grad Norm=2.7966\n",
      "Fold 5, Epoch 15: Train Loss=2.0647, Train Acc=23.54%, Val Loss=2.1254, Val Acc=19.85%, Grad Norm=3.0811\n",
      "Fold 5, Epoch 16: Train Loss=2.0558, Train Acc=24.02%, Val Loss=2.1272, Val Acc=20.11%, Grad Norm=3.3247\n",
      "Fold 5, Epoch 17: Train Loss=2.0460, Train Acc=24.58%, Val Loss=2.1213, Val Acc=20.54%, Grad Norm=3.6099\n",
      "Fold 5, Epoch 18: Train Loss=2.0356, Train Acc=25.29%, Val Loss=2.1256, Val Acc=20.04%, Grad Norm=3.8825\n",
      "Fold 5, Epoch 19: Train Loss=2.0245, Train Acc=25.72%, Val Loss=2.1253, Val Acc=20.40%, Grad Norm=4.2157\n",
      "Fold 5, Epoch 20: Train Loss=2.0120, Train Acc=26.23%, Val Loss=2.1262, Val Acc=20.43%, Grad Norm=4.5761\n",
      "Fold 5, Epoch 21: Train Loss=1.9813, Train Acc=28.03%, Val Loss=2.1354, Val Acc=20.38%, Grad Norm=5.1016\n",
      "Fold 5, Epoch 22: Train Loss=1.9632, Train Acc=28.87%, Val Loss=2.1390, Val Acc=20.47%, Grad Norm=5.7770\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=20.14%\n",
      "\n",
      "SNR -15 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-26_10-35-34_LTE-V_XFR_SNR-15dB_fd655_classes_9_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-20 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 789, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 789\n",
      "[INFO] 训练 block 数: 591, 测试 block 数: 198\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2202, Train Acc=11.34%, Val Loss=2.2151, Val Acc=9.96%, Grad Norm=6.3152\n",
      "Fold 1, Epoch 2: Train Loss=2.2103, Train Acc=11.45%, Val Loss=2.2469, Val Acc=10.18%, Grad Norm=4.4327\n",
      "Fold 1, Epoch 3: Train Loss=2.2035, Train Acc=11.50%, Val Loss=2.2452, Val Acc=10.26%, Grad Norm=2.9341\n",
      "Fold 1, Epoch 4: Train Loss=2.2022, Train Acc=11.41%, Val Loss=2.2207, Val Acc=11.00%, Grad Norm=2.1691\n",
      "Fold 1, Epoch 5: Train Loss=2.1999, Train Acc=11.72%, Val Loss=2.2002, Val Acc=11.55%, Grad Norm=1.6812\n",
      "Fold 1, Epoch 6: Train Loss=2.1983, Train Acc=11.72%, Val Loss=2.2079, Val Acc=9.87%, Grad Norm=1.2929\n",
      "Fold 1, Epoch 7: Train Loss=2.1972, Train Acc=11.90%, Val Loss=2.2107, Val Acc=8.86%, Grad Norm=0.9930\n",
      "Fold 1, Epoch 8: Train Loss=2.1967, Train Acc=11.86%, Val Loss=2.2103, Val Acc=9.58%, Grad Norm=0.7800\n",
      "Fold 1, Epoch 9: Train Loss=2.1956, Train Acc=12.06%, Val Loss=2.2070, Val Acc=9.97%, Grad Norm=0.6572\n",
      "Fold 1, Epoch 10: Train Loss=2.1952, Train Acc=12.23%, Val Loss=2.2060, Val Acc=9.82%, Grad Norm=0.5493\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.19%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2201, Train Acc=11.33%, Val Loss=2.2625, Val Acc=8.48%, Grad Norm=6.3358\n",
      "Fold 2, Epoch 2: Train Loss=2.2118, Train Acc=11.32%, Val Loss=2.2147, Val Acc=11.13%, Grad Norm=4.6075\n",
      "Fold 2, Epoch 3: Train Loss=2.2055, Train Acc=11.25%, Val Loss=2.2302, Val Acc=8.76%, Grad Norm=3.1202\n",
      "Fold 2, Epoch 4: Train Loss=2.2020, Train Acc=11.48%, Val Loss=2.2028, Val Acc=11.39%, Grad Norm=2.3362\n",
      "Fold 2, Epoch 5: Train Loss=2.2007, Train Acc=11.40%, Val Loss=2.2059, Val Acc=10.11%, Grad Norm=1.8042\n",
      "Fold 2, Epoch 6: Train Loss=2.1988, Train Acc=11.64%, Val Loss=2.2013, Val Acc=10.62%, Grad Norm=1.3500\n",
      "Fold 2, Epoch 7: Train Loss=2.1977, Train Acc=11.76%, Val Loss=2.2046, Val Acc=9.72%, Grad Norm=1.0370\n",
      "Fold 2, Epoch 8: Train Loss=2.1968, Train Acc=12.10%, Val Loss=2.2082, Val Acc=10.30%, Grad Norm=0.8077\n",
      "Fold 2, Epoch 9: Train Loss=2.1960, Train Acc=12.25%, Val Loss=2.2044, Val Acc=10.11%, Grad Norm=0.6591\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.29%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2209, Train Acc=11.26%, Val Loss=2.2903, Val Acc=9.96%, Grad Norm=6.3294\n",
      "Fold 3, Epoch 2: Train Loss=2.2114, Train Acc=11.21%, Val Loss=2.2125, Val Acc=11.73%, Grad Norm=4.3565\n",
      "Fold 3, Epoch 3: Train Loss=2.2048, Train Acc=11.49%, Val Loss=2.2494, Val Acc=9.33%, Grad Norm=2.8293\n",
      "Fold 3, Epoch 4: Train Loss=2.2018, Train Acc=11.60%, Val Loss=2.2173, Val Acc=9.50%, Grad Norm=2.1360\n",
      "Fold 3, Epoch 5: Train Loss=2.2001, Train Acc=11.63%, Val Loss=2.2109, Val Acc=12.51%, Grad Norm=1.6362\n",
      "Fold 3, Epoch 6: Train Loss=2.1992, Train Acc=11.61%, Val Loss=2.2005, Val Acc=10.37%, Grad Norm=1.2521\n",
      "Fold 3, Epoch 7: Train Loss=2.1978, Train Acc=11.78%, Val Loss=2.2045, Val Acc=10.30%, Grad Norm=0.9752\n",
      "Fold 3, Epoch 8: Train Loss=2.1968, Train Acc=11.95%, Val Loss=2.2036, Val Acc=10.06%, Grad Norm=0.7882\n",
      "Fold 3, Epoch 9: Train Loss=2.1962, Train Acc=12.12%, Val Loss=2.2080, Val Acc=10.11%, Grad Norm=0.6426\n",
      "Fold 3, Epoch 10: Train Loss=2.1960, Train Acc=12.22%, Val Loss=2.2033, Val Acc=10.23%, Grad Norm=0.5321\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.57%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2211, Train Acc=11.23%, Val Loss=2.2139, Val Acc=11.80%, Grad Norm=6.2616\n",
      "Fold 4, Epoch 2: Train Loss=2.2119, Train Acc=11.40%, Val Loss=2.2305, Val Acc=11.07%, Grad Norm=4.5553\n",
      "Fold 4, Epoch 3: Train Loss=2.2049, Train Acc=11.28%, Val Loss=2.2024, Val Acc=12.72%, Grad Norm=2.9769\n",
      "Fold 4, Epoch 4: Train Loss=2.2025, Train Acc=11.42%, Val Loss=2.2169, Val Acc=9.32%, Grad Norm=2.2543\n",
      "Fold 4, Epoch 5: Train Loss=2.2006, Train Acc=11.55%, Val Loss=2.2167, Val Acc=11.86%, Grad Norm=1.7218\n",
      "Fold 4, Epoch 6: Train Loss=2.1991, Train Acc=11.50%, Val Loss=2.2046, Val Acc=10.51%, Grad Norm=1.2708\n",
      "Fold 4, Epoch 7: Train Loss=2.1976, Train Acc=11.60%, Val Loss=2.2068, Val Acc=11.89%, Grad Norm=0.9737\n",
      "Fold 4, Epoch 8: Train Loss=2.1967, Train Acc=11.86%, Val Loss=2.2049, Val Acc=9.79%, Grad Norm=0.7743\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.18%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2217, Train Acc=11.16%, Val Loss=2.2353, Val Acc=9.82%, Grad Norm=6.3176\n",
      "Fold 5, Epoch 2: Train Loss=2.2115, Train Acc=11.24%, Val Loss=2.2828, Val Acc=8.48%, Grad Norm=4.4852\n",
      "Fold 5, Epoch 3: Train Loss=2.2042, Train Acc=11.53%, Val Loss=2.2630, Val Acc=9.33%, Grad Norm=3.0208\n",
      "Fold 5, Epoch 4: Train Loss=2.2019, Train Acc=11.66%, Val Loss=2.2179, Val Acc=9.33%, Grad Norm=2.2626\n",
      "Fold 5, Epoch 5: Train Loss=2.2005, Train Acc=11.57%, Val Loss=2.2265, Val Acc=10.22%, Grad Norm=1.7619\n",
      "Fold 5, Epoch 6: Train Loss=2.1983, Train Acc=11.76%, Val Loss=2.2030, Val Acc=9.33%, Grad Norm=1.3789\n",
      "Fold 5, Epoch 7: Train Loss=2.1975, Train Acc=11.94%, Val Loss=2.2124, Val Acc=8.88%, Grad Norm=1.0454\n",
      "Fold 5, Epoch 8: Train Loss=2.1962, Train Acc=12.19%, Val Loss=2.2026, Val Acc=9.74%, Grad Norm=0.8175\n",
      "Fold 5, Epoch 9: Train Loss=2.1955, Train Acc=12.43%, Val Loss=2.2077, Val Acc=9.07%, Grad Norm=0.6853\n",
      "Fold 5, Epoch 10: Train Loss=2.1951, Train Acc=12.39%, Val Loss=2.2085, Val Acc=9.58%, Grad Norm=0.5757\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.57%\n",
      "\n",
      "SNR -20 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-26_11-14-21_LTE-V_XFR_SNR-20dB_fd655_classes_9_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-25 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 789, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 789\n",
      "[INFO] 训练 block 数: 591, 测试 block 数: 198\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2213, Train Acc=11.16%, Val Loss=2.2332, Val Acc=10.05%, Grad Norm=6.2561\n",
      "Fold 1, Epoch 2: Train Loss=2.2097, Train Acc=11.40%, Val Loss=2.2136, Val Acc=10.92%, Grad Norm=4.2756\n",
      "Fold 1, Epoch 3: Train Loss=2.2039, Train Acc=11.56%, Val Loss=2.2259, Val Acc=8.50%, Grad Norm=2.8669\n",
      "Fold 1, Epoch 4: Train Loss=2.2019, Train Acc=11.56%, Val Loss=2.2226, Val Acc=10.68%, Grad Norm=2.1714\n",
      "Fold 1, Epoch 5: Train Loss=2.2004, Train Acc=11.57%, Val Loss=2.2232, Val Acc=10.09%, Grad Norm=1.6481\n",
      "Fold 1, Epoch 6: Train Loss=2.1988, Train Acc=11.55%, Val Loss=2.2421, Val Acc=8.40%, Grad Norm=1.2531\n",
      "Fold 1, Epoch 7: Train Loss=2.1977, Train Acc=11.75%, Val Loss=2.2080, Val Acc=8.91%, Grad Norm=0.9634\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.08%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2206, Train Acc=11.19%, Val Loss=2.2071, Val Acc=14.02%, Grad Norm=6.1916\n",
      "Fold 2, Epoch 2: Train Loss=2.2106, Train Acc=11.32%, Val Loss=2.2629, Val Acc=8.48%, Grad Norm=4.3298\n",
      "Fold 2, Epoch 3: Train Loss=2.2044, Train Acc=11.27%, Val Loss=2.2055, Val Acc=10.22%, Grad Norm=2.8803\n",
      "Fold 2, Epoch 4: Train Loss=2.2020, Train Acc=11.37%, Val Loss=2.2137, Val Acc=9.33%, Grad Norm=2.2043\n",
      "Fold 2, Epoch 5: Train Loss=2.2008, Train Acc=11.43%, Val Loss=2.2078, Val Acc=10.05%, Grad Norm=1.6805\n",
      "Fold 2, Epoch 6: Train Loss=2.1991, Train Acc=11.63%, Val Loss=2.2007, Val Acc=10.16%, Grad Norm=1.2943\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.07%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2206, Train Acc=11.15%, Val Loss=2.2187, Val Acc=13.07%, Grad Norm=6.3221\n",
      "Fold 3, Epoch 2: Train Loss=2.2119, Train Acc=11.20%, Val Loss=2.2346, Val Acc=12.71%, Grad Norm=4.3597\n",
      "Fold 3, Epoch 3: Train Loss=2.2049, Train Acc=11.25%, Val Loss=2.2014, Val Acc=11.91%, Grad Norm=2.9212\n",
      "Fold 3, Epoch 4: Train Loss=2.2025, Train Acc=11.26%, Val Loss=2.2108, Val Acc=11.01%, Grad Norm=2.2250\n",
      "Fold 3, Epoch 5: Train Loss=2.2008, Train Acc=11.45%, Val Loss=2.2155, Val Acc=9.52%, Grad Norm=1.6992\n",
      "Fold 3, Epoch 6: Train Loss=2.1995, Train Acc=11.50%, Val Loss=2.2013, Val Acc=10.62%, Grad Norm=1.3152\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=10.96%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2217, Train Acc=11.04%, Val Loss=2.2515, Val Acc=10.08%, Grad Norm=6.2514\n",
      "Fold 4, Epoch 2: Train Loss=2.2114, Train Acc=11.30%, Val Loss=2.2175, Val Acc=11.96%, Grad Norm=4.3555\n",
      "Fold 4, Epoch 3: Train Loss=2.2050, Train Acc=11.21%, Val Loss=2.2110, Val Acc=9.86%, Grad Norm=2.8945\n",
      "Fold 4, Epoch 4: Train Loss=2.2025, Train Acc=11.40%, Val Loss=2.2316, Val Acc=10.88%, Grad Norm=2.2215\n",
      "Fold 4, Epoch 5: Train Loss=2.2007, Train Acc=11.43%, Val Loss=2.2127, Val Acc=9.66%, Grad Norm=1.7345\n",
      "Fold 4, Epoch 6: Train Loss=2.1994, Train Acc=11.50%, Val Loss=2.1983, Val Acc=11.48%, Grad Norm=1.3184\n",
      "Fold 4, Epoch 7: Train Loss=2.1984, Train Acc=11.39%, Val Loss=2.2064, Val Acc=10.30%, Grad Norm=0.9828\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.05%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2209, Train Acc=11.28%, Val Loss=2.2816, Val Acc=9.37%, Grad Norm=6.2801\n",
      "Fold 5, Epoch 2: Train Loss=2.2106, Train Acc=11.40%, Val Loss=2.2654, Val Acc=8.69%, Grad Norm=4.4370\n",
      "Fold 5, Epoch 3: Train Loss=2.2045, Train Acc=11.34%, Val Loss=2.2558, Val Acc=9.12%, Grad Norm=2.8613\n",
      "Fold 5, Epoch 4: Train Loss=2.2023, Train Acc=11.32%, Val Loss=2.2192, Val Acc=8.56%, Grad Norm=2.1421\n",
      "Fold 5, Epoch 5: Train Loss=2.2003, Train Acc=11.51%, Val Loss=2.2199, Val Acc=9.45%, Grad Norm=1.6307\n",
      "Fold 5, Epoch 6: Train Loss=2.1990, Train Acc=11.63%, Val Loss=2.2109, Val Acc=9.38%, Grad Norm=1.2472\n",
      "Fold 5, Epoch 7: Train Loss=2.1981, Train Acc=11.67%, Val Loss=2.2089, Val Acc=11.01%, Grad Norm=0.9396\n",
      "Fold 5, Epoch 8: Train Loss=2.1974, Train Acc=11.86%, Val Loss=2.2124, Val Acc=9.30%, Grad Norm=0.7275\n",
      "Fold 5, Epoch 9: Train Loss=2.1969, Train Acc=11.86%, Val Loss=2.2041, Val Acc=8.63%, Grad Norm=0.5761\n",
      "Fold 5, Epoch 10: Train Loss=2.1967, Train Acc=11.94%, Val Loss=2.2083, Val Acc=9.18%, Grad Norm=0.4606\n",
      "Fold 5, Epoch 11: Train Loss=2.1959, Train Acc=12.08%, Val Loss=2.2065, Val Acc=9.33%, Grad Norm=0.4441\n",
      "Fold 5, Epoch 12: Train Loss=2.1957, Train Acc=12.23%, Val Loss=2.2066, Val Acc=9.09%, Grad Norm=0.4886\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.30%\n",
      "\n",
      "SNR -25 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-26_11-29-34_LTE-V_XFR_SNR-25dB_fd655_classes_9_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-30 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 789, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 789\n",
      "[INFO] 训练 block 数: 591, 测试 block 数: 198\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2205, Train Acc=11.32%, Val Loss=2.2567, Val Acc=9.33%, Grad Norm=6.3051\n",
      "Fold 1, Epoch 2: Train Loss=2.2104, Train Acc=11.39%, Val Loss=2.2320, Val Acc=8.50%, Grad Norm=4.3258\n",
      "Fold 1, Epoch 3: Train Loss=2.2048, Train Acc=11.23%, Val Loss=2.2313, Val Acc=8.82%, Grad Norm=2.8652\n",
      "Fold 1, Epoch 4: Train Loss=2.2016, Train Acc=11.51%, Val Loss=2.2110, Val Acc=11.24%, Grad Norm=2.2142\n",
      "Fold 1, Epoch 5: Train Loss=2.2001, Train Acc=11.47%, Val Loss=2.2033, Val Acc=11.03%, Grad Norm=1.6739\n",
      "Fold 1, Epoch 6: Train Loss=2.1989, Train Acc=11.68%, Val Loss=2.2194, Val Acc=8.43%, Grad Norm=1.2555\n",
      "Fold 1, Epoch 7: Train Loss=2.1978, Train Acc=11.78%, Val Loss=2.2068, Val Acc=9.80%, Grad Norm=0.9646\n",
      "Fold 1, Epoch 8: Train Loss=2.1973, Train Acc=11.80%, Val Loss=2.2132, Val Acc=8.42%, Grad Norm=0.7332\n",
      "Fold 1, Epoch 9: Train Loss=2.1968, Train Acc=11.87%, Val Loss=2.2064, Val Acc=8.40%, Grad Norm=0.5899\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2215, Train Acc=11.21%, Val Loss=2.2870, Val Acc=9.47%, Grad Norm=6.2352\n",
      "Fold 2, Epoch 2: Train Loss=2.2119, Train Acc=11.08%, Val Loss=2.2711, Val Acc=9.32%, Grad Norm=4.3522\n",
      "Fold 2, Epoch 3: Train Loss=2.2052, Train Acc=11.32%, Val Loss=2.2173, Val Acc=9.42%, Grad Norm=2.9397\n",
      "Fold 2, Epoch 4: Train Loss=2.2025, Train Acc=11.29%, Val Loss=2.2081, Val Acc=9.46%, Grad Norm=2.2262\n",
      "Fold 2, Epoch 5: Train Loss=2.2010, Train Acc=11.45%, Val Loss=2.2049, Val Acc=10.01%, Grad Norm=1.7270\n",
      "Fold 2, Epoch 6: Train Loss=2.1997, Train Acc=11.44%, Val Loss=2.2068, Val Acc=9.57%, Grad Norm=1.2958\n",
      "Fold 2, Epoch 7: Train Loss=2.1985, Train Acc=11.60%, Val Loss=2.2116, Val Acc=9.43%, Grad Norm=0.9793\n",
      "Fold 2, Epoch 8: Train Loss=2.1976, Train Acc=11.65%, Val Loss=2.2025, Val Acc=10.29%, Grad Norm=0.7229\n",
      "Fold 2, Epoch 9: Train Loss=2.1973, Train Acc=11.57%, Val Loss=2.2056, Val Acc=10.19%, Grad Norm=0.5770\n",
      "Fold 2, Epoch 10: Train Loss=2.1971, Train Acc=11.60%, Val Loss=2.2017, Val Acc=9.18%, Grad Norm=0.4473\n",
      "Fold 2, Epoch 11: Train Loss=2.1966, Train Acc=11.77%, Val Loss=2.2029, Val Acc=8.77%, Grad Norm=0.3805\n",
      "Fold 2, Epoch 12: Train Loss=2.1964, Train Acc=11.88%, Val Loss=2.2032, Val Acc=9.31%, Grad Norm=0.3878\n",
      "Fold 2, Epoch 13: Train Loss=2.1962, Train Acc=11.88%, Val Loss=2.2035, Val Acc=9.71%, Grad Norm=0.4313\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.05%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2208, Train Acc=11.29%, Val Loss=2.2178, Val Acc=13.05%, Grad Norm=6.2878\n",
      "Fold 3, Epoch 2: Train Loss=2.2118, Train Acc=11.40%, Val Loss=2.2274, Val Acc=12.72%, Grad Norm=4.4973\n",
      "Fold 3, Epoch 3: Train Loss=2.2053, Train Acc=11.17%, Val Loss=2.2182, Val Acc=10.17%, Grad Norm=2.9934\n",
      "Fold 3, Epoch 4: Train Loss=2.2026, Train Acc=11.40%, Val Loss=2.2227, Val Acc=10.17%, Grad Norm=2.2418\n",
      "Fold 3, Epoch 5: Train Loss=2.2007, Train Acc=11.39%, Val Loss=2.2183, Val Acc=9.62%, Grad Norm=1.7537\n",
      "Fold 3, Epoch 6: Train Loss=2.1996, Train Acc=11.55%, Val Loss=2.2111, Val Acc=9.99%, Grad Norm=1.3354\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.21%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2204, Train Acc=11.30%, Val Loss=2.2912, Val Acc=9.82%, Grad Norm=6.3093\n",
      "Fold 4, Epoch 2: Train Loss=2.2119, Train Acc=11.22%, Val Loss=2.2116, Val Acc=10.85%, Grad Norm=4.4550\n",
      "Fold 4, Epoch 3: Train Loss=2.2048, Train Acc=11.29%, Val Loss=2.2084, Val Acc=11.64%, Grad Norm=2.9736\n",
      "Fold 4, Epoch 4: Train Loss=2.2024, Train Acc=11.43%, Val Loss=2.2247, Val Acc=9.51%, Grad Norm=2.2642\n",
      "Fold 4, Epoch 5: Train Loss=2.2008, Train Acc=11.37%, Val Loss=2.2076, Val Acc=11.17%, Grad Norm=1.7145\n",
      "Fold 4, Epoch 6: Train Loss=2.1994, Train Acc=11.40%, Val Loss=2.1988, Val Acc=12.42%, Grad Norm=1.2940\n",
      "Fold 4, Epoch 7: Train Loss=2.1984, Train Acc=11.50%, Val Loss=2.2017, Val Acc=11.56%, Grad Norm=0.9608\n",
      "Fold 4, Epoch 8: Train Loss=2.1977, Train Acc=11.55%, Val Loss=2.2007, Val Acc=10.17%, Grad Norm=0.7264\n",
      "Fold 4, Epoch 9: Train Loss=2.1974, Train Acc=11.52%, Val Loss=2.2049, Val Acc=9.44%, Grad Norm=0.5597\n",
      "Fold 4, Epoch 10: Train Loss=2.1968, Train Acc=11.79%, Val Loss=2.2032, Val Acc=9.16%, Grad Norm=0.4944\n",
      "Fold 4, Epoch 11: Train Loss=2.1965, Train Acc=11.78%, Val Loss=2.2006, Val Acc=10.63%, Grad Norm=0.4271\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.01%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2217, Train Acc=11.14%, Val Loss=2.2154, Val Acc=9.47%, Grad Norm=6.3499\n",
      "Fold 5, Epoch 2: Train Loss=2.2114, Train Acc=11.42%, Val Loss=2.2229, Val Acc=12.69%, Grad Norm=4.5227\n",
      "Fold 5, Epoch 3: Train Loss=2.2046, Train Acc=11.38%, Val Loss=2.2175, Val Acc=9.27%, Grad Norm=3.0121\n",
      "Fold 5, Epoch 4: Train Loss=2.2021, Train Acc=11.42%, Val Loss=2.2212, Val Acc=8.47%, Grad Norm=2.2404\n",
      "Fold 5, Epoch 5: Train Loss=2.2007, Train Acc=11.43%, Val Loss=2.2190, Val Acc=9.32%, Grad Norm=1.6822\n",
      "Fold 5, Epoch 6: Train Loss=2.1990, Train Acc=11.65%, Val Loss=2.2114, Val Acc=9.02%, Grad Norm=1.2691\n",
      "Fold 5, Epoch 7: Train Loss=2.1979, Train Acc=11.66%, Val Loss=2.2024, Val Acc=9.46%, Grad Norm=0.9429\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=10.94%\n",
      "\n",
      "SNR -30 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-26_11-41-44_LTE-V_XFR_SNR-30dB_fd655_classes_9_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-35 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 789, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 789\n",
      "[INFO] 训练 block 数: 591, 测试 block 数: 198\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2198, Train Acc=11.35%, Val Loss=2.2468, Val Acc=11.73%, Grad Norm=6.2281\n",
      "Fold 1, Epoch 2: Train Loss=2.2114, Train Acc=11.29%, Val Loss=2.2345, Val Acc=8.53%, Grad Norm=4.4587\n",
      "Fold 1, Epoch 3: Train Loss=2.2047, Train Acc=11.45%, Val Loss=2.2228, Val Acc=10.32%, Grad Norm=2.9205\n",
      "Fold 1, Epoch 4: Train Loss=2.2019, Train Acc=11.41%, Val Loss=2.2174, Val Acc=10.72%, Grad Norm=2.2331\n",
      "Fold 1, Epoch 5: Train Loss=2.2004, Train Acc=11.52%, Val Loss=2.2202, Val Acc=10.13%, Grad Norm=1.6780\n",
      "Fold 1, Epoch 6: Train Loss=2.1989, Train Acc=11.47%, Val Loss=2.2156, Val Acc=10.66%, Grad Norm=1.2601\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.16%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2209, Train Acc=11.29%, Val Loss=2.2639, Val Acc=10.99%, Grad Norm=6.2782\n",
      "Fold 2, Epoch 2: Train Loss=2.2118, Train Acc=11.23%, Val Loss=2.2047, Val Acc=11.89%, Grad Norm=4.4258\n",
      "Fold 2, Epoch 3: Train Loss=2.2046, Train Acc=11.35%, Val Loss=2.2530, Val Acc=9.32%, Grad Norm=2.9130\n",
      "Fold 2, Epoch 4: Train Loss=2.2025, Train Acc=11.39%, Val Loss=2.2009, Val Acc=11.68%, Grad Norm=2.2395\n",
      "Fold 2, Epoch 5: Train Loss=2.2012, Train Acc=11.28%, Val Loss=2.2123, Val Acc=11.01%, Grad Norm=1.6997\n",
      "Fold 2, Epoch 6: Train Loss=2.1995, Train Acc=11.50%, Val Loss=2.2056, Val Acc=9.53%, Grad Norm=1.2897\n",
      "Fold 2, Epoch 7: Train Loss=2.1985, Train Acc=11.55%, Val Loss=2.2035, Val Acc=10.45%, Grad Norm=0.9861\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.14%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2215, Train Acc=11.15%, Val Loss=2.2418, Val Acc=11.47%, Grad Norm=6.3375\n",
      "Fold 3, Epoch 2: Train Loss=2.2121, Train Acc=11.22%, Val Loss=2.2007, Val Acc=12.89%, Grad Norm=4.4147\n",
      "Fold 3, Epoch 3: Train Loss=2.2053, Train Acc=11.28%, Val Loss=2.1943, Val Acc=13.51%, Grad Norm=2.9751\n",
      "Fold 3, Epoch 4: Train Loss=2.2025, Train Acc=11.28%, Val Loss=2.2186, Val Acc=10.81%, Grad Norm=2.2853\n",
      "Fold 3, Epoch 5: Train Loss=2.2008, Train Acc=11.54%, Val Loss=2.2004, Val Acc=9.76%, Grad Norm=1.7467\n",
      "Fold 3, Epoch 6: Train Loss=2.1997, Train Acc=11.39%, Val Loss=2.2104, Val Acc=9.37%, Grad Norm=1.3243\n",
      "Fold 3, Epoch 7: Train Loss=2.1987, Train Acc=11.36%, Val Loss=2.1959, Val Acc=12.77%, Grad Norm=0.9704\n",
      "Fold 3, Epoch 8: Train Loss=2.1978, Train Acc=11.62%, Val Loss=2.2015, Val Acc=9.38%, Grad Norm=0.7238\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2207, Train Acc=11.25%, Val Loss=2.2228, Val Acc=12.72%, Grad Norm=6.2838\n",
      "Fold 4, Epoch 2: Train Loss=2.2111, Train Acc=11.23%, Val Loss=2.2225, Val Acc=11.37%, Grad Norm=4.3752\n",
      "Fold 4, Epoch 3: Train Loss=2.2046, Train Acc=11.28%, Val Loss=2.2367, Val Acc=9.33%, Grad Norm=2.8495\n",
      "Fold 4, Epoch 4: Train Loss=2.2020, Train Acc=11.34%, Val Loss=2.2139, Val Acc=11.79%, Grad Norm=2.1727\n",
      "Fold 4, Epoch 5: Train Loss=2.2008, Train Acc=11.37%, Val Loss=2.2035, Val Acc=11.87%, Grad Norm=1.5834\n",
      "Fold 4, Epoch 6: Train Loss=2.1995, Train Acc=11.37%, Val Loss=2.2047, Val Acc=9.33%, Grad Norm=1.2040\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.09%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2208, Train Acc=11.33%, Val Loss=2.2155, Val Acc=11.35%, Grad Norm=6.3777\n",
      "Fold 5, Epoch 2: Train Loss=2.2110, Train Acc=11.38%, Val Loss=2.2236, Val Acc=11.65%, Grad Norm=4.4966\n",
      "Fold 5, Epoch 3: Train Loss=2.2045, Train Acc=11.37%, Val Loss=2.2167, Val Acc=9.74%, Grad Norm=2.9906\n",
      "Fold 5, Epoch 4: Train Loss=2.2025, Train Acc=11.26%, Val Loss=2.2149, Val Acc=10.99%, Grad Norm=2.2532\n",
      "Fold 5, Epoch 5: Train Loss=2.2004, Train Acc=11.50%, Val Loss=2.2176, Val Acc=9.32%, Grad Norm=1.7619\n",
      "Fold 5, Epoch 6: Train Loss=2.1995, Train Acc=11.63%, Val Loss=2.2071, Val Acc=9.39%, Grad Norm=1.3451\n",
      "Fold 5, Epoch 7: Train Loss=2.1983, Train Acc=11.59%, Val Loss=2.2096, Val Acc=9.78%, Grad Norm=1.0179\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.10%\n",
      "\n",
      "SNR -35 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-26_11-56-22_LTE-V_XFR_SNR-35dB_fd655_classes_9_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-40 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 789, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 789\n",
      "[INFO] 训练 block 数: 591, 测试 block 数: 198\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2215, Train Acc=11.19%, Val Loss=2.2063, Val Acc=11.77%, Grad Norm=6.2721\n",
      "Fold 1, Epoch 2: Train Loss=2.2105, Train Acc=11.29%, Val Loss=2.2196, Val Acc=10.93%, Grad Norm=4.3748\n",
      "Fold 1, Epoch 3: Train Loss=2.2045, Train Acc=11.38%, Val Loss=2.2258, Val Acc=11.72%, Grad Norm=2.9568\n",
      "Fold 1, Epoch 4: Train Loss=2.2022, Train Acc=11.56%, Val Loss=2.2154, Val Acc=11.77%, Grad Norm=2.2168\n",
      "Fold 1, Epoch 5: Train Loss=2.2005, Train Acc=11.53%, Val Loss=2.2047, Val Acc=10.79%, Grad Norm=1.6945\n",
      "Fold 1, Epoch 6: Train Loss=2.1990, Train Acc=11.59%, Val Loss=2.2057, Val Acc=9.68%, Grad Norm=1.2712\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.03%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2204, Train Acc=11.23%, Val Loss=2.2261, Val Acc=11.16%, Grad Norm=6.3207\n",
      "Fold 2, Epoch 2: Train Loss=2.2108, Train Acc=11.43%, Val Loss=2.2335, Val Acc=10.17%, Grad Norm=4.3502\n",
      "Fold 2, Epoch 3: Train Loss=2.2052, Train Acc=11.11%, Val Loss=2.2284, Val Acc=10.38%, Grad Norm=2.9274\n",
      "Fold 2, Epoch 4: Train Loss=2.2024, Train Acc=11.41%, Val Loss=2.2198, Val Acc=10.77%, Grad Norm=2.2541\n",
      "Fold 2, Epoch 5: Train Loss=2.2004, Train Acc=11.45%, Val Loss=2.2050, Val Acc=9.86%, Grad Norm=1.6862\n",
      "Fold 2, Epoch 6: Train Loss=2.1996, Train Acc=11.38%, Val Loss=2.2117, Val Acc=10.95%, Grad Norm=1.2637\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.07%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2215, Train Acc=11.09%, Val Loss=2.2506, Val Acc=9.92%, Grad Norm=6.3101\n",
      "Fold 3, Epoch 2: Train Loss=2.2131, Train Acc=11.13%, Val Loss=2.2221, Val Acc=11.78%, Grad Norm=4.6248\n",
      "Fold 3, Epoch 3: Train Loss=2.2051, Train Acc=11.25%, Val Loss=2.2588, Val Acc=9.47%, Grad Norm=3.1590\n",
      "Fold 3, Epoch 4: Train Loss=2.2024, Train Acc=11.40%, Val Loss=2.2234, Val Acc=11.10%, Grad Norm=2.3552\n",
      "Fold 3, Epoch 5: Train Loss=2.2011, Train Acc=11.37%, Val Loss=2.2187, Val Acc=10.46%, Grad Norm=1.7814\n",
      "Fold 3, Epoch 6: Train Loss=2.1995, Train Acc=11.63%, Val Loss=2.2058, Val Acc=10.56%, Grad Norm=1.3371\n",
      "Fold 3, Epoch 7: Train Loss=2.1987, Train Acc=11.50%, Val Loss=2.2115, Val Acc=10.18%, Grad Norm=0.9911\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.15%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2206, Train Acc=11.33%, Val Loss=2.2210, Val Acc=10.03%, Grad Norm=6.2560\n",
      "Fold 4, Epoch 2: Train Loss=2.2108, Train Acc=11.21%, Val Loss=2.2041, Val Acc=10.21%, Grad Norm=4.3513\n",
      "Fold 4, Epoch 3: Train Loss=2.2049, Train Acc=11.40%, Val Loss=2.2097, Val Acc=13.10%, Grad Norm=2.9239\n",
      "Fold 4, Epoch 4: Train Loss=2.2025, Train Acc=11.25%, Val Loss=2.2157, Val Acc=9.68%, Grad Norm=2.2274\n",
      "Fold 4, Epoch 5: Train Loss=2.2008, Train Acc=11.32%, Val Loss=2.2063, Val Acc=10.51%, Grad Norm=1.6912\n",
      "Fold 4, Epoch 6: Train Loss=2.1997, Train Acc=11.32%, Val Loss=2.2057, Val Acc=10.26%, Grad Norm=1.2538\n",
      "Fold 4, Epoch 7: Train Loss=2.1984, Train Acc=11.61%, Val Loss=2.1988, Val Acc=10.97%, Grad Norm=0.9267\n",
      "Fold 4, Epoch 8: Train Loss=2.1978, Train Acc=11.49%, Val Loss=2.2046, Val Acc=9.53%, Grad Norm=0.6838\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.09%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2210, Train Acc=11.19%, Val Loss=2.2277, Val Acc=10.96%, Grad Norm=6.2314\n",
      "Fold 5, Epoch 2: Train Loss=2.2118, Train Acc=11.24%, Val Loss=2.2289, Val Acc=9.38%, Grad Norm=4.3605\n",
      "Fold 5, Epoch 3: Train Loss=2.2045, Train Acc=11.48%, Val Loss=2.2265, Val Acc=9.43%, Grad Norm=2.8566\n",
      "Fold 5, Epoch 4: Train Loss=2.2017, Train Acc=11.45%, Val Loss=2.2028, Val Acc=12.10%, Grad Norm=2.2183\n",
      "Fold 5, Epoch 5: Train Loss=2.2004, Train Acc=11.42%, Val Loss=2.2140, Val Acc=9.27%, Grad Norm=1.6791\n",
      "Fold 5, Epoch 6: Train Loss=2.1993, Train Acc=11.53%, Val Loss=2.2036, Val Acc=11.20%, Grad Norm=1.2686\n",
      "Fold 5, Epoch 7: Train Loss=2.1982, Train Acc=11.49%, Val Loss=2.2046, Val Acc=8.61%, Grad Norm=0.9499\n",
      "Fold 5, Epoch 8: Train Loss=2.1973, Train Acc=11.58%, Val Loss=2.2016, Val Acc=9.79%, Grad Norm=0.7035\n",
      "Fold 5, Epoch 9: Train Loss=2.1970, Train Acc=11.74%, Val Loss=2.2132, Val Acc=9.30%, Grad Norm=0.5730\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.08%\n",
      "\n",
      "SNR -40 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-26_12-07-14_LTE-V_XFR_SNR-40dB_fd655_classes_9_ResNet\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SNR vs 测试准确率曲线已保存到 d:\\Program\\MW-RFF\\MW-RFF\\training_results\\SNR_vs_accuracy_2026-01-26_12-07-14.png\n"
     ]
    }
   ],
   "source": [
    "# ResNet 1D 自动 SNR 循环训练脚本（按 block 整体划分训练/测试）\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "from data_utilities import *\n",
    "\n",
    "# ================= 参数设置 =================\n",
    "data_path = \"E:/rf_datasets_IQ_raw/\"  # 数据文件夹\n",
    "fs = 5e6\n",
    "fc = 5.9e9\n",
    "v = 120\n",
    "apply_doppler = True\n",
    "apply_awgn = True\n",
    "\n",
    "# 模型超参数\n",
    "batch_size = 64\n",
    "num_epochs = 300\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-3\n",
    "in_planes = 64\n",
    "dropout = 0.5\n",
    "patience = 5\n",
    "n_splits = 5\n",
    "\n",
    "# ================= 多普勒和AWGN处理函数 =================\n",
    "def compute_doppler_shift(v, fc):\n",
    "    c = 3e8\n",
    "    v = v/3.6\n",
    "    return (v / c) * fc\n",
    "\n",
    "def apply_doppler_shift(signal, fd, fs):\n",
    "    t = np.arange(signal.shape[-1]) / fs\n",
    "    doppler_phase = np.exp(1j * 2 * np.pi * fd * t)\n",
    "    return signal * doppler_phase\n",
    "\n",
    "def add_awgn(signal, snr_db):\n",
    "    signal_power = np.mean(np.abs(signal)**2)\n",
    "    noise_power = signal_power / (10**(snr_db/10))\n",
    "    noise_real = np.random.randn(*signal.shape)\n",
    "    noise_imag = np.random.randn(*signal.shape)\n",
    "    noise = np.sqrt(noise_power/2) * (noise_real + 1j*noise_imag)\n",
    "    return signal + noise\n",
    "\n",
    "# ================= 数据加载（按 block 保存，并翻转 block） =================\n",
    "def load_and_preprocess_with_grouping(mat_folder, group_size=288, apply_doppler=False,\n",
    "                                      target_velocity=30, apply_awgn=False, snr_db=20,\n",
    "                                      fs=5e6, fc=5.9e9):\n",
    "    \"\"\"\n",
    "    改动说明：\n",
    "      - 每个 big_block 保持为整体 (group_size, sample_len, 2)\n",
    "      - 返回 X_blocks: shape (num_blocks, sample_len, group_size, 2)\n",
    "      - 每个 block 内进行翻转，使得每条新“样本”对应原 DMRS 在同一采样点的 IQ\n",
    "      - 返回 y_blocks: shape (num_blocks,) 对应每个 block 的 label（单个整数）\n",
    "    \"\"\"\n",
    "    mat_files = glob.glob(os.path.join(mat_folder, '*.mat'))\n",
    "    print(f\"共找到 {len(mat_files)} 个 .mat 文件\")\n",
    "    fd = compute_doppler_shift(target_velocity, fc)\n",
    "    print(f\"目标速度 {target_velocity} km/h，多普勒频移 {fd:.2f} Hz\")\n",
    "    \n",
    "    X_files, y_files, label_set = [], [], set()\n",
    "    for file in tqdm(mat_files, desc='读取数据'):\n",
    "        with h5py.File(file, 'r') as f:\n",
    "            rfDataset = f['rfDataset']\n",
    "            dmrs_struct = rfDataset['dmrs'][:]\n",
    "            dmrs_complex = dmrs_struct['real'] + 1j * dmrs_struct['imag']\n",
    "            txID_uint16 = rfDataset['txID'][:].flatten()\n",
    "            tx_id = ''.join(chr(c) for c in txID_uint16 if c != 0)\n",
    "            \n",
    "            processed_signals = []\n",
    "            for i in range(dmrs_complex.shape[0]):\n",
    "                sig = dmrs_complex[i, :]\n",
    "                # === step1: 原始信号功率归一化 ===\n",
    "                sig = sig / (np.sqrt(np.mean(np.abs(sig)**2)) + 1e-12)\n",
    "\n",
    "                # === step2: Doppler （只改变相位，不改变功率） ===\n",
    "                if apply_doppler:\n",
    "                    sig = apply_doppler_shift(sig, fd, fs)\n",
    "\n",
    "                # === step3: AWGN（严格按照 SNR 产生噪声） ===\n",
    "                if apply_awgn:\n",
    "                    sig = add_awgn(sig, snr_db)\n",
    "\n",
    "                iq = np.stack((sig.real, sig.imag), axis=-1)  # (sample_len, 2)\n",
    "                processed_signals.append(iq)\n",
    "            processed_signals = np.array(processed_signals)  # (num_samples_file, sample_len, 2)\n",
    "            X_files.append(processed_signals)\n",
    "            y_files.append(tx_id)\n",
    "            label_set.add(tx_id)\n",
    "    \n",
    "    label_list = sorted(list(label_set))\n",
    "    label_to_idx = {label: i for i, label in enumerate(label_list)}\n",
    "    X_blocks_list = []   # 每个元素是一个 big_block (sample_len, group_size, 2)\n",
    "    y_blocks_list = []   # 每个元素是单个 label idx\n",
    "    \n",
    "    for label in label_list:\n",
    "        files_idx = [i for i, y in enumerate(y_files) if y == label]\n",
    "        num_files = len(files_idx)\n",
    "        if num_files == 0:\n",
    "            continue\n",
    "        samples_per_file = group_size // num_files\n",
    "        if samples_per_file == 0:\n",
    "            print(f\"[WARN] 类别 {label} 文件数量过多，导致每文件样本数为0，跳过该类别\")\n",
    "            continue\n",
    "        min_samples = min([X_files[i].shape[0] for i in files_idx])\n",
    "        max_groups = min_samples // samples_per_file\n",
    "        if max_groups == 0:\n",
    "            print(f\"[WARN] 类别 {label} 样本不足，跳过\")\n",
    "            continue\n",
    "        \n",
    "        for group_i in range(max_groups):\n",
    "            pieces = []\n",
    "            for fi in files_idx:\n",
    "                start = group_i * samples_per_file\n",
    "                end = start + samples_per_file\n",
    "                piece = X_files[fi][start:end]  # (samples_per_file, sample_len, 2)\n",
    "                pieces.append(piece)\n",
    "            \n",
    "            # big_block shape: (group_size, sample_len, 2)\n",
    "            big_block = np.concatenate(pieces, axis=0)\n",
    "            \n",
    "            # 翻转 block：每条新样本对应同一采样点的 IQ\n",
    "            big_block = np.transpose(big_block, (1, 0, 2))  # (sample_len, group_size, 2)\n",
    "            \n",
    "            X_blocks_list.append(big_block)\n",
    "            y_blocks_list.append(label_to_idx[label])\n",
    "    \n",
    "    if len(X_blocks_list) == 0:\n",
    "        raise RuntimeError(\"没有生成任何 block，请检查数据/group_size 设置\")\n",
    "    \n",
    "    X_blocks = np.stack(X_blocks_list, axis=0)  # (num_blocks, sample_len, group_size, 2)\n",
    "    y_blocks = np.array(y_blocks_list, dtype=np.int64)  # (num_blocks,)\n",
    "    \n",
    "    print(f\"[INFO] 生成 block 数: {X_blocks.shape[0]}, 每 block 样本数: {X_blocks.shape[2]}, 每样本长度: {X_blocks.shape[1]}\")\n",
    "    return X_blocks, y_blocks, label_to_idx\n",
    "\n",
    "\n",
    "# ================= 1D ResNet18（增加 dropout 和 in_planes） =================\n",
    "class BasicBlock1D(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "class ResNet18_1D(nn.Module):\n",
    "    def __init__(self, num_classes=10, in_planes=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.conv1 = nn.Conv1d(2, in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1, dropout=dropout)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2, dropout=dropout)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2, dropout=dropout)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2, dropout=dropout)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride, dropout):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes)\n",
    "            )\n",
    "        layers = [BasicBlock1D(self.in_planes, planes, stride, downsample, dropout)]\n",
    "        self.in_planes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock1D(self.in_planes, planes, dropout=dropout))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (B, sample_len, 2) -> (B, 2, sample_len)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ================= 辅助函数（保持不变） =================\n",
    "def compute_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm += (p.grad.data.norm(2).item()) ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def moving_average(x, w=5):\n",
    "    x = np.array(x)\n",
    "    if len(x) == 0:\n",
    "        return np.array([])\n",
    "    if w <= 0:\n",
    "        w = 1\n",
    "    if len(x) < w:\n",
    "        w = len(x)\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    acc = 100 * correct / total\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=range(num_classes))\n",
    "    return acc, cm\n",
    "\n",
    "def plot_training_curves(fold_results, save_folder):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    for i, res in enumerate(fold_results):\n",
    "        plt.plot(moving_average(res['train_loss']), label=f'Fold{i+1} Train Loss')\n",
    "        plt.plot(moving_average(res['val_loss']), label=f'Fold{i+1} Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('训练和验证Loss曲线')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(save_folder, 'loss_curves.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_grad_norms(avg_grad_norms, save_folder):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.bar(range(1, len(avg_grad_norms)+1), avg_grad_norms)\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('平均梯度范数')\n",
    "    plt.title('各Fold平均梯度范数')\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(save_folder, 'avg_grad_norms.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(cm, save_path=None):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Reference')\n",
    "    plt.xlabel('Predicted')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def check_block_overlap(train_blocks_idx, val_blocks_idx, test_blocks_idx):\n",
    "    train_set = set(train_blocks_idx)\n",
    "    val_set = set(val_blocks_idx)\n",
    "    test_set = set(test_blocks_idx)\n",
    "\n",
    "    overlap_train_val = train_set & val_set\n",
    "    overlap_train_test = train_set & test_set\n",
    "    overlap_val_test = val_set & test_set\n",
    "\n",
    "    if overlap_train_val or overlap_train_test or overlap_val_test:\n",
    "        raise RuntimeError(f\"[ERROR] Block 重叠检测失败！\"\n",
    "                           f\"\\nTrain-Val overlap: {overlap_train_val}\"\n",
    "                           f\"\\nTrain-Test overlap: {overlap_train_test}\"\n",
    "                           f\"\\nVal-Test overlap: {overlap_val_test}\")\n",
    "    else:\n",
    "        print(\"[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\")\n",
    "\n",
    "# ================= 主训练函数（按 block 划分） =================\n",
    "def train_for_snr(SNR_dB, save_folder, results_file, group_size=288):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] 使用设备: {device}\")\n",
    "\n",
    "    # 1) 加载 block（每个 block 是整体）\n",
    "    X_blocks, y_blocks, label_to_idx = load_and_preprocess_with_grouping(\n",
    "        data_path, group_size=group_size, apply_doppler=apply_doppler,\n",
    "        target_velocity=v, apply_awgn=apply_awgn, snr_db=SNR_dB, fs=fs, fc=fc\n",
    "    )\n",
    "    num_blocks = X_blocks.shape[0]\n",
    "    print(f\"[INFO] 总 block 数: {num_blocks}\")\n",
    "\n",
    "    # 2) 按 block 做 train/test 划分（保证同一 block 不会被拆分）\n",
    "    block_idx = np.arange(num_blocks)\n",
    "    train_block_idx, test_block_idx, y_train_blocks, y_test_blocks = train_test_split(\n",
    "        block_idx, y_blocks, test_size=0.25, stratify=y_blocks, random_state=42\n",
    "    )\n",
    "\n",
    "    X_train_blocks = X_blocks[train_block_idx]  # (num_train_blocks, sample_len, group_size, 2)\n",
    "    y_train_blocks = y_blocks[train_block_idx]  # (num_train_blocks,)\n",
    "    X_test_blocks = X_blocks[test_block_idx]\n",
    "    y_test_blocks = y_blocks[test_block_idx]\n",
    "\n",
    "    # 展开测试 block 用于最终评估\n",
    "    X_test = X_test_blocks.reshape(-1, X_test_blocks.shape[2], X_test_blocks.shape[3])\n",
    "    y_test = np.repeat(y_test_blocks, X_test_blocks.shape[1])\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                 torch.tensor(y_test, dtype=torch.long))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"[INFO] 训练 block 数: {len(train_block_idx)}, 测试 block 数: {len(test_block_idx)}\")\n",
    "\n",
    "    # 3) KFold 按 block 做 fold，而不是样本级\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    fold_test_accs = []\n",
    "\n",
    "    # 对测试集和训练集 block 划分\n",
    "    block_idx = np.arange(num_blocks)\n",
    "    train_block_idx, test_block_idx, y_train_blocks, y_test_blocks = train_test_split(\n",
    "        block_idx, y_blocks, test_size=0.25, stratify=y_blocks, random_state=42\n",
    "    )\n",
    "\n",
    "    # 检查训练集和测试集 block 是否有重叠\n",
    "    check_block_overlap(train_block_idx, [], test_block_idx)\n",
    "\n",
    "    for fold, (train_block_idx_fold, val_block_idx_fold) in enumerate(kfold.split(X_train_blocks)):\n",
    "        print(f\"\\n====== Fold {fold+1}/{n_splits} ======\")\n",
    "        # 展开训练 block\n",
    "        X_train = X_train_blocks[train_block_idx_fold].reshape(-1, X_train_blocks.shape[2], X_train_blocks.shape[3])\n",
    "        y_train = np.repeat(y_train_blocks[train_block_idx_fold], X_train_blocks.shape[1])\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                      torch.tensor(y_train, dtype=torch.long))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # 展开验证 block\n",
    "        X_val = X_train_blocks[val_block_idx_fold].reshape(-1, X_train_blocks.shape[2], X_train_blocks.shape[3])\n",
    "        y_val = np.repeat(y_train_blocks[val_block_idx_fold], X_train_blocks.shape[1])\n",
    "        val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                                    torch.tensor(y_val, dtype=torch.long))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # 模型、损失函数、优化器\n",
    "        model = ResNet18_1D(num_classes=len(label_to_idx), in_planes=in_planes, dropout=dropout).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        best_model_wts = None\n",
    "        train_losses, val_losses, grad_norms = [], [], []\n",
    "\n",
    "        # =================== Epoch 循环保持不变 ===================\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "            batch_grad_norms = []\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                grad_norm = compute_grad_norm(model)\n",
    "                batch_grad_norms.append(grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            train_loss = running_loss / len(train_loader)\n",
    "            train_acc = 100 * correct_train / total_train\n",
    "            avg_grad_norm = np.mean(batch_grad_norms)\n",
    "            train_losses.append(train_loss)\n",
    "            grad_norms.append(avg_grad_norm)\n",
    "\n",
    "            # 验证集 loss & acc\n",
    "            model.eval()\n",
    "            running_val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "            all_val_labels, all_val_preds = [], []\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    loss_val = criterion(val_outputs, val_labels)\n",
    "                    running_val_loss += loss_val.item()\n",
    "                    _, val_predicted = torch.max(val_outputs, 1)\n",
    "                    total_val += val_labels.size(0)\n",
    "                    correct_val += (val_predicted == val_labels).sum().item()\n",
    "                    all_val_labels.extend(val_labels.cpu().numpy())\n",
    "                    all_val_preds.extend(val_predicted.cpu().numpy())\n",
    "            val_loss = running_val_loss / len(val_loader)\n",
    "            val_acc = 100 * correct_val / total_val\n",
    "            val_losses.append(val_loss)\n",
    "            val_cm = confusion_matrix(all_val_labels, all_val_preds, labels=range(len(label_to_idx)))\n",
    "\n",
    "            # 打印 & 写入结果文件\n",
    "            log_msg = (f\"Fold {fold+1}, Epoch {epoch+1}: \"\n",
    "                       f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, \"\n",
    "                       f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%, Grad Norm={avg_grad_norm:.4f}\")\n",
    "            print(log_msg)\n",
    "            with open(results_file, \"a\") as f:\n",
    "                f.write(log_msg + \"\\n\")\n",
    "\n",
    "            # 早停逻辑保持不变\n",
    "            if val_acc > best_val_acc + 0.01:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_model_wts = model.state_dict()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"早停，连续 {patience} 个 epoch 验证集未提升\")\n",
    "                    with open(results_file, \"a\") as f:\n",
    "                        f.write(f\"早停，连续 {patience} 个 epoch 验证集未提升\\n\")\n",
    "                    break\n",
    "            scheduler.step()\n",
    "\n",
    "        # 保存最佳模型 & 测试集评估\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        test_acc, all_test_cm = evaluate_model(model, test_loader, device, len(label_to_idx))\n",
    "        fold_test_accs.append(test_acc)\n",
    "\n",
    "        # 保存训练/验证loss和梯度范数\n",
    "        fold_results.append({\n",
    "            'train_loss': train_losses,\n",
    "            'val_loss': val_losses,\n",
    "            'grad_norms': grad_norms,\n",
    "            'val_cm': val_cm,\n",
    "            'test_cm': all_test_cm\n",
    "        })\n",
    "\n",
    "        # 保存混淆矩阵图片\n",
    "        plot_confusion_matrix(val_cm, save_path=os.path.join(save_folder, f\"confusion_matrix_val_fold{fold+1}.png\"))\n",
    "        plot_confusion_matrix(all_test_cm, save_path=os.path.join(save_folder, f\"confusion_matrix_test_fold{fold+1}.png\"))\n",
    "        torch.save(best_model_wts, os.path.join(save_folder, f\"best_model_fold{fold+1}.pth\"))\n",
    "\n",
    "        print(f\"Fold {fold+1} Test Acc={test_acc:.2f}%\\n\")\n",
    "        with open(results_file, \"a\") as f:\n",
    "            f.write(f\"Fold {fold+1} Test Acc={test_acc:.2f}%\\n\")\n",
    "\n",
    "    # 保存训练曲线 & 梯度范数\n",
    "    plot_training_curves(fold_results, save_folder)\n",
    "    plot_grad_norms([np.mean(f['grad_norms']) for f in fold_results], save_folder)\n",
    "\n",
    "    return np.mean(fold_test_accs)\n",
    "\n",
    "# ================= SNR 循环训练 + 绘制 SNR 曲线 =================\n",
    "if __name__ == \"__main__\":\n",
    "    snr_list = list(range(-5, -45, -5))\n",
    "    snr_accs = []\n",
    "\n",
    "    for snr_db in snr_list:\n",
    "        print(f\"\\n\\n================== 当前实验 SNR={snr_db} dB ==================\\n\")\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        script_name = \"LTE-V_XFR\"\n",
    "        label_to_idx = 9\n",
    "        folder_name = f\"{timestamp}_{script_name}_SNR{snr_db}dB_fd{int(compute_doppler_shift(v, fc))}_classes_{label_to_idx}_ResNet\"\n",
    "        save_folder = os.path.join(os.getcwd(), \"training_results\", folder_name)\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        results_file = os.path.join(save_folder, \"results.txt\")\n",
    "        with open(results_file, \"a\") as f:\n",
    "            f.write(f\"\\n================ SNR={snr_db} dB =================\\n\")\n",
    "        test_acc = train_for_snr(snr_db, save_folder, results_file, group_size=256)\n",
    "        snr_accs.append(test_acc)\n",
    "        print(f\"SNR {snr_db:>3} dB → results in: {save_folder}\")\n",
    "        \n",
    "    # 绘制 SNR vs 测试准确率曲线\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(snr_list, snr_accs, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel(\"SNR (dB)\")\n",
    "    plt.ylabel(\"测试集准确率 (%)\")\n",
    "    plt.title(\"SNR vs 测试集准确率\")\n",
    "    plt.grid(True)\n",
    "    snr_curve_path = os.path.join(os.getcwd(), \"training_results\", f\"SNR_vs_accuracy_{timestamp}.png\")\n",
    "    plt.savefig(snr_curve_path)\n",
    "    plt.show()\n",
    "    print(f\"[INFO] SNR vs 测试准确率曲线已保存到 {snr_curve_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= 网格搜索训练脚本（ResNet 1D，按 block 划分） =================\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# ================= 参数设置 =================\n",
    "data_path = \"E:/rf_datasets/\"\n",
    "fs = 5e6\n",
    "fc = 5.9e9\n",
    "v = 120\n",
    "apply_doppler = True\n",
    "apply_awgn = True\n",
    "patience = 8\n",
    "group_size = 288\n",
    "num_epochs = 200\n",
    "\n",
    "# ================= 多普勒和AWGN处理 =================\n",
    "def compute_doppler_shift(v, fc):\n",
    "    c = 3e8\n",
    "    v = v / 3.6\n",
    "    return (v / c) * fc\n",
    "\n",
    "def apply_doppler_shift(signal, fd, fs):\n",
    "    t = np.arange(signal.shape[-1]) / fs\n",
    "    doppler_phase = np.exp(1j * 2 * np.pi * fd * t)\n",
    "    return signal * doppler_phase\n",
    "\n",
    "def add_awgn(signal, snr_db):\n",
    "    signal_power = np.mean(np.abs(signal)**2)\n",
    "    noise_power = signal_power / (10**(snr_db/10))\n",
    "    noise_real = np.random.randn(*signal.shape)\n",
    "    noise_imag = np.random.randn(*signal.shape)\n",
    "    noise = np.sqrt(noise_power/2) * (noise_real + 1j*noise_imag)\n",
    "    return signal + noise\n",
    "\n",
    "# ================= 数据加载（按 block） =================\n",
    "def load_and_preprocess_with_grouping(mat_folder, group_size=288, apply_doppler=False,\n",
    "                                      target_velocity=30, apply_awgn=False, snr_db=20,\n",
    "                                      fs=5e6, fc=5.9e9):\n",
    "    mat_files = glob.glob(os.path.join(mat_folder, '*.mat'))\n",
    "    print(f\"共找到 {len(mat_files)} 个 .mat 文件\")\n",
    "    fd = compute_doppler_shift(target_velocity, fc)\n",
    "    \n",
    "    X_files, y_files, label_set = [], [], set()\n",
    "    for file in tqdm(mat_files):\n",
    "        with h5py.File(file, 'r') as f:\n",
    "            rfDataset = f['rfDataset']\n",
    "            dmrs_struct = rfDataset['dmrs'][:]\n",
    "            dmrs_complex = dmrs_struct['real'] + 1j*dmrs_struct['imag']\n",
    "            txID_uint16 = rfDataset['txID'][:].flatten()\n",
    "            tx_id = ''.join(chr(c) for c in txID_uint16 if c != 0)\n",
    "            \n",
    "            processed_signals = []\n",
    "            for i in range(dmrs_complex.shape[0]):\n",
    "                sig = dmrs_complex[i, :]\n",
    "                # === step1: 原始信号功率归一化 ===\n",
    "                sig = sig / (np.sqrt(np.mean(np.abs(sig)**2)) + 1e-12)\n",
    "\n",
    "                # === step2: Doppler （只改变相位，不改变功率） ===\n",
    "                if apply_doppler:\n",
    "                    sig = apply_doppler_shift(sig, fd, fs)\n",
    "\n",
    "                # === step3: AWGN（严格按照 SNR 产生噪声） ===\n",
    "                if apply_awgn:\n",
    "                    sig = add_awgn(sig, snr_db)\n",
    "                \n",
    "                iq = np.stack((sig.real, sig.imag), axis=-1)\n",
    "                processed_signals.append(iq)\n",
    "            X_files.append(np.array(processed_signals))\n",
    "            y_files.append(tx_id)\n",
    "            label_set.add(tx_id)\n",
    "    \n",
    "    label_list = sorted(list(label_set))\n",
    "    label_to_idx = {label: i for i, label in enumerate(label_list)}\n",
    "    X_blocks_list, y_blocks_list = [], []\n",
    "    \n",
    "    for label in label_list:\n",
    "        files_idx = [i for i, y in enumerate(y_files) if y == label]\n",
    "        num_files = len(files_idx)\n",
    "        if num_files == 0: continue\n",
    "        samples_per_file = group_size // num_files\n",
    "        min_samples = min([X_files[i].shape[0] for i in files_idx])\n",
    "        max_groups = min_samples // samples_per_file\n",
    "        if max_groups == 0: continue\n",
    "        for g in range(max_groups):\n",
    "            pieces = [X_files[fi][g*samples_per_file:(g+1)*samples_per_file] for fi in files_idx]\n",
    "            big_block = np.concatenate(pieces, axis=0)\n",
    "            X_blocks_list.append(big_block)\n",
    "            y_blocks_list.append(label_to_idx[label])\n",
    "    \n",
    "    X_blocks = np.stack(X_blocks_list)\n",
    "    y_blocks = np.array(y_blocks_list, dtype=np.int64)\n",
    "    return X_blocks, y_blocks, label_to_idx\n",
    "\n",
    "# ================= 1D BasicBlock =================\n",
    "class BasicBlock1D(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_planes, planes, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "# ================= ResNet18_1D 可调超参 =================\n",
    "class ResNet18_1D_Mod(nn.Module):\n",
    "    def __init__(self, num_classes, in_planes=64, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.conv1 = nn.Conv1d(2, in_planes, 7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(in_planes, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(in_planes*2, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(in_planes*4, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(in_planes*8, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc = nn.Linear(in_planes*8, num_classes)\n",
    "    def _make_layer(self, planes, blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_planes, planes, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes)\n",
    "            )\n",
    "        layers = [BasicBlock1D(self.in_planes, planes, stride, downsample)]\n",
    "        self.in_planes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock1D(self.in_planes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ================= 网格搜索训练函数（带日志保存） =================\n",
    "def train_grid_search(X_blocks, y_blocks, label_to_idx, snr_db, params, save_folder):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 日志文件\n",
    "    log_file = os.path.join(save_folder, f\"log_SNR{snr_db}_in{params['in_planes']}_drop{params['dropout_p']}.txt\")\n",
    "    with open(log_file, \"w\") as f_log:\n",
    "        f_log.write(f\"Training log for params: {params}\\n\")\n",
    "    \n",
    "    # 按 block 划分 train/test\n",
    "    num_blocks = X_blocks.shape[0]\n",
    "    block_idx = np.arange(num_blocks)\n",
    "    train_block_idx, test_block_idx, y_train_blocks, y_test_blocks = train_test_split(\n",
    "        block_idx, y_blocks, test_size=0.25, stratify=y_blocks, random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train = X_blocks[train_block_idx].reshape(-1, X_blocks.shape[2], X_blocks.shape[3])\n",
    "    y_train = np.repeat(y_blocks[train_block_idx], X_blocks.shape[1])\n",
    "    X_test = X_blocks[test_block_idx].reshape(-1, X_blocks.shape[2], X_blocks.shape[3])\n",
    "    y_test = np.repeat(y_blocks[test_block_idx], X_blocks.shape[1])\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.tensor(X_train,dtype=torch.float32),\n",
    "                                  torch.tensor(y_train,dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test,dtype=torch.float32),\n",
    "                                 torch.tensor(y_test,dtype=torch.long))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    \n",
    "    model = ResNet18_1D_Mod(num_classes=len(label_to_idx),\n",
    "                            in_planes=params['in_planes'],\n",
    "                            dropout_p=params['dropout_p']).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_wts = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, pred = torch.max(outputs,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (pred==labels).sum().item()\n",
    "        \n",
    "        train_acc = 100*correct/total\n",
    "        \n",
    "        # 验证集\n",
    "        model.eval()\n",
    "        correct_val, total_val = 0,0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, pred = torch.max(outputs,1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (pred==labels).sum().item()\n",
    "        val_acc = 100*correct_val/total_val\n",
    "        \n",
    "        # 打印并保存到日志\n",
    "        log_line = f\"Epoch {epoch+1}/{num_epochs} - Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\"\n",
    "        print(log_line)\n",
    "        with open(log_file, \"a\") as f_log:\n",
    "            f_log.write(log_line + \"\\n\")\n",
    "        \n",
    "        if val_acc > best_val_acc + 0.01:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            best_model_wts = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                with open(log_file, \"a\") as f_log:\n",
    "                    f_log.write(f\"Early stopping at epoch {epoch+1}\\n\")\n",
    "                break\n",
    "        scheduler.step()\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # 测试集最终评估\n",
    "    model.eval()\n",
    "    correct_test, total_test = 0,0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, pred = torch.max(outputs,1)\n",
    "            correct_test += (pred==labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "    test_acc = 100*correct_test/total_test\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=range(len(label_to_idx)))\n",
    "    \n",
    "    # 保存混淆矩阵\n",
    "    cm_path = os.path.join(save_folder,f\"confusion_matrix_SNR{snr_db}_in{params['in_planes']}_drop{params['dropout_p']}.png\")\n",
    "    plot_confusion_matrix(cm, save_path=cm_path)\n",
    "    \n",
    "    # 保存最终测试准确率到日志\n",
    "    with open(log_file, \"a\") as f_log:\n",
    "        f_log.write(f\"Final Test Acc: {test_acc:.2f}%\\n\")\n",
    "    \n",
    "    return test_acc\n",
    "\n",
    "# ================= 绘图函数 =================\n",
    "def plot_confusion_matrix(cm, save_path=None):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Reference')\n",
    "    plt.xlabel('Predicted')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ================= 主函数：随机网格搜索（带文件夹保存） =================\n",
    "if __name__ == \"__main__\":\n",
    "    snr_db = 20  # 固定SNR，可改\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    script_name = \"LTE-V_cross_fluently_ran\"\n",
    "    label_to_idx = 9\n",
    "    folder_name = f\"{timestamp}_{script_name}_SNR{snr_db}dB_fd{int(compute_doppler_shift(v, fc))}_classes_{label_to_idx}_ResNet\"\n",
    "    base_save_folder = os.path.join(os.getcwd(), \"search_results\", folder_name)\n",
    "    os.makedirs(base_save_folder, exist_ok=True)\n",
    "    \n",
    "    # 加载数据\n",
    "    X_blocks, y_blocks, label_to_idx = load_and_preprocess_with_grouping(\n",
    "        data_path, group_size=group_size,\n",
    "        apply_doppler=apply_doppler,\n",
    "        target_velocity=v,\n",
    "        apply_awgn=apply_awgn,\n",
    "        snr_db=snr_db, fs=fs, fc=fc\n",
    "    )\n",
    "    \n",
    "    # 网格超参数\n",
    "    param_grid = {\n",
    "        'in_planes': [16, 32, 64],\n",
    "        'dropout_p': [0.3, 0.5, 0.7],\n",
    "        'weight_decay': [1e-3, 5e-3],\n",
    "        'learning_rate': [5e-4, 1e-4],\n",
    "        'batch_size': [64, 128, 256]\n",
    "    }\n",
    "\n",
    "    # 生成所有组合\n",
    "    all_combinations = list(product(*param_grid.values()))\n",
    "    random.shuffle(all_combinations)  # 打乱顺序\n",
    "    X_experiments = 10  # 随机抽取的组合数\n",
    "    selected_combinations = all_combinations[:X_experiments]\n",
    "    \n",
    "    # 汇总 CSV 文件路径\n",
    "    summary_csv = os.path.join(base_save_folder, f\"summary_SNR{snr_db}.csv\")\n",
    "    with open(summary_csv, \"w\") as f:\n",
    "        f.write(\"in_planes,dropout_p,weight_decay,learning_rate,batch_size,test_acc\\n\")\n",
    "    \n",
    "    for i, values in enumerate(selected_combinations):\n",
    "        params = dict(zip(param_grid.keys(), values))\n",
    "        \n",
    "        # 为每组参数创建子文件夹\n",
    "        param_folder = os.path.join(base_save_folder, f\"exp_{i}_in{params['in_planes']}_drop{params['dropout_p']}\")\n",
    "        os.makedirs(param_folder, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n================ Training Experiment {i} with params: {params} =================\")\n",
    "        test_acc = train_grid_search(X_blocks, y_blocks, label_to_idx, snr_db, params, param_folder)\n",
    "        print(f\"Experiment {i} Test Acc={test_acc:.2f}%\")\n",
    "        \n",
    "        # 保存到汇总 CSV\n",
    "        with open(summary_csv, \"a\") as f:\n",
    "            f.write(f\"{params['in_planes']},{params['dropout_p']},{params['weight_decay']},{params['learning_rate']},{params['batch_size']},{test_acc:.2f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
