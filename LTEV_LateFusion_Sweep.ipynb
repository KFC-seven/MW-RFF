{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfbf89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LTEV_LateFusion_Sweep.py\n",
    "# One script (single run produces all artifacts):\n",
    "#   - Preload all frames once per SNR (shared)\n",
    "#   - BASELINE RAW IQ: train ONE single-frame model per fold (independent of m),\n",
    "#       then compute test-frame logits once, and late-fusion Acc(m) for all m in M_LIST.\n",
    "#   - XFR: for each m, build XFR blocks by m, train one model per fold (row-level),\n",
    "#       compute test row-logits once, evaluate S in S_LIST with rule S>m => all.\n",
    "#   - Output per SNR:\n",
    "#       * long results CSV (fold-level)\n",
    "#       * Table1 (m@99) CSV + LaTeX\n",
    "#       * Table2 (fixed q) CSV + LaTeX\n",
    "#       * Curve data CSV + Plot PNG\n",
    "#       * Raw artifacts: per-fold logits npz, splits, block definitions\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "# =========================\n",
    "# Global config (only edit paths / SNR_LIST if needed)\n",
    "# =========================\n",
    "DATA_PATH = \"E:/rf_datasets/\"   # *.mat folder\n",
    "\n",
    "# PHY\n",
    "FS = 5e6\n",
    "FC = 5.9e9\n",
    "V_KMH = 120\n",
    "APPLY_DOPPLER = True\n",
    "APPLY_AWGN = True\n",
    "\n",
    "# Sweep\n",
    "SNR_LIST = [20, -5]  # e.g. list(range(20, -45, -5))\n",
    "M_LIST = [1, 4, 8, 16, 32, 64, 128, 256]\n",
    "S_LIST = [1, 4, 8, 16, 32, \"all\"]\n",
    "Q_LIST = [1, 4, 8, 16, 32]\n",
    "ACC_TARGET = 99.0\n",
    "\n",
    "# Selection modes\n",
    "ROW_SELECT_MODE = \"linspace\"   # for S rows out of L (=288)\n",
    "BASELINE_BLOCK_SHUFFLE = True  # baseline: shuffle frames within each TX before grouping into blocks\n",
    "BASELINE_BLOCK_SEED_PER_M = 777  # keep blocks deterministic across folds\n",
    "\n",
    "# Safety caps (XFR dataset can explode for small m)\n",
    "MAX_BLOCKS_PER_CLASS = 1200     # None to disable (not recommended for XFR small m)\n",
    "MAX_TOTAL_BLOCKS = 12000        # None to disable\n",
    "\n",
    "# Train\n",
    "SEED = 42\n",
    "N_SPLITS = 5\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 300\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-3\n",
    "IN_PLANES = 64\n",
    "DROPOUT = 0.5\n",
    "PATIENCE = 5\n",
    "\n",
    "# Early stopping metric\n",
    "# baseline: val loss (single-frame) (independent of m)\n",
    "# xfr: fused val acc at S=\"all\"\n",
    "XFR_EARLY_STOP_S = \"all\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Repro\n",
    "# =========================\n",
    "def set_seed(sd=42):\n",
    "    np.random.seed(sd)\n",
    "    torch.manual_seed(sd)\n",
    "    torch.cuda.manual_seed_all(sd)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PHY helpers\n",
    "# =========================\n",
    "def compute_doppler_shift(v_kmh, fc_hz):\n",
    "    c = 3e8\n",
    "    v_ms = v_kmh / 3.6\n",
    "    return (v_ms / c) * fc_hz\n",
    "\n",
    "def apply_doppler_shift(signal_c, fd_hz, fs_hz):\n",
    "    t = np.arange(signal_c.shape[-1], dtype=np.float64) / fs_hz\n",
    "    phase = np.exp(1j * 2 * np.pi * fd_hz * t)\n",
    "    return signal_c * phase\n",
    "\n",
    "def add_awgn(signal_c, snr_db):\n",
    "    p = np.mean(np.abs(signal_c) ** 2)\n",
    "    n_p = p / (10 ** (snr_db / 10))\n",
    "    noise = np.sqrt(n_p / 2) * (np.random.randn(*signal_c.shape) + 1j * np.random.randn(*signal_c.shape))\n",
    "    return signal_c + noise\n",
    "\n",
    "def power_normalize(signal_c, eps=1e-12):\n",
    "    return signal_c / (np.sqrt(np.mean(np.abs(signal_c) ** 2)) + eps)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HDF5 helpers\n",
    "# =========================\n",
    "def read_tx_id_str(rfDataset):\n",
    "    txID_uint16 = rfDataset[\"txID\"][:].flatten()\n",
    "    return \"\".join(chr(int(c)) for c in txID_uint16 if int(c) != 0)\n",
    "\n",
    "def read_dmrs_complex(rfDataset):\n",
    "    dmrs = rfDataset[\"dmrs\"]\n",
    "    if isinstance(dmrs, h5py.Group):\n",
    "        real = dmrs[\"real\"][:]\n",
    "        imag = dmrs[\"imag\"][:]\n",
    "        return real + 1j * imag\n",
    "    arr = dmrs[:]\n",
    "    if hasattr(arr, \"dtype\") and arr.dtype.fields is not None and (\"real\" in arr.dtype.fields) and (\"imag\" in arr.dtype.fields):\n",
    "        return arr[\"real\"] + 1j * arr[\"imag\"]\n",
    "    return arr\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Preload all files once per SNR (shared by baseline + XFR)\n",
    "# Returns:\n",
    "#   tx_list, label_to_idx\n",
    "#   tx_to_seqs: {tx: [arr_file1(N,L,2), ...]}\n",
    "#   baseline_global_frames: X_all (Ntot,L,2), y_all (Ntot,)\n",
    "#   tx_to_global_indices: {tx: np.array(indices in X_all)}\n",
    "# =========================\n",
    "def preload_by_snr(mat_folder, snr_db):\n",
    "    mat_files = glob.glob(os.path.join(mat_folder, \"*.mat\"))\n",
    "    if len(mat_files) == 0:\n",
    "        raise RuntimeError(f\"No .mat in {mat_folder}\")\n",
    "\n",
    "    fd = compute_doppler_shift(V_KMH, FC)\n",
    "\n",
    "    # first pass: group by tx\n",
    "    tx_to_files = {}\n",
    "    for fp in tqdm(mat_files, desc=f\"[SNR{snr_db}] Scan txID\"):\n",
    "        with h5py.File(fp, \"r\") as f:\n",
    "            rfDataset = f[\"rfDataset\"]\n",
    "            tx = read_tx_id_str(rfDataset)\n",
    "        tx_to_files.setdefault(tx, []).append(fp)\n",
    "\n",
    "    tx_list = sorted(tx_to_files.keys())\n",
    "    label_to_idx = {tx: i for i, tx in enumerate(tx_list)}\n",
    "    num_classes = len(tx_list)\n",
    "    print(f\"[SNR{snr_db}] classes={num_classes} | label_to_idx={label_to_idx}\")\n",
    "\n",
    "    tx_to_seqs = {}\n",
    "    X_all_list = []\n",
    "    y_all_list = []\n",
    "    tx_to_global_indices = {}\n",
    "\n",
    "    global_cursor = 0\n",
    "    seg_len_ref = None\n",
    "\n",
    "    for tx in tx_list:\n",
    "        files = sorted(tx_to_files[tx])\n",
    "        seqs = []\n",
    "        tx_global_indices = []\n",
    "\n",
    "        for fp in tqdm(files, desc=f\"[SNR{snr_db}] Load TX={tx}\", leave=False):\n",
    "            with h5py.File(fp, \"r\") as f:\n",
    "                rfDataset = f[\"rfDataset\"]\n",
    "                dmrs_complex = read_dmrs_complex(rfDataset)  # (N,L) complex\n",
    "\n",
    "            if dmrs_complex.ndim != 2:\n",
    "                raise RuntimeError(f\"dmrs shape error: {dmrs_complex.shape} | {fp}\")\n",
    "            N, L = dmrs_complex.shape\n",
    "            if seg_len_ref is None:\n",
    "                seg_len_ref = L\n",
    "            else:\n",
    "                if L != seg_len_ref:\n",
    "                    raise RuntimeError(f\"seg_len mismatch: {seg_len_ref} vs {L} in {fp}\")\n",
    "\n",
    "            processed = np.empty((N, L, 2), dtype=np.float32)\n",
    "            for i in range(N):\n",
    "                sig = dmrs_complex[i, :]\n",
    "                sig = power_normalize(sig)\n",
    "                if APPLY_DOPPLER:\n",
    "                    sig = apply_doppler_shift(sig, fd, FS)\n",
    "                if APPLY_AWGN:\n",
    "                    sig = add_awgn(sig, snr_db)\n",
    "\n",
    "                processed[i, :, 0] = sig.real.astype(np.float32)\n",
    "                processed[i, :, 1] = sig.imag.astype(np.float32)\n",
    "\n",
    "            seqs.append(processed)\n",
    "\n",
    "            # baseline global frames\n",
    "            X_all_list.append(processed)\n",
    "            y_all_list.append(np.full((N,), label_to_idx[tx], dtype=np.int64))\n",
    "\n",
    "            # record global indices for this chunk\n",
    "            idx = np.arange(global_cursor, global_cursor + N, dtype=np.int64)\n",
    "            tx_global_indices.append(idx)\n",
    "            global_cursor += N\n",
    "\n",
    "        tx_to_seqs[tx] = seqs\n",
    "        tx_to_global_indices[tx] = np.concatenate(tx_global_indices, axis=0)\n",
    "\n",
    "    X_all = np.concatenate(X_all_list, axis=0).astype(np.float32)   # (Ntot,L,2)\n",
    "    y_all = np.concatenate(y_all_list, axis=0).astype(np.int64)\n",
    "\n",
    "    print(f\"[SNR{snr_db}] Global frames: X={X_all.shape} y={y_all.shape} | seg_len={seg_len_ref}\")\n",
    "    return tx_list, label_to_idx, tx_to_seqs, X_all, y_all, tx_to_global_indices, seg_len_ref\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utility: pick S rows (for XFR row-fusion)\n",
    "# =========================\n",
    "def pick_rows(L, S, mode=\"linspace\"):\n",
    "    if S == \"all\" or S is None or (isinstance(S, int) and S >= L):\n",
    "        return None\n",
    "    S = int(S)\n",
    "    if mode == \"linspace\":\n",
    "        idx = np.round(np.linspace(0, L - 1, num=S)).astype(int)\n",
    "        idx = np.unique(np.clip(idx, 0, L - 1))\n",
    "        if idx.size < S:\n",
    "            missing = S - idx.size\n",
    "            pool = [i for i in range(L) if i not in set(idx)]\n",
    "            idx = np.concatenate([idx, np.array(pool[:missing], dtype=int)])\n",
    "        idx = np.sort(idx)[:S]\n",
    "        return idx\n",
    "    raise ValueError(f\"Unknown mode={mode}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# XFR block builder by m\n",
    "# =========================\n",
    "def build_xfr_blocks_by_m(tx_list, label_to_idx, tx_to_seqs, m):\n",
    "    X_blocks = []\n",
    "    y_blocks = []\n",
    "\n",
    "    for tx in tx_list:\n",
    "        seqs = tx_to_seqs[tx]\n",
    "        num_files = len(seqs)\n",
    "        pointers = [0] * num_files\n",
    "        blocks_tx = []\n",
    "\n",
    "        b = 0\n",
    "        while True:\n",
    "            start_file = b % num_files\n",
    "            frames = []\n",
    "            ok = True\n",
    "            for t in range(m):\n",
    "                fi = (start_file + t) % num_files\n",
    "                pi = pointers[fi]\n",
    "                if pi >= seqs[fi].shape[0]:\n",
    "                    ok = False\n",
    "                    break\n",
    "                frames.append(seqs[fi][pi])  # (L,2)\n",
    "                pointers[fi] += 1\n",
    "            if not ok:\n",
    "                break\n",
    "            big = np.stack(frames, axis=0)            # (m,L,2)\n",
    "            big = np.transpose(big, (1, 0, 2))        # (L,m,2)\n",
    "            blocks_tx.append(big)\n",
    "            b += 1\n",
    "\n",
    "        if len(blocks_tx) == 0:\n",
    "            continue\n",
    "\n",
    "        X_tx = np.stack(blocks_tx, axis=0).astype(np.float32)  # (Btx,L,m,2)\n",
    "        y_tx = np.full((X_tx.shape[0],), label_to_idx[tx], dtype=np.int64)\n",
    "\n",
    "        X_blocks.append(X_tx)\n",
    "        y_blocks.append(y_tx)\n",
    "\n",
    "    X_blocks = np.concatenate(X_blocks, axis=0)\n",
    "    y_blocks = np.concatenate(y_blocks, axis=0)\n",
    "    return X_blocks, y_blocks\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Baseline block builder by m (on a given pool, not on global)\n",
    "# pool positions are [0..Npool-1], grouped within each TX\n",
    "# Returns: blocks_pos (Nblk,m) positions into pool, y_blocks (Nblk,)\n",
    "# =========================\n",
    "def build_baseline_blocks_from_pool(y_pool, num_classes, m, shuffle=True, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    blocks = []\n",
    "    labels = []\n",
    "    for c in range(num_classes):\n",
    "        pos = np.where(y_pool == c)[0].astype(np.int64)\n",
    "        if pos.size == 0:\n",
    "            continue\n",
    "        if shuffle:\n",
    "            rng.shuffle(pos)\n",
    "        nblk = pos.size // m\n",
    "        if nblk <= 0:\n",
    "            continue\n",
    "        pos = pos[: nblk * m].reshape(nblk, m)\n",
    "        blocks.append(pos)\n",
    "        labels.append(np.full((nblk,), c, dtype=np.int64))\n",
    "    if len(blocks) == 0:\n",
    "        return None, None\n",
    "    return np.concatenate(blocks, axis=0), np.concatenate(labels, axis=0)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Caps: enforce balanced session count for feasibility (XFR)\n",
    "# =========================\n",
    "def cap_blocks_balanced(X_blocks, y_blocks, num_classes):\n",
    "    rng = np.random.default_rng(SEED + 999)\n",
    "\n",
    "    kept = []\n",
    "    for c in range(num_classes):\n",
    "        idx = np.where(y_blocks == c)[0]\n",
    "        if idx.size == 0:\n",
    "            continue\n",
    "        rng.shuffle(idx)\n",
    "        if MAX_BLOCKS_PER_CLASS is not None:\n",
    "            idx = idx[: min(idx.size, MAX_BLOCKS_PER_CLASS)]\n",
    "        kept.append(idx)\n",
    "\n",
    "    if len(kept) == 0:\n",
    "        raise RuntimeError(\"No blocks after per-class capping.\")\n",
    "\n",
    "    kept = np.concatenate(kept, axis=0)\n",
    "    rng.shuffle(kept)\n",
    "\n",
    "    if MAX_TOTAL_BLOCKS is not None and kept.size > MAX_TOTAL_BLOCKS:\n",
    "        kept = kept[:MAX_TOTAL_BLOCKS]\n",
    "\n",
    "    return X_blocks[kept], y_blocks[kept], kept\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Model: ResNet18 1D (supports variable length via AdaptiveAvgPool)\n",
    "#   - baseline input: (L,2) with L=288\n",
    "#   - XFR row input: (m,2) with variable m\n",
    "# =========================\n",
    "class BasicBlock1D(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "class ResNet18_1D(nn.Module):\n",
    "    def __init__(self, num_classes, in_planes=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.conv1 = nn.Conv1d(2, in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1, dropout=dropout)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2, dropout=dropout)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2, dropout=dropout)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2, dropout=dropout)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride, dropout):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes)\n",
    "            )\n",
    "        layers = [BasicBlock1D(self.in_planes, planes, stride, downsample, dropout)]\n",
    "        self.in_planes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock1D(self.in_planes, planes, dropout=dropout))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, 2) -> (B,2,T)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Datasets\n",
    "# =========================\n",
    "class XFRRowDataset(Dataset):\n",
    "    # X_blocks: torch.FloatTensor (Nblk,L,m,2) on CPU\n",
    "    # y_blocks: torch.LongTensor (Nblk,)\n",
    "    def __init__(self, X_blocks, y_blocks, block_indices, L):\n",
    "        self.X = X_blocks\n",
    "        self.y = y_blocks\n",
    "        self.block_indices = np.array(block_indices, dtype=np.int64)\n",
    "        self.L = int(L)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.block_indices.size * self.L\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b = idx // self.L\n",
    "        r = idx % self.L\n",
    "        blk = int(self.block_indices[b])\n",
    "        x = self.X[blk, r]  # (m,2)\n",
    "        y = self.y[blk]\n",
    "        return x, y\n",
    "\n",
    "class FrameIndexDataset(Dataset):\n",
    "    # X_all: torch.FloatTensor (Ntot,L,2), y_all: torch.LongTensor (Ntot,)\n",
    "    def __init__(self, X_all, y_all, frame_indices):\n",
    "        self.X = X_all\n",
    "        self.y = y_all\n",
    "        self.idx = np.array(frame_indices, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.idx.size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        j = int(self.idx[i])\n",
    "        return self.X[j], self.y[j]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Inference helpers\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def infer_logits_for_indices(model, X_all_cpu, frame_indices, num_classes, batch_size=512):\n",
    "    model.eval()\n",
    "    idx = np.array(frame_indices, dtype=np.int64)\n",
    "    out = np.zeros((idx.size, num_classes), dtype=np.float32)\n",
    "    for s in range(0, idx.size, batch_size):\n",
    "        e = min(idx.size, s + batch_size)\n",
    "        xb = X_all_cpu[idx[s:e]].to(DEVICE)  # (B,L,2)\n",
    "        logits = model(xb).detach().cpu().numpy().astype(np.float32)\n",
    "        out[s:e] = logits\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_xfr_row_logits_all(model, X_blk, num_classes, batch_blocks=8):\n",
    "    # X_blk: torch.FloatTensor (Nblk,L,m,2) on CPU\n",
    "    model.eval()\n",
    "    N, L, m, C = X_blk.shape\n",
    "    out = np.zeros((N, L, num_classes), dtype=np.float32)\n",
    "\n",
    "    for s in range(0, N, batch_blocks):\n",
    "        e = min(N, s + batch_blocks)\n",
    "        xb = X_blk[s:e].to(DEVICE)                 # (B,L,m,2)\n",
    "        B = xb.shape[0]\n",
    "        flat = xb.reshape(B * L, m, C)             # (B*L,m,2)\n",
    "        logits = model(flat).reshape(B, L, num_classes)\n",
    "        out[s:e] = logits.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    return out\n",
    "\n",
    "def xfr_fused_acc_from_logits(logits, y_true, m, S):\n",
    "    # rule: if S > m => use all rows\n",
    "    if S != \"all\" and isinstance(S, int) and S > m:\n",
    "        S_eff = \"all\"\n",
    "    else:\n",
    "        S_eff = S\n",
    "\n",
    "    N, L, K = logits.shape\n",
    "    row_idx = pick_rows(L, S_eff, mode=ROW_SELECT_MODE)\n",
    "    if row_idx is None:\n",
    "        fused = logits.mean(axis=1)\n",
    "    else:\n",
    "        fused = logits[:, row_idx, :].mean(axis=1)\n",
    "\n",
    "    y_pred = np.argmax(fused, axis=1)\n",
    "    acc = 100.0 * (y_pred == y_true).mean()\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(K)))\n",
    "    return acc, cm, str(S_eff)\n",
    "\n",
    "def baseline_fused_acc_from_frame_logits(frame_logits, blocks_pos, y_blocks, num_classes):\n",
    "    # frame_logits: (Npool,K), blocks_pos: (Nblk,m) positions into pool\n",
    "    fused = frame_logits[blocks_pos].mean(axis=1)  # (Nblk,K)\n",
    "    pred = np.argmax(fused, axis=1)\n",
    "    acc = 100.0 * (pred == y_blocks).mean()\n",
    "    cm = confusion_matrix(y_blocks, pred, labels=list(range(num_classes)))\n",
    "    return float(acc), cm\n",
    "\n",
    "\n",
    "# =========================\n",
    "# BASELINE: train once per fold (per SNR), evaluate ALL m on same test pool\n",
    "# =========================\n",
    "def train_baseline_shared_all_m(snr_db, baseline_dir, X_all_np, y_all_np, num_classes):\n",
    "    os.makedirs(baseline_dir, exist_ok=True)\n",
    "\n",
    "    X_all = torch.from_numpy(np.ascontiguousarray(X_all_np)).float()  # CPU\n",
    "    y_all = torch.from_numpy(np.ascontiguousarray(y_all_np)).long()   # CPU\n",
    "\n",
    "    # Frame-level split (fixed for all m)\n",
    "    idx_all = np.arange(y_all_np.shape[0], dtype=np.int64)\n",
    "    trval_idx, test_idx = train_test_split(idx_all, test_size=0.25, random_state=SEED, stratify=y_all_np)\n",
    "\n",
    "    y_test_pool = y_all_np[test_idx].astype(np.int64)\n",
    "\n",
    "    np.savez_compressed(os.path.join(baseline_dir, \"split_frames.npz\"),\n",
    "                        trval_idx=trval_idx, test_idx=test_idx, y_test_pool=y_test_pool)\n",
    "\n",
    "    # Pre-build deterministic test blocks for each m (reused across folds)\n",
    "    blocks_dir = os.path.join(baseline_dir, \"test_blocks\")\n",
    "    os.makedirs(blocks_dir, exist_ok=True)\n",
    "    test_blocks_by_m = {}\n",
    "    for m in M_LIST:\n",
    "        blocks_pos, y_blk = build_baseline_blocks_from_pool(\n",
    "            y_pool=y_test_pool,\n",
    "            num_classes=num_classes,\n",
    "            m=m,\n",
    "            shuffle=BASELINE_BLOCK_SHUFFLE,\n",
    "            seed=SEED + BASELINE_BLOCK_SEED_PER_M + m\n",
    "        )\n",
    "        if blocks_pos is None:\n",
    "            raise RuntimeError(f\"[BASE] No test blocks for m={m} (too few samples in some class?)\")\n",
    "        test_blocks_by_m[m] = (blocks_pos, y_blk)\n",
    "        np.savez_compressed(os.path.join(blocks_dir, f\"blocks_m{m:03d}.npz\"),\n",
    "                            blocks_pos=blocks_pos.astype(np.int32),\n",
    "                            y_blocks=y_blk.astype(np.int64),\n",
    "                            m=np.int32(m))\n",
    "\n",
    "    # CV on train_val frames\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    y_trval = y_all_np[trval_idx]\n",
    "\n",
    "    fold_rows = []\n",
    "\n",
    "    for fold, (tr_i, va_i) in enumerate(skf.split(trval_idx, y_trval), start=1):\n",
    "        fold_dir = os.path.join(baseline_dir, f\"fold{fold}\")\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "        train_idx = trval_idx[tr_i]\n",
    "        val_idx   = trval_idx[va_i]\n",
    "\n",
    "        train_ds = FrameIndexDataset(X_all, y_all, train_idx)\n",
    "        val_ds   = FrameIndexDataset(X_all, y_all, val_idx)\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        model = ResNet18_1D(num_classes=num_classes, in_planes=IN_PLANES, dropout=DROPOUT).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_state = None\n",
    "        patience_cnt = 0\n",
    "\n",
    "        log_csv = os.path.join(fold_dir, \"train_log.csv\")\n",
    "        with open(log_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"epoch\", \"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\"])\n",
    "\n",
    "        for epoch in range(1, NUM_EPOCHS + 1):\n",
    "            # train\n",
    "            model.train()\n",
    "            tr_loss = 0.0\n",
    "            tr_cor, tr_tot = 0, 0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tr_loss += loss.item()\n",
    "                pred = torch.argmax(logits, dim=1)\n",
    "                tr_cor += (pred == yb).sum().item()\n",
    "                tr_tot += yb.size(0)\n",
    "            tr_loss /= max(len(train_loader), 1)\n",
    "            tr_acc = 100.0 * tr_cor / max(tr_tot, 1)\n",
    "\n",
    "            # val\n",
    "            model.eval()\n",
    "            va_loss = 0.0\n",
    "            va_cor, va_tot = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                    logits = model(xb)\n",
    "                    loss = criterion(logits, yb)\n",
    "                    va_loss += loss.item()\n",
    "                    pred = torch.argmax(logits, dim=1)\n",
    "                    va_cor += (pred == yb).sum().item()\n",
    "                    va_tot += yb.size(0)\n",
    "            va_loss /= max(len(val_loader), 1)\n",
    "            va_acc = 100.0 * va_cor / max(va_tot, 1)\n",
    "\n",
    "            print(f\"[BASE][SNR{snr_db}] Fold{fold} Ep{epoch:03d} | \"\n",
    "                  f\"TrainAcc {tr_acc:.2f}% | ValAcc {va_acc:.2f}% | ValLoss {va_loss:.4f}\")\n",
    "\n",
    "            with open(log_csv, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                w = csv.writer(f)\n",
    "                w.writerow([epoch, tr_loss, tr_acc, va_loss, va_acc])\n",
    "\n",
    "            # early stop by val loss (independent of m)\n",
    "            if va_loss < best_val_loss - 1e-6:\n",
    "                best_val_loss = va_loss\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "                if patience_cnt >= PATIENCE:\n",
    "                    break\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(fold_dir, \"best_model.pth\"))\n",
    "\n",
    "        # Compute TEST frame logits ONCE for this fold\n",
    "        test_logits = infer_logits_for_indices(model, X_all, test_idx, num_classes, batch_size=512)  # (Ntest,K)\n",
    "\n",
    "        np.savez_compressed(os.path.join(fold_dir, \"test_frame_logits.npz\"),\n",
    "                            logits=test_logits.astype(np.float16),\n",
    "                            y_true=y_test_pool.astype(np.int64),\n",
    "                            test_idx=test_idx.astype(np.int64),\n",
    "                            snr_db=np.int32(snr_db),\n",
    "                            K=np.int32(num_classes))\n",
    "\n",
    "        # Evaluate all m using prebuilt blocks (fast, fair)\n",
    "        for m in M_LIST:\n",
    "            blocks_pos, y_blk = test_blocks_by_m[m]\n",
    "            acc, cm = baseline_fused_acc_from_frame_logits(test_logits, blocks_pos, y_blk, num_classes)\n",
    "            np.save(os.path.join(fold_dir, f\"cm_test_m{m:03d}.npy\"), cm)\n",
    "\n",
    "            fold_rows.append({\n",
    "                \"method\": \"BASELINE_RAWIQ\",\n",
    "                \"snr_db\": int(snr_db),\n",
    "                \"m\": int(m),\n",
    "                \"S\": \"m\",\n",
    "                \"S_eff\": \"m\",\n",
    "                \"fold\": int(fold),\n",
    "                \"acc\": float(acc),\n",
    "                \"val_best\": float(best_val_loss)\n",
    "            })\n",
    "\n",
    "    return fold_rows\n",
    "\n",
    "\n",
    "# =========================\n",
    "# XFR: train one m, test all S\n",
    "# =========================\n",
    "def train_xfr_one_m(snr_db, m, out_dir, tx_list, label_to_idx, tx_to_seqs):\n",
    "    num_classes = len(label_to_idx)\n",
    "\n",
    "    # 1) build blocks by m\n",
    "    X_np, y_np = build_xfr_blocks_by_m(tx_list, label_to_idx, tx_to_seqs, m)\n",
    "    X_np, y_np, kept = cap_blocks_balanced(X_np, y_np, num_classes)\n",
    "\n",
    "    Nblk, L, mm, C = X_np.shape\n",
    "    assert mm == m\n",
    "    print(f\"[XFR][SNR{snr_db}] m={m} | blocks={Nblk} | X={X_np.shape}\")\n",
    "\n",
    "    with open(os.path.join(out_dir, \"blocks_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"snr_db\": snr_db, \"m\": m, \"blocks\": int(Nblk), \"L\": int(L), \"kept_idx_len\": int(len(kept))}, f, indent=2)\n",
    "\n",
    "    X = torch.from_numpy(np.ascontiguousarray(X_np)).float()   # CPU\n",
    "    y = torch.from_numpy(np.ascontiguousarray(y_np)).long()    # CPU\n",
    "\n",
    "    # 2) block split\n",
    "    idx_all = np.arange(Nblk)\n",
    "    trval_idx, test_idx = train_test_split(idx_all, test_size=0.25, random_state=SEED, stratify=y_np)\n",
    "\n",
    "    np.savez_compressed(os.path.join(out_dir, \"split_indices.npz\"),\n",
    "                        trval_idx=trval_idx, test_idx=test_idx, y_blocks=y_np)\n",
    "\n",
    "    X_test_blk = X[test_idx]\n",
    "    y_test_blk = y[test_idx].numpy()\n",
    "\n",
    "    # 3) CV on trval blocks\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    y_trval = y_np[trval_idx]\n",
    "\n",
    "    fold_rows = []\n",
    "    for fold, (tr_i, va_i) in enumerate(skf.split(trval_idx, y_trval), start=1):\n",
    "        fold_dir = os.path.join(out_dir, f\"fold{fold}\")\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "        tr_blocks = trval_idx[tr_i]\n",
    "        va_blocks = trval_idx[va_i]\n",
    "\n",
    "        train_ds = XFRRowDataset(X, y, tr_blocks, L)\n",
    "        val_ds = XFRRowDataset(X, y, va_blocks, L)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        model = ResNet18_1D(num_classes=num_classes, in_planes=IN_PLANES, dropout=DROPOUT).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        best_val_fused = -1.0\n",
    "        best_state = None\n",
    "        patience_cnt = 0\n",
    "\n",
    "        log_csv = os.path.join(fold_dir, \"train_log.csv\")\n",
    "        with open(log_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"epoch\", \"train_loss\", \"train_row_acc\", \"val_loss\", \"val_row_acc\", \"val_fused_acc_Sall\"])\n",
    "\n",
    "        X_va_blk = X[va_blocks]\n",
    "        y_va_blk = y[va_blocks].numpy()\n",
    "\n",
    "        for epoch in range(1, NUM_EPOCHS + 1):\n",
    "            model.train()\n",
    "            tr_loss = 0.0\n",
    "            tr_cor, tr_tot = 0, 0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tr_loss += loss.item()\n",
    "                pred = torch.argmax(logits, dim=1)\n",
    "                tr_cor += (pred == yb).sum().item()\n",
    "                tr_tot += yb.size(0)\n",
    "            tr_loss /= max(len(train_loader), 1)\n",
    "            tr_acc = 100.0 * tr_cor / max(tr_tot, 1)\n",
    "\n",
    "            # val row\n",
    "            model.eval()\n",
    "            va_loss = 0.0\n",
    "            va_cor, va_tot = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                    logits = model(xb)\n",
    "                    loss = criterion(logits, yb)\n",
    "                    va_loss += loss.item()\n",
    "                    pred = torch.argmax(logits, dim=1)\n",
    "                    va_cor += (pred == yb).sum().item()\n",
    "                    va_tot += yb.size(0)\n",
    "            va_loss /= max(len(val_loader), 1)\n",
    "            va_row_acc = 100.0 * va_cor / max(va_tot, 1)\n",
    "\n",
    "            # val fused (S=all)\n",
    "            va_logits = infer_xfr_row_logits_all(model, X_va_blk, num_classes, batch_blocks=8)\n",
    "            va_acc_fused, _, _ = xfr_fused_acc_from_logits(va_logits, y_va_blk, m, XFR_EARLY_STOP_S)\n",
    "\n",
    "            print(f\"[XFR][SNR{snr_db}] m={m:3d} Fold{fold} Ep{epoch:03d} | \"\n",
    "                  f\"TrainRowAcc {tr_acc:.2f}% | ValRowAcc {va_row_acc:.2f}% | ValFusedAcc(S=all) {va_acc_fused:.2f}%\")\n",
    "\n",
    "            with open(log_csv, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                w = csv.writer(f)\n",
    "                w.writerow([epoch, tr_loss, tr_acc, va_loss, va_row_acc, va_acc_fused])\n",
    "\n",
    "            if va_acc_fused > best_val_fused + 1e-6:\n",
    "                best_val_fused = va_acc_fused\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "                if patience_cnt >= PATIENCE:\n",
    "                    break\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "\n",
    "        # ---- TEST: compute logits once, reuse for all S ----\n",
    "        test_logits = infer_xfr_row_logits_all(model, X_test_blk, num_classes, batch_blocks=8)  # (Ntest,L,K)\n",
    "\n",
    "        np.savez_compressed(os.path.join(fold_dir, \"test_row_logits.npz\"),\n",
    "                            logits=test_logits.astype(np.float16),\n",
    "                            y_true=y_test_blk.astype(np.int64),\n",
    "                            snr_db=np.int32(snr_db),\n",
    "                            m=np.int32(m),\n",
    "                            L=np.int32(L),\n",
    "                            K=np.int32(num_classes))\n",
    "\n",
    "        for S in S_LIST:\n",
    "            acc, cm, S_eff = xfr_fused_acc_from_logits(test_logits, y_test_blk, m, S)\n",
    "            np.save(os.path.join(fold_dir, f\"cm_test_S{S}.npy\"), cm)\n",
    "            fold_rows.append({\n",
    "                \"method\": \"XFR\",\n",
    "                \"snr_db\": int(snr_db),\n",
    "                \"m\": int(m),\n",
    "                \"S\": str(S),\n",
    "                \"S_eff\": str(S_eff),\n",
    "                \"fold\": int(fold),\n",
    "                \"acc\": float(acc),\n",
    "                \"val_best\": float(best_val_fused)\n",
    "            })\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(fold_dir, \"best_model.pth\"))\n",
    "\n",
    "    return fold_rows\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Postprocess: aggregate, make tables + plot\n",
    "# =========================\n",
    "def aggregate_results_to_csv(rows, csv_path):\n",
    "    keys = list(rows[0].keys())\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=keys)\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "\n",
    "def aggregate_mean_std(rows):\n",
    "    from collections import defaultdict\n",
    "    agg = defaultdict(list)\n",
    "    for r in rows:\n",
    "        key = (r[\"method\"], int(r[\"snr_db\"]), int(r[\"m\"]), str(r.get(\"S_eff\", r[\"S\"])))\n",
    "        agg[key].append(float(r[\"acc\"]))\n",
    "    out = {}\n",
    "    for k, vals in agg.items():\n",
    "        out[k] = (float(np.mean(vals)), float(np.std(vals)))\n",
    "    return out\n",
    "\n",
    "def find_m_at_target(curve_points, target=99.0):\n",
    "    curve_points = sorted(curve_points, key=lambda x: x[0])\n",
    "    for m, a in curve_points:\n",
    "        if a >= target:\n",
    "            return int(m)\n",
    "    return None\n",
    "\n",
    "def write_latex_table(path, latex_str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(latex_str)\n",
    "\n",
    "def make_tables_and_plot(out_root, snr_db, rows, num_classes):\n",
    "    agg = aggregate_mean_std(rows)\n",
    "\n",
    "    # baseline curve: (m -> mean,std)\n",
    "    baseline_curve = []\n",
    "    for m in M_LIST:\n",
    "        key = (\"BASELINE_RAWIQ\", snr_db, m, \"m\")\n",
    "        if key in agg:\n",
    "            baseline_curve.append((m, agg[key][0], agg[key][1]))\n",
    "\n",
    "    # XFR curves (nominal S, but lookup uses S_eff rule when S>m)\n",
    "    xfr_curves = {str(S): [] for S in S_LIST}\n",
    "    for m in M_LIST:\n",
    "        for S in S_LIST:\n",
    "            if S != \"all\" and isinstance(S, int) and S > m:\n",
    "                S_eff = \"all\"\n",
    "            else:\n",
    "                S_eff = str(S)\n",
    "            key = (\"XFR\", snr_db, m, S_eff)\n",
    "            if key in agg:\n",
    "                mean, std = agg[key]\n",
    "                xfr_curves[str(S)].append((m, mean, std, S_eff))\n",
    "\n",
    "    # ---- Table 1: m@99 ----\n",
    "    baseline_m99 = find_m_at_target([(m, mean) for (m, mean, std) in baseline_curve], target=ACC_TARGET)\n",
    "\n",
    "    xfr_m99 = {}\n",
    "    for S in S_LIST:\n",
    "        pts = [(m, mean) for (m, mean, std, S_eff) in xfr_curves[str(S)]]\n",
    "        xfr_m99[str(S)] = find_m_at_target(pts, target=ACC_TARGET)\n",
    "\n",
    "    table1_csv = os.path.join(out_root, f\"SNR{snr_db}dB_table1_m_at_{int(ACC_TARGET)}.csv\")\n",
    "    with open(table1_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"Method\", \"Budget(q)\", f\"m@{int(ACC_TARGET)}\"])\n",
    "        w.writerow([\"RAW IQ late fusion (baseline)\", \"q=m\", baseline_m99 if baseline_m99 is not None else \"NA\"])\n",
    "        for S in S_LIST:\n",
    "            m99 = xfr_m99[str(S)]\n",
    "            if m99 is None:\n",
    "                q_show = str(S)\n",
    "            else:\n",
    "                if S != \"all\" and isinstance(S, int) and S > m99:\n",
    "                    q_show = \"all\"\n",
    "                else:\n",
    "                    q_show = str(S)\n",
    "            w.writerow([f\"XFR row-fusion (S={S})\", f\"q={q_show}\", m99 if m99 is not None else \"NA\"])\n",
    "\n",
    "    latex1 = []\n",
    "    latex1.append(r\"\\begin{table}[t]\")\n",
    "    latex1.append(r\"\\centering\")\n",
    "    latex1.append(r\"\\caption{Collection budget to reach %d\\%% accuracy ($m@%d$). Baseline uses $q=m$ inferences; XFR uses $q=S$ (if $S>m$, we use all rows).}\" % (int(ACC_TARGET), int(ACC_TARGET)))\n",
    "    latex1.append(r\"\\label{tab:m_at_99}\")\n",
    "    latex1.append(r\"\\begin{tabular}{lcc}\")\n",
    "    latex1.append(r\"\\hline\")\n",
    "    latex1.append(r\"Method & Budget $q$ & $m@%d$ \\\\\" % int(ACC_TARGET))\n",
    "    latex1.append(r\"\\hline\")\n",
    "    latex1.append(r\"RAW IQ late fusion & $q=m$ & %s \\\\\" % (str(baseline_m99) if baseline_m99 is not None else \"N/A\"))\n",
    "    for S in S_LIST:\n",
    "        m99 = xfr_m99[str(S)]\n",
    "        if m99 is None:\n",
    "            q_show = str(S)\n",
    "        else:\n",
    "            if S != \"all\" and isinstance(S, int) and S > m99:\n",
    "                q_show = \"all\"\n",
    "            else:\n",
    "                q_show = str(S)\n",
    "        latex1.append(r\"XFR row-fusion ($S=%s$) & $q=%s$ & %s \\\\\" %\n",
    "                      (str(S), str(q_show), (str(m99) if m99 is not None else \"N/A\")))\n",
    "    latex1.append(r\"\\hline\")\n",
    "    latex1.append(r\"\\end{tabular}\")\n",
    "    latex1.append(r\"\\end{table}\")\n",
    "    write_latex_table(os.path.join(out_root, f\"SNR{snr_db}dB_table1.tex\"), \"\\n\".join(latex1))\n",
    "\n",
    "    # ---- Table 2: fixed inference budget q ----\n",
    "    table2_csv = os.path.join(out_root, f\"SNR{snr_db}dB_table2_fixed_q.csv\")\n",
    "    with open(table2_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"q\", \"Baseline Acc (m=q)\", \"XFR Acc (m=q, S=q)\", \"Delta(XFR-Baseline)\"])\n",
    "        for q in Q_LIST:\n",
    "            kb = (\"BASELINE_RAWIQ\", snr_db, q, \"m\")\n",
    "            b = agg[kb][0] if kb in agg else None\n",
    "            kx = (\"XFR\", snr_db, q, str(q))\n",
    "            x = agg[kx][0] if kx in agg else None\n",
    "            if b is None or x is None:\n",
    "                w.writerow([q, \"NA\", \"NA\", \"NA\"])\n",
    "            else:\n",
    "                w.writerow([q, f\"{b:.2f}\", f\"{x:.2f}\", f\"{(x-b):.2f}\"])\n",
    "\n",
    "    latex2 = []\n",
    "    latex2.append(r\"\\begin{table}[t]\")\n",
    "    latex2.append(r\"\\centering\")\n",
    "    latex2.append(r\"\\caption{Fair comparison under the same inference budget $q$ (number of model calls). Baseline uses $m=q$ late fusion. XFR uses $m=q$ and $S=q$ row fusion.}\")\n",
    "    latex2.append(r\"\\label{tab:fixed_q}\")\n",
    "    latex2.append(r\"\\begin{tabular}{cccc}\")\n",
    "    latex2.append(r\"\\hline\")\n",
    "    latex2.append(r\"$q$ & Baseline Acc (\\%) & XFR Acc (\\%) & $\\Delta$ \\\\\")\n",
    "    latex2.append(r\"\\hline\")\n",
    "    for q in Q_LIST:\n",
    "        kb = (\"BASELINE_RAWIQ\", snr_db, q, \"m\")\n",
    "        kx = (\"XFR\", snr_db, q, str(q))\n",
    "        if kb in agg and kx in agg:\n",
    "            b = agg[kb][0]\n",
    "            x = agg[kx][0]\n",
    "            latex2.append(r\"%d & %.2f & %.2f & %.2f \\\\\" % (q, b, x, x - b))\n",
    "        else:\n",
    "            latex2.append(r\"%d & N/A & N/A & N/A \\\\\" % q)\n",
    "    latex2.append(r\"\\hline\")\n",
    "    latex2.append(r\"\\end{tabular}\")\n",
    "    latex2.append(r\"\\end{table}\")\n",
    "    write_latex_table(os.path.join(out_root, f\"SNR{snr_db}dB_table2.tex\"), \"\\n\".join(latex2))\n",
    "\n",
    "    # ---- Curve data CSV ----\n",
    "    curve_csv = os.path.join(out_root, f\"SNR{snr_db}dB_curve_data.csv\")\n",
    "    with open(curve_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"method\", \"S\", \"m\", \"acc_mean\", \"acc_std\"])\n",
    "        for (m, mean, std) in baseline_curve:\n",
    "            w.writerow([\"BASELINE_RAWIQ\", \"m\", m, f\"{mean:.4f}\", f\"{std:.4f}\"])\n",
    "        for S in S_LIST:\n",
    "            for (m, mean, std, S_eff) in xfr_curves[str(S)]:\n",
    "                w.writerow([\"XFR\", f\"{S}(eff={S_eff})\", m, f\"{mean:.4f}\", f\"{std:.4f}\"])\n",
    "\n",
    "    # ---- Plot ----\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    if len(baseline_curve) > 0:\n",
    "        ms = [x[0] for x in baseline_curve]\n",
    "        accs = [x[1] for x in baseline_curve]\n",
    "        plt.plot(ms, accs, marker=\"o\", label=\"RAW IQ baseline (late fusion)\")\n",
    "\n",
    "    for S in S_LIST:\n",
    "        pts = xfr_curves[str(S)]\n",
    "        if len(pts) == 0:\n",
    "            continue\n",
    "        ms = [p[0] for p in pts]\n",
    "        accs = [p[1] for p in pts]\n",
    "        plt.plot(ms, accs, marker=\"o\", label=f\"XFR (S={S})\")\n",
    "\n",
    "    plt.axhline(ACC_TARGET, linestyle=\"--\")\n",
    "    plt.xlabel(\"m (frames per session)\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(f\"LTE-V: Accuracy vs m (SNR={snr_db} dB)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    fig_path = os.path.join(out_root, f\"SNR{snr_db}dB_acc_vs_m.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    m99_json = {\n",
    "        \"snr_db\": int(snr_db),\n",
    "        \"acc_target\": float(ACC_TARGET),\n",
    "        \"num_classes\": int(num_classes),\n",
    "        \"baseline_m_at_target\": baseline_m99,\n",
    "        \"xfr_m_at_target\": xfr_m99\n",
    "    }\n",
    "    with open(os.path.join(out_root, f\"SNR{snr_db}dB_m_at_{int(ACC_TARGET)}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(m99_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[POST][SNR{snr_db}] Table1: {table1_csv}\")\n",
    "    print(f\"[POST][SNR{snr_db}] Table2: {table2_csv}\")\n",
    "    print(f\"[POST][SNR{snr_db}] Curve data: {curve_csv}\")\n",
    "    print(f\"[POST][SNR{snr_db}] Plot: {fig_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    out_root = os.path.join(os.getcwd(), \"training_results\", f\"{ts}_LTEV_LateFusion_Sweep\")\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "\n",
    "    run_cfg = {\n",
    "        \"DATA_PATH\": DATA_PATH,\n",
    "        \"SNR_LIST\": SNR_LIST,\n",
    "        \"M_LIST\": M_LIST,\n",
    "        \"S_LIST\": S_LIST,\n",
    "        \"Q_LIST\": Q_LIST,\n",
    "        \"ACC_TARGET\": ACC_TARGET,\n",
    "        \"MAX_BLOCKS_PER_CLASS\": MAX_BLOCKS_PER_CLASS,\n",
    "        \"MAX_TOTAL_BLOCKS\": MAX_TOTAL_BLOCKS,\n",
    "        \"PHY\": {\"FS\": FS, \"FC\": FC, \"V_KMH\": V_KMH, \"APPLY_DOPPLER\": APPLY_DOPPLER, \"APPLY_AWGN\": APPLY_AWGN},\n",
    "        \"TRAIN\": {\"SEED\": SEED, \"N_SPLITS\": N_SPLITS, \"BATCH_SIZE\": BATCH_SIZE, \"NUM_EPOCHS\": NUM_EPOCHS,\n",
    "                  \"LR\": LR, \"WEIGHT_DECAY\": WEIGHT_DECAY, \"IN_PLANES\": IN_PLANES, \"DROPOUT\": DROPOUT, \"PATIENCE\": PATIENCE},\n",
    "        \"RULE\": {\"S_gt_m_to_all\": True},\n",
    "        \"BASELINE\": {\"train_once_per_snr\": True, \"blocks_built_on_test_pool\": True},\n",
    "        \"DEVICE\": str(DEVICE)\n",
    "    }\n",
    "    with open(os.path.join(out_root, \"run_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(run_cfg, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    for snr_db in SNR_LIST:\n",
    "        snr_dir = os.path.join(out_root, f\"SNR{snr_db}dB\")\n",
    "        os.makedirs(snr_dir, exist_ok=True)\n",
    "\n",
    "        # preload once per SNR\n",
    "        tx_list, label_to_idx, tx_to_seqs, X_all, y_all, tx_to_global_indices, seg_len = preload_by_snr(DATA_PATH, snr_db)\n",
    "        num_classes = len(label_to_idx)\n",
    "\n",
    "        with open(os.path.join(snr_dir, \"preload_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"snr_db\": int(snr_db), \"num_classes\": int(num_classes), \"seg_len\": int(seg_len), \"label_to_idx\": label_to_idx},\n",
    "                      f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        all_rows = []\n",
    "\n",
    "        # ---- BASELINE: train once per SNR, evaluate all m ----\n",
    "        baseline_dir = os.path.join(snr_dir, \"BASELINE_RAWIQ_SHARED\")\n",
    "        rows_base = train_baseline_shared_all_m(snr_db, baseline_dir, X_all, y_all, num_classes)\n",
    "        all_rows.extend(rows_base)\n",
    "\n",
    "        # ---- XFR: train per m ----\n",
    "        xfr_root = os.path.join(snr_dir, \"XFR\")\n",
    "        os.makedirs(xfr_root, exist_ok=True)\n",
    "        for m in M_LIST:\n",
    "            m_dir = os.path.join(xfr_root, f\"m{m:03d}\")\n",
    "            os.makedirs(m_dir, exist_ok=True)\n",
    "            rows_xfr = train_xfr_one_m(snr_db, m, m_dir, tx_list, label_to_idx, tx_to_seqs)\n",
    "            all_rows.extend(rows_xfr)\n",
    "\n",
    "        # save long results\n",
    "        long_csv = os.path.join(snr_dir, f\"SNR{snr_db}dB_results_long.csv\")\n",
    "        aggregate_results_to_csv(all_rows, long_csv)\n",
    "        print(f\"[SAVE][SNR{snr_db}] long results: {long_csv}\")\n",
    "\n",
    "        # postprocess (tables + plot)\n",
    "        make_tables_and_plot(snr_dir, int(snr_db), all_rows, num_classes)\n",
    "\n",
    "    print(f\"[DONE] All outputs in: {out_root}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"[INFO] device={DEVICE}\")\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
