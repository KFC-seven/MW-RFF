{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a42edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manysig SNR循环实验 ResNet 6TX\n",
    "# === 导入必要库 ===\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from data_utilities import *\n",
    "import cv2\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# === 数据加载和预处理 ===\n",
    "dataset_name = 'ManySig'\n",
    "dataset_path='../ManySig.pkl/'\n",
    "\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path,dataset_name)\n",
    "\n",
    "print(\"数据集发射机数量：\",len(compact_dataset['tx_list']),\"具体为：\",compact_dataset['tx_list'])\n",
    "print(\"数据集接收机数量：\",len(compact_dataset['rx_list']),\"具体为：\",compact_dataset['rx_list'])\n",
    "print(\"数据集采集天数：\",len(compact_dataset['capture_date_list']),\"具体为：\",compact_dataset['capture_date_list'])\n",
    "\n",
    "tx_list = compact_dataset['tx_list']\n",
    "rx_list = compact_dataset['rx_list']\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "\n",
    "n_tx = len(tx_list)\n",
    "n_rx = len(rx_list)\n",
    "\n",
    "train_dates = ['2021_03_15']\n",
    "test_dates  = ['2021_03_01']\n",
    "equalized = 0\n",
    "\n",
    "X_train, y_train, X_test, y_test = preprocess_dataset_for_classification_cross_date(\n",
    "    compact_dataset, tx_list, rx_list, train_dates, test_dates, max_sig=None, equalized=equalized)\n",
    "\n",
    "\n",
    "print(\"训练集所选日期：\",train_dates, \"测试集所选日期：\", test_dates)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test  shape:\", X_test.shape) \n",
    "print(\"y_test  shape:\", y_test.shape)\n",
    "\n",
    "# === 信号处理参数 ===\n",
    "fs = 20e6\n",
    "fc = 2.4e9\n",
    "v = 120\n",
    "Add_noise = True\n",
    "Add_doppler = True\n",
    "\n",
    "def compute_doppler_shift(v, fc):\n",
    "    c = 3e8\n",
    "    v = v / 3.6\n",
    "    return (v / c) * fc\n",
    "\n",
    "fd = compute_doppler_shift(v, fc)\n",
    "\n",
    "def add_doppler_shift(signal, fd, fs):\n",
    "    num_samples = signal.shape[-1]\n",
    "    t = np.arange(num_samples) / fs\n",
    "    doppler_phase = np.exp(1j * 2 * np.pi * fd * t)\n",
    "    return signal * doppler_phase\n",
    "\n",
    "def measure_snr(clean_signal, noisy_signal):\n",
    "    signal_power = np.mean(np.abs(clean_signal) ** 2)\n",
    "    noise = noisy_signal - clean_signal\n",
    "    noise_power = np.mean(np.abs(noise) ** 2)\n",
    "    if noise_power == 0:\n",
    "        return float('inf')\n",
    "    return 10 * np.log10(signal_power / noise_power)\n",
    "\n",
    "def add_complex_awgn(signal, snr_db):\n",
    "    signal_power = np.mean(np.abs(signal) ** 2)\n",
    "    snr_linear = 10 ** (snr_db / 10)\n",
    "    noise_power = signal_power / snr_linear\n",
    "    noise_std = np.sqrt(noise_power / 2)\n",
    "    noise = np.random.normal(0, noise_std, signal.shape) + 1j*np.random.normal(0, noise_std, signal.shape)\n",
    "    return signal + noise, noise\n",
    "\n",
    "def preprocess_iq_data(data_real_imag, snr_db=None, fd=None, fs=None, add_noise=True, add_doppler=True, verify_snr=True):\n",
    "    if add_noise and snr_db is None:\n",
    "        raise ValueError(\"当add_noise=True时，必须提供snr_db参数\")\n",
    "    if add_doppler and (fd is None or fs is None):\n",
    "        raise ValueError(\"当add_doppler=True时，必须提供fd和fs参数\")\n",
    "    data_complex = data_real_imag[...,0] + 1j*data_real_imag[...,1]\n",
    "    processed = []\n",
    "    snr_measured_list = []\n",
    "    for i, sig in enumerate(data_complex):\n",
    "         # 原始信号归一化\n",
    "        sig = sig / (np.sqrt(np.mean(np.abs(sig)**2)) + 1e-12)\n",
    "        current_sig = sig.copy()\n",
    "        if add_doppler:\n",
    "            current_sig = add_doppler_shift(current_sig, fd, fs)\n",
    "        if add_noise:\n",
    "            noisy_sig, _ = add_complex_awgn(current_sig, snr_db)\n",
    "            current_sig = noisy_sig\n",
    "            if verify_snr:\n",
    "                measured_snr = measure_snr(sig, noisy_sig)\n",
    "                snr_measured_list.append(measured_snr)\n",
    "        processed.append(current_sig)\n",
    "    processed = np.array(processed)\n",
    "    processed_real_imag = np.stack([processed.real, processed.imag], axis=-1)\n",
    "    return processed_real_imag, None\n",
    "\n",
    "# === ResNet 1D 模型定义 ===\n",
    "class BasicBlock1D(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "class ResNet18_1D(nn.Module):\n",
    "    def __init__(self, num_classes=10, in_planes=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.conv1 = nn.Conv1d(2, in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1, dropout=dropout)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2, dropout=dropout)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2, dropout=dropout)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2, dropout=dropout)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    def _make_layer(self, planes, blocks, stride, dropout):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes)\n",
    "            )\n",
    "        layers = [BasicBlock1D(self.in_planes, planes, stride, downsample, dropout)]\n",
    "        self.in_planes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock1D(self.in_planes, planes, dropout=dropout))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# === 训练超参数 ===\n",
    "batch_size   = 64\n",
    "num_epochs   = 200\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-3\n",
    "in_planes    = 64\n",
    "dropout      = 0.5\n",
    "patience     = 5\n",
    "n_splits     = 5\n",
    "num_classes  = len(np.unique(y_train))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def compute_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm += (p.grad.data.norm(2).item() ** 2)\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def moving_average(x, w=5):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "# === SNR 循环 ===\n",
    "snr_list = list(range(-25, -45, -5))  # 20,15,...,-40\n",
    "all_results = {}\n",
    "\n",
    "for SNR_dB in snr_list:\n",
    "    print(f\"\\n===== 开始 SNR={SNR_dB} dB 实验 =====\")\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    script_name = \"wisig_time\"\n",
    "    folder_name = f\"{timestamp}_{script_name}_SNR{SNR_dB}dB_fd{int(fd)}_classes_{num_classes}_ResNet18\"\n",
    "    save_folder = os.path.join(os.getcwd(), \"training_results\", folder_name)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    results_file = os.path.join(save_folder, \"results.txt\")\n",
    "    with open(results_file, \"w\") as f:\n",
    "        f.write(f\"=== Experiment Summary ===\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"Total Classes: {num_classes}\\n\")\n",
    "        f.write(f\"SNR: {SNR_dB} dB\\n\")\n",
    "        f.write(f\"fd (Doppler shift): {fd} Hz\\n\")\n",
    "        f.write(f\"equalized: {equalized}\\n\")\n",
    "        f.write(f\"训练集所选日期: {train_dates}\")\n",
    "        f.write(f\"测试集所选日期：{test_dates}\")\n",
    "\n",
    "    # === 数据处理 ===\n",
    "    X_train_processed, _ = preprocess_iq_data(\n",
    "        X_train, snr_db=SNR_dB, fd=fd, fs=fs, add_noise=Add_noise, add_doppler=Add_doppler, verify_snr=False\n",
    "    )\n",
    "    X_test_processed, _ = preprocess_iq_data(\n",
    "        X_test, snr_db=SNR_dB, fd=fd, fs=fs, add_noise=Add_noise, add_doppler=Add_doppler, verify_snr=False\n",
    "    )\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train_processed,dtype=torch.float32),\n",
    "                                  torch.tensor(y_train,dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test_processed,dtype=torch.float32),\n",
    "                                 torch.tensor(y_test,dtype=torch.long))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    test_results = []\n",
    "    avg_grad_norms_per_fold = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{n_splits} ===\")\n",
    "        train_subset = Subset(train_dataset, train_idx)\n",
    "        val_subset = Subset(train_dataset, val_idx)\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "        model = ResNet18_1D(num_classes=num_classes, in_planes=in_planes, dropout=dropout).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        train_losses, val_losses = [], []\n",
    "        train_accuracies, val_accuracies = [], []\n",
    "        grad_norms = []\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "            batch_grad_norms = []\n",
    "            with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as tepoch:\n",
    "                for inputs, labels in tepoch:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    grad_norm = compute_grad_norm(model)\n",
    "                    batch_grad_norms.append(grad_norm)\n",
    "                    optimizer.step()\n",
    "                    running_train_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total_train += labels.size(0)\n",
    "                    correct_train += (predicted == labels).sum().item()\n",
    "                    tepoch.set_postfix(loss=running_train_loss/len(train_loader),\n",
    "                                       accuracy=100*correct_train/total_train,\n",
    "                                       grad_norm=grad_norm)\n",
    "\n",
    "            epoch_train_loss = running_train_loss/len(train_loader)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "            train_accuracies.append(100*correct_train/total_train)\n",
    "            avg_grad_norm = np.mean(batch_grad_norms)\n",
    "            grad_norms.append(avg_grad_norm)\n",
    "\n",
    "            # 验证\n",
    "            model.eval()\n",
    "            running_val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss = criterion(val_outputs, val_labels)\n",
    "                    running_val_loss += val_loss.item()\n",
    "                    _, val_predicted = torch.max(val_outputs, 1)\n",
    "                    total_val += val_labels.size(0)\n",
    "                    correct_val += (val_predicted == val_labels).sum().item()\n",
    "            epoch_val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accuracies.append(100*correct_val/total_val)\n",
    "\n",
    "            with open(results_file,'a') as f:\n",
    "                f.write(f\"Fold{fold+1} Epoch{epoch+1} | TrainAcc={train_accuracies[-1]:.2f}% | ValAcc={val_accuracies[-1]:.2f}% | \"\n",
    "                        f\"TrainLoss={train_losses[-1]:.4f} | ValLoss={val_losses[-1]:.4f} | AvgGrad={avg_grad_norm:.4f}\\n\")\n",
    "\n",
    "\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            scheduler.step()\n",
    "\n",
    "        fold_results.append(max(val_accuracies))\n",
    "        avg_grad_norms_per_fold.append(grad_norms)\n",
    "\n",
    "        # 绘图\n",
    "        plt.figure()\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.plot(moving_average(train_losses), label='Train Loss Smooth', linestyle='--')\n",
    "        plt.plot(moving_average(val_losses), label='Val Loss Smooth', linestyle='--')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Fold {fold+1} Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(save_folder, f\"fold_{fold+1}_loss_curve.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(grad_norms, label='Grad Norm')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Grad Norm')\n",
    "        plt.title(f'Fold {fold+1} Grad Norm')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_folder, f\"fold_{fold+1}_grad_norm.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # 测试集评估\n",
    "        model.eval()\n",
    "        test_preds, test_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for test_inputs, test_labels in test_loader:\n",
    "                test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "                test_outputs = model(test_inputs)\n",
    "                _, predicted = torch.max(test_outputs, 1)\n",
    "                test_preds.extend(predicted.cpu().numpy())\n",
    "                test_true.extend(test_labels.cpu().numpy())\n",
    "        test_preds = np.array(test_preds)\n",
    "        test_true = np.array(test_true)\n",
    "        test_accuracy = 100*np.sum(test_preds==test_true)/len(test_true)\n",
    "        test_results.append(test_accuracy)\n",
    "\n",
    "        print(f\"Fold {fold+1} Test Accuracy: {test_accuracy:.2f}%\")\n",
    "        with open(results_file, \"a\") as f:\n",
    "            f.write(f\"Fold {fold+1} Test Acc: {test_accuracy:.2f}%\\n\")\n",
    "\n",
    "        cm = confusion_matrix(test_true, test_preds)\n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Fold {fold+1} Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.savefig(os.path.join(save_folder, f\"fold_{fold+1}_confusion_matrix.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    avg_val = np.mean(fold_results)\n",
    "    avg_test = np.mean(test_results)\n",
    "    with open(results_file, \"a\") as f:\n",
    "        f.write(f\"\\n=== SNR={SNR_dB} Summary ===\\n\")\n",
    "        f.write(f\"Avg Val Acc: {avg_val:.2f}%, Avg Test Acc: {avg_test:.2f}%\\n\")\n",
    "    print(f\"SNR={SNR_dB} 完成: Avg Val={avg_val:.2f}%, Avg Test={avg_test:.2f}%\")\n",
    "    all_results[SNR_dB] = save_folder\n",
    "\n",
    "print(\"\\n===== 所有 SNR 实验完成 =====\")\n",
    "for snr, folder in all_results.items():\n",
    "    print(f\"SNR={snr} 结果保存在: {folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09fffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始版 ManySig Transfomer 6TX\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from  data_utilities import *\n",
    "import cv2  # OpenCV 用于调整图像大小和颜色处理\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gc  # 引入垃圾回收模块\n",
    "from tqdm.auto import tqdm  # 自动适配环境 导入tqdm进度条库\n",
    "from collections import defaultdict\n",
    "\n",
    "dataset_name = 'ManySig'\n",
    "dataset_path='../ManySig.pkl/'\n",
    "\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path,dataset_name)\n",
    "\n",
    "print(\"数据集发射机数量：\",len(compact_dataset['tx_list']),\"具体为：\",compact_dataset['tx_list'])\n",
    "print(\"数据集接收机数量：\",len(compact_dataset['rx_list']),\"具体为：\",compact_dataset['rx_list'])\n",
    "print(\"数据集采集天数：\",len(compact_dataset['capture_date_list']),\"具体为：\",compact_dataset['capture_date_list'])\n",
    "\n",
    "\n",
    "tx_list = compact_dataset['tx_list']\n",
    "rx_list = compact_dataset['rx_list']\n",
    "equalized = 0\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "\n",
    "\n",
    "n_tx = len(tx_list)\n",
    "n_rx = len(rx_list)\n",
    "print(n_tx,n_rx)\n",
    "\n",
    "\n",
    "train_dates = ['2021_03_15']  # 设定训练日期\n",
    "test_dates  = ['2021_03_01']  # 设定测试日期\n",
    "X_train, y_train, X_test, y_test = preprocess_dataset_for_classification_cross_date(\n",
    "    compact_dataset, tx_list, rx_list, train_dates, test_dates, max_sig=None, equalized = equalized)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test  shape:\", X_test.shape)\n",
    "print(\"y_test  shape:\", y_test.shape)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# === 参数设置 ===\n",
    "SNR_dB = 15           # 信噪比\n",
    "fs = 20e6             # 采样率 (Hz)\n",
    "fc = 2.4e9            # 载波频率 (Hz)\n",
    "v = 120               # 速度 (km/h)\n",
    "Add_noise = True     # 是否添加噪声\n",
    "Add_doppler = True   # 是否添加多普勒频移\n",
    "\n",
    "# === 多普勒频移计算 ===\n",
    "def compute_doppler_shift(v, fc):\n",
    "    c = 3e8  # 光速\n",
    "    v = v / 3.6 # 转换为m/s\n",
    "    return (v / c) * fc\n",
    "\n",
    "fd = compute_doppler_shift(v, fc)\n",
    "print(f\"[INFO] 多普勒频移 fd = {fd:.2f} Hz\")\n",
    "\n",
    "# === 多普勒变换 ===\n",
    "def add_doppler_shift(signal, fd, fs):\n",
    "    num_samples = signal.shape[-1]\n",
    "    t = np.arange(num_samples) / fs\n",
    "    doppler_phase = np.exp(1j * 2 * np.pi * fd * t)\n",
    "    return signal * doppler_phase\n",
    "\n",
    "# === SNR测量函数 ===\n",
    "def measure_snr(clean_signal, noisy_signal):\n",
    "    \"\"\"\n",
    "    测量实际SNR\n",
    "    \"\"\"\n",
    "    signal_power = np.mean(np.abs(clean_signal) ** 2)\n",
    "    noise = noisy_signal - clean_signal\n",
    "    noise_power = np.mean(np.abs(noise) ** 2)\n",
    "    \n",
    "    if noise_power == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    snr_measured = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr_measured\n",
    "\n",
    "# === 复数AWGN噪声添加函数 ===\n",
    "def add_complex_awgn(signal, snr_db):\n",
    "    \"\"\"\n",
    "    为复数信号添加AWGN噪声\n",
    "    \"\"\"\n",
    "    # 计算信号功率\n",
    "    signal_power = np.mean(np.abs(signal) ** 2)\n",
    "    \n",
    "    # 计算噪声功率\n",
    "    snr_linear = 10 ** (snr_db / 10)\n",
    "    noise_power = signal_power / snr_linear\n",
    "    \n",
    "    # 生成复数噪声（实部和虚部独立，各占一半功率）\n",
    "    noise_std = np.sqrt(noise_power / 2)\n",
    "    noise_real = np.random.normal(0, noise_std, signal.shape)\n",
    "    noise_imag = np.random.normal(0, noise_std, signal.shape)\n",
    "    noise = noise_real + 1j * noise_imag\n",
    "    \n",
    "    return signal + noise, noise\n",
    "\n",
    "# === 带验证的预处理函数 ===\n",
    "def preprocess_iq_data(data_real_imag, snr_db=None, fd=None, fs=None, \n",
    "                       add_noise=True, add_doppler=True, verify_snr=True):\n",
    "    \"\"\"\n",
    "    预处理IQ数据：可选择性地添加噪声和多普勒频移\n",
    "    \n",
    "    参数:\n",
    "    - data_real_imag: 输入数据，shape (N, T, 2)\n",
    "    - snr_db: 目标信噪比(dB)，当add_noise=True时必需\n",
    "    - fd: 多普勒频移(Hz)，当add_doppler=True时必需\n",
    "    - fs: 采样率(Hz)，当add_doppler=True时必需\n",
    "    - add_noise: 是否添加噪声 (默认True)\n",
    "    - add_doppler: 是否添加多普勒频移 (默认True)\n",
    "    - verify_snr: 是否验证SNR (仅当add_noise=True时有效)\n",
    "    \n",
    "    返回:\n",
    "    - processed_real_imag: 处理后的数据，shape (N, T, 2)\n",
    "    - snr_info: SNR验证信息（如果verify_snr=True且add_noise=True）\n",
    "    \"\"\"\n",
    "    # 参数检查\n",
    "    if add_noise and snr_db is None:\n",
    "        raise ValueError(\"当add_noise=True时，必须提供snr_db参数\")\n",
    "    \n",
    "    if add_doppler and (fd is None or fs is None):\n",
    "        raise ValueError(\"当add_doppler=True时，必须提供fd和fs参数\")\n",
    "    \n",
    "    # Step 1: 转为复数 IQ，shape: (N, T, 2) → (N, T)\n",
    "    data_complex = data_real_imag[..., 0] + 1j * data_real_imag[..., 1]\n",
    "\n",
    "    processed = []\n",
    "    snr_measured_list = []\n",
    "    \n",
    "    for i, sig in enumerate(data_complex):\n",
    "        current_sig = sig.copy()\n",
    "        \n",
    "        # Step 2: 添加 AWGN 噪声（可选）\n",
    "        if add_noise:\n",
    "            noisy_sig, noise = add_complex_awgn(current_sig, snr_db)\n",
    "            current_sig = noisy_sig\n",
    "            \n",
    "            # SNR验证（可选）\n",
    "            if verify_snr:\n",
    "                measured_snr = measure_snr(sig, noisy_sig)\n",
    "                snr_measured_list.append(measured_snr)\n",
    "                \n",
    "                # 每10000个样本打印一次进度\n",
    "                if i % 10000 == 0 and i > 0:\n",
    "                    avg_snr = np.mean(snr_measured_list[-10000:])\n",
    "                    print(f\"[验证] 样本 {i}, 平均实测SNR: {avg_snr:.2f} dB\")\n",
    "        \n",
    "        # Step 3: 添加多普勒频移（可选）\n",
    "        if add_doppler:\n",
    "            shifted = add_doppler_shift(current_sig, fd, fs)\n",
    "            current_sig = shifted\n",
    "            \n",
    "        processed.append(current_sig)\n",
    "\n",
    "    processed = np.array(processed)\n",
    "    \n",
    "    # Step 4: 转回 [I, Q] 实数格式\n",
    "    processed_real_imag = np.stack([processed.real, processed.imag], axis=-1)\n",
    "    \n",
    "    # SNR验证总结（仅当添加噪声且启用验证时）\n",
    "    snr_info = None\n",
    "    if add_noise and verify_snr and snr_measured_list:\n",
    "        snr_measured_array = np.array(snr_measured_list)\n",
    "        snr_info = {\n",
    "            'target_snr': snr_db,\n",
    "            'measured_mean': np.mean(snr_measured_array),\n",
    "            'measured_std': np.std(snr_measured_array),\n",
    "            'measured_min': np.min(snr_measured_array),\n",
    "            'measured_max': np.max(snr_measured_array),\n",
    "            'samples_count': len(snr_measured_array)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n=== SNR验证结果 ===\")\n",
    "        print(f\"目标SNR: {snr_info['target_snr']} dB\")\n",
    "        print(f\"实测平均SNR: {snr_info['measured_mean']:.2f} dB\")\n",
    "        print(f\"实测标准差: {snr_info['measured_std']:.2f} dB\")\n",
    "        print(f\"实测范围: [{snr_info['measured_min']:.2f}, {snr_info['measured_max']:.2f}] dB\")\n",
    "        print(f\"验证样本数: {snr_info['samples_count']}\")\n",
    "    \n",
    "    return processed_real_imag, snr_info\n",
    "\n",
    "# === 单样本测试函数 ===\n",
    "def test_single_sample_snr(data_real_imag, snr_db, num_tests=10):\n",
    "    \"\"\"\n",
    "    对单个样本进行多次SNR测试，确保噪声添加的正确性\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== 单样本SNR测试 (测试{num_tests}次) ===\")\n",
    "    \n",
    "    # 取第一个样本\n",
    "    sample_complex = data_real_imag[0, :, 0] + 1j * data_real_imag[0, :, 1]\n",
    "    \n",
    "    measured_snrs = []\n",
    "    for i in range(num_tests):\n",
    "        noisy_sample, _ = add_complex_awgn(sample_complex, snr_db)\n",
    "        measured_snr = measure_snr(sample_complex, noisy_sample)\n",
    "        measured_snrs.append(measured_snr)\n",
    "        print(f\"测试 {i+1}: 目标SNR={snr_db} dB, 实测SNR={measured_snr:.2f} dB\")\n",
    "    \n",
    "    measured_snrs = np.array(measured_snrs)\n",
    "    print(f\"\\n测试统计:\")\n",
    "    print(f\"平均值: {np.mean(measured_snrs):.2f} dB\")\n",
    "    print(f\"标准差: {np.std(measured_snrs):.2f} dB\")\n",
    "    print(f\"误差范围: ±{np.abs(np.mean(measured_snrs) - snr_db):.2f} dB\")\n",
    "\n",
    "# === 使用示例 ===\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设 X_train, X_test 已定义\n",
    "    \n",
    "    # 使用全局变量控制处理选项\n",
    "    X_train_processed, _ = preprocess_iq_data(\n",
    "        X_train, snr_db=SNR_dB, fd=fd, fs=fs, \n",
    "        add_noise=Add_noise, add_doppler=Add_doppler, verify_snr=False\n",
    "    )\n",
    "    \n",
    "    # 单样本测试（只有当Add_noise=True时才有意义）\n",
    "    if Add_noise:\n",
    "        test_single_sample_snr(X_train, SNR_dB, num_tests=5)\n",
    "    else:\n",
    "        print(\"\\n=== 跳过单样本SNR测试（未启用噪声添加）===\")\n",
    "    \n",
    "    # 处理测试集（可以根据需要设置不同的选项）\n",
    "    print(f\"\\n=== 处理测试集 ===\")\n",
    "    X_test_processed, test_snr_info = preprocess_iq_data(\n",
    "        X_test, snr_db=SNR_dB, fd=fd, fs=fs, \n",
    "        add_noise=Add_noise, add_doppler=Add_doppler, verify_snr=False\n",
    "    )\n",
    "    \n",
    "    # 查看处理前后前10个点\n",
    "    print(f\"\\n=== 信号对比 ===\")\n",
    "    print(\"原始信号 I 分量：\", X_train[0, :10, 0])\n",
    "    print(\"处理后信号 I 分量：\", X_train_processed[0, :10, 0])\n",
    "    \n",
    "    # 验证多普勒频移效果（只有当Add_doppler=True时才有意义）\n",
    "    if Add_doppler:\n",
    "        print(f\"\\n=== 多普勒频移验证 ===\")\n",
    "        original_phase = np.angle(X_train[0, :10, 0] + 1j * X_train[0, :10, 1])\n",
    "        processed_phase = np.angle(X_train_processed[0, :10, 0] + 1j * X_train_processed[0, :10, 1])\n",
    "        phase_diff = processed_phase - original_phase\n",
    "        print(\"相位变化:\", phase_diff)\n",
    "    else:\n",
    "        print(f\"\\n=== 跳过多普勒频移验证（未启用多普勒频移）===\")\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# 假设 SNR_dB 和 fd 已经定义\n",
    "SNR_dB = globals().get('SNR_dB', 'no')\n",
    "fd = globals().get('fd', 'no')\n",
    "\n",
    "# === 模型与训练参数设置 ===\n",
    "raw_input_dim = 2         # 每个时间步是 I/Q 两个值\n",
    "model_dim = 128           # Transformer 模型内部维度\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "num_classes = len(np.unique(y_train))  # 或 len(tx_list)\n",
    "dropout = 0.1\n",
    "batch_size = 512\n",
    "num_epochs = 200\n",
    "learning_rate = 1e-4\n",
    "patience = 5\n",
    "\n",
    "# === 创建保存目录 ===\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "script_name = \"wisig_time\"\n",
    "folder_name = f\"{timestamp}_{script_name}_SNR{SNR_dB}dB_fd{fd}_classes_{num_classes}_Transformer\"\n",
    "save_folder = os.path.join(os.getcwd(), \"training_results\", folder_name)\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "results_file = os.path.join(save_folder, \"results.txt\")\n",
    "with open(results_file, \"w\") as f:\n",
    "    f.write(f\"=== Experiment Summary ===\\n\")\n",
    "    f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "    f.write(f\"Total Classes: {num_classes}\\n\")\n",
    "    f.write(f\"SNR: {SNR_dB} dB\\n\")\n",
    "    f.write(f\"fd (Doppler shift): {fd} Hz\\n\")\n",
    "    f.write(f\"equalized: {equalized}\\n\")\n",
    "\n",
    "# === 模型定义 ===\n",
    "class SignalTransformer(nn.Module):\n",
    "    def __init__(self, raw_input_dim, model_dim, num_heads, num_layers, num_classes, dropout=0.1):\n",
    "        super(SignalTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(raw_input_dim, model_dim)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# === 假设 X_train, y_train, X_test, y_test 都已定义并 shape 为 (N, L, 2) ===\n",
    "# 若还未定义，可自行加载并 reshape\n",
    "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_processed, dtype=torch.float32),\n",
    "                               torch.tensor(y_train, dtype=torch.long))\n",
    "\n",
    "# === K折交叉验证训练 ===\n",
    "n_splits = 5\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "test_results = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def compute_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def moving_average(x, w=5):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "avg_grad_norms_per_fold = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset)):\n",
    "    print(f\"\\n====== Fold {fold+1}/{n_splits} ======\")\n",
    "\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    val_subset = Subset(train_dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    model = SignalTransformer(raw_input_dim, model_dim, num_heads, num_layers, num_classes, dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    grad_norms = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "        batch_grad_norms = []\n",
    "\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as tepoch:\n",
    "            for inputs, labels in tepoch:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "\n",
    "                grad_norm = compute_grad_norm(model)\n",
    "                batch_grad_norms.append(grad_norm)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                running_train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "                tepoch.set_postfix(loss=running_train_loss / (len(train_loader)),\n",
    "                                   accuracy=100 * correct_train / total_train,\n",
    "                                   grad_norm=grad_norm)\n",
    "\n",
    "        epoch_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(100 * correct_train / total_train)\n",
    "        avg_grad_norm = np.mean(batch_grad_norms)\n",
    "        grad_norms.append(avg_grad_norm)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Average Gradient Norm: {avg_grad_norm:.4f}\")\n",
    "\n",
    "        # === 验证 ===\n",
    "        model.eval()\n",
    "        running_val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "                running_val_loss += val_loss.item()\n",
    "                _, val_predicted = torch.max(val_outputs, 1)\n",
    "                total_val += val_labels.size(0)\n",
    "                correct_val += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accuracies.append(100 * correct_val / total_val)\n",
    "\n",
    "        with open(results_file, \"a\") as f:\n",
    "            f.write(f\"Epoch {epoch+1} | Train Acc: {train_accuracies[-1]:.2f}% | Val Acc: {val_accuracies[-1]:.2f}%\\n\")\n",
    "\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    fold_results.append(max(val_accuracies))\n",
    "    avg_grad_norms_per_fold.append(grad_norms)\n",
    "\n",
    "    # === 绘制 loss 曲线 ===\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.plot(moving_average(train_losses), label='Train Loss (Smooth)', linestyle='--')\n",
    "    plt.plot(moving_average(val_losses), label='Val Loss (Smooth)', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {fold+1} Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_folder, f\"fold_{fold+1}_loss_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # === 绘制 Gradient Norm 曲线 ===\n",
    "    plt.figure()\n",
    "    plt.plot(grad_norms, label='Gradient Norm')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.title(f'Fold {fold+1} Gradient Norm')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_folder, f\"fold_{fold+1}_grad_norm.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # === 测试集评估 ===\n",
    "    model.eval()\n",
    "    test_preds, test_true = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_inputs = test_inputs.to(device)\n",
    "            test_labels = test_labels.to(device)\n",
    "\n",
    "            test_outputs = model(test_inputs)\n",
    "            _, predicted = torch.max(test_outputs, 1)\n",
    "            test_preds.extend(predicted.cpu().numpy())\n",
    "            test_true.extend(test_labels.cpu().numpy())\n",
    "\n",
    "    test_preds = np.array(test_preds)\n",
    "    test_true = np.array(test_true)\n",
    "    test_accuracy = 100.0 * np.sum(test_preds == test_true) / len(test_true)\n",
    "    test_results.append(test_accuracy)\n",
    "\n",
    "    with open(results_file, \"a\") as f:\n",
    "        f.write(f\"Fold {fold+1} Test Accuracy: {test_accuracy:.2f}%\\n\")\n",
    "\n",
    "    cm = confusion_matrix(test_true, test_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Test Confusion Matrix Fold {fold+1}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(os.path.join(save_folder, f\"fold_{fold+1}_test_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# === 总结结果 ===\n",
    "avg_val = np.mean(fold_results)\n",
    "avg_test = np.mean(test_results)\n",
    "\n",
    "with open(results_file, \"a\") as f:\n",
    "    f.write(\"\\n=== Summary ===\\n\")\n",
    "    for i in range(n_splits):\n",
    "        f.write(f\"Fold {i+1}: Val Acc = {fold_results[i]:.2f}%, Test Acc = {test_results[i]:.2f}%\\n\")\n",
    "    f.write(f\"\\nAverage Validation Accuracy: {avg_val:.2f}%\\n\")\n",
    "    f.write(f\"Average Test Accuracy: {avg_test:.2f}%\\n\")\n",
    "\n",
    "print(\"\\n=== Final Summary ===\")\n",
    "for i in range(n_splits):\n",
    "    print(f\"Fold {i+1}: Val = {fold_results[i]:.2f}%, Test = {test_results[i]:.2f}%\")\n",
    "print(f\"Average Val Accuracy: {avg_val:.2f}%\")\n",
    "print(f\"Average Test Accuracy: {avg_test:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495844b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数搜索\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 假设 SNR_dB, fd, equalized 已定义\n",
    "SNR_dB = globals().get('SNR_dB', 'no')\n",
    "fd = globals().get('fd', 'no')\n",
    "equalized = globals().get('equalized', 'no')\n",
    "\n",
    "# 假设 X_train_processed, y_train, X_test_processed, y_test 已定义\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# === 模型定义 ===\n",
    "class SignalTransformer(nn.Module):\n",
    "    def __init__(self, raw_input_dim, model_dim, num_heads, num_layers, num_classes, dropout=0.1):\n",
    "        super(SignalTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(raw_input_dim, model_dim)\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# === 数据准备 ===\n",
    "X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# 划分训练集 / 验证集\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# === 参数空间 ===\n",
    "param_space = {\n",
    "    \"model_dim\": [128, 256, 512],\n",
    "    \"num_heads\": [2, 4, 8],\n",
    "    \"num_layers\": [1, 2, 3],\n",
    "    \"dropout\": [0.1, 0.3, 0.5],\n",
    "    \"learning_rate\": [1e-3, 5e-4, 1e-4],\n",
    "    \"batch_size\": [128, 256, 512]\n",
    "}\n",
    "num_search = 50  # 随机搜索次数\n",
    "patience = 5\n",
    "raw_input_dim = 2\n",
    "num_epochs = 300\n",
    "\n",
    "results_summary = []\n",
    "best_config = None\n",
    "best_val_acc = 0\n",
    "\n",
    "# 计算梯度范数\n",
    "def compute_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "# 平滑曲线\n",
    "def moving_average(x, w=5):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "# === 随机搜索 ===\n",
    "for search_idx in range(num_search):\n",
    "    config = {k: random.choice(v) for k, v in param_space.items()}\n",
    "    print(f\"\\n=== Random Search {search_idx+1}/{num_search} ===\")\n",
    "    print(f\"Params: {config}\")\n",
    "\n",
    "    # 创建保存目录\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    script_name = \"wisig_time_random\"\n",
    "    folder_name = f\"{timestamp}_{script_name}_SNR{SNR_dB}\"\n",
    "    save_folder = os.path.join(\"search_results\", folder_name)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    results_file = os.path.join(save_folder, \"results.txt\")\n",
    "\n",
    "    with open(results_file, \"w\") as f:\n",
    "        f.write(f\"=== Hyperparameters ===\\n{config}\\n\")\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # 模型 & 优化器\n",
    "    model = SignalTransformer(raw_input_dim, config[\"model_dim\"], config[\"num_heads\"],\n",
    "                              config[\"num_layers\"], num_classes, config[\"dropout\"]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    grad_norms = []\n",
    "\n",
    "    best_val = 0\n",
    "    patience_counter = 0\n",
    "    best_model_wts = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "    # 训练\n",
    "        model.train()\n",
    "        running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "        batch_grad_norms = []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            grad_norm = compute_grad_norm(model)\n",
    "            batch_grad_norms.append(grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        train_accuracies.append(100 * correct_train / total_train)\n",
    "        grad_norms.append(np.mean(batch_grad_norms))\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        correct_val, total_val = 0, 0\n",
    "        val_loss_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "                val_loss_sum += val_loss.item()\n",
    "                _, val_pred = torch.max(val_outputs, 1)\n",
    "                total_val += val_labels.size(0)\n",
    "                correct_val += (val_pred == val_labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * correct_val / total_val\n",
    "        val_losses.append(val_loss_sum / len(val_loader))\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # 早停\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            patience_counter = 0\n",
    "            best_model_wts = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    # 恢复最佳权重\n",
    "    if best_model_wts:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # 测试集\n",
    "    model.eval()\n",
    "    test_preds, test_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "            test_outputs = model(test_inputs)\n",
    "            _, predicted = torch.max(test_outputs, 1)\n",
    "            test_preds.extend(predicted.cpu().numpy())\n",
    "            test_true.extend(test_labels.cpu().numpy())\n",
    "\n",
    "    test_acc = 100 * np.sum(np.array(test_preds) == np.array(test_true)) / len(test_true)\n",
    "    with open(results_file, \"a\") as f:\n",
    "        f.write(f\"\\nVal Acc: {val_acc:.2f}% | Test Acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "    # 控制台即时输出\n",
    "    print(f\"[Result] Config {search_idx+1}/{num_search} - Val Acc: {val_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "    # 记录结果\n",
    "    results_summary.append((config, best_val, test_acc))\n",
    "    if best_val > best_val_acc:\n",
    "        best_val_acc = best_val\n",
    "        best_config = (config, test_acc)\n",
    "\n",
    "\n",
    "# === 最佳结果 ===\n",
    "print(\"\\n=== Best Hyperparameters ===\")\n",
    "print(best_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
