{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "907bc930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZY\\.conda\\envs\\MW-RFF\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集发射机数量： 150 具体为： ['1-1', '1-10', '1-11', '1-12', '1-14', '1-15', '1-16', '1-18', '1-19', '1-2', '1-8', '10-1', '10-10', '10-11', '10-17', '10-4', '10-7', '11-1', '11-10', '11-17', '11-19', '11-20', '11-4', '11-7', '12-1', '12-19', '12-20', '12-7', '13-14', '13-18', '13-19', '13-20', '13-3', '13-7', '14-10', '14-11', '14-12', '14-13', '14-14', '14-20', '14-7', '14-8', '14-9', '15-1', '15-19', '15-6', '16-1', '16-16', '16-19', '16-20', '16-5', '17-10', '17-11', '18-1', '18-10', '18-11', '18-12', '18-13', '18-14', '18-15', '18-16', '18-17', '18-2', '18-20', '18-4', '18-5', '18-7', '18-8', '18-9', '19-1', '19-10', '19-11', '19-12', '19-13', '19-14', '19-19', '19-2', '19-20', '19-3', '19-4', '19-6', '19-7', '19-8', '19-9', '2-1', '2-12', '2-13', '2-14', '2-15', '2-16', '2-17', '2-19', '2-20', '2-3', '2-4', '2-5', '2-6', '2-7', '2-8', '20-1', '20-12', '20-14', '20-15', '20-16', '20-18', '20-19', '20-20', '20-3', '20-4', '20-5', '20-7', '20-8', '3-1', '3-13', '3-18', '3-19', '3-2', '3-20', '3-8', '4-1', '4-10', '4-11', '5-1', '5-16', '5-20', '5-5', '6-1', '6-15', '6-6', '7-10', '7-11', '7-12', '7-13', '7-14', '7-20', '7-7', '7-8', '7-9', '8-1', '8-13', '8-14', '8-18', '8-20', '8-3', '8-7', '8-8', '9-1', '9-14', '9-20', '9-7']\n",
      "数据集接收机数量： 18 具体为： ['1-1', '1-19', '1-20', '13-7', '14-7', '18-19', '18-2', '19-1', '19-2', '2-1', '20-1', '20-19', '3-19', '7-14', '7-7', '8-14', '8-7', '8-8']\n",
      "数据集采集天数： 4 具体为： ['2021_03_01', '2021_03_08', '2021_03_15', '2021_03_23']\n",
      "✅ 剩余用于训练的 TX 数量: 144\n",
      "具体为：['1-1', '1-10', '1-11', '1-12', '1-14', '1-15', '1-16', '1-18', '1-19', '1-2', '1-8', '10-1', '10-10', '10-11', '10-17', '10-4', '10-7', '11-1', '11-10', '11-17', '11-19', '11-20', '11-4', '11-7', '12-19', '12-20', '12-7', '13-14', '13-18', '13-19', '13-20', '13-3', '13-7', '14-10', '14-11', '14-12', '14-13', '14-14', '14-20', '14-7', '14-8', '14-9', '15-1', '15-19', '15-6', '16-1', '16-16', '16-19', '16-20', '16-5', '17-10', '17-11', '18-10', '18-11', '18-12', '18-13', '18-14', '18-15', '18-16', '18-17', '18-2', '18-20', '18-4', '18-5', '18-7', '18-8', '18-9', '19-1', '19-10', '19-11', '19-12', '19-13', '19-14', '19-19', '19-2', '19-20', '19-3', '19-6', '19-7', '19-8', '19-9', '2-1', '2-12', '2-13', '2-14', '2-15', '2-16', '2-17', '2-19', '2-20', '2-3', '2-4', '2-5', '2-6', '2-7', '2-8', '20-1', '20-12', '20-14', '20-15', '20-16', '20-18', '20-19', '20-20', '20-4', '20-5', '20-7', '20-8', '3-1', '3-13', '3-18', '3-19', '3-2', '3-20', '3-8', '4-1', '4-10', '4-11', '5-1', '5-16', '5-20', '5-5', '6-1', '6-15', '6-6', '7-10', '7-11', '7-13', '7-14', '7-20', '7-7', '7-8', '7-9', '8-1', '8-13', '8-14', '8-18', '8-20', '8-3', '8-7', '8-8', '9-1', '9-20', '9-7']\n",
      "闭集合训练，所有日期为： ['2021_03_01', '2021_03_08', '2021_03_15', '2021_03_23']\n",
      "144 18\n",
      "✅ 总样本数: 494350\n",
      "✅ 训练样本数: 370763, 测试样本数: 123587\n",
      "X_train shape: (370763, 256, 2)\n",
      "y_train shape: (370763,)\n",
      "X_test  shape: (123587, 256, 2)\n",
      "y_test  shape: (123587,)\n",
      "Using device: cuda\n",
      "\n",
      "===== 开始 SNR=-35 dB 实验 =====\n",
      "数据集已应用IQ差分处理\n",
      "数据集已应用IQ差分处理\n",
      "\n",
      "=== Fold 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 4634/4634 [00:42<00:00, 108.55batch/s, accuracy=0.686, grad_norm=3.25, loss=4.99]\n",
      "Epoch 2/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.62batch/s, accuracy=0.717, grad_norm=1.48, loss=4.98]\n",
      "Epoch 3/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.20batch/s, accuracy=0.729, grad_norm=0.627, loss=4.97]\n",
      "Epoch 4/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.08batch/s, accuracy=0.745, grad_norm=0.314, loss=4.97]\n",
      "Epoch 5/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.10batch/s, accuracy=0.719, grad_norm=0.195, loss=4.97]\n",
      "Epoch 6/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.60batch/s, accuracy=0.729, grad_norm=0.159, loss=4.97]\n",
      "Epoch 7/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.88batch/s, accuracy=0.721, grad_norm=0.161, loss=4.97]\n",
      "Epoch 8/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.98batch/s, accuracy=0.721, grad_norm=0.161, loss=4.97]\n",
      "Epoch 9/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.54batch/s, accuracy=0.728, grad_norm=0.169, loss=4.97]\n",
      "Epoch 10/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.43batch/s, accuracy=0.744, grad_norm=0.173, loss=4.97]\n",
      "Epoch 11/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.64batch/s, accuracy=0.744, grad_norm=0.165, loss=4.97]\n",
      "Epoch 12/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.15batch/s, accuracy=0.742, grad_norm=0.179, loss=4.97]\n",
      "Epoch 13/200: 100%|██████████| 4634/4634 [00:38<00:00, 119.87batch/s, accuracy=0.737, grad_norm=0.165, loss=4.97]\n",
      "Epoch 14/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.57batch/s, accuracy=0.759, grad_norm=0.163, loss=4.97]\n",
      "Epoch 15/200: 100%|██████████| 4634/4634 [00:39<00:00, 118.78batch/s, accuracy=0.721, grad_norm=0.165, loss=4.97]\n",
      "Epoch 16/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.22batch/s, accuracy=0.744, grad_norm=0.181, loss=4.97]\n",
      "Epoch 17/200: 100%|██████████| 4634/4634 [00:39<00:00, 118.49batch/s, accuracy=0.731, grad_norm=0.169, loss=4.97]\n",
      "Epoch 18/200: 100%|██████████| 4634/4634 [00:39<00:00, 118.51batch/s, accuracy=0.752, grad_norm=0.159, loss=4.97]\n",
      "Epoch 19/200: 100%|██████████| 4634/4634 [00:41<00:00, 110.47batch/s, accuracy=0.753, grad_norm=0.171, loss=4.97]\n",
      "Epoch 20/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.26batch/s, accuracy=0.741, grad_norm=0.211, loss=4.97]\n",
      "Epoch 21/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.26batch/s, accuracy=0.762, grad_norm=0.18, loss=4.97] \n",
      "Epoch 22/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.02batch/s, accuracy=0.75, grad_norm=0.183, loss=4.97] \n",
      "Epoch 23/200: 100%|██████████| 4634/4634 [00:38<00:00, 120.43batch/s, accuracy=0.759, grad_norm=0.175, loss=4.97]\n",
      "Epoch 24/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.53batch/s, accuracy=0.753, grad_norm=0.201, loss=4.97]\n",
      "Epoch 25/200: 100%|██████████| 4634/4634 [00:38<00:00, 120.78batch/s, accuracy=0.762, grad_norm=0.169, loss=4.97]\n",
      "Epoch 26/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.44batch/s, accuracy=0.757, grad_norm=0.176, loss=4.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Fold 1 Test Accuracy: 0.66%\n",
      "\n",
      "=== Fold 2/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.71batch/s, accuracy=0.697, grad_norm=3.35, loss=4.99]\n",
      "Epoch 2/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.74batch/s, accuracy=0.733, grad_norm=1.47, loss=4.98]\n",
      "Epoch 3/200: 100%|██████████| 4634/4634 [00:38<00:00, 121.27batch/s, accuracy=0.732, grad_norm=0.663, loss=4.97]\n",
      "Epoch 4/200: 100%|██████████| 4634/4634 [00:38<00:00, 120.16batch/s, accuracy=0.7, grad_norm=0.37, loss=4.97]   \n",
      "Epoch 5/200: 100%|██████████| 4634/4634 [00:38<00:00, 119.05batch/s, accuracy=0.724, grad_norm=0.178, loss=4.97]\n",
      "Epoch 6/200: 100%|██████████| 4634/4634 [00:39<00:00, 118.26batch/s, accuracy=0.728, grad_norm=0.174, loss=4.97]\n",
      "Epoch 7/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.86batch/s, accuracy=0.72, grad_norm=0.16, loss=4.97]  \n",
      "Epoch 8/200: 100%|██████████| 4634/4634 [00:41<00:00, 111.73batch/s, accuracy=0.725, grad_norm=0.164, loss=4.97]\n",
      "Epoch 9/200: 100%|██████████| 4634/4634 [00:38<00:00, 121.48batch/s, accuracy=0.702, grad_norm=0.159, loss=4.97]\n",
      "Epoch 10/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.62batch/s, accuracy=0.738, grad_norm=0.171, loss=4.97]\n",
      "Epoch 11/200: 100%|██████████| 4634/4634 [00:37<00:00, 122.16batch/s, accuracy=0.735, grad_norm=0.165, loss=4.97]\n",
      "Epoch 12/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.45batch/s, accuracy=0.745, grad_norm=0.168, loss=4.97]\n",
      "Epoch 13/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.04batch/s, accuracy=0.741, grad_norm=0.16, loss=4.97] \n",
      "Epoch 14/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.02batch/s, accuracy=0.723, grad_norm=0.175, loss=4.97]\n",
      "Epoch 15/200: 100%|██████████| 4634/4634 [00:42<00:00, 109.35batch/s, accuracy=0.718, grad_norm=0.169, loss=4.97]\n",
      "Epoch 16/200: 100%|██████████| 4634/4634 [00:41<00:00, 111.91batch/s, accuracy=0.73, grad_norm=0.186, loss=4.97] \n",
      "Epoch 17/200: 100%|██████████| 4634/4634 [00:39<00:00, 115.88batch/s, accuracy=0.751, grad_norm=0.179, loss=4.97]\n",
      "Epoch 18/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.64batch/s, accuracy=0.743, grad_norm=0.162, loss=4.97]\n",
      "Epoch 19/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.37batch/s, accuracy=0.751, grad_norm=0.164, loss=4.97]\n",
      "Epoch 20/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.21batch/s, accuracy=0.746, grad_norm=0.155, loss=4.97]\n",
      "Epoch 21/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.06batch/s, accuracy=0.736, grad_norm=0.172, loss=4.97]\n",
      "Epoch 22/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.69batch/s, accuracy=0.751, grad_norm=0.181, loss=4.97]\n",
      "Epoch 23/200: 100%|██████████| 4634/4634 [00:41<00:00, 110.82batch/s, accuracy=0.758, grad_norm=0.176, loss=4.97]\n",
      "Epoch 24/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.24batch/s, accuracy=0.753, grad_norm=0.164, loss=4.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Fold 2 Test Accuracy: 0.66%\n",
      "\n",
      "=== Fold 3/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.91batch/s, accuracy=0.718, grad_norm=3.41, loss=4.99]\n",
      "Epoch 2/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.98batch/s, accuracy=0.718, grad_norm=1.45, loss=4.98] \n",
      "Epoch 3/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.02batch/s, accuracy=0.749, grad_norm=0.614, loss=4.97]\n",
      "Epoch 4/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.77batch/s, accuracy=0.728, grad_norm=0.303, loss=4.97]\n",
      "Epoch 5/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.73batch/s, accuracy=0.725, grad_norm=0.168, loss=4.97]\n",
      "Epoch 6/200: 100%|██████████| 4634/4634 [00:41<00:00, 111.94batch/s, accuracy=0.694, grad_norm=0.179, loss=4.97]\n",
      "Epoch 7/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.45batch/s, accuracy=0.726, grad_norm=0.162, loss=4.97]\n",
      "Epoch 8/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.60batch/s, accuracy=0.71, grad_norm=0.165, loss=4.97] \n",
      "Epoch 9/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.39batch/s, accuracy=0.711, grad_norm=0.162, loss=4.97]\n",
      "Epoch 10/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.02batch/s, accuracy=0.71, grad_norm=0.17, loss=4.97]  \n",
      "Epoch 11/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.02batch/s, accuracy=0.745, grad_norm=0.149, loss=4.97]\n",
      "Epoch 12/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.98batch/s, accuracy=0.714, grad_norm=0.171, loss=4.97]\n",
      "Epoch 13/200: 100%|██████████| 4634/4634 [00:43<00:00, 107.20batch/s, accuracy=0.719, grad_norm=0.171, loss=4.97]\n",
      "Epoch 14/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.53batch/s, accuracy=0.727, grad_norm=0.176, loss=4.97]\n",
      "Epoch 15/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.68batch/s, accuracy=0.727, grad_norm=0.162, loss=4.97]\n",
      "Epoch 16/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.16batch/s, accuracy=0.715, grad_norm=0.173, loss=4.97]\n",
      "Epoch 17/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.67batch/s, accuracy=0.728, grad_norm=0.176, loss=4.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Fold 3 Test Accuracy: 0.66%\n",
      "\n",
      "=== Fold 4/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 4634/4634 [00:39<00:00, 118.34batch/s, accuracy=0.698, grad_norm=3.16, loss=4.99]\n",
      "Epoch 2/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.46batch/s, accuracy=0.744, grad_norm=1.36, loss=4.98]\n",
      "Epoch 3/200: 100%|██████████| 4634/4634 [00:42<00:00, 109.33batch/s, accuracy=0.7, grad_norm=0.627, loss=4.97]  \n",
      "Epoch 4/200: 100%|██████████| 4634/4634 [00:42<00:00, 109.66batch/s, accuracy=0.737, grad_norm=0.361, loss=4.97]\n",
      "Epoch 5/200: 100%|██████████| 4634/4634 [00:39<00:00, 115.87batch/s, accuracy=0.739, grad_norm=0.264, loss=4.97]\n",
      "Epoch 6/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.17batch/s, accuracy=0.718, grad_norm=0.148, loss=4.97]\n",
      "Epoch 7/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.40batch/s, accuracy=0.719, grad_norm=0.168, loss=4.97]\n",
      "Epoch 8/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.65batch/s, accuracy=0.724, grad_norm=0.163, loss=4.97]\n",
      "Epoch 9/200: 100%|██████████| 4634/4634 [00:38<00:00, 119.19batch/s, accuracy=0.733, grad_norm=0.162, loss=4.97]\n",
      "Epoch 10/200: 100%|██████████| 4634/4634 [00:41<00:00, 111.79batch/s, accuracy=0.693, grad_norm=0.165, loss=4.97]\n",
      "Epoch 11/200: 100%|██████████| 4634/4634 [00:43<00:00, 105.64batch/s, accuracy=0.733, grad_norm=0.155, loss=4.97]\n",
      "Epoch 12/200: 100%|██████████| 4634/4634 [00:41<00:00, 111.41batch/s, accuracy=0.73, grad_norm=0.176, loss=4.97] \n",
      "Epoch 13/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.76batch/s, accuracy=0.705, grad_norm=0.193, loss=4.97]\n",
      "Epoch 14/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.28batch/s, accuracy=0.727, grad_norm=0.183, loss=4.97]\n",
      "Epoch 15/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.45batch/s, accuracy=0.709, grad_norm=0.177, loss=4.97]\n",
      "Epoch 16/200: 100%|██████████| 4634/4634 [00:39<00:00, 115.90batch/s, accuracy=0.736, grad_norm=0.186, loss=4.97]\n",
      "Epoch 17/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.80batch/s, accuracy=0.715, grad_norm=0.171, loss=4.97]\n",
      "Epoch 18/200: 100%|██████████| 4634/4634 [00:41<00:00, 110.56batch/s, accuracy=0.73, grad_norm=0.191, loss=4.97] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Fold 4 Test Accuracy: 0.66%\n",
      "\n",
      "=== Fold 5/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.16batch/s, accuracy=0.71, grad_norm=3.47, loss=4.99] \n",
      "Epoch 2/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.60batch/s, accuracy=0.695, grad_norm=1.52, loss=4.98]\n",
      "Epoch 3/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.16batch/s, accuracy=0.713, grad_norm=0.687, loss=4.97]\n",
      "Epoch 4/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.77batch/s, accuracy=0.725, grad_norm=0.401, loss=4.97]\n",
      "Epoch 5/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.37batch/s, accuracy=0.735, grad_norm=0.188, loss=4.97]\n",
      "Epoch 6/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.22batch/s, accuracy=0.746, grad_norm=0.188, loss=4.97]\n",
      "Epoch 7/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.97batch/s, accuracy=0.709, grad_norm=0.155, loss=4.97]\n",
      "Epoch 8/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.87batch/s, accuracy=0.727, grad_norm=0.173, loss=4.97]\n",
      "Epoch 9/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.86batch/s, accuracy=0.735, grad_norm=0.154, loss=4.97]\n",
      "Epoch 10/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.28batch/s, accuracy=0.717, grad_norm=0.164, loss=4.97]\n",
      "Epoch 11/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.01batch/s, accuracy=0.752, grad_norm=0.161, loss=4.97]\n",
      "Epoch 12/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.97batch/s, accuracy=0.743, grad_norm=0.18, loss=4.97] \n",
      "Epoch 13/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.27batch/s, accuracy=0.75, grad_norm=0.161, loss=4.97] \n",
      "Epoch 14/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.91batch/s, accuracy=0.737, grad_norm=0.167, loss=4.97]\n",
      "Epoch 15/200: 100%|██████████| 4634/4634 [00:42<00:00, 108.35batch/s, accuracy=0.718, grad_norm=0.165, loss=4.97]\n",
      "Epoch 16/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.85batch/s, accuracy=0.744, grad_norm=0.165, loss=4.97]\n",
      "Epoch 17/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.47batch/s, accuracy=0.754, grad_norm=0.18, loss=4.97] \n",
      "Epoch 18/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.76batch/s, accuracy=0.753, grad_norm=0.181, loss=4.97]\n",
      "Epoch 19/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.97batch/s, accuracy=0.742, grad_norm=0.168, loss=4.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Fold 5 Test Accuracy: 0.69%\n",
      "SNR=-35 完成: Avg Val=0.77%, Avg Test=0.67%\n",
      "\n",
      "===== 开始 SNR=-40 dB 实验 =====\n",
      "数据集已应用IQ差分处理\n",
      "数据集已应用IQ差分处理\n",
      "\n",
      "=== Fold 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.04batch/s, accuracy=0.732, grad_norm=3.31, loss=4.99]\n",
      "Epoch 2/200: 100%|██████████| 4634/4634 [00:39<00:00, 118.71batch/s, accuracy=0.721, grad_norm=1.23, loss=4.98]\n",
      "Epoch 3/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.61batch/s, accuracy=0.754, grad_norm=0.56, loss=4.97] \n",
      "Epoch 4/200: 100%|██████████| 4634/4634 [00:41<00:00, 111.37batch/s, accuracy=0.731, grad_norm=0.393, loss=4.97]\n",
      "Epoch 5/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.59batch/s, accuracy=0.697, grad_norm=0.207, loss=4.97]\n",
      "Epoch 6/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.49batch/s, accuracy=0.723, grad_norm=0.162, loss=4.97]\n",
      "Epoch 7/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.65batch/s, accuracy=0.727, grad_norm=0.165, loss=4.97]\n",
      "Epoch 8/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.72batch/s, accuracy=0.737, grad_norm=0.159, loss=4.97]\n",
      "Epoch 9/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.48batch/s, accuracy=0.734, grad_norm=0.184, loss=4.97]\n",
      "Epoch 10/200: 100%|██████████| 4634/4634 [00:38<00:00, 118.88batch/s, accuracy=0.729, grad_norm=0.168, loss=4.97]\n",
      "Epoch 11/200: 100%|██████████| 4634/4634 [00:43<00:00, 106.34batch/s, accuracy=0.737, grad_norm=0.167, loss=4.97]\n",
      "Epoch 12/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.49batch/s, accuracy=0.754, grad_norm=0.188, loss=4.97]\n",
      "Epoch 13/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.14batch/s, accuracy=0.749, grad_norm=0.173, loss=4.97]\n",
      "Epoch 14/200: 100%|██████████| 4634/4634 [00:39<00:00, 118.11batch/s, accuracy=0.738, grad_norm=0.168, loss=4.97]\n",
      "Epoch 15/200: 100%|██████████| 4634/4634 [00:39<00:00, 115.87batch/s, accuracy=0.762, grad_norm=0.182, loss=4.97]\n",
      "Epoch 16/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.67batch/s, accuracy=0.756, grad_norm=0.173, loss=4.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Fold 1 Test Accuracy: 0.66%\n",
      "\n",
      "=== Fold 2/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.44batch/s, accuracy=0.723, grad_norm=3.57, loss=4.99]\n",
      "Epoch 2/200: 100%|██████████| 4634/4634 [00:41<00:00, 111.35batch/s, accuracy=0.729, grad_norm=1.54, loss=4.98]\n",
      "Epoch 3/200: 100%|██████████| 4634/4634 [00:43<00:00, 107.55batch/s, accuracy=0.711, grad_norm=0.616, loss=4.97]\n",
      "Epoch 4/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.70batch/s, accuracy=0.737, grad_norm=0.456, loss=4.97]\n",
      "Epoch 5/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.87batch/s, accuracy=0.721, grad_norm=0.213, loss=4.97]\n",
      "Epoch 6/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.55batch/s, accuracy=0.75, grad_norm=0.165, loss=4.97] \n",
      "Epoch 7/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.99batch/s, accuracy=0.71, grad_norm=0.174, loss=4.97] \n",
      "Epoch 8/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.27batch/s, accuracy=0.737, grad_norm=0.175, loss=4.97]\n",
      "Epoch 9/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.55batch/s, accuracy=0.719, grad_norm=0.167, loss=4.97]\n",
      "Epoch 10/200: 100%|██████████| 4634/4634 [00:43<00:00, 107.52batch/s, accuracy=0.716, grad_norm=0.183, loss=4.97]\n",
      "Epoch 11/200: 100%|██████████| 4634/4634 [00:41<00:00, 110.34batch/s, accuracy=0.752, grad_norm=0.171, loss=4.97]\n",
      "Epoch 12/200: 100%|██████████| 4634/4634 [00:39<00:00, 118.40batch/s, accuracy=0.755, grad_norm=0.164, loss=4.97]\n",
      "Epoch 13/200: 100%|██████████| 4634/4634 [00:41<00:00, 110.45batch/s, accuracy=0.746, grad_norm=0.167, loss=4.97]\n",
      "Epoch 14/200: 100%|██████████| 4634/4634 [00:38<00:00, 119.22batch/s, accuracy=0.719, grad_norm=0.165, loss=4.97]\n",
      "Epoch 15/200: 100%|██████████| 4634/4634 [00:41<00:00, 110.78batch/s, accuracy=0.725, grad_norm=0.171, loss=4.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Fold 2 Test Accuracy: 0.66%\n",
      "\n",
      "=== Fold 3/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.07batch/s, accuracy=0.726, grad_norm=3.47, loss=4.99]\n",
      "Epoch 2/200: 100%|██████████| 4634/4634 [00:41<00:00, 111.21batch/s, accuracy=0.698, grad_norm=1.56, loss=4.98]\n",
      "Epoch 3/200: 100%|██████████| 4634/4634 [00:39<00:00, 115.93batch/s, accuracy=0.721, grad_norm=0.675, loss=4.97]\n",
      "Epoch 4/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.01batch/s, accuracy=0.718, grad_norm=0.325, loss=4.97]\n",
      "Epoch 5/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.46batch/s, accuracy=0.695, grad_norm=0.196, loss=4.97]\n",
      "Epoch 6/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.74batch/s, accuracy=0.703, grad_norm=0.17, loss=4.97] \n",
      "Epoch 7/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.41batch/s, accuracy=0.71, grad_norm=0.183, loss=4.97] \n",
      "Epoch 8/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.06batch/s, accuracy=0.748, grad_norm=0.158, loss=4.97]\n",
      "Epoch 9/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.80batch/s, accuracy=0.713, grad_norm=0.155, loss=4.97]\n",
      "Epoch 10/200: 100%|██████████| 4634/4634 [00:41<00:00, 111.21batch/s, accuracy=0.722, grad_norm=0.161, loss=4.97]\n",
      "Epoch 11/200: 100%|██████████| 4634/4634 [00:39<00:00, 118.44batch/s, accuracy=0.711, grad_norm=0.167, loss=4.97]\n",
      "Epoch 12/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.85batch/s, accuracy=0.734, grad_norm=0.171, loss=4.97]\n",
      "Epoch 13/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.21batch/s, accuracy=0.735, grad_norm=0.155, loss=4.97]\n",
      "Epoch 14/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.31batch/s, accuracy=0.737, grad_norm=0.171, loss=4.97]\n",
      "Epoch 15/200: 100%|██████████| 4634/4634 [00:39<00:00, 118.57batch/s, accuracy=0.706, grad_norm=0.176, loss=4.97]\n",
      "Epoch 16/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.58batch/s, accuracy=0.721, grad_norm=0.195, loss=4.97]\n",
      "Epoch 17/200: 100%|██████████| 4634/4634 [00:42<00:00, 109.53batch/s, accuracy=0.723, grad_norm=0.165, loss=4.97]\n",
      "Epoch 18/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.09batch/s, accuracy=0.736, grad_norm=0.182, loss=4.97]\n",
      "Epoch 19/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.26batch/s, accuracy=0.729, grad_norm=0.179, loss=4.97]\n",
      "Epoch 20/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.77batch/s, accuracy=0.707, grad_norm=0.159, loss=4.97]\n",
      "Epoch 21/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.44batch/s, accuracy=0.734, grad_norm=0.16, loss=4.97] \n",
      "Epoch 22/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.76batch/s, accuracy=0.723, grad_norm=0.168, loss=4.97]\n",
      "Epoch 23/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.54batch/s, accuracy=0.722, grad_norm=0.168, loss=4.97]\n",
      "Epoch 24/200: 100%|██████████| 4634/4634 [00:42<00:00, 110.17batch/s, accuracy=0.728, grad_norm=0.172, loss=4.97]\n",
      "Epoch 25/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.84batch/s, accuracy=0.717, grad_norm=0.173, loss=4.97]\n",
      "Epoch 26/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.60batch/s, accuracy=0.719, grad_norm=0.18, loss=4.97] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Fold 3 Test Accuracy: 0.66%\n",
      "\n",
      "=== Fold 4/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.42batch/s, accuracy=0.711, grad_norm=3.5, loss=4.99] \n",
      "Epoch 2/200: 100%|██████████| 4634/4634 [00:39<00:00, 118.30batch/s, accuracy=0.737, grad_norm=1.56, loss=4.98]\n",
      "Epoch 3/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.17batch/s, accuracy=0.748, grad_norm=0.605, loss=4.97]\n",
      "Epoch 4/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.23batch/s, accuracy=0.716, grad_norm=0.306, loss=4.97]\n",
      "Epoch 5/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.69batch/s, accuracy=0.695, grad_norm=0.202, loss=4.97]\n",
      "Epoch 6/200: 100%|██████████| 4634/4634 [00:43<00:00, 106.42batch/s, accuracy=0.715, grad_norm=0.154, loss=4.97]\n",
      "Epoch 7/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.40batch/s, accuracy=0.702, grad_norm=0.183, loss=4.97]\n",
      "Epoch 8/200: 100%|██████████| 4634/4634 [00:39<00:00, 115.97batch/s, accuracy=0.738, grad_norm=0.158, loss=4.97]\n",
      "Epoch 9/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.36batch/s, accuracy=0.725, grad_norm=0.14, loss=4.97] \n",
      "Epoch 10/200: 100%|██████████| 4634/4634 [00:39<00:00, 118.74batch/s, accuracy=0.707, grad_norm=0.168, loss=4.97]\n",
      "Epoch 11/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.51batch/s, accuracy=0.728, grad_norm=0.159, loss=4.97]\n",
      "Epoch 12/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.77batch/s, accuracy=0.71, grad_norm=0.151, loss=4.97] \n",
      "Epoch 13/200: 100%|██████████| 4634/4634 [00:42<00:00, 108.39batch/s, accuracy=0.733, grad_norm=0.157, loss=4.97]\n",
      "Epoch 14/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.12batch/s, accuracy=0.712, grad_norm=0.184, loss=4.97]\n",
      "Epoch 15/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.79batch/s, accuracy=0.723, grad_norm=0.175, loss=4.97]\n",
      "Epoch 16/200: 100%|██████████| 4634/4634 [00:39<00:00, 118.52batch/s, accuracy=0.747, grad_norm=0.158, loss=4.97]\n",
      "Epoch 17/200: 100%|██████████| 4634/4634 [00:39<00:00, 118.51batch/s, accuracy=0.727, grad_norm=0.19, loss=4.97] \n",
      "Epoch 18/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.03batch/s, accuracy=0.717, grad_norm=0.193, loss=4.97]\n",
      "Epoch 19/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.24batch/s, accuracy=0.738, grad_norm=0.179, loss=4.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Fold 4 Test Accuracy: 0.66%\n",
      "\n",
      "=== Fold 5/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.94batch/s, accuracy=0.719, grad_norm=3.54, loss=4.99]\n",
      "Epoch 2/200: 100%|██████████| 4634/4634 [00:42<00:00, 108.96batch/s, accuracy=0.734, grad_norm=1.53, loss=4.98]\n",
      "Epoch 3/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.84batch/s, accuracy=0.691, grad_norm=0.594, loss=4.97]\n",
      "Epoch 4/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.89batch/s, accuracy=0.729, grad_norm=0.382, loss=4.97]\n",
      "Epoch 5/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.46batch/s, accuracy=0.746, grad_norm=0.22, loss=4.97] \n",
      "Epoch 6/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.29batch/s, accuracy=0.74, grad_norm=0.173, loss=4.97] \n",
      "Epoch 7/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.74batch/s, accuracy=0.694, grad_norm=0.177, loss=4.97]\n",
      "Epoch 8/200: 100%|██████████| 4634/4634 [00:40<00:00, 113.47batch/s, accuracy=0.724, grad_norm=0.178, loss=4.97]\n",
      "Epoch 9/200: 100%|██████████| 4634/4634 [00:42<00:00, 109.90batch/s, accuracy=0.72, grad_norm=0.155, loss=4.97] \n",
      "Epoch 10/200: 100%|██████████| 4634/4634 [00:42<00:00, 108.94batch/s, accuracy=0.758, grad_norm=0.158, loss=4.97]\n",
      "Epoch 11/200: 100%|██████████| 4634/4634 [00:38<00:00, 119.30batch/s, accuracy=0.72, grad_norm=0.164, loss=4.97] \n",
      "Epoch 12/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.52batch/s, accuracy=0.743, grad_norm=0.174, loss=4.97]\n",
      "Epoch 13/200: 100%|██████████| 4634/4634 [00:39<00:00, 117.32batch/s, accuracy=0.731, grad_norm=0.179, loss=4.97]\n",
      "Epoch 14/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.43batch/s, accuracy=0.749, grad_norm=0.168, loss=4.97]\n",
      "Epoch 15/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.72batch/s, accuracy=0.739, grad_norm=0.173, loss=4.97]\n",
      "Epoch 16/200: 100%|██████████| 4634/4634 [00:41<00:00, 112.70batch/s, accuracy=0.751, grad_norm=0.169, loss=4.97]\n",
      "Epoch 17/200: 100%|██████████| 4634/4634 [00:42<00:00, 108.48batch/s, accuracy=0.728, grad_norm=0.177, loss=4.97]\n",
      "Epoch 18/200: 100%|██████████| 4634/4634 [00:40<00:00, 115.08batch/s, accuracy=0.732, grad_norm=0.172, loss=4.97]\n",
      "Epoch 19/200: 100%|██████████| 4634/4634 [00:39<00:00, 116.00batch/s, accuracy=0.737, grad_norm=0.157, loss=4.97]\n",
      "Epoch 20/200: 100%|██████████| 4634/4634 [00:40<00:00, 114.24batch/s, accuracy=0.754, grad_norm=0.17, loss=4.97] \n",
      "Epoch 21/200: 100%|██████████| 4634/4634 [00:38<00:00, 119.42batch/s, accuracy=0.736, grad_norm=0.169, loss=4.97]\n",
      "Epoch 22/200: 100%|██████████| 4634/4634 [00:41<00:00, 111.92batch/s, accuracy=0.756, grad_norm=0.188, loss=4.97]\n",
      "Epoch 23/200: 100%|██████████| 4634/4634 [00:38<00:00, 119.11batch/s, accuracy=0.756, grad_norm=0.172, loss=4.97]\n",
      "Epoch 24/200: 100%|██████████| 4634/4634 [00:42<00:00, 110.14batch/s, accuracy=0.748, grad_norm=0.149, loss=4.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Fold 5 Test Accuracy: 0.68%\n",
      "SNR=-40 完成: Avg Val=0.76%, Avg Test=0.66%\n",
      "\n",
      "===== 所有 SNR 实验完成 =====\n",
      "SNR=-35 结果保存在: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2025-12-07_04-45-18_wisig_DCTF_SNR-35dB_fd266_classes_144_ResNet18\n",
      "SNR=-40 结果保存在: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2025-12-07_06-00-03_wisig_DCTF_SNR-40dB_fd266_classes_144_ResNet18\n"
     ]
    }
   ],
   "source": [
    "# ManyTX 全日期闭集 SNR循环实验 ResNet 150TX\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from data_utilities import *\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "dataset_name = 'ManyTx'\n",
    "dataset_path='../ManyTx.pkl/'\n",
    "\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path,dataset_name)\n",
    "\n",
    "print(\"数据集发射机数量：\",len(compact_dataset['tx_list']),\"具体为：\",compact_dataset['tx_list'])\n",
    "print(\"数据集接收机数量：\",len(compact_dataset['rx_list']),\"具体为：\",compact_dataset['rx_list'])\n",
    "print(\"数据集采集天数：\",len(compact_dataset['capture_date_list']),\"具体为：\",compact_dataset['capture_date_list'])\n",
    "\n",
    "# ------------------------ 去掉不参与训练的 TX ------------------------\n",
    "tx_list = compact_dataset['tx_list']\n",
    "exclude_txs = ['12-1', '18-1', '19-4', '20-3', '7-12', '9-14']\n",
    "tx_list = [tx for tx in tx_list if tx not in exclude_txs]\n",
    "\n",
    "print(f\"✅ 剩余用于训练的 TX 数量: {len(tx_list)}\")\n",
    "print(f\"具体为：{tx_list}\")\n",
    "\n",
    "rx_list = compact_dataset['rx_list']\n",
    "equalized = 0\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "\n",
    "print(\"闭集合训练，所有日期为：\", capture_date_list)\n",
    "\n",
    "n_tx = len(tx_list)\n",
    "n_rx = len(rx_list)\n",
    "print(n_tx,n_rx)\n",
    "\n",
    "X_train, y_train, X_test, y_test = preprocess_dataset_for_classification_random_split(\n",
    "    compact_dataset=compact_dataset,\n",
    "    tx_list=tx_list,\n",
    "    rx_list=rx_list,\n",
    "    test_ratio=0.25,\n",
    "    max_sig=None,         # 每个信号最大截取数量，不截取就用 None\n",
    "    equalized=0,          # 使用的数据类型\n",
    "    use_phase_differential=False,  # 是否使用相位差分\n",
    "    seed=42               # 随机种子，可复现\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test  shape:\", X_test.shape)\n",
    "print(\"y_test  shape:\", y_test.shape)\n",
    "\n",
    "# === 信号处理参数 ===\n",
    "fs = 20e6\n",
    "fc = 2.4e9\n",
    "v = 120\n",
    "Add_noise = True\n",
    "Add_doppler = True\n",
    "Add_diff = True\n",
    "\n",
    "def compute_doppler_shift(v, fc):\n",
    "    c = 3e8\n",
    "    v = v / 3.6\n",
    "    return (v / c) * fc\n",
    "\n",
    "fd = compute_doppler_shift(v, fc)\n",
    "\n",
    "def add_doppler_shift(signal, fd, fs):\n",
    "    num_samples = signal.shape[-1]\n",
    "    t = np.arange(num_samples) / fs\n",
    "    doppler_phase = np.exp(1j * 2 * np.pi * fd * t)\n",
    "    return signal * doppler_phase\n",
    "\n",
    "def add_iq_differential(signal):\n",
    "    \"\"\"\n",
    "    对IQ信号进行差分处理\n",
    "    输入: 复数信号数组\n",
    "    输出: 差分后的复数信号数组（长度减1）\n",
    "    \"\"\"\n",
    "    # 对复数信号进行一阶差分\n",
    "    diff_signal = np.diff(signal)\n",
    "    return diff_signal\n",
    "def measure_snr(clean_signal, noisy_signal):\n",
    "    signal_power = np.mean(np.abs(clean_signal) ** 2)\n",
    "    noise = noisy_signal - clean_signal\n",
    "    noise_power = np.mean(np.abs(noise) ** 2)\n",
    "    if noise_power == 0:\n",
    "        return float('inf')\n",
    "    return 10 * np.log10(signal_power / noise_power)\n",
    "\n",
    "def add_complex_awgn(signal, snr_db):\n",
    "    signal_power = np.mean(np.abs(signal) ** 2)\n",
    "    snr_linear = 10 ** (snr_db / 10)\n",
    "    noise_power = signal_power / snr_linear\n",
    "    noise_std = np.sqrt(noise_power / 2)\n",
    "    noise = np.random.normal(0, noise_std, signal.shape) + 1j*np.random.normal(0, noise_std, signal.shape)\n",
    "    return signal + noise, noise\n",
    "\n",
    "def preprocess_iq_data(data_real_imag, snr_db=None, fd=None, fs=None, add_noise=True, add_diff=True, add_doppler=True, verify_snr=True):\n",
    "    if add_noise and snr_db is None:\n",
    "        raise ValueError(\"当add_noise=True时，必须提供snr_db参数\")\n",
    "    if add_doppler and (fd is None or fs is None):\n",
    "        raise ValueError(\"当add_doppler=True时，必须提供fd和fs参数\")\n",
    "    if add_noise:\n",
    "        print(\"数据集已应用IQ差分处理\")\n",
    "        \n",
    "    data_complex = data_real_imag[...,0] + 1j*data_real_imag[...,1]\n",
    "    processed = []\n",
    "    snr_measured_list = []\n",
    "    for i, sig in enumerate(data_complex):\n",
    "        # 原始信号归一化\n",
    "        sig = sig / (np.sqrt(np.mean(np.abs(sig)**2)) + 1e-12)\n",
    "        current_sig = sig.copy()\n",
    "        if add_noise:\n",
    "            noisy_sig, _ = add_complex_awgn(current_sig, snr_db)\n",
    "            current_sig = noisy_sig\n",
    "            if verify_snr:\n",
    "                measured_snr = measure_snr(sig, noisy_sig)\n",
    "                snr_measured_list.append(measured_snr)\n",
    "        if add_doppler:\n",
    "            current_sig = add_doppler_shift(current_sig, fd, fs)\n",
    "        if add_diff:\n",
    "            diff_sig = add_iq_differential(current_sig)\n",
    "            current_sig = diff_sig\n",
    "\n",
    "        processed.append(current_sig)\n",
    "    processed = np.array(processed)\n",
    "    processed_real_imag = np.stack([processed.real, processed.imag], axis=-1)\n",
    "    return processed_real_imag, None\n",
    "\n",
    "# === ResNet 1D 模型定义 ===\n",
    "class BasicBlock1D(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "class ResNet18_1D(nn.Module):\n",
    "    def __init__(self, num_classes=10, in_planes=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.conv1 = nn.Conv1d(2, in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1, dropout=dropout)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2, dropout=dropout)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2, dropout=dropout)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2, dropout=dropout)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    def _make_layer(self, planes, blocks, stride, dropout):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes)\n",
    "            )\n",
    "        layers = [BasicBlock1D(self.in_planes, planes, stride, downsample, dropout)]\n",
    "        self.in_planes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock1D(self.in_planes, planes, dropout=dropout))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# === 训练超参数 ===\n",
    "batch_size   = 64\n",
    "num_epochs   = 200\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-3\n",
    "in_planes    = 64\n",
    "dropout      = 0.5\n",
    "patience     = 5\n",
    "n_splits     = 5\n",
    "num_classes  = len(np.unique(y_train))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def compute_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm += (p.grad.data.norm(2).item() ** 2)\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def moving_average(x, w=5):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "# === SNR 循环 ===\n",
    "snr_list = list(range(-35, -45, -5))  # 20,15,...,-40\n",
    "all_results = {}\n",
    "\n",
    "for SNR_dB in snr_list:\n",
    "    print(f\"\\n===== 开始 SNR={SNR_dB} dB 实验 =====\")\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    script_name = \"wisig_DCTF\"\n",
    "    folder_name = f\"{timestamp}_{script_name}_SNR{SNR_dB}dB_fd{int(fd)}_classes_{num_classes}_ResNet18\"\n",
    "    save_folder = os.path.join(os.getcwd(), \"training_results\", folder_name)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    results_file = os.path.join(save_folder, \"results.txt\")\n",
    "    with open(results_file, \"w\") as f:\n",
    "        f.write(f\"=== Experiment Summary ===\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"Total Classes: {num_classes}\\n\")\n",
    "        f.write(f\"SNR: {SNR_dB} dB\\n\")\n",
    "        f.write(f\"fd (Doppler shift): {fd} Hz\\n\")\n",
    "        f.write(f\"equalized: {equalized}\\n\")\n",
    "        f.write(f\"diff:{Add_diff}\\n\")\n",
    "\n",
    "    # === 数据处理 ===\n",
    "    X_train_processed, _ = preprocess_iq_data(\n",
    "        X_train, snr_db=SNR_dB, fd=fd, fs=fs, add_noise=Add_noise, add_diff=Add_diff, add_doppler=Add_doppler, verify_snr=False\n",
    "    )\n",
    "    X_test_processed, _ = preprocess_iq_data(\n",
    "        X_test, snr_db=SNR_dB, fd=fd, fs=fs, add_noise=Add_noise, add_diff=Add_diff, add_doppler=Add_doppler, verify_snr=False\n",
    "    )\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train_processed,dtype=torch.float32),\n",
    "                                  torch.tensor(y_train,dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test_processed,dtype=torch.float32),\n",
    "                                 torch.tensor(y_test,dtype=torch.long))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    test_results = []\n",
    "    avg_grad_norms_per_fold = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{n_splits} ===\")\n",
    "        train_subset = Subset(train_dataset, train_idx)\n",
    "        val_subset = Subset(train_dataset, val_idx)\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "        model = ResNet18_1D(num_classes=num_classes, in_planes=in_planes, dropout=dropout).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        train_losses, val_losses = [], []\n",
    "        train_accuracies, val_accuracies = [], []\n",
    "        grad_norms = []\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "            batch_grad_norms = []\n",
    "            with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as tepoch:\n",
    "                for inputs, labels in tepoch:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    grad_norm = compute_grad_norm(model)\n",
    "                    batch_grad_norms.append(grad_norm)\n",
    "                    optimizer.step()\n",
    "                    running_train_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total_train += labels.size(0)\n",
    "                    correct_train += (predicted == labels).sum().item()\n",
    "                    tepoch.set_postfix(loss=running_train_loss/len(train_loader),\n",
    "                                       accuracy=100*correct_train/total_train,\n",
    "                                       grad_norm=grad_norm)\n",
    "\n",
    "            epoch_train_loss = running_train_loss/len(train_loader)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "            train_accuracies.append(100*correct_train/total_train)\n",
    "            avg_grad_norm = np.mean(batch_grad_norms)\n",
    "            grad_norms.append(avg_grad_norm)\n",
    "\n",
    "            # 验证\n",
    "            model.eval()\n",
    "            running_val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss = criterion(val_outputs, val_labels)\n",
    "                    running_val_loss += val_loss.item()\n",
    "                    _, val_predicted = torch.max(val_outputs, 1)\n",
    "                    total_val += val_labels.size(0)\n",
    "                    correct_val += (val_predicted == val_labels).sum().item()\n",
    "            epoch_val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accuracies.append(100*correct_val/total_val)\n",
    "\n",
    "            with open(results_file,'a') as f:\n",
    "                f.write(f\"Fold{fold+1} Epoch{epoch+1} | TrainAcc={train_accuracies[-1]:.2f}% | ValAcc={val_accuracies[-1]:.2f}% | \"\n",
    "                        f\"TrainLoss={train_losses[-1]:.4f} | ValLoss={val_losses[-1]:.4f} | AvgGrad={avg_grad_norm:.4f}\\n\")\n",
    "\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            scheduler.step()\n",
    "\n",
    "        fold_results.append(max(val_accuracies))\n",
    "        avg_grad_norms_per_fold.append(grad_norms)\n",
    "\n",
    "        # 绘图\n",
    "        plt.figure()\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.plot(moving_average(train_losses), label='Train Loss Smooth', linestyle='--')\n",
    "        plt.plot(moving_average(val_losses), label='Val Loss Smooth', linestyle='--')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Fold {fold+1} Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(save_folder, f\"fold_{fold+1}_loss_curve.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(grad_norms, label='Grad Norm')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Grad Norm')\n",
    "        plt.title(f'Fold {fold+1} Grad Norm')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_folder, f\"fold_{fold+1}_grad_norm.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # 测试集评估\n",
    "        model.eval()\n",
    "        test_preds, test_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for test_inputs, test_labels in test_loader:\n",
    "                test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "                test_outputs = model(test_inputs)\n",
    "                _, predicted = torch.max(test_outputs, 1)\n",
    "                test_preds.extend(predicted.cpu().numpy())\n",
    "                test_true.extend(test_labels.cpu().numpy())\n",
    "        test_preds = np.array(test_preds)\n",
    "        test_true = np.array(test_true)\n",
    "        test_accuracy = 100*np.sum(test_preds==test_true)/len(test_true)\n",
    "        test_results.append(test_accuracy)\n",
    "\n",
    "        print(f\"Fold {fold+1} Test Accuracy: {test_accuracy:.2f}%\")\n",
    "        with open(results_file, \"a\") as f:\n",
    "            f.write(f\"Fold {fold+1} Test Acc: {test_accuracy:.2f}%\\n\")\n",
    "\n",
    "        cm = confusion_matrix(test_true, test_preds)\n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Fold {fold+1} Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.savefig(os.path.join(save_folder, f\"fold_{fold+1}_confusion_matrix.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    avg_val = np.mean(fold_results)\n",
    "    avg_test = np.mean(test_results)\n",
    "    with open(results_file, \"a\") as f:\n",
    "        f.write(f\"\\n=== SNR={SNR_dB} Summary ===\\n\")\n",
    "        f.write(f\"Avg Val Acc: {avg_val:.2f}%, Avg Test Acc: {avg_test:.2f}%\\n\")\n",
    "    print(f\"SNR={SNR_dB} 完成: Avg Val={avg_val:.2f}%, Avg Test={avg_test:.2f}%\")\n",
    "    all_results[SNR_dB] = save_folder\n",
    "\n",
    "print(\"\\n===== 所有 SNR 实验完成 =====\")\n",
    "for snr, folder in all_results.items():\n",
    "    print(f\"SNR={snr} 结果保存在: {folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09fffc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZY\\.conda\\envs\\MW-RFF\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集发射机数量： 150 具体为： ['1-1', '1-10', '1-11', '1-12', '1-14', '1-15', '1-16', '1-18', '1-19', '1-2', '1-8', '10-1', '10-10', '10-11', '10-17', '10-4', '10-7', '11-1', '11-10', '11-17', '11-19', '11-20', '11-4', '11-7', '12-1', '12-19', '12-20', '12-7', '13-14', '13-18', '13-19', '13-20', '13-3', '13-7', '14-10', '14-11', '14-12', '14-13', '14-14', '14-20', '14-7', '14-8', '14-9', '15-1', '15-19', '15-6', '16-1', '16-16', '16-19', '16-20', '16-5', '17-10', '17-11', '18-1', '18-10', '18-11', '18-12', '18-13', '18-14', '18-15', '18-16', '18-17', '18-2', '18-20', '18-4', '18-5', '18-7', '18-8', '18-9', '19-1', '19-10', '19-11', '19-12', '19-13', '19-14', '19-19', '19-2', '19-20', '19-3', '19-4', '19-6', '19-7', '19-8', '19-9', '2-1', '2-12', '2-13', '2-14', '2-15', '2-16', '2-17', '2-19', '2-20', '2-3', '2-4', '2-5', '2-6', '2-7', '2-8', '20-1', '20-12', '20-14', '20-15', '20-16', '20-18', '20-19', '20-20', '20-3', '20-4', '20-5', '20-7', '20-8', '3-1', '3-13', '3-18', '3-19', '3-2', '3-20', '3-8', '4-1', '4-10', '4-11', '5-1', '5-16', '5-20', '5-5', '6-1', '6-15', '6-6', '7-10', '7-11', '7-12', '7-13', '7-14', '7-20', '7-7', '7-8', '7-9', '8-1', '8-13', '8-14', '8-18', '8-20', '8-3', '8-7', '8-8', '9-1', '9-14', '9-20', '9-7']\n",
      "数据集接收机数量： 18 具体为： ['1-1', '1-19', '1-20', '13-7', '14-7', '18-19', '18-2', '19-1', '19-2', '2-1', '20-1', '20-19', '3-19', '7-14', '7-7', '8-14', '8-7', '8-8']\n",
      "数据集采集天数： 4 具体为： ['2021_03_01', '2021_03_08', '2021_03_15', '2021_03_23']\n",
      "✅ 剩余用于训练的 TX 数量: 144\n",
      "具体为：['1-1', '1-10', '1-11', '1-12', '1-14', '1-15', '1-16', '1-18', '1-19', '1-2', '1-8', '10-1', '10-10', '10-11', '10-17', '10-4', '10-7', '11-1', '11-10', '11-17', '11-19', '11-20', '11-4', '11-7', '12-19', '12-20', '12-7', '13-14', '13-18', '13-19', '13-20', '13-3', '13-7', '14-10', '14-11', '14-12', '14-13', '14-14', '14-20', '14-7', '14-8', '14-9', '15-1', '15-19', '15-6', '16-1', '16-16', '16-19', '16-20', '16-5', '17-10', '17-11', '18-10', '18-11', '18-12', '18-13', '18-14', '18-15', '18-16', '18-17', '18-2', '18-20', '18-4', '18-5', '18-7', '18-8', '18-9', '19-1', '19-10', '19-11', '19-12', '19-13', '19-14', '19-19', '19-2', '19-20', '19-3', '19-6', '19-7', '19-8', '19-9', '2-1', '2-12', '2-13', '2-14', '2-15', '2-16', '2-17', '2-19', '2-20', '2-3', '2-4', '2-5', '2-6', '2-7', '2-8', '20-1', '20-12', '20-14', '20-15', '20-16', '20-18', '20-19', '20-20', '20-4', '20-5', '20-7', '20-8', '3-1', '3-13', '3-18', '3-19', '3-2', '3-20', '3-8', '4-1', '4-10', '4-11', '5-1', '5-16', '5-20', '5-5', '6-1', '6-15', '6-6', '7-10', '7-11', '7-13', '7-14', '7-20', '7-7', '7-8', '7-9', '8-1', '8-13', '8-14', '8-18', '8-20', '8-3', '8-7', '8-8', '9-1', '9-20', '9-7']\n",
      "150 18\n",
      "=== 相位差分前后对比 ===\n",
      "差分前的前5个IQ样本值:\n",
      "  样本0: I=-0.003845, Q=0.005188\n",
      "  样本1: I=-0.004028, Q=0.004425\n",
      "  样本2: I=-0.006684, Q=0.006745\n",
      "  样本3: I=-0.005249, Q=-0.002045\n",
      "  样本4: I=0.004303, Q=-0.009705\n",
      "\n",
      "相位差分后的前5个样本值:\n",
      "  样本0: I=0.005954, Q=0.000601\n",
      "  样本1: I=0.009487, Q=0.000402\n",
      "  样本2: I=0.002242, Q=0.005168\n",
      "  样本3: I=-0.000487, Q=0.010605\n",
      "  样本4: I=0.001958, Q=0.007879\n",
      "\n",
      "数值范围信息:\n",
      "  原始信号I范围: [-0.010590, 0.010468]\n",
      "  原始信号Q范围: [-0.011322, 0.009796]\n",
      "  差分后I范围: [-0.006994, 0.010417]\n",
      "  差分后Q范围: [-0.010326, 0.011780]\n",
      "=== 相位差分前后对比 ===\n",
      "差分前的前5个IQ样本值:\n",
      "  样本0: I=0.006256, Q=-0.001160\n",
      "  样本1: I=-0.011353, Q=0.062593\n",
      "  样本2: I=-0.121890, Q=-0.042207\n",
      "  样本3: I=-0.004364, Q=-0.204442\n",
      "  样本4: I=0.166111, Q=0.037446\n",
      "\n",
      "相位差分后的前5个样本值:\n",
      "  样本0: I=-0.022571, Q=0.059476\n",
      "  样本1: I=-0.019776, Q=0.127466\n",
      "  样本2: I=0.071019, Q=0.191760\n",
      "  样本3: I=-0.040983, Q=0.165274\n",
      "  样本4: I=0.142943, Q=0.160174\n",
      "\n",
      "数值范围信息:\n",
      "  原始信号I范围: [-0.259314, 0.244574]\n",
      "  原始信号Q范围: [-0.237524, 0.235724]\n",
      "  差分后I范围: [-0.163636, 0.193649]\n",
      "  差分后Q范围: [-0.219788, 0.264972]\n",
      "✅ 训练样本数: 383986, 测试样本数: 127529\n",
      "✅ 已启用相位差分功能\n",
      "✅ 相位差分：保持幅度信息，消除载波频偏\n",
      "✅ 差分后样本长度: 255\n",
      "X_train shape: (383986, 255, 2)\n",
      "y_train shape: (383986,)\n",
      "[INFO] 多普勒频移 fd = 266.67 Hz\n",
      "应用IQ差分处理\n",
      "\n",
      "=== 单样本SNR测试 (测试5次) ===\n",
      "测试 1: 目标SNR=20 dB, 实测SNR=19.98 dB\n",
      "测试 2: 目标SNR=20 dB, 实测SNR=19.94 dB\n",
      "测试 3: 目标SNR=20 dB, 实测SNR=20.30 dB\n",
      "测试 4: 目标SNR=20 dB, 实测SNR=20.07 dB\n",
      "测试 5: 目标SNR=20 dB, 实测SNR=20.02 dB\n",
      "\n",
      "测试统计:\n",
      "平均值: 20.06 dB\n",
      "标准差: 0.13 dB\n",
      "误差范围: ±0.06 dB\n",
      "\n",
      "=== 处理测试集 ===\n",
      "应用IQ差分处理\n",
      "\n",
      "=== 信号对比 ===\n",
      "原始信号 I 分量： [ 0.00595387  0.00948666  0.00224241 -0.00048707  0.00195766 -0.00303293\n",
      " -0.00189895  0.00251106 -0.00012568  0.00786993]\n",
      "处理后信号 I 分量： [ 0.00335539 -0.00776075 -0.00276553  0.00333597 -0.00535168  0.0020114\n",
      "  0.00389046 -0.00384794  0.00883419 -0.00301261]\n",
      "\n",
      "=== 多普勒频移验证 ===\n",
      "相位变化: [-0.11713952  2.58107067  0.86468917 -2.2016199  -4.17843878 -0.78851323\n",
      " -2.6116998   1.2004772  -2.26994232 -2.5357652 ]\n",
      "Using device: cuda\n",
      "\n",
      "====== Fold 1/5 ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 599/599 [02:20<00:00,  4.26batch/s, accuracy=0.677, grad_norm=0.641, loss=5.02] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Average Gradient Norm: 0.6740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200: 100%|██████████| 599/599 [02:23<00:00,  4.17batch/s, accuracy=1.01, grad_norm=2.11, loss=4.93]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Average Gradient Norm: 1.2128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200:  13%|█▎        | 75/599 [00:18<02:05,  4.17batch/s, accuracy=2.04, grad_norm=0.72, loss=0.587] "
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from  data_utilities import *\n",
    "import cv2  # OpenCV 用于调整图像大小和颜色处理\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gc  # 引入垃圾回收模块\n",
    "from tqdm.auto import tqdm  # 自动适配环境 导入tqdm进度条库\n",
    "from collections import defaultdict\n",
    "\n",
    "dataset_name = 'ManyTx'\n",
    "dataset_path='../ManyTx.pkl/'\n",
    "\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path,dataset_name)\n",
    "\n",
    "print(\"数据集发射机数量：\",len(compact_dataset['tx_list']),\"具体为：\",compact_dataset['tx_list'])\n",
    "print(\"数据集接收机数量：\",len(compact_dataset['rx_list']),\"具体为：\",compact_dataset['rx_list'])\n",
    "print(\"数据集采集天数：\",len(compact_dataset['capture_date_list']),\"具体为：\",compact_dataset['capture_date_list'])\n",
    "\n",
    "# ------------------------ 去掉不参与训练的 TX ------------------------\n",
    "tx_list = compact_dataset['tx_list']\n",
    "exclude_txs = ['12-1', '18-1', '19-4', '20-3', '7-12', '9-14']\n",
    "tx_list = [tx for tx in tx_list if tx not in exclude_txs]\n",
    "\n",
    "print(f\"✅ 剩余用于训练的 TX 数量: {len(tx_list)}\")\n",
    "print(f\"具体为：{tx_list}\")\n",
    "\n",
    "tx_list = compact_dataset['tx_list']\n",
    "rx_list = compact_dataset['rx_list']\n",
    "equalized = 0\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "\n",
    "\n",
    "n_tx = len(tx_list)\n",
    "n_rx = len(rx_list)\n",
    "print(n_tx,n_rx)\n",
    "\n",
    "\n",
    "train_dates = ['2021_03_01', '2021_03_08', '2021_03_15']  # 设定你想用的训练日期\n",
    "X_train, y_train, X_test, y_test = preprocess_dataset_for_classification(\n",
    "    compact_dataset, tx_list, rx_list, train_dates, max_sig=None, equalized = equalized, use_phase_differential=True)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)  # (num_blocks, 256, 250, 2)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# === 参数设置 ===\n",
    "SNR_dB = 20           # 信噪比\n",
    "fs = 20e6             # 采样率 (Hz)\n",
    "fc = 2.4e9            # 载波频率 (Hz)\n",
    "v = 120               # 速度 (km/h)\n",
    "Add_noise = True     # 是否添加噪声\n",
    "Add_doppler = True   # 是否添加多普勒频移\n",
    "Add_diff = True      # 是否进行差分\n",
    "\n",
    "# === 多普勒频移计算 ===\n",
    "def compute_doppler_shift(v, fc):\n",
    "    c = 3e8  # 光速\n",
    "    v = v / 3.6 # 转换为m/s\n",
    "    return (v / c) * fc\n",
    "\n",
    "fd = compute_doppler_shift(v, fc)\n",
    "print(f\"[INFO] 多普勒频移 fd = {fd:.2f} Hz\")\n",
    "\n",
    "# === 多普勒变换 ===\n",
    "def add_doppler_shift(signal, fd, fs):\n",
    "    num_samples = signal.shape[-1]\n",
    "    t = np.arange(num_samples) / fs\n",
    "    doppler_phase = np.exp(1j * 2 * np.pi * fd * t)\n",
    "    return signal * doppler_phase\n",
    "\n",
    "# === SNR测量函数 ===\n",
    "def measure_snr(clean_signal, noisy_signal):\n",
    "    \"\"\"\n",
    "    测量实际SNR\n",
    "    \"\"\"\n",
    "    signal_power = np.mean(np.abs(clean_signal) ** 2)\n",
    "    noise = noisy_signal - clean_signal\n",
    "    noise_power = np.mean(np.abs(noise) ** 2)\n",
    "    \n",
    "    if noise_power == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    snr_measured = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr_measured\n",
    "\n",
    "# === 复数AWGN噪声添加函数 ===\n",
    "def add_complex_awgn(signal, snr_db):\n",
    "    \"\"\"\n",
    "    为复数信号添加AWGN噪声\n",
    "    \"\"\"\n",
    "    # 计算信号功率\n",
    "    signal_power = np.mean(np.abs(signal) ** 2)\n",
    "    \n",
    "    # 计算噪声功率\n",
    "    snr_linear = 10 ** (snr_db / 10)\n",
    "    noise_power = signal_power / snr_linear\n",
    "    \n",
    "    # 生成复数噪声（实部和虚部独立，各占一半功率）\n",
    "    noise_std = np.sqrt(noise_power / 2)\n",
    "    noise_real = np.random.normal(0, noise_std, signal.shape)\n",
    "    noise_imag = np.random.normal(0, noise_std, signal.shape)\n",
    "    noise = noise_real + 1j * noise_imag\n",
    "    \n",
    "    return signal + noise, noise\n",
    "\n",
    "# === IQ信号差分处理 ===\n",
    "def add_iq_differential(signal):\n",
    "    \"\"\"\n",
    "    对IQ信号进行差分处理\n",
    "    输入: 复数信号数组\n",
    "    输出: 差分后的复数信号数组（长度减1）\n",
    "    \"\"\"\n",
    "    # 对复数信号进行一阶差分\n",
    "    diff_signal = np.diff(signal)\n",
    "    return diff_signal\n",
    "\n",
    "# === 带验证的预处理函数 ===\n",
    "def preprocess_iq_data(data_real_imag, snr_db=None, fd=None, fs=None, \n",
    "                       add_noise=True, add_diff=True, add_doppler=True, verify_snr=True):\n",
    "    \"\"\"\n",
    "    预处理IQ数据：可选择性地添加噪声和多普勒频移\n",
    "    \n",
    "    参数:\n",
    "    - data_real_imag: 输入数据，shape (N, T, 2)\n",
    "    - snr_db: 目标信噪比(dB)，当add_noise=True时必需\n",
    "    - fd: 多普勒频移(Hz)，当add_doppler=True时必需\n",
    "    - fs: 采样率(Hz)，当add_doppler=True时必需\n",
    "    - add_noise: 是否添加噪声 (默认True)\n",
    "    - add_doppler: 是否添加多普勒频移 (默认True)\n",
    "    - verify_snr: 是否验证SNR (仅当add_noise=True时有效)\n",
    "    \n",
    "    返回:\n",
    "    - processed_real_imag: 处理后的数据，shape (N, T, 2)\n",
    "    - snr_info: SNR验证信息（如果verify_snr=True且add_noise=True）\n",
    "    \"\"\"\n",
    "    # 参数检查\n",
    "    if add_noise and snr_db is None:\n",
    "        raise ValueError(\"当add_noise=True时，必须提供snr_db参数\")\n",
    "    \n",
    "    if add_doppler and (fd is None or fs is None):\n",
    "        raise ValueError(\"当add_doppler=True时，必须提供fd和fs参数\")\n",
    "    \n",
    "    if Add_diff:\n",
    "        print(\"应用IQ差分处理\")\n",
    "    \n",
    "    # Step 1: 转为复数 IQ，shape: (N, T, 2) → (N, T)\n",
    "    data_complex = data_real_imag[..., 0] + 1j * data_real_imag[..., 1]\n",
    "\n",
    "    processed = []\n",
    "    snr_measured_list = []\n",
    "    \n",
    "    for i, sig in enumerate(data_complex):\n",
    "        current_sig = sig.copy()\n",
    "        \n",
    "        # Step 2: 添加 AWGN 噪声（可选）\n",
    "        if add_noise:\n",
    "            noisy_sig, noise = add_complex_awgn(current_sig, snr_db)\n",
    "            current_sig = noisy_sig\n",
    "            \n",
    "            # SNR验证（可选）\n",
    "            if verify_snr:\n",
    "                measured_snr = measure_snr(sig, noisy_sig)\n",
    "                snr_measured_list.append(measured_snr)\n",
    "                \n",
    "                # 每10000个样本打印一次进度\n",
    "                if i % 10000 == 0 and i > 0:\n",
    "                    avg_snr = np.mean(snr_measured_list[-10000:])\n",
    "                    print(f\"[验证] 样本 {i}, 平均实测SNR: {avg_snr:.2f} dB\")\n",
    "        \n",
    "        # Step 3: 添加多普勒频移（可选）\n",
    "        if add_doppler:\n",
    "            shifted = add_doppler_shift(current_sig, fd, fs)\n",
    "            current_sig = shifted\n",
    "        \n",
    "        # 应用IQ差分\n",
    "        if Add_diff:\n",
    "            diffed = add_iq_differential(current_sig)\n",
    "            current_sig = diffed\n",
    "\n",
    "        processed.append(current_sig)\n",
    "\n",
    "    processed = np.array(processed)\n",
    "    \n",
    "    # Step 4: 转回 [I, Q] 实数格式\n",
    "    processed_real_imag = np.stack([processed.real, processed.imag], axis=-1)\n",
    "    \n",
    "    # SNR验证总结（仅当添加噪声且启用验证时）\n",
    "    snr_info = None\n",
    "    if add_noise and verify_snr and snr_measured_list:\n",
    "        snr_measured_array = np.array(snr_measured_list)\n",
    "        snr_info = {\n",
    "            'target_snr': snr_db,\n",
    "            'measured_mean': np.mean(snr_measured_array),\n",
    "            'measured_std': np.std(snr_measured_array),\n",
    "            'measured_min': np.min(snr_measured_array),\n",
    "            'measured_max': np.max(snr_measured_array),\n",
    "            'samples_count': len(snr_measured_array)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n=== SNR验证结果 ===\")\n",
    "        print(f\"目标SNR: {snr_info['target_snr']} dB\")\n",
    "        print(f\"实测平均SNR: {snr_info['measured_mean']:.2f} dB\")\n",
    "        print(f\"实测标准差: {snr_info['measured_std']:.2f} dB\")\n",
    "        print(f\"实测范围: [{snr_info['measured_min']:.2f}, {snr_info['measured_max']:.2f}] dB\")\n",
    "        print(f\"验证样本数: {snr_info['samples_count']}\")\n",
    "    \n",
    "    return processed_real_imag, snr_info\n",
    "\n",
    "# === 单样本测试函数 ===\n",
    "def test_single_sample_snr(data_real_imag, snr_db, num_tests=10):\n",
    "    \"\"\"\n",
    "    对单个样本进行多次SNR测试，确保噪声添加的正确性\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== 单样本SNR测试 (测试{num_tests}次) ===\")\n",
    "    \n",
    "    # 取第一个样本\n",
    "    sample_complex = data_real_imag[0, :, 0] + 1j * data_real_imag[0, :, 1]\n",
    "    \n",
    "    measured_snrs = []\n",
    "    for i in range(num_tests):\n",
    "        noisy_sample, _ = add_complex_awgn(sample_complex, snr_db)\n",
    "        measured_snr = measure_snr(sample_complex, noisy_sample)\n",
    "        measured_snrs.append(measured_snr)\n",
    "        print(f\"测试 {i+1}: 目标SNR={snr_db} dB, 实测SNR={measured_snr:.2f} dB\")\n",
    "    \n",
    "    measured_snrs = np.array(measured_snrs)\n",
    "    print(f\"\\n测试统计:\")\n",
    "    print(f\"平均值: {np.mean(measured_snrs):.2f} dB\")\n",
    "    print(f\"标准差: {np.std(measured_snrs):.2f} dB\")\n",
    "    print(f\"误差范围: ±{np.abs(np.mean(measured_snrs) - snr_db):.2f} dB\")\n",
    "\n",
    "# === 使用示例 ===\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设 X_train, X_test 已定义\n",
    "    \n",
    "    # 使用全局变量控制处理选项\n",
    "    X_train_processed, _ = preprocess_iq_data(\n",
    "        X_train, snr_db=SNR_dB, fd=fd, fs=fs, \n",
    "        add_noise=Add_noise, add_diff=Add_diff, add_doppler=Add_doppler, verify_snr=False\n",
    "    )\n",
    "    \n",
    "    # 单样本测试（只有当Add_noise=True时才有意义）\n",
    "    if Add_noise:\n",
    "        test_single_sample_snr(X_train, SNR_dB, num_tests=5)\n",
    "    else:\n",
    "        print(\"\\n=== 跳过单样本SNR测试（未启用噪声添加）===\")\n",
    "    \n",
    "    # 处理测试集（可以根据需要设置不同的选项）\n",
    "    print(f\"\\n=== 处理测试集 ===\")\n",
    "    X_test_processed, test_snr_info = preprocess_iq_data(\n",
    "        X_test, snr_db=SNR_dB, fd=fd, fs=fs, \n",
    "        add_noise=Add_noise, add_diff=Add_diff, add_doppler=Add_doppler, verify_snr=False\n",
    "    )\n",
    "    \n",
    "    # 查看处理前后前10个点\n",
    "    print(f\"\\n=== 信号对比 ===\")\n",
    "    print(\"原始信号 I 分量：\", X_train[0, :10, 0])\n",
    "    print(\"处理后信号 I 分量：\", X_train_processed[0, :10, 0])\n",
    "    \n",
    "    # 验证多普勒频移效果（只有当Add_doppler=True时才有意义）\n",
    "    if Add_doppler:\n",
    "        print(f\"\\n=== 多普勒频移验证 ===\")\n",
    "        original_phase = np.angle(X_train[0, :10, 0] + 1j * X_train[0, :10, 1])\n",
    "        processed_phase = np.angle(X_train_processed[0, :10, 0] + 1j * X_train_processed[0, :10, 1])\n",
    "        phase_diff = processed_phase - original_phase\n",
    "        print(\"相位变化:\", phase_diff)\n",
    "    else:\n",
    "        print(f\"\\n=== 跳过多普勒频移验证（未启用多普勒频移）===\")\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# 假设 SNR_dB 和 fd 已经定义\n",
    "SNR_dB = globals().get('SNR_dB', 'no')\n",
    "fd = globals().get('fd', 'no')\n",
    "\n",
    "# === 模型与训练参数设置 ===\n",
    "raw_input_dim = 2         # 每个时间步是 I/Q 两个值\n",
    "model_dim = 128           # Transformer 模型内部维度\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "num_classes = len(np.unique(tx_list))  # 或 len(tx_list)\n",
    "dropout = 0.1\n",
    "batch_size = 512\n",
    "num_epochs = 200\n",
    "learning_rate = 1e-4\n",
    "patience = 5\n",
    "\n",
    "# === 创建保存目录 ===\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "script_name = \"wisig_DCTF\"\n",
    "folder_name = f\"{timestamp}_{script_name}_SNR{SNR_dB}dB_fd{int(fd)}_classes_{num_classes}_Transformer\"\n",
    "save_folder = os.path.join(os.getcwd(), \"training_results\", folder_name)\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "results_file = os.path.join(save_folder, \"results.txt\")\n",
    "with open(results_file, \"w\") as f:\n",
    "    f.write(f\"=== Experiment Summary ===\\n\")\n",
    "    f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "    f.write(f\"Total Classes: {num_classes}\\n\")\n",
    "    f.write(f\"SNR: {SNR_dB} dB\\n\")\n",
    "    f.write(f\"fd (Doppler shift): {fd} Hz\\n\")\n",
    "    f.write(f\"equalized: {equalized}\\n\")\n",
    "\n",
    "# === 模型定义 ===\n",
    "class SignalTransformer(nn.Module):\n",
    "    def __init__(self, raw_input_dim, model_dim, num_heads, num_layers, num_classes, dropout=0.1):\n",
    "        super(SignalTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(raw_input_dim, model_dim)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# === 假设 X_train, y_train, X_test, y_test 都已定义并 shape 为 (N, L, 2) ===\n",
    "# 若还未定义，可自行加载并 reshape\n",
    "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_processed, dtype=torch.float32),\n",
    "                               torch.tensor(y_train, dtype=torch.long))\n",
    "\n",
    "# === K折交叉验证训练 ===\n",
    "n_splits = 5\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "test_results = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def compute_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def moving_average(x, w=5):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "avg_grad_norms_per_fold = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset)):\n",
    "    print(f\"\\n====== Fold {fold+1}/{n_splits} ======\")\n",
    "\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    val_subset = Subset(train_dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    model = SignalTransformer(raw_input_dim, model_dim, num_heads, num_layers, num_classes, dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    grad_norms = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "        batch_grad_norms = []\n",
    "\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as tepoch:\n",
    "            for inputs, labels in tepoch:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "\n",
    "                grad_norm = compute_grad_norm(model)\n",
    "                batch_grad_norms.append(grad_norm)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                running_train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "                tepoch.set_postfix(loss=running_train_loss / (len(train_loader)),\n",
    "                                   accuracy=100 * correct_train / total_train,\n",
    "                                   grad_norm=grad_norm)\n",
    "\n",
    "        epoch_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(100 * correct_train / total_train)\n",
    "        avg_grad_norm = np.mean(batch_grad_norms)\n",
    "        grad_norms.append(avg_grad_norm)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Average Gradient Norm: {avg_grad_norm:.4f}\")\n",
    "\n",
    "        # === 验证 ===\n",
    "        model.eval()\n",
    "        running_val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "                running_val_loss += val_loss.item()\n",
    "                _, val_predicted = torch.max(val_outputs, 1)\n",
    "                total_val += val_labels.size(0)\n",
    "                correct_val += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accuracies.append(100 * correct_val / total_val)\n",
    "\n",
    "        with open(results_file, \"a\") as f:\n",
    "            f.write(f\"Epoch {epoch+1} | Train Acc: {train_accuracies[-1]:.2f}% | Val Acc: {val_accuracies[-1]:.2f}%\\n\")\n",
    "\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    fold_results.append(max(val_accuracies))\n",
    "    avg_grad_norms_per_fold.append(grad_norms)\n",
    "\n",
    "    # === 绘制 loss 曲线 ===\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.plot(moving_average(train_losses), label='Train Loss (Smooth)', linestyle='--')\n",
    "    plt.plot(moving_average(val_losses), label='Val Loss (Smooth)', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {fold+1} Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_folder, f\"fold_{fold+1}_loss_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # === 绘制 Gradient Norm 曲线 ===\n",
    "    plt.figure()\n",
    "    plt.plot(grad_norms, label='Gradient Norm')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.title(f'Fold {fold+1} Gradient Norm')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_folder, f\"fold_{fold+1}_grad_norm.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # === 测试集评估 ===\n",
    "    model.eval()\n",
    "    test_preds, test_true = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_inputs = test_inputs.to(device)\n",
    "            test_labels = test_labels.to(device)\n",
    "\n",
    "            test_outputs = model(test_inputs)\n",
    "            _, predicted = torch.max(test_outputs, 1)\n",
    "            test_preds.extend(predicted.cpu().numpy())\n",
    "            test_true.extend(test_labels.cpu().numpy())\n",
    "\n",
    "    test_preds = np.array(test_preds)\n",
    "    test_true = np.array(test_true)\n",
    "    test_accuracy = 100.0 * np.sum(test_preds == test_true) / len(test_true)\n",
    "    test_results.append(test_accuracy)\n",
    "\n",
    "    with open(results_file, \"a\") as f:\n",
    "        f.write(f\"Fold {fold+1} Test Accuracy: {test_accuracy:.2f}%\\n\")\n",
    "\n",
    "    cm = confusion_matrix(test_true, test_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Test Confusion Matrix Fold {fold+1}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(os.path.join(save_folder, f\"fold_{fold+1}_test_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# === 总结结果 ===\n",
    "avg_val = np.mean(fold_results)\n",
    "avg_test = np.mean(test_results)\n",
    "\n",
    "with open(results_file, \"a\") as f:\n",
    "    f.write(\"\\n=== Summary ===\\n\")\n",
    "    for i in range(n_splits):\n",
    "        f.write(f\"Fold {i+1}: Val Acc = {fold_results[i]:.2f}%, Test Acc = {test_results[i]:.2f}%\\n\")\n",
    "    f.write(f\"\\nAverage Validation Accuracy: {avg_val:.2f}%\\n\")\n",
    "    f.write(f\"Average Test Accuracy: {avg_test:.2f}%\\n\")\n",
    "\n",
    "print(\"\\n=== Final Summary ===\")\n",
    "for i in range(n_splits):\n",
    "    print(f\"Fold {i+1}: Val = {fold_results[i]:.2f}%, Test = {test_results[i]:.2f}%\")\n",
    "print(f\"Average Val Accuracy: {avg_val:.2f}%\")\n",
    "print(f\"Average Test Accuracy: {avg_test:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
