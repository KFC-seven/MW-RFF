{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b307805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共找到 72 个 .mat 文件\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取与处理数据: 100%|██████████| 72/72 [00:03<00:00, 20.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据维度: X=(215928, 288, 2), y=(215928,)\n",
      "类别映射: {'001': 0, '002': 1, '003': 2, '004': 3, '005': 4, '006': 5, '007': 6, '008': 7, '009': 8}\n",
      "\n",
      "===== 随机搜索 1/30 参数: {'model_dim': 128, 'num_heads': 2, 'num_layers': 2, 'dropout': 0.5, 'batch_size': 128, 'learning_rate': 0.0005, 'weight_decay': 0.0001, 'num_epochs': 200, 'patience': 5} =====\n",
      "Epoch 1: Val Loss=2.1957, Val Acc=12.18%\n",
      "Epoch 2: Val Loss=2.1863, Val Acc=12.85%\n",
      "Epoch 3: Val Loss=2.1787, Val Acc=13.51%\n",
      "Epoch 4: Val Loss=2.1697, Val Acc=14.02%\n",
      "Epoch 5: Val Loss=2.1689, Val Acc=14.55%\n",
      "Epoch 6: Val Loss=2.1605, Val Acc=15.38%\n",
      "Epoch 7: Val Loss=2.1581, Val Acc=14.41%\n",
      "Epoch 8: Val Loss=2.1612, Val Acc=14.95%\n",
      "Epoch 9: Val Loss=2.1558, Val Acc=14.92%\n",
      "Epoch 10: Val Loss=2.1622, Val Acc=15.26%\n",
      "Epoch 11: Val Loss=2.1459, Val Acc=14.29%\n",
      "Epoch 12: Val Loss=2.1508, Val Acc=15.10%\n",
      "Epoch 13: Val Loss=2.1414, Val Acc=15.87%\n",
      "Epoch 14: Val Loss=2.1370, Val Acc=16.15%\n",
      "Epoch 15: Val Loss=2.1382, Val Acc=16.43%\n",
      "Epoch 16: Val Loss=2.1364, Val Acc=16.58%\n",
      "Epoch 17: Val Loss=2.1204, Val Acc=17.19%\n",
      "Epoch 18: Val Loss=2.1327, Val Acc=16.82%\n",
      "Epoch 19: Val Loss=2.1318, Val Acc=16.79%\n",
      "Epoch 20: Val Loss=2.1243, Val Acc=17.53%\n",
      "Epoch 21: Val Loss=2.1144, Val Acc=17.55%\n",
      "Epoch 22: Val Loss=2.1103, Val Acc=17.37%\n",
      "Epoch 23: Val Loss=2.1316, Val Acc=17.17%\n",
      "Epoch 24: Val Loss=2.1249, Val Acc=17.23%\n",
      "Epoch 25: Val Loss=2.1172, Val Acc=17.44%\n",
      "Epoch 26: Val Loss=2.1084, Val Acc=18.12%\n",
      "Epoch 27: Val Loss=2.0991, Val Acc=18.27%\n",
      "Epoch 28: Val Loss=2.1152, Val Acc=18.01%\n",
      "Epoch 29: Val Loss=2.1145, Val Acc=17.78%\n",
      "Epoch 30: Val Loss=2.1262, Val Acc=17.45%\n",
      "Epoch 31: Val Loss=2.1291, Val Acc=17.45%\n",
      "Epoch 32: Val Loss=2.1118, Val Acc=18.01%\n",
      "早停触发\n",
      "\n",
      "===== 随机搜索 2/30 参数: {'model_dim': 128, 'num_heads': 2, 'num_layers': 1, 'dropout': 0.5, 'batch_size': 256, 'learning_rate': 0.0005, 'weight_decay': 0.0005, 'num_epochs': 200, 'patience': 5} =====\n",
      "Epoch 1: Val Loss=2.1880, Val Acc=11.80%\n",
      "Epoch 2: Val Loss=2.1857, Val Acc=11.70%\n",
      "Epoch 3: Val Loss=2.1848, Val Acc=12.45%\n",
      "Epoch 4: Val Loss=2.1849, Val Acc=11.71%\n",
      "Epoch 5: Val Loss=2.1834, Val Acc=12.23%\n",
      "Epoch 6: Val Loss=2.1842, Val Acc=12.34%\n",
      "Epoch 7: Val Loss=2.1822, Val Acc=11.17%\n",
      "Epoch 8: Val Loss=2.1831, Val Acc=12.49%\n",
      "Epoch 9: Val Loss=2.1823, Val Acc=12.43%\n",
      "Epoch 10: Val Loss=2.1813, Val Acc=11.85%\n",
      "Epoch 11: Val Loss=2.1816, Val Acc=11.32%\n",
      "Epoch 12: Val Loss=2.1808, Val Acc=12.27%\n",
      "Epoch 13: Val Loss=2.1829, Val Acc=12.47%\n",
      "Epoch 14: Val Loss=2.1795, Val Acc=12.45%\n",
      "Epoch 15: Val Loss=2.1801, Val Acc=12.27%\n",
      "Epoch 16: Val Loss=2.1798, Val Acc=11.91%\n",
      "Epoch 17: Val Loss=2.1804, Val Acc=12.46%\n",
      "Epoch 18: Val Loss=2.1794, Val Acc=12.03%\n",
      "Epoch 19: Val Loss=2.1792, Val Acc=12.53%\n",
      "Epoch 20: Val Loss=2.1817, Val Acc=12.50%\n",
      "Epoch 21: Val Loss=2.1807, Val Acc=12.42%\n",
      "Epoch 22: Val Loss=2.1806, Val Acc=12.80%\n",
      "Epoch 23: Val Loss=2.1803, Val Acc=12.13%\n",
      "Epoch 24: Val Loss=2.1800, Val Acc=12.49%\n",
      "早停触发\n",
      "\n",
      "===== 随机搜索 3/30 参数: {'model_dim': 32, 'num_heads': 4, 'num_layers': 2, 'dropout': 0.5, 'batch_size': 256, 'learning_rate': 0.0005, 'weight_decay': 0.0001, 'num_epochs': 200, 'patience': 5} =====\n",
      "Epoch 1: Val Loss=2.1923, Val Acc=11.78%\n",
      "Epoch 2: Val Loss=2.1911, Val Acc=11.63%\n",
      "Epoch 3: Val Loss=2.1893, Val Acc=12.43%\n",
      "Epoch 4: Val Loss=2.1815, Val Acc=12.57%\n",
      "Epoch 5: Val Loss=2.1702, Val Acc=13.62%\n",
      "Epoch 6: Val Loss=2.1697, Val Acc=13.46%\n",
      "Epoch 7: Val Loss=2.1641, Val Acc=13.68%\n",
      "Epoch 8: Val Loss=2.1651, Val Acc=13.82%\n",
      "Epoch 9: Val Loss=2.1594, Val Acc=14.62%\n",
      "Epoch 10: Val Loss=2.1574, Val Acc=14.16%\n",
      "Epoch 11: Val Loss=2.1582, Val Acc=14.12%\n",
      "Epoch 12: Val Loss=2.1517, Val Acc=14.08%\n",
      "Epoch 13: Val Loss=2.1485, Val Acc=15.72%\n",
      "Epoch 14: Val Loss=2.1444, Val Acc=15.69%\n",
      "Epoch 15: Val Loss=2.1425, Val Acc=15.12%\n",
      "Epoch 16: Val Loss=2.1438, Val Acc=15.52%\n",
      "Epoch 17: Val Loss=2.1426, Val Acc=15.86%\n",
      "Epoch 18: Val Loss=2.1402, Val Acc=15.46%\n",
      "Epoch 19: Val Loss=2.1368, Val Acc=15.86%\n",
      "Epoch 20: Val Loss=2.1364, Val Acc=16.09%\n",
      "Epoch 21: Val Loss=2.1402, Val Acc=15.66%\n",
      "Epoch 22: Val Loss=2.1358, Val Acc=16.27%\n",
      "Epoch 23: Val Loss=2.1324, Val Acc=16.46%\n",
      "Epoch 24: Val Loss=2.1349, Val Acc=16.19%\n",
      "Epoch 25: Val Loss=2.1324, Val Acc=16.75%\n",
      "Epoch 26: Val Loss=2.1280, Val Acc=16.95%\n",
      "Epoch 27: Val Loss=2.1401, Val Acc=16.53%\n",
      "Epoch 28: Val Loss=2.1260, Val Acc=17.14%\n",
      "Epoch 29: Val Loss=2.1208, Val Acc=17.05%\n",
      "Epoch 30: Val Loss=2.1235, Val Acc=16.89%\n",
      "Epoch 31: Val Loss=2.1117, Val Acc=17.59%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "# ================= 数据路径和信号参数 =================\n",
    "data_path = \"E:/rf_datasets/\"\n",
    "SNR_dB = 10\n",
    "fs = 20e6\n",
    "fc = 2.4e9\n",
    "v = 120\n",
    "apply_doppler = False\n",
    "apply_awgn = False\n",
    "\n",
    "# ================= 训练参数 =================\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "batch_size = 256\n",
    "weight_decay = 5e-4\n",
    "\n",
    "# ================= 数据处理函数 =================\n",
    "def compute_doppler_shift(v, fc):\n",
    "    c = 3e8\n",
    "    return (v / c) * fc\n",
    "\n",
    "def apply_doppler_shift(signal, fd, fs):\n",
    "    t = np.arange(signal.shape[-1]) / fs\n",
    "    doppler_phase = np.exp(1j * 2 * np.pi * fd * t)\n",
    "    return signal * doppler_phase\n",
    "\n",
    "def add_awgn(signal, snr_db):\n",
    "    signal_power = np.mean(np.abs(signal)**2)\n",
    "    noise_power = signal_power / (10**(snr_db/10))\n",
    "    noise = np.sqrt(noise_power/2) * (np.random.randn(*signal.shape) + 1j*np.random.randn(*signal.shape))\n",
    "    return signal + noise\n",
    "\n",
    "def load_and_preprocess(mat_folder, apply_doppler=False, target_velocity=30, apply_awgn=False, snr_db=20, fs=20e6, fc=2.4e9):\n",
    "    mat_files = glob.glob(os.path.join(mat_folder, '*.mat'))\n",
    "    print(f\"共找到 {len(mat_files)} 个 .mat 文件\")\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    fd = compute_doppler_shift(target_velocity, fc)\n",
    "\n",
    "    for file in tqdm(mat_files, desc='读取与处理数据'):\n",
    "        with h5py.File(file, 'r') as f:\n",
    "            rfDataset = f['rfDataset']\n",
    "            dmrs_struct = rfDataset['dmrs'][:]\n",
    "            dmrs_complex = dmrs_struct['real'] + 1j * dmrs_struct['imag']\n",
    "            txID_uint16 = rfDataset['txID'][:].flatten()\n",
    "            tx_id = ''.join(chr(c) for c in txID_uint16 if c != 0)\n",
    "\n",
    "        processed_signals = []\n",
    "        for sig in dmrs_complex:\n",
    "            if apply_doppler:\n",
    "                sig = apply_doppler_shift(sig, fd, fs)\n",
    "            if apply_awgn:\n",
    "                sig = add_awgn(sig, snr_db)\n",
    "            iq = np.stack((sig.real, sig.imag), axis=-1)\n",
    "            processed_signals.append(iq)\n",
    "\n",
    "        processed_signals = np.array(processed_signals)\n",
    "        X_list.append(processed_signals)\n",
    "        y_list.append(tx_id)\n",
    "\n",
    "    unique_labels = sorted(list(set(y_list)))\n",
    "    label_to_idx = {lab: i for i, lab in enumerate(unique_labels)}\n",
    "    y_idx = np.array([label_to_idx[lab] for lab in y_list])\n",
    "\n",
    "    X_all = np.concatenate(X_list, axis=0)\n",
    "    y_all = np.repeat(y_idx, dmrs_complex.shape[0])\n",
    "\n",
    "    print(f\"数据维度: X={X_all.shape}, y={y_all.shape}\")\n",
    "    print(f\"类别映射: {label_to_idx}\")\n",
    "    return X_all, y_all, label_to_idx\n",
    "\n",
    "# ================= 模型 =================\n",
    "class SignalTransformer(nn.Module):\n",
    "    def __init__(self, raw_input_dim, model_dim, num_heads, num_layers, num_classes, dropout=0.4):\n",
    "        super(SignalTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(raw_input_dim, model_dim)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ================= 工具函数 =================\n",
    "def compute_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=range(num_classes))\n",
    "    return acc, cm\n",
    "\n",
    "# ================= 随机搜索训练 =================\n",
    "def random_search_train(X_all, y_all, label_to_idx, param_grid, n_iter=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_tensor = torch.tensor(X_all, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_all, dtype=torch.long)\n",
    "    full_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "    # 划分 60% 训练，20% 验证，20% 测试\n",
    "    indices = np.arange(len(full_dataset))\n",
    "    train_idx, temp_idx, y_train, y_temp = train_test_split(\n",
    "        indices, y_all, test_size=0.4, stratify=y_all, random_state=42\n",
    "    )\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        temp_idx, test_size=0.5, stratify=y_temp, random_state=42\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(Subset(full_dataset, train_idx), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(Subset(full_dataset, val_idx), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(Subset(full_dataset, test_idx), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_params = None\n",
    "    best_model_state = None\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "        print(f\"\\n===== 随机搜索 {i+1}/{n_iter} 参数: {params} =====\")\n",
    "\n",
    "        model = SignalTransformer(\n",
    "            raw_input_dim=2,\n",
    "            model_dim=params[\"model_dim\"],\n",
    "            num_heads=params[\"num_heads\"],\n",
    "            num_layers=params[\"num_layers\"],\n",
    "            num_classes=len(label_to_idx),\n",
    "            dropout=params[\"dropout\"]\n",
    "        ).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"], weight_decay=weight_decay)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # 验证\n",
    "            model.eval()\n",
    "            val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    total_val += labels.size(0)\n",
    "                    correct_val += (preds == labels).sum().item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_acc = 100 * correct_val / total_val\n",
    "            # print(f\"Epoch {epoch+1}: Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%\")\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                model_state = model.state_dict()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"早停触发\")\n",
    "                    break\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_params = params\n",
    "            best_model_state = model_state\n",
    "\n",
    "    print(f\"\\n最佳验证集准确率: {best_acc:.2f}% 参数: {best_params}\")\n",
    "\n",
    "    # 用最佳参数在测试集评估\n",
    "    best_model = SignalTransformer(\n",
    "        raw_input_dim=2,\n",
    "        model_dim=best_params[\"model_dim\"],\n",
    "        num_heads=best_params[\"num_heads\"],\n",
    "        num_layers=best_params[\"num_layers\"],\n",
    "        num_classes=len(label_to_idx),\n",
    "        dropout=best_params[\"dropout\"]\n",
    "    ).to(device)\n",
    "    best_model.load_state_dict(best_model_state)\n",
    "    test_acc, cm = evaluate_model(best_model, test_loader, device, len(label_to_idx))\n",
    "    print(f\"测试集准确率: {test_acc:.2f}%\")\n",
    "\n",
    "    return best_params, best_acc, test_acc\n",
    "\n",
    "# ================= 主函数 =================\n",
    "if __name__ == \"__main__\":\n",
    "    X_all, y_all, label_to_idx = load_and_preprocess(\n",
    "        data_path,\n",
    "        apply_doppler=apply_doppler,\n",
    "        target_velocity=v,\n",
    "        apply_awgn=apply_awgn,\n",
    "        snr_db=SNR_dB,\n",
    "        fs=fs,\n",
    "        fc=fc\n",
    "    )\n",
    "\n",
    "    # 搜索空间\n",
    "    param_grid = {\n",
    "        'model_dim': [32, 64, 128, 256],\n",
    "        'num_heads': [2, 4, 8],\n",
    "        'num_layers': [1, 2],\n",
    "        'dropout': [0.1, 0.3, 0.5],\n",
    "        'batch_size': [128, 256],\n",
    "        'learning_rate': [1e-4, 5e-4, 1e-3],\n",
    "        'weight_decay': [1e-4, 5e-4],\n",
    "        'num_epochs': [200],\n",
    "        'patience': [5]\n",
    "    }\n",
    "\n",
    "    random_search_train(X_all, y_all, label_to_idx, param_grid, n_iter=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
