{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4117f770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================== 当前实验 SNR=20 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "[INFO] 共找到 72 个 .mat 文件（含子目录）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 可用 TX 类别数（train/test 交集）: 9\n",
      "[INFO] Train blocks: 360, Test blocks: 360, sample_len: 288, group_size: 288\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.0356, Train Acc=22.80%, Val Loss=2.3679, Val Acc=24.69%, Grad Norm=5.7252\n",
      "Fold 1, Epoch 2: Train Loss=1.7018, Train Acc=36.65%, Val Loss=1.9641, Val Acc=31.39%, Grad Norm=4.6021\n",
      "Fold 1, Epoch 3: Train Loss=1.5346, Train Acc=42.70%, Val Loss=1.8102, Val Acc=36.90%, Grad Norm=4.2821\n",
      "Fold 1, Epoch 4: Train Loss=1.3939, Train Acc=48.26%, Val Loss=1.6346, Val Acc=43.98%, Grad Norm=4.2528\n",
      "Fold 1, Epoch 5: Train Loss=1.2901, Train Acc=52.38%, Val Loss=1.5332, Val Acc=47.33%, Grad Norm=4.1628\n",
      "Fold 1, Epoch 6: Train Loss=1.2191, Train Acc=55.22%, Val Loss=1.4974, Val Acc=49.17%, Grad Norm=4.0656\n",
      "Fold 1, Epoch 7: Train Loss=1.1647, Train Acc=57.41%, Val Loss=1.4768, Val Acc=49.74%, Grad Norm=4.0455\n",
      "Fold 1, Epoch 8: Train Loss=1.1168, Train Acc=59.44%, Val Loss=1.4112, Val Acc=51.29%, Grad Norm=4.0635\n",
      "Fold 1, Epoch 9: Train Loss=1.0640, Train Acc=61.27%, Val Loss=1.4250, Val Acc=53.31%, Grad Norm=4.1444\n",
      "Fold 1, Epoch 10: Train Loss=1.0264, Train Acc=62.51%, Val Loss=1.3726, Val Acc=53.79%, Grad Norm=4.2341\n",
      "Fold 1, Epoch 11: Train Loss=0.9472, Train Acc=65.87%, Val Loss=1.3010, Val Acc=56.17%, Grad Norm=4.4102\n",
      "Fold 1, Epoch 12: Train Loss=0.9103, Train Acc=67.25%, Val Loss=1.2439, Val Acc=58.25%, Grad Norm=4.7049\n",
      "Fold 1, Epoch 13: Train Loss=0.8751, Train Acc=68.71%, Val Loss=1.2191, Val Acc=59.16%, Grad Norm=4.9767\n",
      "Fold 1, Epoch 14: Train Loss=0.8395, Train Acc=69.98%, Val Loss=1.2679, Val Acc=57.51%, Grad Norm=5.1887\n",
      "Fold 1, Epoch 15: Train Loss=0.8107, Train Acc=71.17%, Val Loss=1.2241, Val Acc=59.36%, Grad Norm=5.4516\n",
      "Fold 1, Epoch 16: Train Loss=0.7807, Train Acc=72.22%, Val Loss=1.1987, Val Acc=60.46%, Grad Norm=5.6935\n",
      "Fold 1, Epoch 17: Train Loss=0.7498, Train Acc=73.31%, Val Loss=1.1989, Val Acc=61.23%, Grad Norm=5.9425\n",
      "Fold 1, Epoch 18: Train Loss=0.7200, Train Acc=74.55%, Val Loss=1.1819, Val Acc=61.91%, Grad Norm=6.1982\n",
      "Fold 1, Epoch 19: Train Loss=0.6908, Train Acc=75.50%, Val Loss=1.1542, Val Acc=61.76%, Grad Norm=6.4039\n",
      "Fold 1, Epoch 20: Train Loss=0.6682, Train Acc=76.44%, Val Loss=1.2060, Val Acc=61.04%, Grad Norm=6.6619\n",
      "Fold 1, Epoch 21: Train Loss=0.6041, Train Acc=78.61%, Val Loss=1.1466, Val Acc=63.52%, Grad Norm=6.8507\n",
      "Fold 1, Epoch 22: Train Loss=0.5801, Train Acc=79.60%, Val Loss=1.1554, Val Acc=63.45%, Grad Norm=7.2557\n",
      "Fold 1, Epoch 23: Train Loss=0.5567, Train Acc=80.55%, Val Loss=1.1411, Val Acc=64.05%, Grad Norm=7.4969\n",
      "Fold 1, Epoch 24: Train Loss=0.5390, Train Acc=81.18%, Val Loss=1.1460, Val Acc=63.50%, Grad Norm=7.8232\n",
      "Fold 1, Epoch 25: Train Loss=0.5192, Train Acc=81.81%, Val Loss=1.1852, Val Acc=63.55%, Grad Norm=8.0710\n",
      "Fold 1, Epoch 26: Train Loss=0.5055, Train Acc=82.13%, Val Loss=1.1482, Val Acc=63.85%, Grad Norm=8.3465\n",
      "Fold 1, Epoch 27: Train Loss=0.4857, Train Acc=83.00%, Val Loss=1.1574, Val Acc=64.02%, Grad Norm=8.5330\n",
      "Fold 1, Epoch 28: Train Loss=0.4674, Train Acc=83.69%, Val Loss=1.1412, Val Acc=64.47%, Grad Norm=8.7971\n",
      "Fold 1, Epoch 29: Train Loss=0.4476, Train Acc=84.33%, Val Loss=1.1494, Val Acc=64.78%, Grad Norm=9.0000\n",
      "Fold 1, Epoch 30: Train Loss=0.4322, Train Acc=84.96%, Val Loss=1.1442, Val Acc=64.58%, Grad Norm=9.2082\n",
      "Fold 1, Epoch 31: Train Loss=0.4025, Train Acc=85.98%, Val Loss=1.1317, Val Acc=64.89%, Grad Norm=9.3335\n",
      "Fold 1, Epoch 32: Train Loss=0.3876, Train Acc=86.68%, Val Loss=1.1882, Val Acc=64.19%, Grad Norm=9.5230\n",
      "Fold 1, Epoch 33: Train Loss=0.3741, Train Acc=87.00%, Val Loss=1.1629, Val Acc=64.88%, Grad Norm=9.6510\n",
      "Fold 1, Epoch 34: Train Loss=0.3651, Train Acc=87.49%, Val Loss=1.2046, Val Acc=64.18%, Grad Norm=9.8497\n",
      "Fold 1, Epoch 35: Train Loss=0.3517, Train Acc=88.09%, Val Loss=1.1849, Val Acc=64.96%, Grad Norm=9.9478\n",
      "Fold 1, Epoch 36: Train Loss=0.3438, Train Acc=88.18%, Val Loss=1.2099, Val Acc=64.61%, Grad Norm=10.1562\n",
      "Fold 1, Epoch 37: Train Loss=0.3336, Train Acc=88.61%, Val Loss=1.1958, Val Acc=64.41%, Grad Norm=10.3177\n",
      "Fold 1, Epoch 38: Train Loss=0.3257, Train Acc=88.80%, Val Loss=1.1948, Val Acc=64.87%, Grad Norm=10.4402\n",
      "Fold 1, Epoch 39: Train Loss=0.3155, Train Acc=89.28%, Val Loss=1.2232, Val Acc=64.38%, Grad Norm=10.4982\n",
      "Fold 1, Epoch 40: Train Loss=0.3103, Train Acc=89.50%, Val Loss=1.2713, Val Acc=64.06%, Grad Norm=10.7245\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc (Test=10km/h) = 14.85%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.0387, Train Acc=22.02%, Val Loss=2.6002, Val Acc=19.16%, Grad Norm=5.4073\n",
      "Fold 2, Epoch 2: Train Loss=1.7168, Train Acc=34.97%, Val Loss=1.9426, Val Acc=33.37%, Grad Norm=4.5527\n",
      "Fold 2, Epoch 3: Train Loss=1.5363, Train Acc=42.18%, Val Loss=1.6940, Val Acc=40.71%, Grad Norm=4.3056\n",
      "Fold 2, Epoch 4: Train Loss=1.3850, Train Acc=48.41%, Val Loss=1.5518, Val Acc=46.20%, Grad Norm=4.3301\n",
      "Fold 2, Epoch 5: Train Loss=1.2705, Train Acc=53.02%, Val Loss=1.3692, Val Acc=49.86%, Grad Norm=4.2686\n",
      "Fold 2, Epoch 6: Train Loss=1.1951, Train Acc=55.95%, Val Loss=1.3498, Val Acc=51.85%, Grad Norm=4.1672\n",
      "Fold 2, Epoch 7: Train Loss=1.1362, Train Acc=58.20%, Val Loss=1.3189, Val Acc=52.42%, Grad Norm=4.1726\n",
      "Fold 2, Epoch 8: Train Loss=1.0873, Train Acc=60.13%, Val Loss=1.2141, Val Acc=56.15%, Grad Norm=4.2138\n",
      "Fold 2, Epoch 9: Train Loss=1.0365, Train Acc=62.07%, Val Loss=1.2462, Val Acc=55.90%, Grad Norm=4.2360\n",
      "Fold 2, Epoch 10: Train Loss=0.9941, Train Acc=63.82%, Val Loss=1.1894, Val Acc=56.94%, Grad Norm=4.3289\n",
      "Fold 2, Epoch 11: Train Loss=0.9049, Train Acc=67.56%, Val Loss=1.1620, Val Acc=57.95%, Grad Norm=4.4889\n",
      "Fold 2, Epoch 12: Train Loss=0.8691, Train Acc=68.88%, Val Loss=1.1621, Val Acc=58.56%, Grad Norm=4.7680\n",
      "Fold 2, Epoch 13: Train Loss=0.8359, Train Acc=69.91%, Val Loss=1.1057, Val Acc=61.54%, Grad Norm=5.0242\n",
      "Fold 2, Epoch 14: Train Loss=0.8068, Train Acc=71.23%, Val Loss=1.1231, Val Acc=61.18%, Grad Norm=5.2808\n",
      "Fold 2, Epoch 15: Train Loss=0.7800, Train Acc=72.29%, Val Loss=1.1126, Val Acc=61.69%, Grad Norm=5.4878\n",
      "Fold 2, Epoch 16: Train Loss=0.7469, Train Acc=73.49%, Val Loss=1.0916, Val Acc=62.52%, Grad Norm=5.7692\n",
      "Fold 2, Epoch 17: Train Loss=0.7183, Train Acc=74.49%, Val Loss=1.0498, Val Acc=64.36%, Grad Norm=5.9693\n",
      "Fold 2, Epoch 18: Train Loss=0.6880, Train Acc=75.61%, Val Loss=1.1048, Val Acc=62.86%, Grad Norm=6.1940\n",
      "Fold 2, Epoch 19: Train Loss=0.6655, Train Acc=76.53%, Val Loss=1.0358, Val Acc=64.73%, Grad Norm=6.4163\n",
      "Fold 2, Epoch 20: Train Loss=0.6368, Train Acc=77.46%, Val Loss=1.0735, Val Acc=64.33%, Grad Norm=6.6825\n",
      "Fold 2, Epoch 21: Train Loss=0.5755, Train Acc=79.78%, Val Loss=1.0285, Val Acc=65.22%, Grad Norm=6.8468\n",
      "Fold 2, Epoch 22: Train Loss=0.5532, Train Acc=80.63%, Val Loss=1.0526, Val Acc=65.22%, Grad Norm=7.1589\n",
      "Fold 2, Epoch 23: Train Loss=0.5307, Train Acc=81.49%, Val Loss=1.0152, Val Acc=66.55%, Grad Norm=7.4564\n",
      "Fold 2, Epoch 24: Train Loss=0.5115, Train Acc=82.05%, Val Loss=1.0085, Val Acc=66.60%, Grad Norm=7.7155\n",
      "Fold 2, Epoch 25: Train Loss=0.4952, Train Acc=82.74%, Val Loss=1.0448, Val Acc=65.97%, Grad Norm=7.9434\n",
      "Fold 2, Epoch 26: Train Loss=0.4739, Train Acc=83.43%, Val Loss=1.0385, Val Acc=65.96%, Grad Norm=8.1829\n",
      "Fold 2, Epoch 27: Train Loss=0.4541, Train Acc=84.08%, Val Loss=1.0334, Val Acc=65.82%, Grad Norm=8.4026\n",
      "Fold 2, Epoch 28: Train Loss=0.4434, Train Acc=84.56%, Val Loss=1.0778, Val Acc=65.89%, Grad Norm=8.6783\n",
      "Fold 2, Epoch 29: Train Loss=0.4254, Train Acc=85.23%, Val Loss=1.0830, Val Acc=66.39%, Grad Norm=8.8866\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc (Test=10km/h) = 16.41%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.0342, Train Acc=21.80%, Val Loss=2.9805, Val Acc=19.50%, Grad Norm=5.9762\n",
      "Fold 3, Epoch 2: Train Loss=1.7318, Train Acc=34.92%, Val Loss=2.0229, Val Acc=32.35%, Grad Norm=4.5993\n",
      "Fold 3, Epoch 3: Train Loss=1.5602, Train Acc=41.52%, Val Loss=1.7406, Val Acc=39.05%, Grad Norm=4.2147\n",
      "Fold 3, Epoch 4: Train Loss=1.4544, Train Acc=45.73%, Val Loss=1.5692, Val Acc=43.31%, Grad Norm=4.1125\n",
      "Fold 3, Epoch 5: Train Loss=1.3219, Train Acc=51.31%, Val Loss=1.5089, Val Acc=45.64%, Grad Norm=4.1874\n",
      "Fold 3, Epoch 6: Train Loss=1.2396, Train Acc=54.33%, Val Loss=1.3176, Val Acc=52.06%, Grad Norm=4.1228\n",
      "Fold 3, Epoch 7: Train Loss=1.1803, Train Acc=56.66%, Val Loss=1.3343, Val Acc=52.05%, Grad Norm=4.1160\n",
      "Fold 3, Epoch 8: Train Loss=1.1263, Train Acc=58.73%, Val Loss=1.2439, Val Acc=55.06%, Grad Norm=4.1228\n",
      "Fold 3, Epoch 9: Train Loss=1.0747, Train Acc=60.76%, Val Loss=1.2079, Val Acc=56.30%, Grad Norm=4.1927\n",
      "Fold 3, Epoch 10: Train Loss=1.0203, Train Acc=62.93%, Val Loss=1.1538, Val Acc=58.77%, Grad Norm=4.3137\n",
      "Fold 3, Epoch 11: Train Loss=0.9408, Train Acc=66.25%, Val Loss=1.0773, Val Acc=62.04%, Grad Norm=4.4353\n",
      "Fold 3, Epoch 12: Train Loss=0.9018, Train Acc=67.52%, Val Loss=1.0726, Val Acc=62.34%, Grad Norm=4.7563\n",
      "Fold 3, Epoch 13: Train Loss=0.8665, Train Acc=69.01%, Val Loss=1.0737, Val Acc=62.89%, Grad Norm=4.9927\n",
      "Fold 3, Epoch 14: Train Loss=0.8365, Train Acc=70.07%, Val Loss=1.0380, Val Acc=64.34%, Grad Norm=5.2179\n",
      "Fold 3, Epoch 15: Train Loss=0.8089, Train Acc=70.88%, Val Loss=1.0401, Val Acc=64.28%, Grad Norm=5.4829\n",
      "Fold 3, Epoch 16: Train Loss=0.7790, Train Acc=72.23%, Val Loss=0.9998, Val Acc=65.56%, Grad Norm=5.7184\n",
      "Fold 3, Epoch 17: Train Loss=0.7487, Train Acc=73.46%, Val Loss=0.9863, Val Acc=66.27%, Grad Norm=5.9588\n",
      "Fold 3, Epoch 18: Train Loss=0.7195, Train Acc=74.39%, Val Loss=1.0401, Val Acc=65.42%, Grad Norm=6.2143\n",
      "Fold 3, Epoch 19: Train Loss=0.6869, Train Acc=75.79%, Val Loss=0.9726, Val Acc=67.63%, Grad Norm=6.4514\n",
      "Fold 3, Epoch 20: Train Loss=0.6581, Train Acc=76.79%, Val Loss=0.9547, Val Acc=67.89%, Grad Norm=6.6735\n",
      "Fold 3, Epoch 21: Train Loss=0.6030, Train Acc=78.81%, Val Loss=0.9270, Val Acc=69.28%, Grad Norm=6.8770\n",
      "Fold 3, Epoch 22: Train Loss=0.5781, Train Acc=79.62%, Val Loss=0.9357, Val Acc=68.79%, Grad Norm=7.2172\n",
      "Fold 3, Epoch 23: Train Loss=0.5560, Train Acc=80.47%, Val Loss=0.9898, Val Acc=67.70%, Grad Norm=7.5136\n",
      "Fold 3, Epoch 24: Train Loss=0.5352, Train Acc=81.33%, Val Loss=0.9556, Val Acc=68.85%, Grad Norm=7.7668\n",
      "Fold 3, Epoch 25: Train Loss=0.5214, Train Acc=81.83%, Val Loss=0.9764, Val Acc=68.56%, Grad Norm=8.0538\n",
      "Fold 3, Epoch 26: Train Loss=0.5026, Train Acc=82.33%, Val Loss=0.9723, Val Acc=68.35%, Grad Norm=8.3061\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc (Test=10km/h) = 16.14%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.0567, Train Acc=21.83%, Val Loss=2.5593, Val Acc=21.97%, Grad Norm=5.3452\n",
      "Fold 4, Epoch 2: Train Loss=1.7327, Train Acc=35.50%, Val Loss=1.8490, Val Acc=33.74%, Grad Norm=4.4998\n",
      "Fold 4, Epoch 3: Train Loss=1.5692, Train Acc=41.81%, Val Loss=1.7414, Val Acc=35.71%, Grad Norm=4.1559\n",
      "Fold 4, Epoch 4: Train Loss=1.4694, Train Acc=45.77%, Val Loss=1.6314, Val Acc=40.21%, Grad Norm=4.0703\n",
      "Fold 4, Epoch 5: Train Loss=1.3577, Train Acc=50.02%, Val Loss=1.4950, Val Acc=47.60%, Grad Norm=4.1636\n",
      "Fold 4, Epoch 6: Train Loss=1.2593, Train Acc=53.87%, Val Loss=1.3007, Val Acc=53.40%, Grad Norm=4.1470\n",
      "Fold 4, Epoch 7: Train Loss=1.1927, Train Acc=56.31%, Val Loss=1.2667, Val Acc=54.21%, Grad Norm=4.1226\n",
      "Fold 4, Epoch 8: Train Loss=1.1366, Train Acc=58.68%, Val Loss=1.2810, Val Acc=53.88%, Grad Norm=4.1471\n",
      "Fold 4, Epoch 9: Train Loss=1.0934, Train Acc=60.34%, Val Loss=1.2249, Val Acc=56.44%, Grad Norm=4.1755\n",
      "Fold 4, Epoch 10: Train Loss=1.0491, Train Acc=61.95%, Val Loss=1.1891, Val Acc=57.62%, Grad Norm=4.3155\n",
      "Fold 4, Epoch 11: Train Loss=0.9680, Train Acc=65.08%, Val Loss=1.1204, Val Acc=60.83%, Grad Norm=4.4770\n",
      "Fold 4, Epoch 12: Train Loss=0.9247, Train Acc=66.93%, Val Loss=1.0848, Val Acc=61.68%, Grad Norm=4.8003\n",
      "Fold 4, Epoch 13: Train Loss=0.8861, Train Acc=68.39%, Val Loss=1.0980, Val Acc=61.80%, Grad Norm=5.1096\n",
      "Fold 4, Epoch 14: Train Loss=0.8532, Train Acc=69.56%, Val Loss=1.0797, Val Acc=62.96%, Grad Norm=5.3397\n",
      "Fold 4, Epoch 15: Train Loss=0.8247, Train Acc=70.57%, Val Loss=1.0651, Val Acc=63.67%, Grad Norm=5.6032\n",
      "Fold 4, Epoch 16: Train Loss=0.7958, Train Acc=71.60%, Val Loss=1.0569, Val Acc=63.55%, Grad Norm=5.8270\n",
      "Fold 4, Epoch 17: Train Loss=0.7627, Train Acc=72.95%, Val Loss=1.0342, Val Acc=64.86%, Grad Norm=6.1048\n",
      "Fold 4, Epoch 18: Train Loss=0.7336, Train Acc=73.94%, Val Loss=1.0505, Val Acc=64.90%, Grad Norm=6.3784\n",
      "Fold 4, Epoch 19: Train Loss=0.7018, Train Acc=75.20%, Val Loss=1.0529, Val Acc=64.57%, Grad Norm=6.5974\n",
      "Fold 4, Epoch 20: Train Loss=0.6722, Train Acc=76.24%, Val Loss=1.0179, Val Acc=65.76%, Grad Norm=6.8440\n",
      "Fold 4, Epoch 21: Train Loss=0.6107, Train Acc=78.70%, Val Loss=0.9898, Val Acc=66.92%, Grad Norm=6.9973\n",
      "Fold 4, Epoch 22: Train Loss=0.5857, Train Acc=79.48%, Val Loss=1.0455, Val Acc=66.50%, Grad Norm=7.4285\n",
      "Fold 4, Epoch 23: Train Loss=0.5639, Train Acc=80.27%, Val Loss=1.0052, Val Acc=67.03%, Grad Norm=7.6571\n",
      "Fold 4, Epoch 24: Train Loss=0.5441, Train Acc=81.08%, Val Loss=1.0113, Val Acc=66.86%, Grad Norm=7.9720\n",
      "Fold 4, Epoch 25: Train Loss=0.5266, Train Acc=81.69%, Val Loss=0.9888, Val Acc=67.81%, Grad Norm=8.2276\n",
      "Fold 4, Epoch 26: Train Loss=0.5103, Train Acc=82.17%, Val Loss=1.0556, Val Acc=66.62%, Grad Norm=8.4921\n",
      "Fold 4, Epoch 27: Train Loss=0.4924, Train Acc=82.89%, Val Loss=1.0421, Val Acc=66.96%, Grad Norm=8.6929\n",
      "Fold 4, Epoch 28: Train Loss=0.4740, Train Acc=83.43%, Val Loss=1.0193, Val Acc=67.28%, Grad Norm=8.9541\n",
      "Fold 4, Epoch 29: Train Loss=0.4598, Train Acc=83.98%, Val Loss=1.0196, Val Acc=67.92%, Grad Norm=9.1844\n",
      "Fold 4, Epoch 30: Train Loss=0.4366, Train Acc=84.66%, Val Loss=1.0224, Val Acc=67.69%, Grad Norm=9.3051\n",
      "Fold 4, Epoch 31: Train Loss=0.4029, Train Acc=85.98%, Val Loss=1.0449, Val Acc=67.28%, Grad Norm=9.4037\n",
      "Fold 4, Epoch 32: Train Loss=0.3915, Train Acc=86.43%, Val Loss=1.0595, Val Acc=67.39%, Grad Norm=9.7008\n",
      "Fold 4, Epoch 33: Train Loss=0.3805, Train Acc=86.97%, Val Loss=1.0817, Val Acc=66.89%, Grad Norm=9.8037\n",
      "Fold 4, Epoch 34: Train Loss=0.3688, Train Acc=87.39%, Val Loss=1.0967, Val Acc=67.08%, Grad Norm=10.0027\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc (Test=10km/h) = 16.05%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.0473, Train Acc=21.63%, Val Loss=2.6821, Val Acc=21.99%, Grad Norm=5.5330\n",
      "Fold 5, Epoch 2: Train Loss=1.7220, Train Acc=35.15%, Val Loss=2.1059, Val Acc=30.58%, Grad Norm=4.5373\n",
      "Fold 5, Epoch 3: Train Loss=1.5578, Train Acc=41.38%, Val Loss=2.0220, Val Acc=36.02%, Grad Norm=4.1867\n",
      "Fold 5, Epoch 4: Train Loss=1.4222, Train Acc=46.83%, Val Loss=1.5491, Val Acc=45.89%, Grad Norm=4.2368\n",
      "Fold 5, Epoch 5: Train Loss=1.2998, Train Acc=51.80%, Val Loss=1.5922, Val Acc=46.74%, Grad Norm=4.1951\n",
      "Fold 5, Epoch 6: Train Loss=1.2221, Train Acc=54.85%, Val Loss=1.5066, Val Acc=48.88%, Grad Norm=4.1328\n",
      "Fold 5, Epoch 7: Train Loss=1.1654, Train Acc=56.96%, Val Loss=1.5558, Val Acc=49.62%, Grad Norm=4.1057\n",
      "Fold 5, Epoch 8: Train Loss=1.1114, Train Acc=59.20%, Val Loss=1.3961, Val Acc=52.30%, Grad Norm=4.1185\n",
      "Fold 5, Epoch 9: Train Loss=1.0711, Train Acc=60.71%, Val Loss=1.3729, Val Acc=54.95%, Grad Norm=4.1829\n",
      "Fold 5, Epoch 10: Train Loss=1.0207, Train Acc=62.71%, Val Loss=1.3811, Val Acc=54.86%, Grad Norm=4.2680\n",
      "Fold 5, Epoch 11: Train Loss=0.9368, Train Acc=66.14%, Val Loss=1.3303, Val Acc=56.79%, Grad Norm=4.4395\n",
      "Fold 5, Epoch 12: Train Loss=0.8945, Train Acc=67.72%, Val Loss=1.3027, Val Acc=58.47%, Grad Norm=4.7536\n",
      "Fold 5, Epoch 13: Train Loss=0.8676, Train Acc=68.70%, Val Loss=1.3692, Val Acc=57.11%, Grad Norm=5.0222\n",
      "Fold 5, Epoch 14: Train Loss=0.8343, Train Acc=69.98%, Val Loss=1.2506, Val Acc=59.76%, Grad Norm=5.2426\n",
      "Fold 5, Epoch 15: Train Loss=0.8103, Train Acc=71.03%, Val Loss=1.3243, Val Acc=59.11%, Grad Norm=5.4898\n",
      "Fold 5, Epoch 16: Train Loss=0.7794, Train Acc=72.10%, Val Loss=1.3021, Val Acc=59.54%, Grad Norm=5.7222\n",
      "Fold 5, Epoch 17: Train Loss=0.7507, Train Acc=73.35%, Val Loss=1.2308, Val Acc=60.72%, Grad Norm=5.9712\n",
      "Fold 5, Epoch 18: Train Loss=0.7250, Train Acc=74.15%, Val Loss=1.2450, Val Acc=61.64%, Grad Norm=6.2456\n",
      "Fold 5, Epoch 19: Train Loss=0.6925, Train Acc=75.40%, Val Loss=1.2439, Val Acc=62.76%, Grad Norm=6.4491\n",
      "Fold 5, Epoch 20: Train Loss=0.6641, Train Acc=76.45%, Val Loss=1.2449, Val Acc=61.99%, Grad Norm=6.6903\n",
      "Fold 5, Epoch 21: Train Loss=0.6089, Train Acc=78.52%, Val Loss=1.1934, Val Acc=63.56%, Grad Norm=6.9499\n",
      "Fold 5, Epoch 22: Train Loss=0.5798, Train Acc=79.52%, Val Loss=1.2309, Val Acc=62.98%, Grad Norm=7.2797\n",
      "Fold 5, Epoch 23: Train Loss=0.5627, Train Acc=80.15%, Val Loss=1.1971, Val Acc=63.91%, Grad Norm=7.6000\n",
      "Fold 5, Epoch 24: Train Loss=0.5411, Train Acc=81.04%, Val Loss=1.2418, Val Acc=63.74%, Grad Norm=7.8507\n",
      "Fold 5, Epoch 25: Train Loss=0.5229, Train Acc=81.66%, Val Loss=1.1980, Val Acc=64.71%, Grad Norm=8.1233\n",
      "Fold 5, Epoch 26: Train Loss=0.5052, Train Acc=82.28%, Val Loss=1.2118, Val Acc=64.40%, Grad Norm=8.3886\n",
      "Fold 5, Epoch 27: Train Loss=0.4881, Train Acc=82.89%, Val Loss=1.2529, Val Acc=63.91%, Grad Norm=8.6762\n",
      "Fold 5, Epoch 28: Train Loss=0.4725, Train Acc=83.50%, Val Loss=1.2476, Val Acc=64.28%, Grad Norm=8.9089\n",
      "Fold 5, Epoch 29: Train Loss=0.4553, Train Acc=84.19%, Val Loss=1.2431, Val Acc=64.41%, Grad Norm=9.1001\n",
      "Fold 5, Epoch 30: Train Loss=0.4398, Train Acc=84.72%, Val Loss=1.2555, Val Acc=64.69%, Grad Norm=9.2629\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc (Test=10km/h) = 15.80%\n",
      "\n",
      "SNR  20 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-23_22-36-39_LTE-V_XFR_SpeedSplit_FileBlock_Train20kmh_Test10kmh_SNR20dB_fdTrain655_fdTest655_group288_ResNet1D\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=15 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "[INFO] 共找到 72 个 .mat 文件（含子目录）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 可用 TX 类别数（train/test 交集）: 9\n",
      "[INFO] Train blocks: 360, Test blocks: 360, sample_len: 288, group_size: 288\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.0411, Train Acc=22.50%, Val Loss=2.7752, Val Acc=19.17%, Grad Norm=5.8207\n",
      "Fold 1, Epoch 2: Train Loss=1.7208, Train Acc=36.09%, Val Loss=2.1172, Val Acc=31.11%, Grad Norm=4.6940\n",
      "Fold 1, Epoch 3: Train Loss=1.5370, Train Acc=43.05%, Val Loss=1.8249, Val Acc=36.57%, Grad Norm=4.2640\n",
      "Fold 1, Epoch 4: Train Loss=1.4326, Train Acc=46.92%, Val Loss=1.7085, Val Acc=40.77%, Grad Norm=4.1493\n",
      "Fold 1, Epoch 5: Train Loss=1.3255, Train Acc=51.33%, Val Loss=1.5605, Val Acc=46.61%, Grad Norm=4.1262\n",
      "Fold 1, Epoch 6: Train Loss=1.2440, Train Acc=54.42%, Val Loss=1.5540, Val Acc=47.62%, Grad Norm=4.0802\n",
      "Fold 1, Epoch 7: Train Loss=1.1811, Train Acc=56.73%, Val Loss=1.4128, Val Acc=51.33%, Grad Norm=4.0639\n",
      "Fold 1, Epoch 8: Train Loss=1.1288, Train Acc=58.81%, Val Loss=1.5113, Val Acc=50.00%, Grad Norm=4.0773\n",
      "Fold 1, Epoch 9: Train Loss=1.0916, Train Acc=60.19%, Val Loss=1.3724, Val Acc=53.00%, Grad Norm=4.0955\n",
      "Fold 1, Epoch 10: Train Loss=1.0523, Train Acc=61.72%, Val Loss=1.4066, Val Acc=53.92%, Grad Norm=4.1593\n",
      "Fold 1, Epoch 11: Train Loss=0.9816, Train Acc=64.38%, Val Loss=1.4319, Val Acc=55.62%, Grad Norm=4.3472\n",
      "Fold 1, Epoch 12: Train Loss=0.9416, Train Acc=65.90%, Val Loss=1.3505, Val Acc=55.77%, Grad Norm=4.6254\n",
      "Fold 1, Epoch 13: Train Loss=0.9148, Train Acc=66.99%, Val Loss=1.2812, Val Acc=58.37%, Grad Norm=4.9006\n",
      "Fold 1, Epoch 14: Train Loss=0.8847, Train Acc=68.42%, Val Loss=1.2920, Val Acc=57.76%, Grad Norm=5.1272\n",
      "Fold 1, Epoch 15: Train Loss=0.8546, Train Acc=69.44%, Val Loss=1.2819, Val Acc=58.81%, Grad Norm=5.3395\n",
      "Fold 1, Epoch 16: Train Loss=0.8223, Train Acc=70.70%, Val Loss=1.2504, Val Acc=59.75%, Grad Norm=5.5729\n",
      "Fold 1, Epoch 17: Train Loss=0.7987, Train Acc=71.60%, Val Loss=1.2036, Val Acc=61.08%, Grad Norm=5.8130\n",
      "Fold 1, Epoch 18: Train Loss=0.7680, Train Acc=72.77%, Val Loss=1.1978, Val Acc=60.51%, Grad Norm=6.0640\n",
      "Fold 1, Epoch 19: Train Loss=0.7455, Train Acc=73.52%, Val Loss=1.1992, Val Acc=61.90%, Grad Norm=6.2680\n",
      "Fold 1, Epoch 20: Train Loss=0.7105, Train Acc=74.87%, Val Loss=1.1768, Val Acc=61.90%, Grad Norm=6.5014\n",
      "Fold 1, Epoch 21: Train Loss=0.6558, Train Acc=77.00%, Val Loss=1.1900, Val Acc=62.48%, Grad Norm=6.6717\n",
      "Fold 1, Epoch 22: Train Loss=0.6356, Train Acc=77.60%, Val Loss=1.1516, Val Acc=62.40%, Grad Norm=7.0527\n",
      "Fold 1, Epoch 23: Train Loss=0.6135, Train Acc=78.42%, Val Loss=1.1340, Val Acc=63.40%, Grad Norm=7.3370\n",
      "Fold 1, Epoch 24: Train Loss=0.5933, Train Acc=79.18%, Val Loss=1.1887, Val Acc=62.79%, Grad Norm=7.5681\n",
      "Fold 1, Epoch 25: Train Loss=0.5759, Train Acc=79.69%, Val Loss=1.1281, Val Acc=63.61%, Grad Norm=7.8271\n",
      "Fold 1, Epoch 26: Train Loss=0.5572, Train Acc=80.50%, Val Loss=1.1579, Val Acc=62.62%, Grad Norm=8.0960\n",
      "Fold 1, Epoch 27: Train Loss=0.5415, Train Acc=80.99%, Val Loss=1.1410, Val Acc=63.63%, Grad Norm=8.3241\n",
      "Fold 1, Epoch 28: Train Loss=0.5253, Train Acc=81.49%, Val Loss=1.1521, Val Acc=63.62%, Grad Norm=8.5504\n",
      "Fold 1, Epoch 29: Train Loss=0.5110, Train Acc=81.99%, Val Loss=1.1401, Val Acc=63.95%, Grad Norm=8.7732\n",
      "Fold 1, Epoch 30: Train Loss=0.4919, Train Acc=82.79%, Val Loss=1.1741, Val Acc=63.32%, Grad Norm=9.0078\n",
      "Fold 1, Epoch 31: Train Loss=0.4610, Train Acc=83.94%, Val Loss=1.1487, Val Acc=63.62%, Grad Norm=9.0334\n",
      "Fold 1, Epoch 32: Train Loss=0.4483, Train Acc=84.46%, Val Loss=1.1447, Val Acc=64.53%, Grad Norm=9.3198\n",
      "Fold 1, Epoch 33: Train Loss=0.4346, Train Acc=84.86%, Val Loss=1.1590, Val Acc=63.66%, Grad Norm=9.4773\n",
      "Fold 1, Epoch 34: Train Loss=0.4271, Train Acc=85.12%, Val Loss=1.1724, Val Acc=63.70%, Grad Norm=9.6932\n",
      "Fold 1, Epoch 35: Train Loss=0.4165, Train Acc=85.54%, Val Loss=1.1489, Val Acc=64.19%, Grad Norm=9.8954\n",
      "Fold 1, Epoch 36: Train Loss=0.4086, Train Acc=85.82%, Val Loss=1.1663, Val Acc=63.77%, Grad Norm=10.0540\n",
      "Fold 1, Epoch 37: Train Loss=0.3999, Train Acc=85.99%, Val Loss=1.1871, Val Acc=63.83%, Grad Norm=10.2378\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc (Test=10km/h) = 14.86%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.0649, Train Acc=20.88%, Val Loss=2.7733, Val Acc=18.28%, Grad Norm=5.6231\n",
      "Fold 2, Epoch 2: Train Loss=1.7469, Train Acc=33.66%, Val Loss=2.1216, Val Acc=29.13%, Grad Norm=4.6131\n",
      "Fold 2, Epoch 3: Train Loss=1.5686, Train Acc=40.82%, Val Loss=1.8134, Val Acc=38.37%, Grad Norm=4.3320\n",
      "Fold 2, Epoch 4: Train Loss=1.4183, Train Acc=47.12%, Val Loss=1.5599, Val Acc=46.21%, Grad Norm=4.3246\n",
      "Fold 2, Epoch 5: Train Loss=1.3039, Train Acc=51.67%, Val Loss=1.5186, Val Acc=46.60%, Grad Norm=4.2441\n",
      "Fold 2, Epoch 6: Train Loss=1.2248, Train Acc=54.90%, Val Loss=1.4310, Val Acc=50.08%, Grad Norm=4.1634\n",
      "Fold 2, Epoch 7: Train Loss=1.1679, Train Acc=56.99%, Val Loss=1.3445, Val Acc=52.20%, Grad Norm=4.1619\n",
      "Fold 2, Epoch 8: Train Loss=1.1130, Train Acc=59.08%, Val Loss=1.3213, Val Acc=52.68%, Grad Norm=4.1911\n",
      "Fold 2, Epoch 9: Train Loss=1.0670, Train Acc=61.20%, Val Loss=1.2607, Val Acc=55.57%, Grad Norm=4.2388\n",
      "Fold 2, Epoch 10: Train Loss=1.0083, Train Acc=63.35%, Val Loss=1.2423, Val Acc=55.52%, Grad Norm=4.3706\n",
      "Fold 2, Epoch 11: Train Loss=0.9274, Train Acc=66.67%, Val Loss=1.1826, Val Acc=58.64%, Grad Norm=4.4998\n",
      "Fold 2, Epoch 12: Train Loss=0.8869, Train Acc=68.28%, Val Loss=1.2038, Val Acc=57.88%, Grad Norm=4.8038\n",
      "Fold 2, Epoch 13: Train Loss=0.8564, Train Acc=69.40%, Val Loss=1.1579, Val Acc=59.43%, Grad Norm=5.0813\n",
      "Fold 2, Epoch 14: Train Loss=0.8277, Train Acc=70.20%, Val Loss=1.1490, Val Acc=59.80%, Grad Norm=5.3099\n",
      "Fold 2, Epoch 15: Train Loss=0.7994, Train Acc=71.33%, Val Loss=1.1464, Val Acc=59.92%, Grad Norm=5.5528\n",
      "Fold 2, Epoch 16: Train Loss=0.7726, Train Acc=72.55%, Val Loss=1.0770, Val Acc=62.31%, Grad Norm=5.7563\n",
      "Fold 2, Epoch 17: Train Loss=0.7399, Train Acc=73.65%, Val Loss=1.0959, Val Acc=61.70%, Grad Norm=6.0008\n",
      "Fold 2, Epoch 18: Train Loss=0.7158, Train Acc=74.64%, Val Loss=1.0952, Val Acc=62.48%, Grad Norm=6.3093\n",
      "Fold 2, Epoch 19: Train Loss=0.6836, Train Acc=75.75%, Val Loss=1.0793, Val Acc=63.03%, Grad Norm=6.4776\n",
      "Fold 2, Epoch 20: Train Loss=0.6549, Train Acc=77.13%, Val Loss=1.0919, Val Acc=63.10%, Grad Norm=6.6907\n",
      "Fold 2, Epoch 21: Train Loss=0.5956, Train Acc=79.11%, Val Loss=1.0622, Val Acc=64.53%, Grad Norm=6.8782\n",
      "Fold 2, Epoch 22: Train Loss=0.5741, Train Acc=79.69%, Val Loss=1.0554, Val Acc=64.38%, Grad Norm=7.2775\n",
      "Fold 2, Epoch 23: Train Loss=0.5553, Train Acc=80.46%, Val Loss=1.0407, Val Acc=65.50%, Grad Norm=7.5431\n",
      "Fold 2, Epoch 24: Train Loss=0.5345, Train Acc=81.26%, Val Loss=1.0784, Val Acc=64.51%, Grad Norm=7.7727\n",
      "Fold 2, Epoch 25: Train Loss=0.5189, Train Acc=81.86%, Val Loss=1.0674, Val Acc=65.25%, Grad Norm=8.0257\n",
      "Fold 2, Epoch 26: Train Loss=0.5024, Train Acc=82.42%, Val Loss=1.0703, Val Acc=65.39%, Grad Norm=8.2563\n",
      "Fold 2, Epoch 27: Train Loss=0.4868, Train Acc=82.88%, Val Loss=1.0727, Val Acc=65.47%, Grad Norm=8.4832\n",
      "Fold 2, Epoch 28: Train Loss=0.4694, Train Acc=83.59%, Val Loss=1.0667, Val Acc=65.64%, Grad Norm=8.7443\n",
      "Fold 2, Epoch 29: Train Loss=0.4535, Train Acc=84.21%, Val Loss=1.0660, Val Acc=65.75%, Grad Norm=8.9091\n",
      "Fold 2, Epoch 30: Train Loss=0.4398, Train Acc=84.57%, Val Loss=1.1196, Val Acc=64.67%, Grad Norm=9.1303\n",
      "Fold 2, Epoch 31: Train Loss=0.4055, Train Acc=86.14%, Val Loss=1.0983, Val Acc=65.47%, Grad Norm=9.1359\n",
      "Fold 2, Epoch 32: Train Loss=0.3905, Train Acc=86.52%, Val Loss=1.1265, Val Acc=65.01%, Grad Norm=9.3420\n",
      "Fold 2, Epoch 33: Train Loss=0.3804, Train Acc=86.95%, Val Loss=1.1016, Val Acc=65.83%, Grad Norm=9.5066\n",
      "Fold 2, Epoch 34: Train Loss=0.3718, Train Acc=87.16%, Val Loss=1.0937, Val Acc=65.72%, Grad Norm=9.7311\n",
      "Fold 2, Epoch 35: Train Loss=0.3642, Train Acc=87.30%, Val Loss=1.1205, Val Acc=65.29%, Grad Norm=9.8902\n",
      "Fold 2, Epoch 36: Train Loss=0.3567, Train Acc=87.76%, Val Loss=1.0765, Val Acc=66.28%, Grad Norm=10.0587\n",
      "Fold 2, Epoch 37: Train Loss=0.3450, Train Acc=88.09%, Val Loss=1.1139, Val Acc=66.18%, Grad Norm=10.0899\n",
      "Fold 2, Epoch 38: Train Loss=0.3373, Train Acc=88.31%, Val Loss=1.1382, Val Acc=65.53%, Grad Norm=10.2792\n",
      "Fold 2, Epoch 39: Train Loss=0.3294, Train Acc=88.71%, Val Loss=1.1308, Val Acc=65.59%, Grad Norm=10.3913\n",
      "Fold 2, Epoch 40: Train Loss=0.3218, Train Acc=88.91%, Val Loss=1.1599, Val Acc=65.37%, Grad Norm=10.5338\n",
      "Fold 2, Epoch 41: Train Loss=0.3025, Train Acc=89.74%, Val Loss=1.1343, Val Acc=66.03%, Grad Norm=10.4168\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc (Test=10km/h) = 16.27%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.0571, Train Acc=20.85%, Val Loss=2.5978, Val Acc=18.36%, Grad Norm=5.4034\n",
      "Fold 3, Epoch 2: Train Loss=1.7700, Train Acc=32.98%, Val Loss=1.9472, Val Acc=34.73%, Grad Norm=4.5892\n",
      "Fold 3, Epoch 3: Train Loss=1.5784, Train Acc=40.52%, Val Loss=1.6813, Val Acc=40.40%, Grad Norm=4.2597\n",
      "Fold 3, Epoch 4: Train Loss=1.4715, Train Acc=44.80%, Val Loss=1.5855, Val Acc=42.64%, Grad Norm=4.1364\n",
      "Fold 3, Epoch 5: Train Loss=1.3530, Train Acc=49.63%, Val Loss=1.4357, Val Acc=47.91%, Grad Norm=4.2071\n",
      "Fold 3, Epoch 6: Train Loss=1.2606, Train Acc=53.43%, Val Loss=1.3951, Val Acc=49.70%, Grad Norm=4.1610\n",
      "Fold 3, Epoch 7: Train Loss=1.1974, Train Acc=55.84%, Val Loss=1.3049, Val Acc=52.42%, Grad Norm=4.1116\n",
      "Fold 3, Epoch 8: Train Loss=1.1453, Train Acc=58.04%, Val Loss=1.2203, Val Acc=55.70%, Grad Norm=4.1206\n",
      "Fold 3, Epoch 9: Train Loss=1.0969, Train Acc=59.89%, Val Loss=1.1735, Val Acc=57.89%, Grad Norm=4.1766\n",
      "Fold 3, Epoch 10: Train Loss=1.0530, Train Acc=61.55%, Val Loss=1.1543, Val Acc=58.30%, Grad Norm=4.2882\n",
      "Fold 3, Epoch 11: Train Loss=0.9691, Train Acc=64.96%, Val Loss=1.0663, Val Acc=62.16%, Grad Norm=4.4385\n",
      "Fold 3, Epoch 12: Train Loss=0.9336, Train Acc=66.31%, Val Loss=1.0848, Val Acc=62.39%, Grad Norm=4.7549\n",
      "Fold 3, Epoch 13: Train Loss=0.9026, Train Acc=67.61%, Val Loss=1.0656, Val Acc=62.24%, Grad Norm=5.0151\n",
      "Fold 3, Epoch 14: Train Loss=0.8722, Train Acc=68.60%, Val Loss=1.0362, Val Acc=63.65%, Grad Norm=5.2104\n",
      "Fold 3, Epoch 15: Train Loss=0.8401, Train Acc=69.92%, Val Loss=1.0496, Val Acc=64.36%, Grad Norm=5.4739\n",
      "Fold 3, Epoch 16: Train Loss=0.8133, Train Acc=70.89%, Val Loss=1.0199, Val Acc=64.63%, Grad Norm=5.6950\n",
      "Fold 3, Epoch 17: Train Loss=0.7858, Train Acc=72.00%, Val Loss=1.0171, Val Acc=65.45%, Grad Norm=5.9394\n",
      "Fold 3, Epoch 18: Train Loss=0.7541, Train Acc=73.20%, Val Loss=0.9794, Val Acc=66.17%, Grad Norm=6.1647\n",
      "Fold 3, Epoch 19: Train Loss=0.7256, Train Acc=74.39%, Val Loss=1.0280, Val Acc=65.62%, Grad Norm=6.3780\n",
      "Fold 3, Epoch 20: Train Loss=0.6938, Train Acc=75.32%, Val Loss=1.0130, Val Acc=66.01%, Grad Norm=6.5936\n",
      "Fold 3, Epoch 21: Train Loss=0.6398, Train Acc=77.55%, Val Loss=0.9864, Val Acc=67.24%, Grad Norm=6.7434\n",
      "Fold 3, Epoch 22: Train Loss=0.6169, Train Acc=78.27%, Val Loss=0.9904, Val Acc=67.90%, Grad Norm=7.1141\n",
      "Fold 3, Epoch 23: Train Loss=0.5975, Train Acc=78.95%, Val Loss=0.9926, Val Acc=67.68%, Grad Norm=7.4585\n",
      "Fold 3, Epoch 24: Train Loss=0.5777, Train Acc=79.64%, Val Loss=0.9775, Val Acc=68.02%, Grad Norm=7.6635\n",
      "Fold 3, Epoch 25: Train Loss=0.5621, Train Acc=80.33%, Val Loss=0.9703, Val Acc=68.48%, Grad Norm=7.9346\n",
      "Fold 3, Epoch 26: Train Loss=0.5440, Train Acc=81.01%, Val Loss=0.9641, Val Acc=68.40%, Grad Norm=8.1480\n",
      "Fold 3, Epoch 27: Train Loss=0.5282, Train Acc=81.51%, Val Loss=0.9962, Val Acc=67.76%, Grad Norm=8.3652\n",
      "Fold 3, Epoch 28: Train Loss=0.5114, Train Acc=81.91%, Val Loss=0.9523, Val Acc=68.86%, Grad Norm=8.5831\n",
      "Fold 3, Epoch 29: Train Loss=0.4919, Train Acc=82.75%, Val Loss=0.9708, Val Acc=68.35%, Grad Norm=8.8612\n",
      "Fold 3, Epoch 30: Train Loss=0.4797, Train Acc=83.18%, Val Loss=0.9662, Val Acc=68.57%, Grad Norm=9.0909\n",
      "Fold 3, Epoch 31: Train Loss=0.4467, Train Acc=84.55%, Val Loss=0.9810, Val Acc=68.67%, Grad Norm=9.1734\n",
      "Fold 3, Epoch 32: Train Loss=0.4345, Train Acc=84.91%, Val Loss=1.0138, Val Acc=68.53%, Grad Norm=9.3953\n",
      "Fold 3, Epoch 33: Train Loss=0.4203, Train Acc=85.47%, Val Loss=1.0079, Val Acc=68.37%, Grad Norm=9.5645\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc (Test=10km/h) = 15.90%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.0338, Train Acc=22.65%, Val Loss=2.5326, Val Acc=20.82%, Grad Norm=5.8191\n",
      "Fold 4, Epoch 2: Train Loss=1.7552, Train Acc=34.60%, Val Loss=1.8717, Val Acc=33.84%, Grad Norm=4.5295\n",
      "Fold 4, Epoch 3: Train Loss=1.5753, Train Acc=41.51%, Val Loss=1.7576, Val Acc=36.69%, Grad Norm=4.1403\n",
      "Fold 4, Epoch 4: Train Loss=1.4796, Train Acc=45.24%, Val Loss=1.5880, Val Acc=43.12%, Grad Norm=4.0011\n",
      "Fold 4, Epoch 5: Train Loss=1.3631, Train Acc=49.79%, Val Loss=1.4584, Val Acc=47.91%, Grad Norm=4.0900\n",
      "Fold 4, Epoch 6: Train Loss=1.2815, Train Acc=52.91%, Val Loss=1.3436, Val Acc=51.10%, Grad Norm=4.0294\n",
      "Fold 4, Epoch 7: Train Loss=1.2196, Train Acc=55.37%, Val Loss=1.3066, Val Acc=52.85%, Grad Norm=4.0196\n",
      "Fold 4, Epoch 8: Train Loss=1.1701, Train Acc=57.25%, Val Loss=1.1940, Val Acc=55.80%, Grad Norm=4.0594\n",
      "Fold 4, Epoch 9: Train Loss=1.1227, Train Acc=59.28%, Val Loss=1.2676, Val Acc=54.98%, Grad Norm=4.1161\n",
      "Fold 4, Epoch 10: Train Loss=1.0755, Train Acc=61.09%, Val Loss=1.1528, Val Acc=58.38%, Grad Norm=4.2159\n",
      "Fold 4, Epoch 11: Train Loss=0.9918, Train Acc=64.34%, Val Loss=1.1108, Val Acc=60.45%, Grad Norm=4.3855\n",
      "Fold 4, Epoch 12: Train Loss=0.9556, Train Acc=65.71%, Val Loss=1.1614, Val Acc=60.19%, Grad Norm=4.7068\n",
      "Fold 4, Epoch 13: Train Loss=0.9210, Train Acc=67.08%, Val Loss=1.1138, Val Acc=61.49%, Grad Norm=4.9489\n",
      "Fold 4, Epoch 14: Train Loss=0.8900, Train Acc=68.31%, Val Loss=1.0816, Val Acc=62.57%, Grad Norm=5.2005\n",
      "Fold 4, Epoch 15: Train Loss=0.8632, Train Acc=69.33%, Val Loss=1.0697, Val Acc=62.91%, Grad Norm=5.4586\n",
      "Fold 4, Epoch 16: Train Loss=0.8288, Train Acc=70.40%, Val Loss=1.0326, Val Acc=64.41%, Grad Norm=5.6522\n",
      "Fold 4, Epoch 17: Train Loss=0.8024, Train Acc=71.58%, Val Loss=1.0343, Val Acc=64.41%, Grad Norm=5.8906\n",
      "Fold 4, Epoch 18: Train Loss=0.7749, Train Acc=72.48%, Val Loss=0.9966, Val Acc=65.86%, Grad Norm=6.1630\n",
      "Fold 4, Epoch 19: Train Loss=0.7457, Train Acc=73.72%, Val Loss=1.0124, Val Acc=64.48%, Grad Norm=6.3571\n",
      "Fold 4, Epoch 20: Train Loss=0.7183, Train Acc=74.69%, Val Loss=1.0119, Val Acc=65.31%, Grad Norm=6.5650\n",
      "Fold 4, Epoch 21: Train Loss=0.6633, Train Acc=76.77%, Val Loss=1.0206, Val Acc=66.12%, Grad Norm=6.7301\n",
      "Fold 4, Epoch 22: Train Loss=0.6361, Train Acc=77.62%, Val Loss=0.9932, Val Acc=66.86%, Grad Norm=7.1553\n",
      "Fold 4, Epoch 23: Train Loss=0.6165, Train Acc=78.27%, Val Loss=0.9951, Val Acc=66.56%, Grad Norm=7.4175\n",
      "Fold 4, Epoch 24: Train Loss=0.5999, Train Acc=78.94%, Val Loss=0.9957, Val Acc=66.85%, Grad Norm=7.6523\n",
      "Fold 4, Epoch 25: Train Loss=0.5827, Train Acc=79.65%, Val Loss=1.0021, Val Acc=66.81%, Grad Norm=7.9242\n",
      "Fold 4, Epoch 26: Train Loss=0.5657, Train Acc=80.20%, Val Loss=1.0013, Val Acc=67.27%, Grad Norm=8.1610\n",
      "Fold 4, Epoch 27: Train Loss=0.5503, Train Acc=80.74%, Val Loss=1.0227, Val Acc=66.36%, Grad Norm=8.3847\n",
      "Fold 4, Epoch 28: Train Loss=0.5304, Train Acc=81.49%, Val Loss=0.9888, Val Acc=67.90%, Grad Norm=8.5978\n",
      "Fold 4, Epoch 29: Train Loss=0.5193, Train Acc=81.83%, Val Loss=1.0124, Val Acc=67.14%, Grad Norm=8.8658\n",
      "Fold 4, Epoch 30: Train Loss=0.5023, Train Acc=82.52%, Val Loss=1.0506, Val Acc=66.82%, Grad Norm=9.0856\n",
      "Fold 4, Epoch 31: Train Loss=0.4711, Train Acc=83.66%, Val Loss=1.0659, Val Acc=66.37%, Grad Norm=9.2134\n",
      "Fold 4, Epoch 32: Train Loss=0.4579, Train Acc=84.12%, Val Loss=1.0411, Val Acc=67.00%, Grad Norm=9.4429\n",
      "Fold 4, Epoch 33: Train Loss=0.4436, Train Acc=84.49%, Val Loss=1.0416, Val Acc=67.27%, Grad Norm=9.6149\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc (Test=10km/h) = 15.63%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.0651, Train Acc=20.65%, Val Loss=2.9888, Val Acc=20.66%, Grad Norm=5.6341\n",
      "Fold 5, Epoch 2: Train Loss=1.7924, Train Acc=32.26%, Val Loss=2.2748, Val Acc=29.70%, Grad Norm=4.5131\n",
      "Fold 5, Epoch 3: Train Loss=1.5837, Train Acc=40.43%, Val Loss=2.0525, Val Acc=35.61%, Grad Norm=4.2370\n",
      "Fold 5, Epoch 4: Train Loss=1.4388, Train Acc=46.48%, Val Loss=1.6245, Val Acc=44.00%, Grad Norm=4.2647\n",
      "Fold 5, Epoch 5: Train Loss=1.3171, Train Acc=51.18%, Val Loss=1.5530, Val Acc=47.30%, Grad Norm=4.1720\n",
      "Fold 5, Epoch 6: Train Loss=1.2407, Train Acc=54.12%, Val Loss=1.5427, Val Acc=49.30%, Grad Norm=4.1082\n",
      "Fold 5, Epoch 7: Train Loss=1.1808, Train Acc=56.59%, Val Loss=1.4353, Val Acc=50.93%, Grad Norm=4.0761\n",
      "Fold 5, Epoch 8: Train Loss=1.1288, Train Acc=58.38%, Val Loss=1.4141, Val Acc=52.40%, Grad Norm=4.0978\n",
      "Fold 5, Epoch 9: Train Loss=1.0863, Train Acc=60.30%, Val Loss=1.4074, Val Acc=53.02%, Grad Norm=4.1544\n",
      "Fold 5, Epoch 10: Train Loss=1.0373, Train Acc=62.25%, Val Loss=1.3353, Val Acc=56.59%, Grad Norm=4.2611\n",
      "Fold 5, Epoch 11: Train Loss=0.9541, Train Acc=65.56%, Val Loss=1.2712, Val Acc=58.30%, Grad Norm=4.4194\n",
      "Fold 5, Epoch 12: Train Loss=0.9157, Train Acc=66.88%, Val Loss=1.2428, Val Acc=58.95%, Grad Norm=4.7259\n",
      "Fold 5, Epoch 13: Train Loss=0.8838, Train Acc=68.33%, Val Loss=1.2752, Val Acc=59.04%, Grad Norm=4.9826\n",
      "Fold 5, Epoch 14: Train Loss=0.8532, Train Acc=69.34%, Val Loss=1.3060, Val Acc=58.52%, Grad Norm=5.2150\n",
      "Fold 5, Epoch 15: Train Loss=0.8248, Train Acc=70.33%, Val Loss=1.2535, Val Acc=59.47%, Grad Norm=5.4413\n",
      "Fold 5, Epoch 16: Train Loss=0.8008, Train Acc=71.24%, Val Loss=1.2613, Val Acc=60.30%, Grad Norm=5.6764\n",
      "Fold 5, Epoch 17: Train Loss=0.7738, Train Acc=72.39%, Val Loss=1.2062, Val Acc=61.10%, Grad Norm=5.8645\n",
      "Fold 5, Epoch 18: Train Loss=0.7463, Train Acc=73.24%, Val Loss=1.2484, Val Acc=60.31%, Grad Norm=6.1464\n",
      "Fold 5, Epoch 19: Train Loss=0.7231, Train Acc=74.22%, Val Loss=1.2253, Val Acc=61.64%, Grad Norm=6.3602\n",
      "Fold 5, Epoch 20: Train Loss=0.6934, Train Acc=75.31%, Val Loss=1.2301, Val Acc=61.59%, Grad Norm=6.5859\n",
      "Fold 5, Epoch 21: Train Loss=0.6393, Train Acc=77.38%, Val Loss=1.2189, Val Acc=62.33%, Grad Norm=6.7893\n",
      "Fold 5, Epoch 22: Train Loss=0.6139, Train Acc=78.40%, Val Loss=1.2200, Val Acc=62.33%, Grad Norm=7.1327\n",
      "Fold 5, Epoch 23: Train Loss=0.5963, Train Acc=78.99%, Val Loss=1.2442, Val Acc=62.42%, Grad Norm=7.4299\n",
      "Fold 5, Epoch 24: Train Loss=0.5773, Train Acc=79.62%, Val Loss=1.2552, Val Acc=62.67%, Grad Norm=7.6741\n",
      "Fold 5, Epoch 25: Train Loss=0.5615, Train Acc=80.35%, Val Loss=1.2607, Val Acc=63.33%, Grad Norm=7.9406\n",
      "Fold 5, Epoch 26: Train Loss=0.5417, Train Acc=81.03%, Val Loss=1.2565, Val Acc=63.19%, Grad Norm=8.2104\n",
      "Fold 5, Epoch 27: Train Loss=0.5283, Train Acc=81.34%, Val Loss=1.1837, Val Acc=64.28%, Grad Norm=8.4790\n",
      "Fold 5, Epoch 28: Train Loss=0.5108, Train Acc=82.01%, Val Loss=1.2633, Val Acc=62.57%, Grad Norm=8.6942\n",
      "Fold 5, Epoch 29: Train Loss=0.4922, Train Acc=82.80%, Val Loss=1.2808, Val Acc=63.25%, Grad Norm=8.9095\n",
      "Fold 5, Epoch 30: Train Loss=0.4803, Train Acc=83.31%, Val Loss=1.2449, Val Acc=63.76%, Grad Norm=9.1722\n",
      "Fold 5, Epoch 31: Train Loss=0.4432, Train Acc=84.60%, Val Loss=1.2335, Val Acc=64.12%, Grad Norm=9.2313\n",
      "Fold 5, Epoch 32: Train Loss=0.4273, Train Acc=85.21%, Val Loss=1.2691, Val Acc=63.40%, Grad Norm=9.4187\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc (Test=10km/h) = 15.80%\n",
      "\n",
      "SNR  15 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-23_22-36-39_LTE-V_XFR_SpeedSplit_FileBlock_Train20kmh_Test10kmh_SNR15dB_fdTrain655_fdTest655_group288_ResNet1D\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=10 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "[INFO] 共找到 72 个 .mat 文件（含子目录）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 可用 TX 类别数（train/test 交集）: 9\n",
      "[INFO] Train blocks: 360, Test blocks: 360, sample_len: 288, group_size: 288\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.0658, Train Acc=21.36%, Val Loss=2.6661, Val Acc=19.25%, Grad Norm=5.4511\n",
      "Fold 1, Epoch 2: Train Loss=1.8391, Train Acc=31.50%, Val Loss=2.0907, Val Acc=28.02%, Grad Norm=4.2769\n",
      "Fold 1, Epoch 3: Train Loss=1.6316, Train Acc=39.28%, Val Loss=1.8910, Val Acc=34.40%, Grad Norm=4.1574\n",
      "Fold 1, Epoch 4: Train Loss=1.5189, Train Acc=43.79%, Val Loss=1.7674, Val Acc=37.63%, Grad Norm=4.0172\n",
      "Fold 1, Epoch 5: Train Loss=1.4029, Train Acc=48.27%, Val Loss=1.5670, Val Acc=43.41%, Grad Norm=4.0717\n",
      "Fold 1, Epoch 6: Train Loss=1.3193, Train Acc=51.32%, Val Loss=1.5765, Val Acc=44.84%, Grad Norm=3.9976\n",
      "Fold 1, Epoch 7: Train Loss=1.2593, Train Acc=53.77%, Val Loss=1.4978, Val Acc=48.49%, Grad Norm=3.9672\n",
      "Fold 1, Epoch 8: Train Loss=1.2124, Train Acc=55.63%, Val Loss=1.4724, Val Acc=49.25%, Grad Norm=3.9809\n",
      "Fold 1, Epoch 9: Train Loss=1.1681, Train Acc=57.61%, Val Loss=1.4999, Val Acc=48.86%, Grad Norm=4.0356\n",
      "Fold 1, Epoch 10: Train Loss=1.1209, Train Acc=59.50%, Val Loss=1.4060, Val Acc=52.50%, Grad Norm=4.1243\n",
      "Fold 1, Epoch 11: Train Loss=1.0424, Train Acc=62.45%, Val Loss=1.3931, Val Acc=54.89%, Grad Norm=4.2934\n",
      "Fold 1, Epoch 12: Train Loss=1.0083, Train Acc=63.77%, Val Loss=1.3545, Val Acc=55.57%, Grad Norm=4.5928\n",
      "Fold 1, Epoch 13: Train Loss=0.9768, Train Acc=64.79%, Val Loss=1.3374, Val Acc=55.79%, Grad Norm=4.8267\n",
      "Fold 1, Epoch 14: Train Loss=0.9516, Train Acc=65.74%, Val Loss=1.2879, Val Acc=57.45%, Grad Norm=5.0547\n",
      "Fold 1, Epoch 15: Train Loss=0.9243, Train Acc=66.81%, Val Loss=1.3566, Val Acc=56.52%, Grad Norm=5.3011\n",
      "Fold 1, Epoch 16: Train Loss=0.8990, Train Acc=67.96%, Val Loss=1.3049, Val Acc=57.68%, Grad Norm=5.4823\n",
      "Fold 1, Epoch 17: Train Loss=0.8734, Train Acc=68.87%, Val Loss=1.3175, Val Acc=56.80%, Grad Norm=5.7617\n",
      "Fold 1, Epoch 18: Train Loss=0.8486, Train Acc=69.66%, Val Loss=1.2632, Val Acc=59.04%, Grad Norm=5.9583\n",
      "Fold 1, Epoch 19: Train Loss=0.8237, Train Acc=70.69%, Val Loss=1.2686, Val Acc=58.56%, Grad Norm=6.1582\n",
      "Fold 1, Epoch 20: Train Loss=0.8017, Train Acc=71.40%, Val Loss=1.2574, Val Acc=59.54%, Grad Norm=6.3855\n",
      "Fold 1, Epoch 21: Train Loss=0.7457, Train Acc=73.56%, Val Loss=1.2307, Val Acc=60.62%, Grad Norm=6.5772\n",
      "Fold 1, Epoch 22: Train Loss=0.7222, Train Acc=74.49%, Val Loss=1.2448, Val Acc=60.62%, Grad Norm=6.8937\n",
      "Fold 1, Epoch 23: Train Loss=0.7045, Train Acc=75.07%, Val Loss=1.2558, Val Acc=60.81%, Grad Norm=7.1815\n",
      "Fold 1, Epoch 24: Train Loss=0.6884, Train Acc=75.67%, Val Loss=1.2494, Val Acc=61.28%, Grad Norm=7.4564\n",
      "Fold 1, Epoch 25: Train Loss=0.6733, Train Acc=76.07%, Val Loss=1.2263, Val Acc=61.32%, Grad Norm=7.6925\n",
      "Fold 1, Epoch 26: Train Loss=0.6569, Train Acc=76.76%, Val Loss=1.2282, Val Acc=61.33%, Grad Norm=7.9382\n",
      "Fold 1, Epoch 27: Train Loss=0.6441, Train Acc=77.32%, Val Loss=1.2168, Val Acc=61.64%, Grad Norm=8.1179\n",
      "Fold 1, Epoch 28: Train Loss=0.6225, Train Acc=78.09%, Val Loss=1.2367, Val Acc=61.90%, Grad Norm=8.3498\n",
      "Fold 1, Epoch 29: Train Loss=0.6132, Train Acc=78.42%, Val Loss=1.2748, Val Acc=60.99%, Grad Norm=8.6188\n",
      "Fold 1, Epoch 30: Train Loss=0.5964, Train Acc=78.91%, Val Loss=1.2518, Val Acc=61.34%, Grad Norm=8.7996\n",
      "Fold 1, Epoch 31: Train Loss=0.5718, Train Acc=79.89%, Val Loss=1.2489, Val Acc=62.26%, Grad Norm=8.9817\n",
      "Fold 1, Epoch 32: Train Loss=0.5495, Train Acc=80.64%, Val Loss=1.2434, Val Acc=61.25%, Grad Norm=9.1515\n",
      "Fold 1, Epoch 33: Train Loss=0.5392, Train Acc=80.93%, Val Loss=1.2212, Val Acc=62.26%, Grad Norm=9.3781\n",
      "Fold 1, Epoch 34: Train Loss=0.5314, Train Acc=81.37%, Val Loss=1.2417, Val Acc=62.44%, Grad Norm=9.5610\n",
      "Fold 1, Epoch 35: Train Loss=0.5219, Train Acc=81.65%, Val Loss=1.2327, Val Acc=62.35%, Grad Norm=9.7566\n",
      "Fold 1, Epoch 36: Train Loss=0.5121, Train Acc=82.03%, Val Loss=1.2654, Val Acc=62.34%, Grad Norm=9.9297\n",
      "Fold 1, Epoch 37: Train Loss=0.5029, Train Acc=82.49%, Val Loss=1.2625, Val Acc=62.50%, Grad Norm=10.1150\n",
      "Fold 1, Epoch 38: Train Loss=0.4944, Train Acc=82.64%, Val Loss=1.2548, Val Acc=62.23%, Grad Norm=10.3124\n",
      "Fold 1, Epoch 39: Train Loss=0.4857, Train Acc=83.05%, Val Loss=1.2624, Val Acc=62.02%, Grad Norm=10.4786\n",
      "Fold 1, Epoch 40: Train Loss=0.4792, Train Acc=83.19%, Val Loss=1.2736, Val Acc=62.13%, Grad Norm=10.6401\n",
      "Fold 1, Epoch 41: Train Loss=0.4584, Train Acc=83.93%, Val Loss=1.2624, Val Acc=62.03%, Grad Norm=10.6512\n",
      "Fold 1, Epoch 42: Train Loss=0.4532, Train Acc=84.07%, Val Loss=1.2376, Val Acc=62.67%, Grad Norm=10.7885\n",
      "Fold 1, Epoch 43: Train Loss=0.4471, Train Acc=84.34%, Val Loss=1.2857, Val Acc=61.87%, Grad Norm=10.9152\n",
      "Fold 1, Epoch 44: Train Loss=0.4419, Train Acc=84.57%, Val Loss=1.2802, Val Acc=61.81%, Grad Norm=11.0164\n",
      "Fold 1, Epoch 45: Train Loss=0.4370, Train Acc=84.85%, Val Loss=1.2528, Val Acc=62.48%, Grad Norm=11.0926\n",
      "Fold 1, Epoch 46: Train Loss=0.4341, Train Acc=84.84%, Val Loss=1.2806, Val Acc=62.40%, Grad Norm=11.2899\n",
      "Fold 1, Epoch 47: Train Loss=0.4270, Train Acc=85.29%, Val Loss=1.2723, Val Acc=62.32%, Grad Norm=11.3113\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc (Test=10km/h) = 14.67%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.0646, Train Acc=20.93%, Val Loss=2.6584, Val Acc=19.10%, Grad Norm=5.5646\n",
      "Fold 2, Epoch 2: Train Loss=1.7854, Train Acc=32.29%, Val Loss=2.0450, Val Acc=28.81%, Grad Norm=4.6222\n",
      "Fold 2, Epoch 3: Train Loss=1.6144, Train Acc=39.12%, Val Loss=1.8627, Val Acc=36.61%, Grad Norm=4.2860\n",
      "Fold 2, Epoch 4: Train Loss=1.4920, Train Acc=44.05%, Val Loss=1.6277, Val Acc=42.47%, Grad Norm=4.2517\n",
      "Fold 2, Epoch 5: Train Loss=1.3809, Train Acc=48.67%, Val Loss=1.5436, Val Acc=45.17%, Grad Norm=4.1975\n",
      "Fold 2, Epoch 6: Train Loss=1.3041, Train Acc=51.62%, Val Loss=1.4442, Val Acc=47.66%, Grad Norm=4.0843\n",
      "Fold 2, Epoch 7: Train Loss=1.2454, Train Acc=53.99%, Val Loss=1.4590, Val Acc=48.16%, Grad Norm=4.0745\n",
      "Fold 2, Epoch 8: Train Loss=1.1986, Train Acc=55.88%, Val Loss=1.3789, Val Acc=50.27%, Grad Norm=4.1031\n",
      "Fold 2, Epoch 9: Train Loss=1.1457, Train Acc=58.14%, Val Loss=1.3317, Val Acc=52.31%, Grad Norm=4.1718\n",
      "Fold 2, Epoch 10: Train Loss=1.0919, Train Acc=60.20%, Val Loss=1.3400, Val Acc=52.20%, Grad Norm=4.2427\n",
      "Fold 2, Epoch 11: Train Loss=1.0124, Train Acc=63.43%, Val Loss=1.2749, Val Acc=55.03%, Grad Norm=4.4271\n",
      "Fold 2, Epoch 12: Train Loss=0.9811, Train Acc=64.54%, Val Loss=1.2078, Val Acc=56.93%, Grad Norm=4.7089\n",
      "Fold 2, Epoch 13: Train Loss=0.9486, Train Acc=65.79%, Val Loss=1.2031, Val Acc=57.55%, Grad Norm=4.9227\n",
      "Fold 2, Epoch 14: Train Loss=0.9186, Train Acc=67.04%, Val Loss=1.1793, Val Acc=57.90%, Grad Norm=5.1514\n",
      "Fold 2, Epoch 15: Train Loss=0.8954, Train Acc=68.00%, Val Loss=1.1639, Val Acc=58.81%, Grad Norm=5.3848\n",
      "Fold 2, Epoch 16: Train Loss=0.8705, Train Acc=68.76%, Val Loss=1.2058, Val Acc=57.71%, Grad Norm=5.5556\n",
      "Fold 2, Epoch 17: Train Loss=0.8439, Train Acc=69.89%, Val Loss=1.1498, Val Acc=59.66%, Grad Norm=5.7865\n",
      "Fold 2, Epoch 18: Train Loss=0.8201, Train Acc=70.85%, Val Loss=1.1460, Val Acc=59.88%, Grad Norm=5.9715\n",
      "Fold 2, Epoch 19: Train Loss=0.7956, Train Acc=71.74%, Val Loss=1.1249, Val Acc=60.72%, Grad Norm=6.1563\n",
      "Fold 2, Epoch 20: Train Loss=0.7704, Train Acc=72.69%, Val Loss=1.0992, Val Acc=62.05%, Grad Norm=6.4153\n",
      "Fold 2, Epoch 21: Train Loss=0.7193, Train Acc=74.37%, Val Loss=1.1123, Val Acc=61.44%, Grad Norm=6.6243\n",
      "Fold 2, Epoch 22: Train Loss=0.6969, Train Acc=75.36%, Val Loss=1.1135, Val Acc=61.93%, Grad Norm=6.9650\n",
      "Fold 2, Epoch 23: Train Loss=0.6800, Train Acc=75.94%, Val Loss=1.1108, Val Acc=61.66%, Grad Norm=7.1856\n",
      "Fold 2, Epoch 24: Train Loss=0.6590, Train Acc=76.70%, Val Loss=1.0913, Val Acc=62.50%, Grad Norm=7.4136\n",
      "Fold 2, Epoch 25: Train Loss=0.6457, Train Acc=77.12%, Val Loss=1.0621, Val Acc=63.57%, Grad Norm=7.6784\n",
      "Fold 2, Epoch 26: Train Loss=0.6273, Train Acc=77.88%, Val Loss=1.1321, Val Acc=62.46%, Grad Norm=7.8951\n",
      "Fold 2, Epoch 27: Train Loss=0.6120, Train Acc=78.50%, Val Loss=1.0687, Val Acc=64.26%, Grad Norm=8.1238\n",
      "Fold 2, Epoch 28: Train Loss=0.6011, Train Acc=78.82%, Val Loss=1.0773, Val Acc=63.61%, Grad Norm=8.3596\n",
      "Fold 2, Epoch 29: Train Loss=0.5863, Train Acc=79.38%, Val Loss=1.0452, Val Acc=64.52%, Grad Norm=8.5938\n",
      "Fold 2, Epoch 30: Train Loss=0.5706, Train Acc=79.89%, Val Loss=1.0753, Val Acc=63.95%, Grad Norm=8.8053\n",
      "Fold 2, Epoch 31: Train Loss=0.5357, Train Acc=81.26%, Val Loss=1.0742, Val Acc=64.39%, Grad Norm=8.8479\n",
      "Fold 2, Epoch 32: Train Loss=0.5237, Train Acc=81.75%, Val Loss=1.0841, Val Acc=64.46%, Grad Norm=9.0729\n",
      "Fold 2, Epoch 33: Train Loss=0.5128, Train Acc=82.10%, Val Loss=1.0836, Val Acc=64.09%, Grad Norm=9.3163\n",
      "Fold 2, Epoch 34: Train Loss=0.5002, Train Acc=82.53%, Val Loss=1.1016, Val Acc=64.09%, Grad Norm=9.5366\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc (Test=10km/h) = 16.05%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.0909, Train Acc=19.58%, Val Loss=3.1366, Val Acc=15.51%, Grad Norm=5.7086\n",
      "Fold 3, Epoch 2: Train Loss=1.8724, Train Acc=29.47%, Val Loss=2.2107, Val Acc=27.16%, Grad Norm=4.3757\n",
      "Fold 3, Epoch 3: Train Loss=1.6822, Train Acc=36.70%, Val Loss=1.8329, Val Acc=34.75%, Grad Norm=4.1307\n",
      "Fold 3, Epoch 4: Train Loss=1.5647, Train Acc=41.43%, Val Loss=1.7110, Val Acc=39.50%, Grad Norm=3.9693\n",
      "Fold 3, Epoch 5: Train Loss=1.4582, Train Acc=45.85%, Val Loss=1.5719, Val Acc=43.05%, Grad Norm=4.0456\n",
      "Fold 3, Epoch 6: Train Loss=1.3579, Train Acc=49.77%, Val Loss=1.4268, Val Acc=48.69%, Grad Norm=4.0448\n",
      "Fold 3, Epoch 7: Train Loss=1.2926, Train Acc=52.14%, Val Loss=1.3984, Val Acc=49.22%, Grad Norm=3.9890\n",
      "Fold 3, Epoch 8: Train Loss=1.2379, Train Acc=54.41%, Val Loss=1.3403, Val Acc=50.67%, Grad Norm=4.0019\n",
      "Fold 3, Epoch 9: Train Loss=1.1924, Train Acc=56.13%, Val Loss=1.3432, Val Acc=51.15%, Grad Norm=4.0258\n",
      "Fold 3, Epoch 10: Train Loss=1.1525, Train Acc=57.94%, Val Loss=1.2617, Val Acc=54.81%, Grad Norm=4.0987\n",
      "Fold 3, Epoch 11: Train Loss=1.0747, Train Acc=60.90%, Val Loss=1.2216, Val Acc=56.30%, Grad Norm=4.2708\n",
      "Fold 3, Epoch 12: Train Loss=1.0393, Train Acc=62.47%, Val Loss=1.1843, Val Acc=57.36%, Grad Norm=4.5633\n",
      "Fold 3, Epoch 13: Train Loss=1.0116, Train Acc=63.47%, Val Loss=1.1931, Val Acc=57.93%, Grad Norm=4.7632\n",
      "Fold 3, Epoch 14: Train Loss=0.9852, Train Acc=64.41%, Val Loss=1.2051, Val Acc=57.72%, Grad Norm=4.9582\n",
      "Fold 3, Epoch 15: Train Loss=0.9579, Train Acc=65.45%, Val Loss=1.1416, Val Acc=59.27%, Grad Norm=5.1826\n",
      "Fold 3, Epoch 16: Train Loss=0.9328, Train Acc=66.31%, Val Loss=1.1620, Val Acc=59.48%, Grad Norm=5.3708\n",
      "Fold 3, Epoch 17: Train Loss=0.9044, Train Acc=67.55%, Val Loss=1.1394, Val Acc=60.37%, Grad Norm=5.5550\n",
      "Fold 3, Epoch 18: Train Loss=0.8788, Train Acc=68.55%, Val Loss=1.1217, Val Acc=61.38%, Grad Norm=5.8151\n",
      "Fold 3, Epoch 19: Train Loss=0.8554, Train Acc=69.50%, Val Loss=1.1215, Val Acc=61.44%, Grad Norm=6.0429\n",
      "Fold 3, Epoch 20: Train Loss=0.8310, Train Acc=70.15%, Val Loss=1.1004, Val Acc=62.44%, Grad Norm=6.2757\n",
      "Fold 3, Epoch 21: Train Loss=0.7770, Train Acc=72.58%, Val Loss=1.0766, Val Acc=63.86%, Grad Norm=6.4554\n",
      "Fold 3, Epoch 22: Train Loss=0.7544, Train Acc=73.17%, Val Loss=1.0800, Val Acc=63.52%, Grad Norm=6.7899\n",
      "Fold 3, Epoch 23: Train Loss=0.7380, Train Acc=73.92%, Val Loss=1.0728, Val Acc=63.70%, Grad Norm=7.0349\n",
      "Fold 3, Epoch 24: Train Loss=0.7156, Train Acc=74.67%, Val Loss=1.0629, Val Acc=64.30%, Grad Norm=7.3351\n",
      "Fold 3, Epoch 25: Train Loss=0.7030, Train Acc=75.10%, Val Loss=1.0391, Val Acc=64.67%, Grad Norm=7.6249\n",
      "Fold 3, Epoch 26: Train Loss=0.6849, Train Acc=75.90%, Val Loss=1.0627, Val Acc=64.50%, Grad Norm=7.8703\n",
      "Fold 3, Epoch 27: Train Loss=0.6679, Train Acc=76.36%, Val Loss=1.0636, Val Acc=64.73%, Grad Norm=8.1256\n",
      "Fold 3, Epoch 28: Train Loss=0.6559, Train Acc=76.72%, Val Loss=1.0402, Val Acc=65.56%, Grad Norm=8.3158\n",
      "Fold 3, Epoch 29: Train Loss=0.6413, Train Acc=77.42%, Val Loss=1.0564, Val Acc=65.08%, Grad Norm=8.5601\n",
      "Fold 3, Epoch 30: Train Loss=0.6239, Train Acc=78.05%, Val Loss=1.0823, Val Acc=64.36%, Grad Norm=8.7900\n",
      "Fold 3, Epoch 31: Train Loss=0.5897, Train Acc=79.25%, Val Loss=1.0548, Val Acc=65.49%, Grad Norm=8.9349\n",
      "Fold 3, Epoch 32: Train Loss=0.5777, Train Acc=79.69%, Val Loss=1.0699, Val Acc=65.46%, Grad Norm=9.1749\n",
      "Fold 3, Epoch 33: Train Loss=0.5645, Train Acc=80.32%, Val Loss=1.0621, Val Acc=65.79%, Grad Norm=9.4032\n",
      "Fold 3, Epoch 34: Train Loss=0.5570, Train Acc=80.38%, Val Loss=1.0610, Val Acc=66.16%, Grad Norm=9.6449\n",
      "Fold 3, Epoch 35: Train Loss=0.5456, Train Acc=80.89%, Val Loss=1.0738, Val Acc=65.69%, Grad Norm=9.8473\n",
      "Fold 3, Epoch 36: Train Loss=0.5409, Train Acc=80.96%, Val Loss=1.0674, Val Acc=65.76%, Grad Norm=10.0426\n",
      "Fold 3, Epoch 37: Train Loss=0.5280, Train Acc=81.73%, Val Loss=1.0633, Val Acc=66.24%, Grad Norm=10.1790\n",
      "Fold 3, Epoch 38: Train Loss=0.5217, Train Acc=81.61%, Val Loss=1.0805, Val Acc=65.58%, Grad Norm=10.4147\n",
      "Fold 3, Epoch 39: Train Loss=0.5103, Train Acc=82.16%, Val Loss=1.0627, Val Acc=66.14%, Grad Norm=10.5270\n",
      "Fold 3, Epoch 40: Train Loss=0.5019, Train Acc=82.40%, Val Loss=1.0976, Val Acc=65.89%, Grad Norm=10.7513\n",
      "Fold 3, Epoch 41: Train Loss=0.4821, Train Acc=83.41%, Val Loss=1.0956, Val Acc=65.74%, Grad Norm=10.7806\n",
      "Fold 3, Epoch 42: Train Loss=0.4743, Train Acc=83.49%, Val Loss=1.0874, Val Acc=66.33%, Grad Norm=10.9660\n",
      "Fold 3, Epoch 43: Train Loss=0.4682, Train Acc=83.82%, Val Loss=1.1046, Val Acc=65.83%, Grad Norm=11.0828\n",
      "Fold 3, Epoch 44: Train Loss=0.4609, Train Acc=83.94%, Val Loss=1.1070, Val Acc=65.89%, Grad Norm=11.1388\n",
      "Fold 3, Epoch 45: Train Loss=0.4568, Train Acc=84.28%, Val Loss=1.0977, Val Acc=66.26%, Grad Norm=11.2889\n",
      "Fold 3, Epoch 46: Train Loss=0.4538, Train Acc=84.17%, Val Loss=1.1120, Val Acc=66.00%, Grad Norm=11.4598\n",
      "Fold 3, Epoch 47: Train Loss=0.4509, Train Acc=84.18%, Val Loss=1.1054, Val Acc=66.15%, Grad Norm=11.5668\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc (Test=10km/h) = 15.31%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.0746, Train Acc=20.88%, Val Loss=2.8254, Val Acc=20.05%, Grad Norm=5.9069\n",
      "Fold 4, Epoch 2: Train Loss=1.7901, Train Acc=33.42%, Val Loss=2.0112, Val Acc=30.78%, Grad Norm=4.7092\n",
      "Fold 4, Epoch 3: Train Loss=1.6332, Train Acc=39.23%, Val Loss=1.7977, Val Acc=35.81%, Grad Norm=4.1518\n",
      "Fold 4, Epoch 4: Train Loss=1.5480, Train Acc=42.44%, Val Loss=1.7383, Val Acc=38.02%, Grad Norm=3.9510\n",
      "Fold 4, Epoch 5: Train Loss=1.4480, Train Acc=46.51%, Val Loss=1.5489, Val Acc=44.68%, Grad Norm=4.0356\n",
      "Fold 4, Epoch 6: Train Loss=1.3567, Train Acc=49.96%, Val Loss=1.4440, Val Acc=48.10%, Grad Norm=3.9893\n",
      "Fold 4, Epoch 7: Train Loss=1.2989, Train Acc=52.17%, Val Loss=1.4396, Val Acc=48.87%, Grad Norm=3.9558\n",
      "Fold 4, Epoch 8: Train Loss=1.2502, Train Acc=54.32%, Val Loss=1.3216, Val Acc=52.31%, Grad Norm=3.9960\n",
      "Fold 4, Epoch 9: Train Loss=1.2074, Train Acc=55.97%, Val Loss=1.3638, Val Acc=51.32%, Grad Norm=3.9892\n",
      "Fold 4, Epoch 10: Train Loss=1.1705, Train Acc=57.38%, Val Loss=1.2259, Val Acc=56.06%, Grad Norm=4.0862\n",
      "Fold 4, Epoch 11: Train Loss=1.0934, Train Acc=60.25%, Val Loss=1.1901, Val Acc=57.59%, Grad Norm=4.1999\n",
      "Fold 4, Epoch 12: Train Loss=1.0584, Train Acc=61.66%, Val Loss=1.1569, Val Acc=58.71%, Grad Norm=4.5097\n",
      "Fold 4, Epoch 13: Train Loss=1.0226, Train Acc=63.09%, Val Loss=1.1203, Val Acc=59.73%, Grad Norm=4.7800\n",
      "Fold 4, Epoch 14: Train Loss=0.9999, Train Acc=64.08%, Val Loss=1.1809, Val Acc=58.81%, Grad Norm=4.9847\n",
      "Fold 4, Epoch 15: Train Loss=0.9710, Train Acc=65.13%, Val Loss=1.1148, Val Acc=60.68%, Grad Norm=5.1861\n",
      "Fold 4, Epoch 16: Train Loss=0.9455, Train Acc=66.12%, Val Loss=1.1489, Val Acc=60.45%, Grad Norm=5.4114\n",
      "Fold 4, Epoch 17: Train Loss=0.9181, Train Acc=67.09%, Val Loss=1.1809, Val Acc=59.32%, Grad Norm=5.6337\n",
      "Fold 4, Epoch 18: Train Loss=0.8948, Train Acc=67.99%, Val Loss=1.1037, Val Acc=61.33%, Grad Norm=5.8559\n",
      "Fold 4, Epoch 19: Train Loss=0.8648, Train Acc=69.20%, Val Loss=1.1200, Val Acc=61.27%, Grad Norm=6.0742\n",
      "Fold 4, Epoch 20: Train Loss=0.8380, Train Acc=70.12%, Val Loss=1.0822, Val Acc=62.16%, Grad Norm=6.3037\n",
      "Fold 4, Epoch 21: Train Loss=0.7842, Train Acc=72.35%, Val Loss=1.0783, Val Acc=63.03%, Grad Norm=6.5262\n",
      "Fold 4, Epoch 22: Train Loss=0.7670, Train Acc=72.82%, Val Loss=1.0835, Val Acc=63.08%, Grad Norm=6.9140\n",
      "Fold 4, Epoch 23: Train Loss=0.7456, Train Acc=73.81%, Val Loss=1.0636, Val Acc=64.10%, Grad Norm=7.1727\n",
      "Fold 4, Epoch 24: Train Loss=0.7256, Train Acc=74.17%, Val Loss=1.0676, Val Acc=63.90%, Grad Norm=7.4185\n",
      "Fold 4, Epoch 25: Train Loss=0.7121, Train Acc=75.01%, Val Loss=1.0921, Val Acc=63.63%, Grad Norm=7.6380\n",
      "Fold 4, Epoch 26: Train Loss=0.6934, Train Acc=75.51%, Val Loss=1.0507, Val Acc=64.36%, Grad Norm=7.8739\n",
      "Fold 4, Epoch 27: Train Loss=0.6815, Train Acc=75.91%, Val Loss=1.0445, Val Acc=64.90%, Grad Norm=8.0728\n",
      "Fold 4, Epoch 28: Train Loss=0.6643, Train Acc=76.73%, Val Loss=1.0646, Val Acc=64.46%, Grad Norm=8.3210\n",
      "Fold 4, Epoch 29: Train Loss=0.6446, Train Acc=77.32%, Val Loss=1.0593, Val Acc=65.23%, Grad Norm=8.5784\n",
      "Fold 4, Epoch 30: Train Loss=0.6322, Train Acc=77.82%, Val Loss=1.0550, Val Acc=65.05%, Grad Norm=8.7618\n",
      "Fold 4, Epoch 31: Train Loss=0.6006, Train Acc=78.87%, Val Loss=1.0464, Val Acc=65.28%, Grad Norm=8.9458\n",
      "Fold 4, Epoch 32: Train Loss=0.5846, Train Acc=79.54%, Val Loss=1.0359, Val Acc=65.33%, Grad Norm=9.1571\n",
      "Fold 4, Epoch 33: Train Loss=0.5722, Train Acc=79.92%, Val Loss=1.0571, Val Acc=65.40%, Grad Norm=9.3441\n",
      "Fold 4, Epoch 34: Train Loss=0.5600, Train Acc=80.48%, Val Loss=1.0634, Val Acc=65.25%, Grad Norm=9.5399\n",
      "Fold 4, Epoch 35: Train Loss=0.5535, Train Acc=80.53%, Val Loss=1.0582, Val Acc=65.62%, Grad Norm=9.7583\n",
      "Fold 4, Epoch 36: Train Loss=0.5474, Train Acc=80.75%, Val Loss=1.0745, Val Acc=65.54%, Grad Norm=9.9657\n",
      "Fold 4, Epoch 37: Train Loss=0.5369, Train Acc=81.21%, Val Loss=1.0844, Val Acc=65.10%, Grad Norm=10.1666\n",
      "Fold 4, Epoch 38: Train Loss=0.5278, Train Acc=81.55%, Val Loss=1.0748, Val Acc=65.57%, Grad Norm=10.3237\n",
      "Fold 4, Epoch 39: Train Loss=0.5185, Train Acc=81.84%, Val Loss=1.0658, Val Acc=65.63%, Grad Norm=10.4972\n",
      "Fold 4, Epoch 40: Train Loss=0.5086, Train Acc=82.22%, Val Loss=1.0712, Val Acc=65.51%, Grad Norm=10.6862\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc (Test=10km/h) = 15.55%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.0778, Train Acc=19.77%, Val Loss=3.7820, Val Acc=17.21%, Grad Norm=5.7623\n",
      "Fold 5, Epoch 2: Train Loss=1.8089, Train Acc=31.74%, Val Loss=2.2221, Val Acc=29.48%, Grad Norm=4.5320\n",
      "Fold 5, Epoch 3: Train Loss=1.6342, Train Acc=38.50%, Val Loss=2.0419, Val Acc=34.37%, Grad Norm=4.1466\n",
      "Fold 5, Epoch 4: Train Loss=1.5211, Train Acc=43.29%, Val Loss=1.9814, Val Acc=36.86%, Grad Norm=4.0834\n",
      "Fold 5, Epoch 5: Train Loss=1.4027, Train Acc=47.96%, Val Loss=1.7284, Val Acc=42.37%, Grad Norm=4.1077\n",
      "Fold 5, Epoch 6: Train Loss=1.3214, Train Acc=51.19%, Val Loss=1.6063, Val Acc=44.97%, Grad Norm=4.0484\n",
      "Fold 5, Epoch 7: Train Loss=1.2622, Train Acc=53.37%, Val Loss=1.5457, Val Acc=47.63%, Grad Norm=3.9882\n",
      "Fold 5, Epoch 8: Train Loss=1.2149, Train Acc=55.07%, Val Loss=1.5214, Val Acc=48.68%, Grad Norm=3.9748\n",
      "Fold 5, Epoch 9: Train Loss=1.1777, Train Acc=56.82%, Val Loss=1.4497, Val Acc=50.55%, Grad Norm=4.0336\n",
      "Fold 5, Epoch 10: Train Loss=1.1436, Train Acc=58.09%, Val Loss=1.4235, Val Acc=51.48%, Grad Norm=4.0627\n",
      "Fold 5, Epoch 11: Train Loss=1.0659, Train Acc=60.99%, Val Loss=1.3972, Val Acc=52.38%, Grad Norm=4.2470\n",
      "Fold 5, Epoch 12: Train Loss=1.0346, Train Acc=62.32%, Val Loss=1.4102, Val Acc=53.31%, Grad Norm=4.5160\n",
      "Fold 5, Epoch 13: Train Loss=1.0056, Train Acc=63.50%, Val Loss=1.3506, Val Acc=54.48%, Grad Norm=4.7475\n",
      "Fold 5, Epoch 14: Train Loss=0.9795, Train Acc=64.57%, Val Loss=1.3761, Val Acc=54.72%, Grad Norm=4.9741\n",
      "Fold 5, Epoch 15: Train Loss=0.9486, Train Acc=65.70%, Val Loss=1.3752, Val Acc=55.51%, Grad Norm=5.1824\n",
      "Fold 5, Epoch 16: Train Loss=0.9264, Train Acc=66.56%, Val Loss=1.3599, Val Acc=56.72%, Grad Norm=5.4006\n",
      "Fold 5, Epoch 17: Train Loss=0.9008, Train Acc=67.53%, Val Loss=1.3000, Val Acc=56.94%, Grad Norm=5.6131\n",
      "Fold 5, Epoch 18: Train Loss=0.8755, Train Acc=68.44%, Val Loss=1.3591, Val Acc=56.70%, Grad Norm=5.8248\n",
      "Fold 5, Epoch 19: Train Loss=0.8511, Train Acc=69.37%, Val Loss=1.3633, Val Acc=57.25%, Grad Norm=6.0552\n",
      "Fold 5, Epoch 20: Train Loss=0.8309, Train Acc=70.35%, Val Loss=1.3243, Val Acc=57.84%, Grad Norm=6.2376\n",
      "Fold 5, Epoch 21: Train Loss=0.7753, Train Acc=72.42%, Val Loss=1.2680, Val Acc=59.25%, Grad Norm=6.4492\n",
      "Fold 5, Epoch 22: Train Loss=0.7529, Train Acc=73.23%, Val Loss=1.2671, Val Acc=60.13%, Grad Norm=6.8291\n",
      "Fold 5, Epoch 23: Train Loss=0.7300, Train Acc=74.02%, Val Loss=1.2853, Val Acc=59.55%, Grad Norm=7.1203\n",
      "Fold 5, Epoch 24: Train Loss=0.7142, Train Acc=74.62%, Val Loss=1.2597, Val Acc=60.30%, Grad Norm=7.3987\n",
      "Fold 5, Epoch 25: Train Loss=0.6995, Train Acc=75.12%, Val Loss=1.2586, Val Acc=61.39%, Grad Norm=7.6844\n",
      "Fold 5, Epoch 26: Train Loss=0.6813, Train Acc=75.91%, Val Loss=1.2773, Val Acc=60.89%, Grad Norm=7.9103\n",
      "Fold 5, Epoch 27: Train Loss=0.6641, Train Acc=76.39%, Val Loss=1.2638, Val Acc=60.84%, Grad Norm=8.1164\n",
      "Fold 5, Epoch 28: Train Loss=0.6490, Train Acc=77.05%, Val Loss=1.3103, Val Acc=60.19%, Grad Norm=8.4313\n",
      "Fold 5, Epoch 29: Train Loss=0.6333, Train Acc=77.52%, Val Loss=1.2559, Val Acc=61.66%, Grad Norm=8.6898\n",
      "Fold 5, Epoch 30: Train Loss=0.6171, Train Acc=78.18%, Val Loss=1.2988, Val Acc=61.13%, Grad Norm=8.8450\n",
      "Fold 5, Epoch 31: Train Loss=0.5839, Train Acc=79.55%, Val Loss=1.2995, Val Acc=61.24%, Grad Norm=9.0253\n",
      "Fold 5, Epoch 32: Train Loss=0.5733, Train Acc=79.83%, Val Loss=1.3125, Val Acc=60.74%, Grad Norm=9.3210\n",
      "Fold 5, Epoch 33: Train Loss=0.5583, Train Acc=80.50%, Val Loss=1.2999, Val Acc=61.53%, Grad Norm=9.5447\n",
      "Fold 5, Epoch 34: Train Loss=0.5472, Train Acc=80.75%, Val Loss=1.3058, Val Acc=61.02%, Grad Norm=9.7536\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc (Test=10km/h) = 15.78%\n",
      "\n",
      "SNR  10 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-23_22-36-39_LTE-V_XFR_SpeedSplit_FileBlock_Train20kmh_Test10kmh_SNR10dB_fdTrain655_fdTest655_group288_ResNet1D\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=5 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "[INFO] 共找到 72 个 .mat 文件（含子目录）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 可用 TX 类别数（train/test 交集）: 9\n",
      "[INFO] Train blocks: 360, Test blocks: 360, sample_len: 288, group_size: 288\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.1057, Train Acc=19.26%, Val Loss=2.7683, Val Acc=18.40%, Grad Norm=5.4609\n",
      "Fold 1, Epoch 2: Train Loss=1.9375, Train Acc=27.24%, Val Loss=2.5434, Val Acc=18.79%, Grad Norm=3.9443\n",
      "Fold 1, Epoch 3: Train Loss=1.8401, Train Acc=31.61%, Val Loss=2.1410, Val Acc=22.74%, Grad Norm=3.6151\n",
      "Fold 1, Epoch 4: Train Loss=1.7286, Train Acc=36.00%, Val Loss=1.9542, Val Acc=29.57%, Grad Norm=3.7503\n",
      "Fold 1, Epoch 5: Train Loss=1.6245, Train Acc=39.87%, Val Loss=1.8744, Val Acc=32.85%, Grad Norm=3.6975\n",
      "Fold 1, Epoch 6: Train Loss=1.5601, Train Acc=42.40%, Val Loss=1.7844, Val Acc=34.65%, Grad Norm=3.6211\n",
      "Fold 1, Epoch 7: Train Loss=1.4965, Train Acc=44.83%, Val Loss=1.7011, Val Acc=39.63%, Grad Norm=3.7095\n",
      "Fold 1, Epoch 8: Train Loss=1.4399, Train Acc=47.10%, Val Loss=1.6580, Val Acc=42.15%, Grad Norm=3.7285\n",
      "Fold 1, Epoch 9: Train Loss=1.3911, Train Acc=49.03%, Val Loss=1.6363, Val Acc=41.47%, Grad Norm=3.7454\n",
      "Fold 1, Epoch 10: Train Loss=1.3564, Train Acc=50.41%, Val Loss=1.6404, Val Acc=43.77%, Grad Norm=3.7988\n",
      "Fold 1, Epoch 11: Train Loss=1.2906, Train Acc=52.77%, Val Loss=1.5722, Val Acc=45.27%, Grad Norm=3.9087\n",
      "Fold 1, Epoch 12: Train Loss=1.2656, Train Acc=53.78%, Val Loss=1.5784, Val Acc=46.05%, Grad Norm=4.1571\n",
      "Fold 1, Epoch 13: Train Loss=1.2475, Train Acc=54.44%, Val Loss=1.5317, Val Acc=46.79%, Grad Norm=4.3620\n",
      "Fold 1, Epoch 14: Train Loss=1.2202, Train Acc=55.47%, Val Loss=1.5424, Val Acc=47.28%, Grad Norm=4.5421\n",
      "Fold 1, Epoch 15: Train Loss=1.2028, Train Acc=56.43%, Val Loss=1.5232, Val Acc=47.34%, Grad Norm=4.7399\n",
      "Fold 1, Epoch 16: Train Loss=1.1796, Train Acc=57.33%, Val Loss=1.5277, Val Acc=47.37%, Grad Norm=4.9117\n",
      "Fold 1, Epoch 17: Train Loss=1.1562, Train Acc=58.41%, Val Loss=1.5056, Val Acc=48.39%, Grad Norm=5.1207\n",
      "Fold 1, Epoch 18: Train Loss=1.1381, Train Acc=58.91%, Val Loss=1.5352, Val Acc=48.98%, Grad Norm=5.3271\n",
      "Fold 1, Epoch 19: Train Loss=1.1167, Train Acc=60.02%, Val Loss=1.5008, Val Acc=49.22%, Grad Norm=5.5138\n",
      "Fold 1, Epoch 20: Train Loss=1.0980, Train Acc=60.46%, Val Loss=1.4544, Val Acc=50.38%, Grad Norm=5.6844\n",
      "Fold 1, Epoch 21: Train Loss=1.0469, Train Acc=62.58%, Val Loss=1.4329, Val Acc=51.52%, Grad Norm=5.9431\n",
      "Fold 1, Epoch 22: Train Loss=1.0323, Train Acc=63.08%, Val Loss=1.4327, Val Acc=51.24%, Grad Norm=6.3009\n",
      "Fold 1, Epoch 23: Train Loss=1.0136, Train Acc=63.72%, Val Loss=1.4126, Val Acc=51.88%, Grad Norm=6.5329\n",
      "Fold 1, Epoch 24: Train Loss=0.9989, Train Acc=64.35%, Val Loss=1.4183, Val Acc=52.31%, Grad Norm=6.7816\n",
      "Fold 1, Epoch 25: Train Loss=0.9875, Train Acc=64.75%, Val Loss=1.4295, Val Acc=52.17%, Grad Norm=7.0035\n",
      "Fold 1, Epoch 26: Train Loss=0.9713, Train Acc=65.43%, Val Loss=1.4380, Val Acc=51.96%, Grad Norm=7.2799\n",
      "Fold 1, Epoch 27: Train Loss=0.9550, Train Acc=65.99%, Val Loss=1.4254, Val Acc=52.43%, Grad Norm=7.5205\n",
      "Fold 1, Epoch 28: Train Loss=0.9418, Train Acc=66.50%, Val Loss=1.4084, Val Acc=53.17%, Grad Norm=7.7987\n",
      "Fold 1, Epoch 29: Train Loss=0.9271, Train Acc=67.07%, Val Loss=1.4467, Val Acc=52.75%, Grad Norm=7.9877\n",
      "Fold 1, Epoch 30: Train Loss=0.9170, Train Acc=67.48%, Val Loss=1.4143, Val Acc=52.67%, Grad Norm=8.2813\n",
      "Fold 1, Epoch 31: Train Loss=0.8839, Train Acc=68.78%, Val Loss=1.3943, Val Acc=53.26%, Grad Norm=8.4520\n",
      "Fold 1, Epoch 32: Train Loss=0.8697, Train Acc=69.25%, Val Loss=1.3981, Val Acc=53.77%, Grad Norm=8.7433\n",
      "Fold 1, Epoch 33: Train Loss=0.8588, Train Acc=69.53%, Val Loss=1.4016, Val Acc=54.08%, Grad Norm=9.0244\n",
      "Fold 1, Epoch 34: Train Loss=0.8541, Train Acc=69.78%, Val Loss=1.4138, Val Acc=53.08%, Grad Norm=9.2577\n",
      "Fold 1, Epoch 35: Train Loss=0.8417, Train Acc=70.17%, Val Loss=1.4237, Val Acc=53.40%, Grad Norm=9.4808\n",
      "Fold 1, Epoch 36: Train Loss=0.8316, Train Acc=70.70%, Val Loss=1.3868, Val Acc=54.32%, Grad Norm=9.6735\n",
      "Fold 1, Epoch 37: Train Loss=0.8234, Train Acc=70.83%, Val Loss=1.4261, Val Acc=53.40%, Grad Norm=9.9015\n",
      "Fold 1, Epoch 38: Train Loss=0.8177, Train Acc=70.98%, Val Loss=1.3843, Val Acc=54.00%, Grad Norm=10.1026\n",
      "Fold 1, Epoch 39: Train Loss=0.8063, Train Acc=71.43%, Val Loss=1.4092, Val Acc=53.57%, Grad Norm=10.2664\n",
      "Fold 1, Epoch 40: Train Loss=0.8008, Train Acc=71.74%, Val Loss=1.4223, Val Acc=53.17%, Grad Norm=10.5831\n",
      "Fold 1, Epoch 41: Train Loss=0.7808, Train Acc=72.46%, Val Loss=1.4336, Val Acc=53.63%, Grad Norm=10.7031\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc (Test=10km/h) = 14.25%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.1158, Train Acc=18.45%, Val Loss=3.2789, Val Acc=13.30%, Grad Norm=5.3798\n",
      "Fold 2, Epoch 2: Train Loss=1.9507, Train Acc=25.76%, Val Loss=2.5852, Val Acc=17.61%, Grad Norm=4.0538\n",
      "Fold 2, Epoch 3: Train Loss=1.8047, Train Acc=31.79%, Val Loss=2.1215, Val Acc=27.02%, Grad Norm=3.9852\n",
      "Fold 2, Epoch 4: Train Loss=1.6999, Train Acc=36.01%, Val Loss=2.0512, Val Acc=28.86%, Grad Norm=3.7803\n",
      "Fold 2, Epoch 5: Train Loss=1.6245, Train Acc=39.01%, Val Loss=1.8872, Val Acc=31.75%, Grad Norm=3.6654\n",
      "Fold 2, Epoch 6: Train Loss=1.5741, Train Acc=41.19%, Val Loss=1.8075, Val Acc=34.88%, Grad Norm=3.6280\n",
      "Fold 2, Epoch 7: Train Loss=1.5263, Train Acc=43.13%, Val Loss=1.7535, Val Acc=36.70%, Grad Norm=3.6448\n",
      "Fold 2, Epoch 8: Train Loss=1.4674, Train Acc=45.63%, Val Loss=1.6514, Val Acc=40.68%, Grad Norm=3.7586\n",
      "Fold 2, Epoch 9: Train Loss=1.4146, Train Acc=47.73%, Val Loss=1.6221, Val Acc=42.04%, Grad Norm=3.8089\n",
      "Fold 2, Epoch 10: Train Loss=1.3704, Train Acc=49.52%, Val Loss=1.4959, Val Acc=45.66%, Grad Norm=3.8446\n",
      "Fold 2, Epoch 11: Train Loss=1.3064, Train Acc=52.23%, Val Loss=1.4646, Val Acc=47.47%, Grad Norm=3.9873\n",
      "Fold 2, Epoch 12: Train Loss=1.2786, Train Acc=53.13%, Val Loss=1.4611, Val Acc=47.39%, Grad Norm=4.2298\n",
      "Fold 2, Epoch 13: Train Loss=1.2563, Train Acc=53.99%, Val Loss=1.4506, Val Acc=47.62%, Grad Norm=4.4117\n",
      "Fold 2, Epoch 14: Train Loss=1.2328, Train Acc=54.84%, Val Loss=1.4406, Val Acc=48.51%, Grad Norm=4.5991\n",
      "Fold 2, Epoch 15: Train Loss=1.2157, Train Acc=55.67%, Val Loss=1.4243, Val Acc=49.17%, Grad Norm=4.7637\n",
      "Fold 2, Epoch 16: Train Loss=1.1947, Train Acc=56.56%, Val Loss=1.3931, Val Acc=50.16%, Grad Norm=4.9325\n",
      "Fold 2, Epoch 17: Train Loss=1.1737, Train Acc=57.30%, Val Loss=1.4079, Val Acc=50.06%, Grad Norm=5.1244\n",
      "Fold 2, Epoch 18: Train Loss=1.1526, Train Acc=57.94%, Val Loss=1.3774, Val Acc=50.34%, Grad Norm=5.3162\n",
      "Fold 2, Epoch 19: Train Loss=1.1281, Train Acc=59.21%, Val Loss=1.3696, Val Acc=51.28%, Grad Norm=5.4813\n",
      "Fold 2, Epoch 20: Train Loss=1.1050, Train Acc=59.95%, Val Loss=1.3384, Val Acc=52.03%, Grad Norm=5.7168\n",
      "Fold 2, Epoch 21: Train Loss=1.0603, Train Acc=61.99%, Val Loss=1.3425, Val Acc=52.22%, Grad Norm=5.9186\n",
      "Fold 2, Epoch 22: Train Loss=1.0381, Train Acc=62.66%, Val Loss=1.3379, Val Acc=52.34%, Grad Norm=6.2666\n",
      "Fold 2, Epoch 23: Train Loss=1.0213, Train Acc=63.39%, Val Loss=1.3193, Val Acc=53.14%, Grad Norm=6.5282\n",
      "Fold 2, Epoch 24: Train Loss=1.0049, Train Acc=64.12%, Val Loss=1.3469, Val Acc=52.36%, Grad Norm=6.8255\n",
      "Fold 2, Epoch 25: Train Loss=0.9910, Train Acc=64.54%, Val Loss=1.3362, Val Acc=52.92%, Grad Norm=7.0116\n",
      "Fold 2, Epoch 26: Train Loss=0.9797, Train Acc=64.97%, Val Loss=1.3109, Val Acc=53.60%, Grad Norm=7.2665\n",
      "Fold 2, Epoch 27: Train Loss=0.9625, Train Acc=65.43%, Val Loss=1.3166, Val Acc=53.70%, Grad Norm=7.4904\n",
      "Fold 2, Epoch 28: Train Loss=0.9479, Train Acc=66.12%, Val Loss=1.2990, Val Acc=54.73%, Grad Norm=7.7321\n",
      "Fold 2, Epoch 29: Train Loss=0.9339, Train Acc=66.55%, Val Loss=1.2855, Val Acc=54.89%, Grad Norm=7.9951\n",
      "Fold 2, Epoch 30: Train Loss=0.9210, Train Acc=67.14%, Val Loss=1.3171, Val Acc=54.25%, Grad Norm=8.2539\n",
      "Fold 2, Epoch 31: Train Loss=0.8863, Train Acc=68.53%, Val Loss=1.2828, Val Acc=55.67%, Grad Norm=8.4487\n",
      "Fold 2, Epoch 32: Train Loss=0.8763, Train Acc=68.93%, Val Loss=1.2883, Val Acc=55.22%, Grad Norm=8.7506\n",
      "Fold 2, Epoch 33: Train Loss=0.8656, Train Acc=69.29%, Val Loss=1.2795, Val Acc=55.57%, Grad Norm=8.9633\n",
      "Fold 2, Epoch 34: Train Loss=0.8565, Train Acc=69.54%, Val Loss=1.2852, Val Acc=55.11%, Grad Norm=9.2063\n",
      "Fold 2, Epoch 35: Train Loss=0.8418, Train Acc=70.25%, Val Loss=1.3037, Val Acc=54.94%, Grad Norm=9.4061\n",
      "Fold 2, Epoch 36: Train Loss=0.8390, Train Acc=70.18%, Val Loss=1.2890, Val Acc=55.57%, Grad Norm=9.7115\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc (Test=10km/h) = 15.38%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.1163, Train Acc=18.01%, Val Loss=2.6412, Val Acc=14.87%, Grad Norm=5.4694\n",
      "Fold 3, Epoch 2: Train Loss=1.9393, Train Acc=26.44%, Val Loss=2.3112, Val Acc=23.74%, Grad Norm=4.0814\n",
      "Fold 3, Epoch 3: Train Loss=1.7837, Train Acc=32.62%, Val Loss=2.0516, Val Acc=29.48%, Grad Norm=3.8689\n",
      "Fold 3, Epoch 4: Train Loss=1.6812, Train Acc=36.87%, Val Loss=1.9795, Val Acc=32.53%, Grad Norm=3.7113\n",
      "Fold 3, Epoch 5: Train Loss=1.6178, Train Acc=39.57%, Val Loss=1.8116, Val Acc=35.95%, Grad Norm=3.6380\n",
      "Fold 3, Epoch 6: Train Loss=1.5531, Train Acc=42.07%, Val Loss=1.6840, Val Acc=39.05%, Grad Norm=3.7036\n",
      "Fold 3, Epoch 7: Train Loss=1.4891, Train Acc=44.82%, Val Loss=1.5609, Val Acc=43.12%, Grad Norm=3.7310\n",
      "Fold 3, Epoch 8: Train Loss=1.4365, Train Acc=46.91%, Val Loss=1.5184, Val Acc=44.86%, Grad Norm=3.7522\n",
      "Fold 3, Epoch 9: Train Loss=1.3938, Train Acc=48.78%, Val Loss=1.4813, Val Acc=45.75%, Grad Norm=3.7842\n",
      "Fold 3, Epoch 10: Train Loss=1.3614, Train Acc=50.20%, Val Loss=1.4194, Val Acc=48.23%, Grad Norm=3.8124\n",
      "Fold 3, Epoch 11: Train Loss=1.3005, Train Acc=52.53%, Val Loss=1.4062, Val Acc=49.10%, Grad Norm=3.9444\n",
      "Fold 3, Epoch 12: Train Loss=1.2731, Train Acc=53.56%, Val Loss=1.3606, Val Acc=51.16%, Grad Norm=4.1901\n",
      "Fold 3, Epoch 13: Train Loss=1.2516, Train Acc=54.28%, Val Loss=1.3763, Val Acc=50.58%, Grad Norm=4.3908\n",
      "Fold 3, Epoch 14: Train Loss=1.2301, Train Acc=55.05%, Val Loss=1.3439, Val Acc=51.27%, Grad Norm=4.5604\n",
      "Fold 3, Epoch 15: Train Loss=1.2094, Train Acc=56.08%, Val Loss=1.3388, Val Acc=51.99%, Grad Norm=4.7296\n",
      "Fold 3, Epoch 16: Train Loss=1.1857, Train Acc=56.98%, Val Loss=1.3319, Val Acc=52.51%, Grad Norm=4.9372\n",
      "Fold 3, Epoch 17: Train Loss=1.1668, Train Acc=57.59%, Val Loss=1.3121, Val Acc=53.76%, Grad Norm=5.1252\n",
      "Fold 3, Epoch 18: Train Loss=1.1453, Train Acc=58.52%, Val Loss=1.2841, Val Acc=54.52%, Grad Norm=5.3207\n",
      "Fold 3, Epoch 19: Train Loss=1.1213, Train Acc=59.51%, Val Loss=1.3015, Val Acc=53.83%, Grad Norm=5.5031\n",
      "Fold 3, Epoch 20: Train Loss=1.1014, Train Acc=60.12%, Val Loss=1.2723, Val Acc=54.74%, Grad Norm=5.7316\n",
      "Fold 3, Epoch 21: Train Loss=1.0516, Train Acc=62.11%, Val Loss=1.2470, Val Acc=56.23%, Grad Norm=5.9574\n",
      "Fold 3, Epoch 22: Train Loss=1.0358, Train Acc=62.60%, Val Loss=1.2677, Val Acc=55.52%, Grad Norm=6.3145\n",
      "Fold 3, Epoch 23: Train Loss=1.0176, Train Acc=63.48%, Val Loss=1.2528, Val Acc=56.45%, Grad Norm=6.5759\n",
      "Fold 3, Epoch 24: Train Loss=1.0015, Train Acc=64.13%, Val Loss=1.2550, Val Acc=56.22%, Grad Norm=6.8427\n",
      "Fold 3, Epoch 25: Train Loss=0.9923, Train Acc=64.52%, Val Loss=1.2691, Val Acc=56.32%, Grad Norm=7.0617\n",
      "Fold 3, Epoch 26: Train Loss=0.9757, Train Acc=65.13%, Val Loss=1.2463, Val Acc=56.60%, Grad Norm=7.3186\n",
      "Fold 3, Epoch 27: Train Loss=0.9609, Train Acc=65.93%, Val Loss=1.2545, Val Acc=56.27%, Grad Norm=7.5399\n",
      "Fold 3, Epoch 28: Train Loss=0.9446, Train Acc=66.22%, Val Loss=1.2552, Val Acc=56.96%, Grad Norm=7.8221\n",
      "Fold 3, Epoch 29: Train Loss=0.9369, Train Acc=66.70%, Val Loss=1.2388, Val Acc=57.22%, Grad Norm=8.0588\n",
      "Fold 3, Epoch 30: Train Loss=0.9168, Train Acc=67.36%, Val Loss=1.2449, Val Acc=57.61%, Grad Norm=8.2862\n",
      "Fold 3, Epoch 31: Train Loss=0.8880, Train Acc=68.57%, Val Loss=1.2334, Val Acc=57.80%, Grad Norm=8.5010\n",
      "Fold 3, Epoch 32: Train Loss=0.8747, Train Acc=68.84%, Val Loss=1.2456, Val Acc=57.46%, Grad Norm=8.7990\n",
      "Fold 3, Epoch 33: Train Loss=0.8652, Train Acc=69.36%, Val Loss=1.2434, Val Acc=57.85%, Grad Norm=9.0027\n",
      "Fold 3, Epoch 34: Train Loss=0.8553, Train Acc=69.54%, Val Loss=1.2408, Val Acc=58.02%, Grad Norm=9.2511\n",
      "Fold 3, Epoch 35: Train Loss=0.8442, Train Acc=70.12%, Val Loss=1.2518, Val Acc=57.62%, Grad Norm=9.5026\n",
      "Fold 3, Epoch 36: Train Loss=0.8361, Train Acc=70.42%, Val Loss=1.2402, Val Acc=58.13%, Grad Norm=9.6874\n",
      "Fold 3, Epoch 37: Train Loss=0.8265, Train Acc=70.76%, Val Loss=1.2626, Val Acc=57.45%, Grad Norm=9.9017\n",
      "Fold 3, Epoch 38: Train Loss=0.8209, Train Acc=70.96%, Val Loss=1.2383, Val Acc=58.09%, Grad Norm=10.1087\n",
      "Fold 3, Epoch 39: Train Loss=0.8108, Train Acc=71.36%, Val Loss=1.2634, Val Acc=57.46%, Grad Norm=10.2909\n",
      "Fold 3, Epoch 40: Train Loss=0.7984, Train Acc=71.77%, Val Loss=1.2635, Val Acc=58.06%, Grad Norm=10.5242\n",
      "Fold 3, Epoch 41: Train Loss=0.7830, Train Acc=72.26%, Val Loss=1.2562, Val Acc=57.94%, Grad Norm=10.6769\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc (Test=10km/h) = 15.06%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1101, Train Acc=18.96%, Val Loss=2.6835, Val Acc=17.66%, Grad Norm=5.5347\n",
      "Fold 4, Epoch 2: Train Loss=1.9391, Train Acc=27.30%, Val Loss=2.2952, Val Acc=21.84%, Grad Norm=4.1583\n",
      "Fold 4, Epoch 3: Train Loss=1.7934, Train Acc=33.09%, Val Loss=1.8733, Val Acc=32.24%, Grad Norm=3.9531\n",
      "Fold 4, Epoch 4: Train Loss=1.6881, Train Acc=37.02%, Val Loss=1.8060, Val Acc=34.02%, Grad Norm=3.7613\n",
      "Fold 4, Epoch 5: Train Loss=1.6271, Train Acc=39.60%, Val Loss=1.6869, Val Acc=37.33%, Grad Norm=3.6232\n",
      "Fold 4, Epoch 6: Train Loss=1.5808, Train Acc=41.33%, Val Loss=1.6955, Val Acc=36.74%, Grad Norm=3.5369\n",
      "Fold 4, Epoch 7: Train Loss=1.5421, Train Acc=42.93%, Val Loss=1.5666, Val Acc=41.41%, Grad Norm=3.5509\n",
      "Fold 4, Epoch 8: Train Loss=1.4864, Train Acc=45.19%, Val Loss=1.5000, Val Acc=45.33%, Grad Norm=3.6695\n",
      "Fold 4, Epoch 9: Train Loss=1.4295, Train Acc=47.59%, Val Loss=1.5074, Val Acc=45.15%, Grad Norm=3.7329\n",
      "Fold 4, Epoch 10: Train Loss=1.3894, Train Acc=49.17%, Val Loss=1.4803, Val Acc=45.64%, Grad Norm=3.7608\n",
      "Fold 4, Epoch 11: Train Loss=1.3285, Train Acc=51.52%, Val Loss=1.4135, Val Acc=48.70%, Grad Norm=3.8967\n",
      "Fold 4, Epoch 12: Train Loss=1.3032, Train Acc=52.46%, Val Loss=1.3778, Val Acc=49.71%, Grad Norm=4.1581\n",
      "Fold 4, Epoch 13: Train Loss=1.2795, Train Acc=53.68%, Val Loss=1.3940, Val Acc=49.20%, Grad Norm=4.3308\n",
      "Fold 4, Epoch 14: Train Loss=1.2572, Train Acc=54.46%, Val Loss=1.3401, Val Acc=51.69%, Grad Norm=4.5289\n",
      "Fold 4, Epoch 15: Train Loss=1.2389, Train Acc=55.08%, Val Loss=1.3453, Val Acc=51.56%, Grad Norm=4.6900\n",
      "Fold 4, Epoch 16: Train Loss=1.2133, Train Acc=56.07%, Val Loss=1.3431, Val Acc=52.06%, Grad Norm=4.8802\n",
      "Fold 4, Epoch 17: Train Loss=1.1953, Train Acc=56.74%, Val Loss=1.3283, Val Acc=52.40%, Grad Norm=5.0476\n",
      "Fold 4, Epoch 18: Train Loss=1.1715, Train Acc=57.67%, Val Loss=1.2947, Val Acc=53.62%, Grad Norm=5.2480\n",
      "Fold 4, Epoch 19: Train Loss=1.1522, Train Acc=58.49%, Val Loss=1.2988, Val Acc=54.03%, Grad Norm=5.4449\n",
      "Fold 4, Epoch 20: Train Loss=1.1301, Train Acc=59.41%, Val Loss=1.2830, Val Acc=54.45%, Grad Norm=5.6546\n",
      "Fold 4, Epoch 21: Train Loss=1.0842, Train Acc=61.19%, Val Loss=1.2677, Val Acc=55.28%, Grad Norm=5.8615\n",
      "Fold 4, Epoch 22: Train Loss=1.0642, Train Acc=61.75%, Val Loss=1.2536, Val Acc=55.71%, Grad Norm=6.2340\n",
      "Fold 4, Epoch 23: Train Loss=1.0480, Train Acc=62.54%, Val Loss=1.2476, Val Acc=55.82%, Grad Norm=6.4658\n",
      "Fold 4, Epoch 24: Train Loss=1.0328, Train Acc=63.12%, Val Loss=1.2529, Val Acc=55.96%, Grad Norm=6.7540\n",
      "Fold 4, Epoch 25: Train Loss=1.0192, Train Acc=63.61%, Val Loss=1.2583, Val Acc=56.43%, Grad Norm=6.9943\n",
      "Fold 4, Epoch 26: Train Loss=1.0050, Train Acc=64.30%, Val Loss=1.2408, Val Acc=56.67%, Grad Norm=7.2011\n",
      "Fold 4, Epoch 27: Train Loss=0.9935, Train Acc=64.48%, Val Loss=1.2501, Val Acc=56.57%, Grad Norm=7.4789\n",
      "Fold 4, Epoch 28: Train Loss=0.9788, Train Acc=65.28%, Val Loss=1.2614, Val Acc=56.37%, Grad Norm=7.7041\n",
      "Fold 4, Epoch 29: Train Loss=0.9649, Train Acc=65.76%, Val Loss=1.2511, Val Acc=56.60%, Grad Norm=7.9272\n",
      "Fold 4, Epoch 30: Train Loss=0.9525, Train Acc=66.35%, Val Loss=1.2476, Val Acc=56.71%, Grad Norm=8.1964\n",
      "Fold 4, Epoch 31: Train Loss=0.9211, Train Acc=67.50%, Val Loss=1.2293, Val Acc=57.68%, Grad Norm=8.3453\n",
      "Fold 4, Epoch 32: Train Loss=0.9053, Train Acc=67.93%, Val Loss=1.2311, Val Acc=57.16%, Grad Norm=8.6226\n",
      "Fold 4, Epoch 33: Train Loss=0.8971, Train Acc=68.32%, Val Loss=1.2230, Val Acc=57.89%, Grad Norm=8.8731\n",
      "Fold 4, Epoch 34: Train Loss=0.8852, Train Acc=68.77%, Val Loss=1.2272, Val Acc=57.65%, Grad Norm=9.0898\n",
      "Fold 4, Epoch 35: Train Loss=0.8783, Train Acc=68.89%, Val Loss=1.2374, Val Acc=57.73%, Grad Norm=9.3398\n",
      "Fold 4, Epoch 36: Train Loss=0.8689, Train Acc=69.32%, Val Loss=1.2386, Val Acc=57.74%, Grad Norm=9.5347\n",
      "Fold 4, Epoch 37: Train Loss=0.8586, Train Acc=69.57%, Val Loss=1.2410, Val Acc=57.52%, Grad Norm=9.7392\n",
      "Fold 4, Epoch 38: Train Loss=0.8513, Train Acc=70.00%, Val Loss=1.2385, Val Acc=57.59%, Grad Norm=9.9474\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc (Test=10km/h) = 15.35%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.1220, Train Acc=17.58%, Val Loss=3.1435, Val Acc=16.01%, Grad Norm=5.5600\n",
      "Fold 5, Epoch 2: Train Loss=1.9329, Train Acc=27.07%, Val Loss=2.3870, Val Acc=23.62%, Grad Norm=4.1053\n",
      "Fold 5, Epoch 3: Train Loss=1.7898, Train Acc=32.83%, Val Loss=2.0451, Val Acc=30.05%, Grad Norm=3.9648\n",
      "Fold 5, Epoch 4: Train Loss=1.6761, Train Acc=37.09%, Val Loss=1.8540, Val Acc=35.19%, Grad Norm=3.7779\n",
      "Fold 5, Epoch 5: Train Loss=1.6118, Train Acc=39.90%, Val Loss=1.9584, Val Acc=33.39%, Grad Norm=3.6813\n",
      "Fold 5, Epoch 6: Train Loss=1.5460, Train Acc=42.41%, Val Loss=1.7512, Val Acc=38.45%, Grad Norm=3.7152\n",
      "Fold 5, Epoch 7: Train Loss=1.4759, Train Acc=45.11%, Val Loss=1.6911, Val Acc=41.10%, Grad Norm=3.7322\n",
      "Fold 5, Epoch 8: Train Loss=1.4261, Train Acc=47.42%, Val Loss=1.6786, Val Acc=42.20%, Grad Norm=3.7124\n",
      "Fold 5, Epoch 9: Train Loss=1.3853, Train Acc=48.57%, Val Loss=1.6304, Val Acc=44.37%, Grad Norm=3.7306\n",
      "Fold 5, Epoch 10: Train Loss=1.3537, Train Acc=50.11%, Val Loss=1.5928, Val Acc=45.19%, Grad Norm=3.7484\n",
      "Fold 5, Epoch 11: Train Loss=1.2932, Train Acc=52.34%, Val Loss=1.6112, Val Acc=45.16%, Grad Norm=3.8961\n",
      "Fold 5, Epoch 12: Train Loss=1.2680, Train Acc=53.45%, Val Loss=1.5539, Val Acc=47.17%, Grad Norm=4.1415\n",
      "Fold 5, Epoch 13: Train Loss=1.2467, Train Acc=54.31%, Val Loss=1.5160, Val Acc=47.25%, Grad Norm=4.3162\n",
      "Fold 5, Epoch 14: Train Loss=1.2301, Train Acc=54.72%, Val Loss=1.5513, Val Acc=47.76%, Grad Norm=4.5016\n",
      "Fold 5, Epoch 15: Train Loss=1.2095, Train Acc=55.77%, Val Loss=1.5505, Val Acc=48.16%, Grad Norm=4.6740\n",
      "Fold 5, Epoch 16: Train Loss=1.1895, Train Acc=56.39%, Val Loss=1.5273, Val Acc=48.36%, Grad Norm=4.8753\n",
      "Fold 5, Epoch 17: Train Loss=1.1715, Train Acc=57.38%, Val Loss=1.5071, Val Acc=49.44%, Grad Norm=5.0470\n",
      "Fold 5, Epoch 18: Train Loss=1.1471, Train Acc=58.18%, Val Loss=1.4522, Val Acc=50.73%, Grad Norm=5.2651\n",
      "Fold 5, Epoch 19: Train Loss=1.1255, Train Acc=59.25%, Val Loss=1.4872, Val Acc=50.40%, Grad Norm=5.4274\n",
      "Fold 5, Epoch 20: Train Loss=1.1081, Train Acc=59.75%, Val Loss=1.4437, Val Acc=51.33%, Grad Norm=5.6048\n",
      "Fold 5, Epoch 21: Train Loss=1.0579, Train Acc=61.81%, Val Loss=1.4310, Val Acc=52.33%, Grad Norm=5.8342\n",
      "Fold 5, Epoch 22: Train Loss=1.0372, Train Acc=62.53%, Val Loss=1.4624, Val Acc=51.10%, Grad Norm=6.2228\n",
      "Fold 5, Epoch 23: Train Loss=1.0246, Train Acc=63.05%, Val Loss=1.4729, Val Acc=51.63%, Grad Norm=6.5328\n",
      "Fold 5, Epoch 24: Train Loss=1.0068, Train Acc=63.85%, Val Loss=1.4311, Val Acc=52.45%, Grad Norm=6.7398\n",
      "Fold 5, Epoch 25: Train Loss=0.9968, Train Acc=64.08%, Val Loss=1.4267, Val Acc=52.77%, Grad Norm=6.9917\n",
      "Fold 5, Epoch 26: Train Loss=0.9812, Train Acc=64.81%, Val Loss=1.4596, Val Acc=52.05%, Grad Norm=7.2111\n",
      "Fold 5, Epoch 27: Train Loss=0.9680, Train Acc=65.20%, Val Loss=1.4444, Val Acc=52.58%, Grad Norm=7.4887\n",
      "Fold 5, Epoch 28: Train Loss=0.9566, Train Acc=65.69%, Val Loss=1.4400, Val Acc=52.73%, Grad Norm=7.7000\n",
      "Fold 5, Epoch 29: Train Loss=0.9435, Train Acc=66.25%, Val Loss=1.4463, Val Acc=52.71%, Grad Norm=7.9300\n",
      "Fold 5, Epoch 30: Train Loss=0.9270, Train Acc=66.75%, Val Loss=1.4581, Val Acc=52.39%, Grad Norm=8.1446\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc (Test=10km/h) = 15.59%\n",
      "\n",
      "SNR   5 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-23_22-36-39_LTE-V_XFR_SpeedSplit_FileBlock_Train20kmh_Test10kmh_SNR5dB_fdTrain655_fdTest655_group288_ResNet1D\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=0 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "[INFO] 共找到 72 个 .mat 文件（含子目录）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 可用 TX 类别数（train/test 交集）: 9\n",
      "[INFO] Train blocks: 360, Test blocks: 360, sample_len: 288, group_size: 288\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.1681, Train Acc=15.86%, Val Loss=3.0875, Val Acc=11.60%, Grad Norm=5.4160\n",
      "Fold 1, Epoch 2: Train Loss=2.0543, Train Acc=21.81%, Val Loss=2.5064, Val Acc=14.32%, Grad Norm=3.7021\n",
      "Fold 1, Epoch 3: Train Loss=1.9933, Train Acc=24.99%, Val Loss=2.2474, Val Acc=17.04%, Grad Norm=3.1328\n",
      "Fold 1, Epoch 4: Train Loss=1.9462, Train Acc=26.93%, Val Loss=2.1321, Val Acc=19.92%, Grad Norm=2.9644\n",
      "Fold 1, Epoch 5: Train Loss=1.9026, Train Acc=29.14%, Val Loss=2.1141, Val Acc=21.35%, Grad Norm=2.9503\n",
      "Fold 1, Epoch 6: Train Loss=1.8490, Train Acc=30.97%, Val Loss=1.9453, Val Acc=26.63%, Grad Norm=3.0756\n",
      "Fold 1, Epoch 7: Train Loss=1.8058, Train Acc=32.77%, Val Loss=2.0207, Val Acc=25.64%, Grad Norm=3.0749\n",
      "Fold 1, Epoch 8: Train Loss=1.7742, Train Acc=34.12%, Val Loss=1.9490, Val Acc=27.96%, Grad Norm=3.0592\n",
      "Fold 1, Epoch 9: Train Loss=1.7485, Train Acc=35.14%, Val Loss=1.9340, Val Acc=27.94%, Grad Norm=3.0359\n",
      "Fold 1, Epoch 10: Train Loss=1.7317, Train Acc=35.77%, Val Loss=1.9579, Val Acc=27.23%, Grad Norm=3.0452\n",
      "Fold 1, Epoch 11: Train Loss=1.6922, Train Acc=37.32%, Val Loss=1.8753, Val Acc=30.20%, Grad Norm=3.1663\n",
      "Fold 1, Epoch 12: Train Loss=1.6747, Train Acc=38.00%, Val Loss=1.8927, Val Acc=29.84%, Grad Norm=3.3556\n",
      "Fold 1, Epoch 13: Train Loss=1.6582, Train Acc=38.90%, Val Loss=1.9139, Val Acc=29.64%, Grad Norm=3.5357\n",
      "Fold 1, Epoch 14: Train Loss=1.6442, Train Acc=39.41%, Val Loss=1.8707, Val Acc=30.23%, Grad Norm=3.6916\n",
      "Fold 1, Epoch 15: Train Loss=1.6314, Train Acc=39.97%, Val Loss=1.8896, Val Acc=30.47%, Grad Norm=3.8127\n",
      "Fold 1, Epoch 16: Train Loss=1.6209, Train Acc=40.31%, Val Loss=1.8760, Val Acc=31.58%, Grad Norm=3.9204\n",
      "Fold 1, Epoch 17: Train Loss=1.6054, Train Acc=40.85%, Val Loss=1.8672, Val Acc=31.52%, Grad Norm=4.0294\n",
      "Fold 1, Epoch 18: Train Loss=1.5942, Train Acc=41.47%, Val Loss=1.8344, Val Acc=31.98%, Grad Norm=4.1721\n",
      "Fold 1, Epoch 19: Train Loss=1.5824, Train Acc=41.87%, Val Loss=1.8384, Val Acc=32.53%, Grad Norm=4.3182\n",
      "Fold 1, Epoch 20: Train Loss=1.5714, Train Acc=42.35%, Val Loss=1.8572, Val Acc=32.43%, Grad Norm=4.4464\n",
      "Fold 1, Epoch 21: Train Loss=1.5419, Train Acc=43.71%, Val Loss=1.8464, Val Acc=32.90%, Grad Norm=4.6384\n",
      "Fold 1, Epoch 22: Train Loss=1.5295, Train Acc=44.04%, Val Loss=1.8416, Val Acc=33.22%, Grad Norm=4.9564\n",
      "Fold 1, Epoch 23: Train Loss=1.5187, Train Acc=44.50%, Val Loss=1.8312, Val Acc=33.26%, Grad Norm=5.1719\n",
      "Fold 1, Epoch 24: Train Loss=1.5111, Train Acc=44.89%, Val Loss=1.8175, Val Acc=33.91%, Grad Norm=5.3914\n",
      "Fold 1, Epoch 25: Train Loss=1.4994, Train Acc=45.30%, Val Loss=1.8431, Val Acc=33.60%, Grad Norm=5.5787\n",
      "Fold 1, Epoch 26: Train Loss=1.4926, Train Acc=45.63%, Val Loss=1.8372, Val Acc=33.69%, Grad Norm=5.8307\n",
      "Fold 1, Epoch 27: Train Loss=1.4836, Train Acc=45.98%, Val Loss=1.8176, Val Acc=34.21%, Grad Norm=5.9995\n",
      "Fold 1, Epoch 28: Train Loss=1.4741, Train Acc=46.35%, Val Loss=1.8349, Val Acc=34.39%, Grad Norm=6.1851\n",
      "Fold 1, Epoch 29: Train Loss=1.4637, Train Acc=46.92%, Val Loss=1.8118, Val Acc=34.37%, Grad Norm=6.4448\n",
      "Fold 1, Epoch 30: Train Loss=1.4557, Train Acc=47.28%, Val Loss=1.8380, Val Acc=34.00%, Grad Norm=6.6874\n",
      "Fold 1, Epoch 31: Train Loss=1.4320, Train Acc=48.08%, Val Loss=1.8168, Val Acc=34.92%, Grad Norm=6.8976\n",
      "Fold 1, Epoch 32: Train Loss=1.4194, Train Acc=48.54%, Val Loss=1.8382, Val Acc=34.64%, Grad Norm=7.2125\n",
      "Fold 1, Epoch 33: Train Loss=1.4162, Train Acc=48.81%, Val Loss=1.8205, Val Acc=35.07%, Grad Norm=7.5214\n",
      "Fold 1, Epoch 34: Train Loss=1.4059, Train Acc=49.20%, Val Loss=1.8226, Val Acc=35.27%, Grad Norm=7.6989\n",
      "Fold 1, Epoch 35: Train Loss=1.3987, Train Acc=49.47%, Val Loss=1.8276, Val Acc=34.96%, Grad Norm=7.9288\n",
      "Fold 1, Epoch 36: Train Loss=1.3946, Train Acc=49.59%, Val Loss=1.8372, Val Acc=34.84%, Grad Norm=8.1687\n",
      "Fold 1, Epoch 37: Train Loss=1.3829, Train Acc=50.18%, Val Loss=1.8332, Val Acc=35.12%, Grad Norm=8.3709\n",
      "Fold 1, Epoch 38: Train Loss=1.3741, Train Acc=50.60%, Val Loss=1.8321, Val Acc=35.47%, Grad Norm=8.6327\n",
      "Fold 1, Epoch 39: Train Loss=1.3713, Train Acc=50.52%, Val Loss=1.8235, Val Acc=35.57%, Grad Norm=8.8975\n",
      "Fold 1, Epoch 40: Train Loss=1.3591, Train Acc=51.06%, Val Loss=1.8338, Val Acc=35.52%, Grad Norm=9.1092\n",
      "Fold 1, Epoch 41: Train Loss=1.3467, Train Acc=51.61%, Val Loss=1.8371, Val Acc=35.08%, Grad Norm=9.3272\n",
      "Fold 1, Epoch 42: Train Loss=1.3412, Train Acc=51.80%, Val Loss=1.8465, Val Acc=35.29%, Grad Norm=9.5484\n",
      "Fold 1, Epoch 43: Train Loss=1.3355, Train Acc=52.01%, Val Loss=1.8341, Val Acc=35.55%, Grad Norm=9.7195\n",
      "Fold 1, Epoch 44: Train Loss=1.3308, Train Acc=52.35%, Val Loss=1.8454, Val Acc=35.48%, Grad Norm=9.9209\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc (Test=10km/h) = 14.35%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.1722, Train Acc=15.13%, Val Loss=3.5733, Val Acc=11.76%, Grad Norm=5.4761\n",
      "Fold 2, Epoch 2: Train Loss=2.0720, Train Acc=20.31%, Val Loss=2.7390, Val Acc=13.81%, Grad Norm=3.6862\n",
      "Fold 2, Epoch 3: Train Loss=2.0060, Train Acc=23.35%, Val Loss=2.4902, Val Acc=15.14%, Grad Norm=3.0833\n",
      "Fold 2, Epoch 4: Train Loss=1.9573, Train Acc=25.58%, Val Loss=2.2797, Val Acc=17.90%, Grad Norm=2.9536\n",
      "Fold 2, Epoch 5: Train Loss=1.9073, Train Acc=27.96%, Val Loss=2.1466, Val Acc=23.01%, Grad Norm=3.0245\n",
      "Fold 2, Epoch 6: Train Loss=1.8543, Train Acc=29.96%, Val Loss=2.1401, Val Acc=22.78%, Grad Norm=3.0344\n",
      "Fold 2, Epoch 7: Train Loss=1.8185, Train Acc=31.59%, Val Loss=2.0807, Val Acc=24.14%, Grad Norm=2.9978\n",
      "Fold 2, Epoch 8: Train Loss=1.7899, Train Acc=32.93%, Val Loss=2.0066, Val Acc=26.54%, Grad Norm=2.9992\n",
      "Fold 2, Epoch 9: Train Loss=1.7662, Train Acc=34.03%, Val Loss=2.0076, Val Acc=26.55%, Grad Norm=3.0207\n",
      "Fold 2, Epoch 10: Train Loss=1.7452, Train Acc=34.57%, Val Loss=1.9280, Val Acc=28.76%, Grad Norm=3.0116\n",
      "Fold 2, Epoch 11: Train Loss=1.7093, Train Acc=36.15%, Val Loss=1.8998, Val Acc=30.08%, Grad Norm=3.1501\n",
      "Fold 2, Epoch 12: Train Loss=1.6948, Train Acc=36.95%, Val Loss=1.8941, Val Acc=30.27%, Grad Norm=3.3396\n",
      "Fold 2, Epoch 13: Train Loss=1.6803, Train Acc=37.64%, Val Loss=1.8743, Val Acc=29.96%, Grad Norm=3.5031\n",
      "Fold 2, Epoch 14: Train Loss=1.6666, Train Acc=38.11%, Val Loss=1.8961, Val Acc=31.40%, Grad Norm=3.6515\n",
      "Fold 2, Epoch 15: Train Loss=1.6506, Train Acc=38.75%, Val Loss=1.8872, Val Acc=30.78%, Grad Norm=3.8314\n",
      "Fold 2, Epoch 16: Train Loss=1.6372, Train Acc=39.32%, Val Loss=1.8652, Val Acc=32.15%, Grad Norm=3.9557\n",
      "Fold 2, Epoch 17: Train Loss=1.6210, Train Acc=39.95%, Val Loss=1.8296, Val Acc=33.56%, Grad Norm=4.1073\n",
      "Fold 2, Epoch 18: Train Loss=1.6067, Train Acc=40.62%, Val Loss=1.8362, Val Acc=33.84%, Grad Norm=4.2763\n",
      "Fold 2, Epoch 19: Train Loss=1.5949, Train Acc=41.19%, Val Loss=1.8024, Val Acc=34.61%, Grad Norm=4.4054\n",
      "Fold 2, Epoch 20: Train Loss=1.5834, Train Acc=41.85%, Val Loss=1.8438, Val Acc=34.33%, Grad Norm=4.5370\n",
      "Fold 2, Epoch 21: Train Loss=1.5497, Train Acc=43.19%, Val Loss=1.8007, Val Acc=34.83%, Grad Norm=4.7526\n",
      "Fold 2, Epoch 22: Train Loss=1.5378, Train Acc=43.38%, Val Loss=1.7712, Val Acc=36.33%, Grad Norm=5.0573\n",
      "Fold 2, Epoch 23: Train Loss=1.5276, Train Acc=43.98%, Val Loss=1.7869, Val Acc=35.16%, Grad Norm=5.2604\n",
      "Fold 2, Epoch 24: Train Loss=1.5157, Train Acc=44.68%, Val Loss=1.7505, Val Acc=36.37%, Grad Norm=5.4824\n",
      "Fold 2, Epoch 25: Train Loss=1.5082, Train Acc=45.04%, Val Loss=1.7522, Val Acc=36.56%, Grad Norm=5.6822\n",
      "Fold 2, Epoch 26: Train Loss=1.5000, Train Acc=45.24%, Val Loss=1.7580, Val Acc=36.82%, Grad Norm=5.8832\n",
      "Fold 2, Epoch 27: Train Loss=1.4883, Train Acc=45.83%, Val Loss=1.7824, Val Acc=35.85%, Grad Norm=6.0703\n",
      "Fold 2, Epoch 28: Train Loss=1.4826, Train Acc=46.01%, Val Loss=1.7577, Val Acc=36.97%, Grad Norm=6.2568\n",
      "Fold 2, Epoch 29: Train Loss=1.4735, Train Acc=46.24%, Val Loss=1.7438, Val Acc=37.23%, Grad Norm=6.4828\n",
      "Fold 2, Epoch 30: Train Loss=1.4598, Train Acc=46.84%, Val Loss=1.7860, Val Acc=36.26%, Grad Norm=6.7358\n",
      "Fold 2, Epoch 31: Train Loss=1.4412, Train Acc=47.64%, Val Loss=1.7384, Val Acc=37.57%, Grad Norm=6.9501\n",
      "Fold 2, Epoch 32: Train Loss=1.4272, Train Acc=48.29%, Val Loss=1.7378, Val Acc=37.44%, Grad Norm=7.2463\n",
      "Fold 2, Epoch 33: Train Loss=1.4215, Train Acc=48.55%, Val Loss=1.7420, Val Acc=37.77%, Grad Norm=7.5065\n",
      "Fold 2, Epoch 34: Train Loss=1.4128, Train Acc=48.84%, Val Loss=1.7429, Val Acc=37.83%, Grad Norm=7.7198\n",
      "Fold 2, Epoch 35: Train Loss=1.4046, Train Acc=49.08%, Val Loss=1.7365, Val Acc=37.78%, Grad Norm=7.9537\n",
      "Fold 2, Epoch 36: Train Loss=1.3994, Train Acc=49.45%, Val Loss=1.7511, Val Acc=37.53%, Grad Norm=8.1841\n",
      "Fold 2, Epoch 37: Train Loss=1.3896, Train Acc=49.73%, Val Loss=1.7440, Val Acc=37.89%, Grad Norm=8.4052\n",
      "Fold 2, Epoch 38: Train Loss=1.3856, Train Acc=49.95%, Val Loss=1.7361, Val Acc=38.03%, Grad Norm=8.6661\n",
      "Fold 2, Epoch 39: Train Loss=1.3786, Train Acc=50.19%, Val Loss=1.7385, Val Acc=38.27%, Grad Norm=8.8733\n",
      "Fold 2, Epoch 40: Train Loss=1.3712, Train Acc=50.61%, Val Loss=1.7398, Val Acc=38.29%, Grad Norm=9.0762\n",
      "Fold 2, Epoch 41: Train Loss=1.3514, Train Acc=51.34%, Val Loss=1.7373, Val Acc=38.48%, Grad Norm=9.2916\n",
      "Fold 2, Epoch 42: Train Loss=1.3495, Train Acc=51.46%, Val Loss=1.7432, Val Acc=38.37%, Grad Norm=9.5765\n",
      "Fold 2, Epoch 43: Train Loss=1.3434, Train Acc=51.73%, Val Loss=1.7429, Val Acc=38.38%, Grad Norm=9.7587\n",
      "Fold 2, Epoch 44: Train Loss=1.3366, Train Acc=51.80%, Val Loss=1.7552, Val Acc=38.15%, Grad Norm=9.9259\n",
      "Fold 2, Epoch 45: Train Loss=1.3325, Train Acc=52.07%, Val Loss=1.7531, Val Acc=37.95%, Grad Norm=10.1271\n",
      "Fold 2, Epoch 46: Train Loss=1.3301, Train Acc=52.02%, Val Loss=1.7551, Val Acc=38.28%, Grad Norm=10.3167\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc (Test=10km/h) = 15.22%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.1687, Train Acc=15.43%, Val Loss=3.2812, Val Acc=13.47%, Grad Norm=5.5949\n",
      "Fold 3, Epoch 2: Train Loss=2.0781, Train Acc=19.86%, Val Loss=2.6621, Val Acc=15.29%, Grad Norm=3.5669\n",
      "Fold 3, Epoch 3: Train Loss=2.0279, Train Acc=22.25%, Val Loss=2.4243, Val Acc=15.37%, Grad Norm=2.9889\n",
      "Fold 3, Epoch 4: Train Loss=1.9784, Train Acc=24.69%, Val Loss=2.3499, Val Acc=17.77%, Grad Norm=2.9123\n",
      "Fold 3, Epoch 5: Train Loss=1.9229, Train Acc=27.25%, Val Loss=2.1113, Val Acc=23.51%, Grad Norm=3.0021\n",
      "Fold 3, Epoch 6: Train Loss=1.8714, Train Acc=29.49%, Val Loss=2.0908, Val Acc=25.97%, Grad Norm=3.0037\n",
      "Fold 3, Epoch 7: Train Loss=1.8343, Train Acc=31.05%, Val Loss=1.9652, Val Acc=28.15%, Grad Norm=2.9644\n",
      "Fold 3, Epoch 8: Train Loss=1.8055, Train Acc=32.24%, Val Loss=1.8907, Val Acc=30.35%, Grad Norm=2.9359\n",
      "Fold 3, Epoch 9: Train Loss=1.7832, Train Acc=32.99%, Val Loss=1.8651, Val Acc=30.30%, Grad Norm=2.9641\n",
      "Fold 3, Epoch 10: Train Loss=1.7649, Train Acc=33.79%, Val Loss=1.9056, Val Acc=29.10%, Grad Norm=2.9443\n",
      "Fold 3, Epoch 11: Train Loss=1.7265, Train Acc=35.75%, Val Loss=1.8512, Val Acc=32.01%, Grad Norm=3.0920\n",
      "Fold 3, Epoch 12: Train Loss=1.7088, Train Acc=36.24%, Val Loss=1.8608, Val Acc=32.55%, Grad Norm=3.2709\n",
      "Fold 3, Epoch 13: Train Loss=1.6960, Train Acc=36.69%, Val Loss=1.8205, Val Acc=33.69%, Grad Norm=3.4482\n",
      "Fold 3, Epoch 14: Train Loss=1.6826, Train Acc=37.59%, Val Loss=1.8181, Val Acc=34.38%, Grad Norm=3.5919\n",
      "Fold 3, Epoch 15: Train Loss=1.6645, Train Acc=38.42%, Val Loss=1.7699, Val Acc=35.67%, Grad Norm=3.7978\n",
      "Fold 3, Epoch 16: Train Loss=1.6485, Train Acc=39.06%, Val Loss=1.7968, Val Acc=34.81%, Grad Norm=3.9398\n",
      "Fold 3, Epoch 17: Train Loss=1.6370, Train Acc=39.51%, Val Loss=1.7797, Val Acc=35.25%, Grad Norm=4.0744\n",
      "Fold 3, Epoch 18: Train Loss=1.6225, Train Acc=40.17%, Val Loss=1.7460, Val Acc=37.18%, Grad Norm=4.2251\n",
      "Fold 3, Epoch 19: Train Loss=1.6083, Train Acc=41.01%, Val Loss=1.7703, Val Acc=35.56%, Grad Norm=4.3494\n",
      "Fold 3, Epoch 20: Train Loss=1.6003, Train Acc=41.07%, Val Loss=1.7343, Val Acc=36.91%, Grad Norm=4.4977\n",
      "Fold 3, Epoch 21: Train Loss=1.5671, Train Acc=42.68%, Val Loss=1.7089, Val Acc=37.95%, Grad Norm=4.6758\n",
      "Fold 3, Epoch 22: Train Loss=1.5536, Train Acc=43.12%, Val Loss=1.7158, Val Acc=37.80%, Grad Norm=4.9860\n",
      "Fold 3, Epoch 23: Train Loss=1.5443, Train Acc=43.48%, Val Loss=1.6965, Val Acc=38.31%, Grad Norm=5.2344\n",
      "Fold 3, Epoch 24: Train Loss=1.5347, Train Acc=43.93%, Val Loss=1.7223, Val Acc=37.60%, Grad Norm=5.4198\n",
      "Fold 3, Epoch 25: Train Loss=1.5239, Train Acc=44.29%, Val Loss=1.6966, Val Acc=38.72%, Grad Norm=5.6349\n",
      "Fold 3, Epoch 26: Train Loss=1.5143, Train Acc=44.87%, Val Loss=1.7327, Val Acc=37.77%, Grad Norm=5.8579\n",
      "Fold 3, Epoch 27: Train Loss=1.5042, Train Acc=45.27%, Val Loss=1.7205, Val Acc=37.86%, Grad Norm=6.0349\n",
      "Fold 3, Epoch 28: Train Loss=1.4943, Train Acc=45.46%, Val Loss=1.6908, Val Acc=38.85%, Grad Norm=6.3024\n",
      "Fold 3, Epoch 29: Train Loss=1.4848, Train Acc=46.07%, Val Loss=1.7033, Val Acc=38.53%, Grad Norm=6.4956\n",
      "Fold 3, Epoch 30: Train Loss=1.4805, Train Acc=45.98%, Val Loss=1.6808, Val Acc=39.42%, Grad Norm=6.6992\n",
      "Fold 3, Epoch 31: Train Loss=1.4519, Train Acc=47.31%, Val Loss=1.6902, Val Acc=39.22%, Grad Norm=6.9402\n",
      "Fold 3, Epoch 32: Train Loss=1.4455, Train Acc=47.51%, Val Loss=1.6700, Val Acc=39.64%, Grad Norm=7.2583\n",
      "Fold 3, Epoch 33: Train Loss=1.4339, Train Acc=48.06%, Val Loss=1.6966, Val Acc=39.38%, Grad Norm=7.5073\n",
      "Fold 3, Epoch 34: Train Loss=1.4276, Train Acc=48.31%, Val Loss=1.6915, Val Acc=39.55%, Grad Norm=7.7770\n",
      "Fold 3, Epoch 35: Train Loss=1.4218, Train Acc=48.56%, Val Loss=1.6898, Val Acc=39.57%, Grad Norm=7.9774\n",
      "Fold 3, Epoch 36: Train Loss=1.4139, Train Acc=48.84%, Val Loss=1.6904, Val Acc=39.83%, Grad Norm=8.2233\n",
      "Fold 3, Epoch 37: Train Loss=1.4061, Train Acc=49.14%, Val Loss=1.7128, Val Acc=39.21%, Grad Norm=8.4414\n",
      "Fold 3, Epoch 38: Train Loss=1.4000, Train Acc=49.35%, Val Loss=1.7069, Val Acc=39.20%, Grad Norm=8.7186\n",
      "Fold 3, Epoch 39: Train Loss=1.3934, Train Acc=49.64%, Val Loss=1.7167, Val Acc=39.17%, Grad Norm=8.8898\n",
      "Fold 3, Epoch 40: Train Loss=1.3843, Train Acc=50.01%, Val Loss=1.6975, Val Acc=39.71%, Grad Norm=9.1741\n",
      "Fold 3, Epoch 41: Train Loss=1.3681, Train Acc=50.62%, Val Loss=1.6863, Val Acc=39.90%, Grad Norm=9.3666\n",
      "Fold 3, Epoch 42: Train Loss=1.3656, Train Acc=50.95%, Val Loss=1.6965, Val Acc=39.93%, Grad Norm=9.6212\n",
      "Fold 3, Epoch 43: Train Loss=1.3551, Train Acc=51.33%, Val Loss=1.6917, Val Acc=39.62%, Grad Norm=9.8205\n",
      "Fold 3, Epoch 44: Train Loss=1.3529, Train Acc=51.40%, Val Loss=1.7047, Val Acc=39.74%, Grad Norm=10.0060\n",
      "Fold 3, Epoch 45: Train Loss=1.3472, Train Acc=51.59%, Val Loss=1.7225, Val Acc=39.34%, Grad Norm=10.2361\n",
      "Fold 3, Epoch 46: Train Loss=1.3448, Train Acc=51.66%, Val Loss=1.7029, Val Acc=39.78%, Grad Norm=10.4531\n",
      "Fold 3, Epoch 47: Train Loss=1.3390, Train Acc=51.87%, Val Loss=1.6993, Val Acc=39.85%, Grad Norm=10.6035\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc (Test=10km/h) = 14.92%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1684, Train Acc=15.73%, Val Loss=3.0090, Val Acc=12.36%, Grad Norm=5.5658\n",
      "Fold 4, Epoch 2: Train Loss=2.0715, Train Acc=20.87%, Val Loss=2.7134, Val Acc=15.22%, Grad Norm=3.7368\n",
      "Fold 4, Epoch 3: Train Loss=2.0159, Train Acc=23.65%, Val Loss=2.2315, Val Acc=19.30%, Grad Norm=3.1300\n",
      "Fold 4, Epoch 4: Train Loss=1.9748, Train Acc=25.48%, Val Loss=2.0970, Val Acc=20.16%, Grad Norm=2.9560\n",
      "Fold 4, Epoch 5: Train Loss=1.9307, Train Acc=27.73%, Val Loss=2.1510, Val Acc=21.71%, Grad Norm=2.9103\n",
      "Fold 4, Epoch 6: Train Loss=1.8837, Train Acc=29.89%, Val Loss=2.0110, Val Acc=25.11%, Grad Norm=2.9896\n",
      "Fold 4, Epoch 7: Train Loss=1.8413, Train Acc=31.66%, Val Loss=1.9323, Val Acc=26.59%, Grad Norm=2.9746\n",
      "Fold 4, Epoch 8: Train Loss=1.8081, Train Acc=32.95%, Val Loss=1.8990, Val Acc=29.04%, Grad Norm=2.9830\n",
      "Fold 4, Epoch 9: Train Loss=1.7859, Train Acc=33.71%, Val Loss=1.8676, Val Acc=28.78%, Grad Norm=2.9708\n",
      "Fold 4, Epoch 10: Train Loss=1.7652, Train Acc=34.81%, Val Loss=1.8489, Val Acc=29.97%, Grad Norm=2.9882\n",
      "Fold 4, Epoch 11: Train Loss=1.7283, Train Acc=36.17%, Val Loss=1.7940, Val Acc=31.86%, Grad Norm=3.1131\n",
      "Fold 4, Epoch 12: Train Loss=1.7111, Train Acc=36.75%, Val Loss=1.7957, Val Acc=32.62%, Grad Norm=3.2914\n",
      "Fold 4, Epoch 13: Train Loss=1.6990, Train Acc=37.20%, Val Loss=1.7614, Val Acc=33.29%, Grad Norm=3.4261\n",
      "Fold 4, Epoch 14: Train Loss=1.6871, Train Acc=37.77%, Val Loss=1.7482, Val Acc=33.82%, Grad Norm=3.5775\n",
      "Fold 4, Epoch 15: Train Loss=1.6726, Train Acc=38.38%, Val Loss=1.7734, Val Acc=33.47%, Grad Norm=3.7321\n",
      "Fold 4, Epoch 16: Train Loss=1.6593, Train Acc=39.03%, Val Loss=1.7451, Val Acc=34.23%, Grad Norm=3.9198\n",
      "Fold 4, Epoch 17: Train Loss=1.6455, Train Acc=39.71%, Val Loss=1.7407, Val Acc=34.57%, Grad Norm=4.0527\n",
      "Fold 4, Epoch 18: Train Loss=1.6287, Train Acc=40.25%, Val Loss=1.7496, Val Acc=34.92%, Grad Norm=4.2029\n",
      "Fold 4, Epoch 19: Train Loss=1.6194, Train Acc=40.69%, Val Loss=1.7188, Val Acc=35.83%, Grad Norm=4.3433\n",
      "Fold 4, Epoch 20: Train Loss=1.6035, Train Acc=41.46%, Val Loss=1.7155, Val Acc=35.53%, Grad Norm=4.5317\n",
      "Fold 4, Epoch 21: Train Loss=1.5748, Train Acc=42.63%, Val Loss=1.7144, Val Acc=35.88%, Grad Norm=4.7779\n",
      "Fold 4, Epoch 22: Train Loss=1.5607, Train Acc=43.15%, Val Loss=1.6919, Val Acc=36.62%, Grad Norm=5.0442\n",
      "Fold 4, Epoch 23: Train Loss=1.5495, Train Acc=43.74%, Val Loss=1.6840, Val Acc=37.55%, Grad Norm=5.2775\n",
      "Fold 4, Epoch 24: Train Loss=1.5391, Train Acc=43.97%, Val Loss=1.6938, Val Acc=36.64%, Grad Norm=5.5175\n",
      "Fold 4, Epoch 25: Train Loss=1.5287, Train Acc=44.41%, Val Loss=1.6877, Val Acc=37.44%, Grad Norm=5.7118\n",
      "Fold 4, Epoch 26: Train Loss=1.5210, Train Acc=44.69%, Val Loss=1.6780, Val Acc=37.81%, Grad Norm=5.9344\n",
      "Fold 4, Epoch 27: Train Loss=1.5107, Train Acc=45.21%, Val Loss=1.6996, Val Acc=37.18%, Grad Norm=6.1449\n",
      "Fold 4, Epoch 28: Train Loss=1.4986, Train Acc=45.71%, Val Loss=1.6798, Val Acc=38.29%, Grad Norm=6.3460\n",
      "Fold 4, Epoch 29: Train Loss=1.4916, Train Acc=46.09%, Val Loss=1.6651, Val Acc=39.09%, Grad Norm=6.5740\n",
      "Fold 4, Epoch 30: Train Loss=1.4825, Train Acc=46.30%, Val Loss=1.6715, Val Acc=38.50%, Grad Norm=6.8192\n",
      "Fold 4, Epoch 31: Train Loss=1.4538, Train Acc=47.43%, Val Loss=1.6633, Val Acc=38.85%, Grad Norm=6.9898\n",
      "Fold 4, Epoch 32: Train Loss=1.4474, Train Acc=47.83%, Val Loss=1.6720, Val Acc=39.30%, Grad Norm=7.3219\n",
      "Fold 4, Epoch 33: Train Loss=1.4356, Train Acc=48.33%, Val Loss=1.6538, Val Acc=39.69%, Grad Norm=7.6015\n",
      "Fold 4, Epoch 34: Train Loss=1.4278, Train Acc=48.69%, Val Loss=1.6705, Val Acc=39.26%, Grad Norm=7.8352\n",
      "Fold 4, Epoch 35: Train Loss=1.4251, Train Acc=48.63%, Val Loss=1.6629, Val Acc=39.54%, Grad Norm=8.0613\n",
      "Fold 4, Epoch 36: Train Loss=1.4157, Train Acc=49.14%, Val Loss=1.6496, Val Acc=40.13%, Grad Norm=8.2995\n",
      "Fold 4, Epoch 37: Train Loss=1.4039, Train Acc=49.49%, Val Loss=1.6613, Val Acc=39.74%, Grad Norm=8.5429\n",
      "Fold 4, Epoch 38: Train Loss=1.4009, Train Acc=49.71%, Val Loss=1.6573, Val Acc=40.02%, Grad Norm=8.8358\n",
      "Fold 4, Epoch 39: Train Loss=1.3936, Train Acc=49.93%, Val Loss=1.6573, Val Acc=39.80%, Grad Norm=8.9697\n",
      "Fold 4, Epoch 40: Train Loss=1.3830, Train Acc=50.65%, Val Loss=1.6739, Val Acc=40.00%, Grad Norm=9.2318\n",
      "Fold 4, Epoch 41: Train Loss=1.3689, Train Acc=50.91%, Val Loss=1.6692, Val Acc=39.89%, Grad Norm=9.4877\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc (Test=10km/h) = 14.78%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.1731, Train Acc=14.67%, Val Loss=3.7524, Val Acc=9.98%, Grad Norm=5.4760\n",
      "Fold 5, Epoch 2: Train Loss=2.0897, Train Acc=19.49%, Val Loss=3.0843, Val Acc=18.33%, Grad Norm=3.7558\n",
      "Fold 5, Epoch 3: Train Loss=2.0099, Train Acc=23.26%, Val Loss=2.7029, Val Acc=18.43%, Grad Norm=3.2283\n",
      "Fold 5, Epoch 4: Train Loss=1.9639, Train Acc=25.24%, Val Loss=2.4484, Val Acc=21.49%, Grad Norm=2.9977\n",
      "Fold 5, Epoch 5: Train Loss=1.9166, Train Acc=27.45%, Val Loss=2.1594, Val Acc=24.54%, Grad Norm=3.0246\n",
      "Fold 5, Epoch 6: Train Loss=1.8597, Train Acc=29.96%, Val Loss=2.0023, Val Acc=27.29%, Grad Norm=3.0721\n",
      "Fold 5, Epoch 7: Train Loss=1.8250, Train Acc=31.41%, Val Loss=1.9611, Val Acc=30.10%, Grad Norm=3.0057\n",
      "Fold 5, Epoch 8: Train Loss=1.7995, Train Acc=32.39%, Val Loss=1.9981, Val Acc=28.40%, Grad Norm=2.9853\n",
      "Fold 5, Epoch 9: Train Loss=1.7748, Train Acc=33.39%, Val Loss=1.9808, Val Acc=29.73%, Grad Norm=2.9876\n",
      "Fold 5, Epoch 10: Train Loss=1.7549, Train Acc=34.25%, Val Loss=1.9553, Val Acc=29.31%, Grad Norm=3.0035\n",
      "Fold 5, Epoch 11: Train Loss=1.7161, Train Acc=36.11%, Val Loss=1.9596, Val Acc=30.92%, Grad Norm=3.1436\n",
      "Fold 5, Epoch 12: Train Loss=1.6972, Train Acc=36.98%, Val Loss=1.9151, Val Acc=32.32%, Grad Norm=3.3360\n",
      "Fold 5, Epoch 13: Train Loss=1.6838, Train Acc=37.47%, Val Loss=1.9005, Val Acc=32.80%, Grad Norm=3.5073\n",
      "Fold 5, Epoch 14: Train Loss=1.6684, Train Acc=38.00%, Val Loss=1.9059, Val Acc=32.91%, Grad Norm=3.6582\n",
      "Fold 5, Epoch 15: Train Loss=1.6520, Train Acc=39.10%, Val Loss=1.8805, Val Acc=33.89%, Grad Norm=3.8003\n",
      "Fold 5, Epoch 16: Train Loss=1.6373, Train Acc=39.61%, Val Loss=1.8751, Val Acc=34.12%, Grad Norm=3.9118\n",
      "Fold 5, Epoch 17: Train Loss=1.6231, Train Acc=40.05%, Val Loss=1.8510, Val Acc=34.24%, Grad Norm=4.0910\n",
      "Fold 5, Epoch 18: Train Loss=1.6121, Train Acc=40.48%, Val Loss=1.8679, Val Acc=34.51%, Grad Norm=4.2048\n",
      "Fold 5, Epoch 19: Train Loss=1.6007, Train Acc=41.20%, Val Loss=1.8210, Val Acc=36.10%, Grad Norm=4.3212\n",
      "Fold 5, Epoch 20: Train Loss=1.5896, Train Acc=41.44%, Val Loss=1.8383, Val Acc=35.88%, Grad Norm=4.4557\n",
      "Fold 5, Epoch 21: Train Loss=1.5554, Train Acc=42.84%, Val Loss=1.8055, Val Acc=36.31%, Grad Norm=4.6893\n",
      "Fold 5, Epoch 22: Train Loss=1.5459, Train Acc=43.43%, Val Loss=1.8457, Val Acc=35.82%, Grad Norm=4.9670\n",
      "Fold 5, Epoch 23: Train Loss=1.5341, Train Acc=43.81%, Val Loss=1.8317, Val Acc=35.95%, Grad Norm=5.2015\n",
      "Fold 5, Epoch 24: Train Loss=1.5239, Train Acc=44.11%, Val Loss=1.8147, Val Acc=36.86%, Grad Norm=5.4317\n",
      "Fold 5, Epoch 25: Train Loss=1.5160, Train Acc=44.53%, Val Loss=1.8401, Val Acc=36.29%, Grad Norm=5.6441\n",
      "Fold 5, Epoch 26: Train Loss=1.5064, Train Acc=44.82%, Val Loss=1.8472, Val Acc=36.12%, Grad Norm=5.8329\n",
      "Fold 5, Epoch 27: Train Loss=1.4989, Train Acc=45.24%, Val Loss=1.8274, Val Acc=37.09%, Grad Norm=6.0217\n",
      "Fold 5, Epoch 28: Train Loss=1.4863, Train Acc=45.78%, Val Loss=1.8147, Val Acc=37.00%, Grad Norm=6.2798\n",
      "Fold 5, Epoch 29: Train Loss=1.4767, Train Acc=46.11%, Val Loss=1.8507, Val Acc=36.48%, Grad Norm=6.4927\n",
      "Fold 5, Epoch 30: Train Loss=1.4681, Train Acc=46.59%, Val Loss=1.7800, Val Acc=37.55%, Grad Norm=6.6981\n",
      "Fold 5, Epoch 31: Train Loss=1.4479, Train Acc=47.52%, Val Loss=1.8057, Val Acc=37.55%, Grad Norm=6.9501\n",
      "Fold 5, Epoch 32: Train Loss=1.4350, Train Acc=47.79%, Val Loss=1.7964, Val Acc=37.70%, Grad Norm=7.2522\n",
      "Fold 5, Epoch 33: Train Loss=1.4276, Train Acc=48.38%, Val Loss=1.8226, Val Acc=37.21%, Grad Norm=7.4928\n",
      "Fold 5, Epoch 34: Train Loss=1.4216, Train Acc=48.44%, Val Loss=1.8222, Val Acc=37.49%, Grad Norm=7.7274\n",
      "Fold 5, Epoch 35: Train Loss=1.4132, Train Acc=48.93%, Val Loss=1.8252, Val Acc=37.25%, Grad Norm=8.0161\n",
      "Fold 5, Epoch 36: Train Loss=1.4085, Train Acc=49.13%, Val Loss=1.8475, Val Acc=36.96%, Grad Norm=8.2362\n",
      "Fold 5, Epoch 37: Train Loss=1.3960, Train Acc=49.52%, Val Loss=1.8139, Val Acc=37.59%, Grad Norm=8.4725\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc (Test=10km/h) = 15.68%\n",
      "\n",
      "SNR   0 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-23_22-36-39_LTE-V_XFR_SpeedSplit_FileBlock_Train20kmh_Test10kmh_SNR0dB_fdTrain655_fdTest655_group288_ResNet1D\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-5 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "[INFO] 共找到 72 个 .mat 文件（含子目录）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 可用 TX 类别数（train/test 交集）: 9\n",
      "[INFO] Train blocks: 360, Test blocks: 360, sample_len: 288, group_size: 288\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2106, Train Acc=12.74%, Val Loss=3.1096, Val Acc=15.07%, Grad Norm=6.2819\n",
      "Fold 1, Epoch 2: Train Loss=2.1709, Train Acc=15.55%, Val Loss=2.3649, Val Acc=11.52%, Grad Norm=4.3343\n",
      "Fold 1, Epoch 3: Train Loss=2.1304, Train Acc=17.83%, Val Loss=2.3319, Val Acc=12.19%, Grad Norm=2.9830\n",
      "Fold 1, Epoch 4: Train Loss=2.1069, Train Acc=19.17%, Val Loss=2.2634, Val Acc=14.35%, Grad Norm=2.5312\n",
      "Fold 1, Epoch 5: Train Loss=2.0884, Train Acc=20.19%, Val Loss=2.1607, Val Acc=16.50%, Grad Norm=2.2844\n",
      "Fold 1, Epoch 6: Train Loss=2.0759, Train Acc=20.65%, Val Loss=2.1396, Val Acc=14.86%, Grad Norm=2.1220\n",
      "Fold 1, Epoch 7: Train Loss=2.0643, Train Acc=21.41%, Val Loss=2.1369, Val Acc=16.23%, Grad Norm=2.0463\n",
      "Fold 1, Epoch 8: Train Loss=2.0551, Train Acc=21.79%, Val Loss=2.1027, Val Acc=17.33%, Grad Norm=1.9703\n",
      "Fold 1, Epoch 9: Train Loss=2.0469, Train Acc=22.23%, Val Loss=2.1166, Val Acc=16.54%, Grad Norm=1.9195\n",
      "Fold 1, Epoch 10: Train Loss=2.0383, Train Acc=22.72%, Val Loss=2.0877, Val Acc=16.79%, Grad Norm=1.9147\n",
      "Fold 1, Epoch 11: Train Loss=2.0254, Train Acc=23.34%, Val Loss=2.0898, Val Acc=17.51%, Grad Norm=1.9574\n",
      "Fold 1, Epoch 12: Train Loss=2.0190, Train Acc=23.58%, Val Loss=2.0937, Val Acc=18.50%, Grad Norm=2.0636\n",
      "Fold 1, Epoch 13: Train Loss=2.0144, Train Acc=23.85%, Val Loss=2.0823, Val Acc=18.04%, Grad Norm=2.1461\n",
      "Fold 1, Epoch 14: Train Loss=2.0083, Train Acc=24.25%, Val Loss=2.0931, Val Acc=17.34%, Grad Norm=2.2426\n",
      "Fold 1, Epoch 15: Train Loss=2.0054, Train Acc=24.37%, Val Loss=2.0887, Val Acc=18.13%, Grad Norm=2.3341\n",
      "Fold 1, Epoch 16: Train Loss=2.0004, Train Acc=24.69%, Val Loss=2.0690, Val Acc=18.78%, Grad Norm=2.4197\n",
      "Fold 1, Epoch 17: Train Loss=1.9948, Train Acc=24.82%, Val Loss=2.0795, Val Acc=19.03%, Grad Norm=2.5235\n",
      "Fold 1, Epoch 18: Train Loss=1.9901, Train Acc=25.15%, Val Loss=2.0858, Val Acc=18.18%, Grad Norm=2.6490\n",
      "Fold 1, Epoch 19: Train Loss=1.9855, Train Acc=25.51%, Val Loss=2.0739, Val Acc=19.02%, Grad Norm=2.7777\n",
      "Fold 1, Epoch 20: Train Loss=1.9812, Train Acc=25.70%, Val Loss=2.0666, Val Acc=18.94%, Grad Norm=2.8753\n",
      "Fold 1, Epoch 21: Train Loss=1.9657, Train Acc=26.48%, Val Loss=2.0798, Val Acc=18.65%, Grad Norm=3.1593\n",
      "Fold 1, Epoch 22: Train Loss=1.9605, Train Acc=26.77%, Val Loss=2.0740, Val Acc=19.09%, Grad Norm=3.4107\n",
      "Fold 1, Epoch 23: Train Loss=1.9559, Train Acc=27.11%, Val Loss=2.0687, Val Acc=19.16%, Grad Norm=3.5697\n",
      "Fold 1, Epoch 24: Train Loss=1.9511, Train Acc=27.25%, Val Loss=2.0607, Val Acc=20.08%, Grad Norm=3.7833\n",
      "Fold 1, Epoch 25: Train Loss=1.9461, Train Acc=27.34%, Val Loss=2.0705, Val Acc=19.34%, Grad Norm=3.9825\n",
      "Fold 1, Epoch 26: Train Loss=1.9393, Train Acc=27.96%, Val Loss=2.0641, Val Acc=19.92%, Grad Norm=4.1701\n",
      "Fold 1, Epoch 27: Train Loss=1.9357, Train Acc=27.98%, Val Loss=2.0867, Val Acc=19.06%, Grad Norm=4.3365\n",
      "Fold 1, Epoch 28: Train Loss=1.9295, Train Acc=28.31%, Val Loss=2.0743, Val Acc=19.61%, Grad Norm=4.5740\n",
      "Fold 1, Epoch 29: Train Loss=1.9242, Train Acc=28.56%, Val Loss=2.0790, Val Acc=19.50%, Grad Norm=4.7605\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc (Test=10km/h) = 12.41%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2119, Train Acc=12.24%, Val Loss=3.7822, Val Acc=11.02%, Grad Norm=6.1879\n",
      "Fold 2, Epoch 2: Train Loss=2.1878, Train Acc=13.86%, Val Loss=2.7969, Val Acc=11.10%, Grad Norm=4.2666\n",
      "Fold 2, Epoch 3: Train Loss=2.1481, Train Acc=16.44%, Val Loss=2.5517, Val Acc=12.28%, Grad Norm=2.9564\n",
      "Fold 2, Epoch 4: Train Loss=2.1180, Train Acc=17.95%, Val Loss=2.5192, Val Acc=12.79%, Grad Norm=2.4320\n",
      "Fold 2, Epoch 5: Train Loss=2.0977, Train Acc=18.81%, Val Loss=2.3571, Val Acc=13.32%, Grad Norm=2.1797\n",
      "Fold 2, Epoch 6: Train Loss=2.0857, Train Acc=19.46%, Val Loss=2.2678, Val Acc=13.60%, Grad Norm=1.9968\n",
      "Fold 2, Epoch 7: Train Loss=2.0758, Train Acc=19.78%, Val Loss=2.2358, Val Acc=14.05%, Grad Norm=1.8824\n",
      "Fold 2, Epoch 8: Train Loss=2.0675, Train Acc=20.53%, Val Loss=2.2267, Val Acc=14.90%, Grad Norm=1.8190\n",
      "Fold 2, Epoch 9: Train Loss=2.0592, Train Acc=20.77%, Val Loss=2.1929, Val Acc=15.58%, Grad Norm=1.7934\n",
      "Fold 2, Epoch 10: Train Loss=2.0520, Train Acc=20.96%, Val Loss=2.1816, Val Acc=16.56%, Grad Norm=1.7769\n",
      "Fold 2, Epoch 11: Train Loss=2.0371, Train Acc=21.67%, Val Loss=2.1456, Val Acc=17.05%, Grad Norm=1.8696\n",
      "Fold 2, Epoch 12: Train Loss=2.0309, Train Acc=22.49%, Val Loss=2.1637, Val Acc=17.28%, Grad Norm=1.9867\n",
      "Fold 2, Epoch 13: Train Loss=2.0254, Train Acc=22.63%, Val Loss=2.1567, Val Acc=17.08%, Grad Norm=2.1067\n",
      "Fold 2, Epoch 14: Train Loss=2.0186, Train Acc=22.98%, Val Loss=2.1545, Val Acc=17.49%, Grad Norm=2.2416\n",
      "Fold 2, Epoch 15: Train Loss=2.0133, Train Acc=23.39%, Val Loss=2.1659, Val Acc=17.53%, Grad Norm=2.3425\n",
      "Fold 2, Epoch 16: Train Loss=2.0068, Train Acc=23.62%, Val Loss=2.1719, Val Acc=18.14%, Grad Norm=2.4919\n",
      "Fold 2, Epoch 17: Train Loss=2.0001, Train Acc=24.20%, Val Loss=2.1362, Val Acc=18.75%, Grad Norm=2.6286\n",
      "Fold 2, Epoch 18: Train Loss=1.9943, Train Acc=24.38%, Val Loss=2.1561, Val Acc=18.00%, Grad Norm=2.7457\n",
      "Fold 2, Epoch 19: Train Loss=1.9889, Train Acc=24.64%, Val Loss=2.1773, Val Acc=17.65%, Grad Norm=2.8856\n",
      "Fold 2, Epoch 20: Train Loss=1.9811, Train Acc=25.07%, Val Loss=2.1777, Val Acc=17.60%, Grad Norm=3.0142\n",
      "Fold 2, Epoch 21: Train Loss=1.9664, Train Acc=25.97%, Val Loss=2.1647, Val Acc=18.91%, Grad Norm=3.2771\n",
      "Fold 2, Epoch 22: Train Loss=1.9601, Train Acc=26.15%, Val Loss=2.1430, Val Acc=19.12%, Grad Norm=3.5655\n",
      "Fold 2, Epoch 23: Train Loss=1.9545, Train Acc=26.55%, Val Loss=2.1557, Val Acc=18.95%, Grad Norm=3.7684\n",
      "Fold 2, Epoch 24: Train Loss=1.9506, Train Acc=26.81%, Val Loss=2.1554, Val Acc=18.91%, Grad Norm=3.9321\n",
      "Fold 2, Epoch 25: Train Loss=1.9423, Train Acc=27.13%, Val Loss=2.1482, Val Acc=18.99%, Grad Norm=4.1385\n",
      "Fold 2, Epoch 26: Train Loss=1.9398, Train Acc=27.24%, Val Loss=2.1450, Val Acc=19.23%, Grad Norm=4.2884\n",
      "Fold 2, Epoch 27: Train Loss=1.9320, Train Acc=27.66%, Val Loss=2.1563, Val Acc=19.54%, Grad Norm=4.5118\n",
      "Fold 2, Epoch 28: Train Loss=1.9266, Train Acc=28.10%, Val Loss=2.1754, Val Acc=19.29%, Grad Norm=4.7117\n",
      "Fold 2, Epoch 29: Train Loss=1.9197, Train Acc=28.36%, Val Loss=2.1480, Val Acc=19.85%, Grad Norm=4.9561\n",
      "Fold 2, Epoch 30: Train Loss=1.9149, Train Acc=28.57%, Val Loss=2.1489, Val Acc=19.36%, Grad Norm=5.1453\n",
      "Fold 2, Epoch 31: Train Loss=1.9019, Train Acc=29.26%, Val Loss=2.1345, Val Acc=19.48%, Grad Norm=5.4103\n",
      "Fold 2, Epoch 32: Train Loss=1.8965, Train Acc=29.56%, Val Loss=2.1481, Val Acc=19.42%, Grad Norm=5.7342\n",
      "Fold 2, Epoch 33: Train Loss=1.8878, Train Acc=30.07%, Val Loss=2.1414, Val Acc=19.54%, Grad Norm=5.9884\n",
      "Fold 2, Epoch 34: Train Loss=1.8857, Train Acc=30.00%, Val Loss=2.1608, Val Acc=19.62%, Grad Norm=6.2403\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc (Test=10km/h) = 12.59%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2094, Train Acc=12.65%, Val Loss=2.9898, Val Acc=13.71%, Grad Norm=6.1274\n",
      "Fold 3, Epoch 2: Train Loss=2.1850, Train Acc=13.96%, Val Loss=2.7418, Val Acc=8.34%, Grad Norm=3.8366\n",
      "Fold 3, Epoch 3: Train Loss=2.1529, Train Acc=16.15%, Val Loss=2.5031, Val Acc=12.87%, Grad Norm=2.8277\n",
      "Fold 3, Epoch 4: Train Loss=2.1245, Train Acc=17.35%, Val Loss=2.4271, Val Acc=10.87%, Grad Norm=2.4268\n",
      "Fold 3, Epoch 5: Train Loss=2.1092, Train Acc=18.22%, Val Loss=2.2756, Val Acc=14.87%, Grad Norm=2.1727\n",
      "Fold 3, Epoch 6: Train Loss=2.0952, Train Acc=18.73%, Val Loss=2.2732, Val Acc=15.95%, Grad Norm=2.0403\n",
      "Fold 3, Epoch 7: Train Loss=2.0845, Train Acc=19.59%, Val Loss=2.1777, Val Acc=15.49%, Grad Norm=1.9516\n",
      "Fold 3, Epoch 8: Train Loss=2.0751, Train Acc=19.99%, Val Loss=2.1771, Val Acc=16.97%, Grad Norm=1.8786\n",
      "Fold 3, Epoch 9: Train Loss=2.0683, Train Acc=20.56%, Val Loss=2.1504, Val Acc=17.85%, Grad Norm=1.8713\n",
      "Fold 3, Epoch 10: Train Loss=2.0594, Train Acc=20.80%, Val Loss=2.1378, Val Acc=17.94%, Grad Norm=1.8632\n",
      "Fold 3, Epoch 11: Train Loss=2.0461, Train Acc=21.65%, Val Loss=2.1529, Val Acc=18.46%, Grad Norm=1.9439\n",
      "Fold 3, Epoch 12: Train Loss=2.0392, Train Acc=21.82%, Val Loss=2.1058, Val Acc=20.28%, Grad Norm=2.0418\n",
      "Fold 3, Epoch 13: Train Loss=2.0328, Train Acc=22.28%, Val Loss=2.1515, Val Acc=18.33%, Grad Norm=2.1293\n",
      "Fold 3, Epoch 14: Train Loss=2.0258, Train Acc=22.53%, Val Loss=2.0956, Val Acc=20.91%, Grad Norm=2.2381\n",
      "Fold 3, Epoch 15: Train Loss=2.0207, Train Acc=23.04%, Val Loss=2.1219, Val Acc=18.98%, Grad Norm=2.3766\n",
      "Fold 3, Epoch 16: Train Loss=2.0162, Train Acc=23.14%, Val Loss=2.1139, Val Acc=18.70%, Grad Norm=2.5002\n",
      "Fold 3, Epoch 17: Train Loss=2.0108, Train Acc=23.72%, Val Loss=2.1151, Val Acc=21.11%, Grad Norm=2.6011\n",
      "Fold 3, Epoch 18: Train Loss=2.0035, Train Acc=23.75%, Val Loss=2.1468, Val Acc=20.64%, Grad Norm=2.7441\n",
      "Fold 3, Epoch 19: Train Loss=1.9989, Train Acc=24.15%, Val Loss=2.0950, Val Acc=21.10%, Grad Norm=2.8492\n",
      "Fold 3, Epoch 20: Train Loss=1.9940, Train Acc=24.52%, Val Loss=2.0783, Val Acc=21.59%, Grad Norm=2.9596\n",
      "Fold 3, Epoch 21: Train Loss=1.9771, Train Acc=25.27%, Val Loss=2.0916, Val Acc=21.61%, Grad Norm=3.2131\n",
      "Fold 3, Epoch 22: Train Loss=1.9712, Train Acc=25.66%, Val Loss=2.0915, Val Acc=22.08%, Grad Norm=3.4724\n",
      "Fold 3, Epoch 23: Train Loss=1.9665, Train Acc=25.86%, Val Loss=2.0958, Val Acc=21.87%, Grad Norm=3.6491\n",
      "Fold 3, Epoch 24: Train Loss=1.9605, Train Acc=26.30%, Val Loss=2.0817, Val Acc=21.74%, Grad Norm=3.8333\n",
      "Fold 3, Epoch 25: Train Loss=1.9551, Train Acc=26.62%, Val Loss=2.1006, Val Acc=21.22%, Grad Norm=4.0228\n",
      "Fold 3, Epoch 26: Train Loss=1.9489, Train Acc=26.93%, Val Loss=2.0907, Val Acc=22.00%, Grad Norm=4.2073\n",
      "Fold 3, Epoch 27: Train Loss=1.9444, Train Acc=27.01%, Val Loss=2.0736, Val Acc=22.56%, Grad Norm=4.4223\n",
      "Fold 3, Epoch 28: Train Loss=1.9380, Train Acc=27.44%, Val Loss=2.0942, Val Acc=21.86%, Grad Norm=4.6067\n",
      "Fold 3, Epoch 29: Train Loss=1.9353, Train Acc=27.45%, Val Loss=2.0891, Val Acc=22.11%, Grad Norm=4.7738\n",
      "Fold 3, Epoch 30: Train Loss=1.9296, Train Acc=27.91%, Val Loss=2.0844, Val Acc=22.53%, Grad Norm=4.9573\n",
      "Fold 3, Epoch 31: Train Loss=1.9170, Train Acc=28.54%, Val Loss=2.0744, Val Acc=22.54%, Grad Norm=5.2169\n",
      "Fold 3, Epoch 32: Train Loss=1.9080, Train Acc=28.91%, Val Loss=2.0848, Val Acc=22.11%, Grad Norm=5.5045\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc (Test=10km/h) = 13.14%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2094, Train Acc=12.62%, Val Loss=2.8885, Val Acc=11.06%, Grad Norm=6.0543\n",
      "Fold 4, Epoch 2: Train Loss=2.1880, Train Acc=13.85%, Val Loss=2.8330, Val Acc=10.64%, Grad Norm=3.9001\n",
      "Fold 4, Epoch 3: Train Loss=2.1571, Train Acc=16.16%, Val Loss=2.3970, Val Acc=13.17%, Grad Norm=2.9499\n",
      "Fold 4, Epoch 4: Train Loss=2.1257, Train Acc=17.86%, Val Loss=2.3258, Val Acc=14.69%, Grad Norm=2.5667\n",
      "Fold 4, Epoch 5: Train Loss=2.1055, Train Acc=18.99%, Val Loss=2.2325, Val Acc=15.27%, Grad Norm=2.3148\n",
      "Fold 4, Epoch 6: Train Loss=2.0912, Train Acc=19.64%, Val Loss=2.1813, Val Acc=16.44%, Grad Norm=2.1465\n",
      "Fold 4, Epoch 7: Train Loss=2.0795, Train Acc=20.20%, Val Loss=2.1582, Val Acc=17.06%, Grad Norm=2.0351\n",
      "Fold 4, Epoch 8: Train Loss=2.0694, Train Acc=20.98%, Val Loss=2.1453, Val Acc=17.20%, Grad Norm=1.9618\n",
      "Fold 4, Epoch 9: Train Loss=2.0591, Train Acc=21.59%, Val Loss=2.1314, Val Acc=17.40%, Grad Norm=1.9309\n",
      "Fold 4, Epoch 10: Train Loss=2.0522, Train Acc=21.90%, Val Loss=2.1354, Val Acc=18.19%, Grad Norm=1.9243\n",
      "Fold 4, Epoch 11: Train Loss=2.0373, Train Acc=22.78%, Val Loss=2.1036, Val Acc=18.32%, Grad Norm=1.9834\n",
      "Fold 4, Epoch 12: Train Loss=2.0304, Train Acc=23.03%, Val Loss=2.1066, Val Acc=18.58%, Grad Norm=2.0796\n",
      "Fold 4, Epoch 13: Train Loss=2.0249, Train Acc=23.42%, Val Loss=2.0954, Val Acc=18.70%, Grad Norm=2.1773\n",
      "Fold 4, Epoch 14: Train Loss=2.0203, Train Acc=23.54%, Val Loss=2.0953, Val Acc=18.76%, Grad Norm=2.2591\n",
      "Fold 4, Epoch 15: Train Loss=2.0176, Train Acc=23.85%, Val Loss=2.0998, Val Acc=19.03%, Grad Norm=2.3416\n",
      "Fold 4, Epoch 16: Train Loss=2.0131, Train Acc=23.87%, Val Loss=2.0980, Val Acc=18.83%, Grad Norm=2.4196\n",
      "Fold 4, Epoch 17: Train Loss=2.0066, Train Acc=24.51%, Val Loss=2.1010, Val Acc=18.49%, Grad Norm=2.5610\n",
      "Fold 4, Epoch 18: Train Loss=2.0023, Train Acc=24.57%, Val Loss=2.0941, Val Acc=19.28%, Grad Norm=2.6564\n",
      "Fold 4, Epoch 19: Train Loss=1.9964, Train Acc=24.81%, Val Loss=2.0862, Val Acc=19.54%, Grad Norm=2.8058\n",
      "Fold 4, Epoch 20: Train Loss=1.9906, Train Acc=25.35%, Val Loss=2.0797, Val Acc=18.79%, Grad Norm=2.9432\n",
      "Fold 4, Epoch 21: Train Loss=1.9758, Train Acc=26.00%, Val Loss=2.0825, Val Acc=19.70%, Grad Norm=3.1486\n",
      "Fold 4, Epoch 22: Train Loss=1.9690, Train Acc=26.33%, Val Loss=2.0707, Val Acc=19.65%, Grad Norm=3.4574\n",
      "Fold 4, Epoch 23: Train Loss=1.9637, Train Acc=26.57%, Val Loss=2.0848, Val Acc=19.49%, Grad Norm=3.6667\n",
      "Fold 4, Epoch 24: Train Loss=1.9591, Train Acc=26.87%, Val Loss=2.0610, Val Acc=20.27%, Grad Norm=3.8458\n",
      "Fold 4, Epoch 25: Train Loss=1.9519, Train Acc=27.10%, Val Loss=2.0730, Val Acc=20.11%, Grad Norm=3.9905\n",
      "Fold 4, Epoch 26: Train Loss=1.9480, Train Acc=27.51%, Val Loss=2.0764, Val Acc=19.81%, Grad Norm=4.2171\n",
      "Fold 4, Epoch 27: Train Loss=1.9431, Train Acc=27.67%, Val Loss=2.0973, Val Acc=20.22%, Grad Norm=4.3789\n",
      "Fold 4, Epoch 28: Train Loss=1.9387, Train Acc=27.88%, Val Loss=2.0889, Val Acc=19.71%, Grad Norm=4.5163\n",
      "Fold 4, Epoch 29: Train Loss=1.9333, Train Acc=28.16%, Val Loss=2.0917, Val Acc=20.13%, Grad Norm=4.7469\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc (Test=10km/h) = 12.56%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2068, Train Acc=12.43%, Val Loss=3.7564, Val Acc=8.26%, Grad Norm=5.9585\n",
      "Fold 5, Epoch 2: Train Loss=2.1840, Train Acc=13.96%, Val Loss=2.6303, Val Acc=13.02%, Grad Norm=3.6724\n",
      "Fold 5, Epoch 3: Train Loss=2.1601, Train Acc=15.22%, Val Loss=2.6839, Val Acc=10.01%, Grad Norm=2.6256\n",
      "Fold 5, Epoch 4: Train Loss=2.1401, Train Acc=16.54%, Val Loss=2.5519, Val Acc=12.35%, Grad Norm=2.3825\n",
      "Fold 5, Epoch 5: Train Loss=2.1130, Train Acc=17.98%, Val Loss=2.3497, Val Acc=15.04%, Grad Norm=2.1989\n",
      "Fold 5, Epoch 6: Train Loss=2.0961, Train Acc=19.03%, Val Loss=2.2375, Val Acc=18.52%, Grad Norm=2.0359\n",
      "Fold 5, Epoch 7: Train Loss=2.0846, Train Acc=19.32%, Val Loss=2.2005, Val Acc=17.69%, Grad Norm=1.9406\n",
      "Fold 5, Epoch 8: Train Loss=2.0758, Train Acc=20.08%, Val Loss=2.1709, Val Acc=18.05%, Grad Norm=1.8803\n",
      "Fold 5, Epoch 9: Train Loss=2.0663, Train Acc=20.35%, Val Loss=2.1393, Val Acc=18.55%, Grad Norm=1.8547\n",
      "Fold 5, Epoch 10: Train Loss=2.0604, Train Acc=20.96%, Val Loss=2.1626, Val Acc=17.94%, Grad Norm=1.8557\n",
      "Fold 5, Epoch 11: Train Loss=2.0449, Train Acc=21.71%, Val Loss=2.1470, Val Acc=19.06%, Grad Norm=1.9465\n",
      "Fold 5, Epoch 12: Train Loss=2.0366, Train Acc=22.17%, Val Loss=2.1672, Val Acc=19.66%, Grad Norm=2.0768\n",
      "Fold 5, Epoch 13: Train Loss=2.0301, Train Acc=22.73%, Val Loss=2.1442, Val Acc=20.83%, Grad Norm=2.2001\n",
      "Fold 5, Epoch 14: Train Loss=2.0243, Train Acc=22.92%, Val Loss=2.1475, Val Acc=20.68%, Grad Norm=2.3179\n",
      "Fold 5, Epoch 15: Train Loss=2.0197, Train Acc=23.00%, Val Loss=2.1269, Val Acc=20.08%, Grad Norm=2.4077\n",
      "Fold 5, Epoch 16: Train Loss=2.0134, Train Acc=23.52%, Val Loss=2.1423, Val Acc=20.05%, Grad Norm=2.5362\n",
      "Fold 5, Epoch 17: Train Loss=2.0070, Train Acc=23.87%, Val Loss=2.1278, Val Acc=20.49%, Grad Norm=2.6624\n",
      "Fold 5, Epoch 18: Train Loss=2.0030, Train Acc=24.13%, Val Loss=2.1035, Val Acc=22.00%, Grad Norm=2.7656\n",
      "Fold 5, Epoch 19: Train Loss=1.9958, Train Acc=24.31%, Val Loss=2.1038, Val Acc=20.86%, Grad Norm=2.9406\n",
      "Fold 5, Epoch 20: Train Loss=1.9906, Train Acc=24.94%, Val Loss=2.1103, Val Acc=22.11%, Grad Norm=3.0532\n",
      "Fold 5, Epoch 21: Train Loss=1.9735, Train Acc=25.53%, Val Loss=2.1144, Val Acc=21.97%, Grad Norm=3.3522\n",
      "Fold 5, Epoch 22: Train Loss=1.9670, Train Acc=25.93%, Val Loss=2.1120, Val Acc=22.81%, Grad Norm=3.5962\n",
      "Fold 5, Epoch 23: Train Loss=1.9624, Train Acc=26.13%, Val Loss=2.1008, Val Acc=22.46%, Grad Norm=3.7850\n",
      "Fold 5, Epoch 24: Train Loss=1.9547, Train Acc=26.66%, Val Loss=2.1093, Val Acc=21.56%, Grad Norm=3.9560\n",
      "Fold 5, Epoch 25: Train Loss=1.9519, Train Acc=26.68%, Val Loss=2.0943, Val Acc=22.08%, Grad Norm=4.1156\n",
      "Fold 5, Epoch 26: Train Loss=1.9469, Train Acc=27.02%, Val Loss=2.1122, Val Acc=22.95%, Grad Norm=4.3178\n",
      "Fold 5, Epoch 27: Train Loss=1.9396, Train Acc=27.75%, Val Loss=2.1011, Val Acc=22.76%, Grad Norm=4.5280\n",
      "Fold 5, Epoch 28: Train Loss=1.9341, Train Acc=27.85%, Val Loss=2.1244, Val Acc=22.31%, Grad Norm=4.7263\n",
      "Fold 5, Epoch 29: Train Loss=1.9297, Train Acc=27.99%, Val Loss=2.0971, Val Acc=22.46%, Grad Norm=4.9691\n",
      "Fold 5, Epoch 30: Train Loss=1.9241, Train Acc=28.28%, Val Loss=2.1091, Val Acc=22.25%, Grad Norm=5.1491\n",
      "Fold 5, Epoch 31: Train Loss=1.9075, Train Acc=29.22%, Val Loss=2.1004, Val Acc=22.96%, Grad Norm=5.4128\n",
      "Fold 5, Epoch 32: Train Loss=1.9025, Train Acc=29.41%, Val Loss=2.1316, Val Acc=22.55%, Grad Norm=5.7438\n",
      "Fold 5, Epoch 33: Train Loss=1.8977, Train Acc=29.65%, Val Loss=2.1309, Val Acc=22.62%, Grad Norm=5.9811\n",
      "Fold 5, Epoch 34: Train Loss=1.8899, Train Acc=30.13%, Val Loss=2.1190, Val Acc=22.41%, Grad Norm=6.2121\n",
      "Fold 5, Epoch 35: Train Loss=1.8892, Train Acc=30.06%, Val Loss=2.1260, Val Acc=22.77%, Grad Norm=6.4933\n",
      "Fold 5, Epoch 36: Train Loss=1.8819, Train Acc=30.52%, Val Loss=2.1099, Val Acc=22.37%, Grad Norm=6.7163\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc (Test=10km/h) = 12.34%\n",
      "\n",
      "SNR  -5 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-23_22-36-39_LTE-V_XFR_SpeedSplit_FileBlock_Train20kmh_Test10kmh_SNR-5dB_fdTrain655_fdTest655_group288_ResNet1D\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-10 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "[INFO] 共找到 72 个 .mat 文件（含子目录）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:11<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 可用 TX 类别数（train/test 交集）: 9\n",
      "[INFO] Train blocks: 360, Test blocks: 360, sample_len: 288, group_size: 288\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2169, Train Acc=11.72%, Val Loss=2.3863, Val Acc=15.25%, Grad Norm=6.2903\n",
      "Fold 1, Epoch 2: Train Loss=2.2086, Train Acc=12.29%, Val Loss=2.2628, Val Acc=15.18%, Grad Norm=5.0773\n",
      "Fold 1, Epoch 3: Train Loss=2.2024, Train Acc=12.43%, Val Loss=2.2753, Val Acc=11.06%, Grad Norm=3.7144\n",
      "Fold 1, Epoch 4: Train Loss=2.1964, Train Acc=12.78%, Val Loss=2.1770, Val Acc=12.59%, Grad Norm=2.6093\n",
      "Fold 1, Epoch 5: Train Loss=2.1919, Train Acc=13.06%, Val Loss=2.2106, Val Acc=14.75%, Grad Norm=2.0448\n",
      "Fold 1, Epoch 6: Train Loss=2.1871, Train Acc=13.36%, Val Loss=2.1876, Val Acc=15.30%, Grad Norm=1.7340\n",
      "Fold 1, Epoch 7: Train Loss=2.1805, Train Acc=14.30%, Val Loss=2.1960, Val Acc=11.42%, Grad Norm=1.5736\n",
      "Fold 1, Epoch 8: Train Loss=2.1709, Train Acc=15.25%, Val Loss=2.1841, Val Acc=15.80%, Grad Norm=1.4886\n",
      "Fold 1, Epoch 9: Train Loss=2.1616, Train Acc=15.98%, Val Loss=2.1974, Val Acc=12.31%, Grad Norm=1.4000\n",
      "Fold 1, Epoch 10: Train Loss=2.1566, Train Acc=16.28%, Val Loss=2.1780, Val Acc=15.11%, Grad Norm=1.2972\n",
      "Fold 1, Epoch 11: Train Loss=2.1497, Train Acc=16.61%, Val Loss=2.1988, Val Acc=13.05%, Grad Norm=1.2895\n",
      "Fold 1, Epoch 12: Train Loss=2.1483, Train Acc=16.83%, Val Loss=2.1849, Val Acc=11.91%, Grad Norm=1.3144\n",
      "Fold 1, Epoch 13: Train Loss=2.1441, Train Acc=17.15%, Val Loss=2.1875, Val Acc=12.46%, Grad Norm=1.3767\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc (Test=10km/h) = 11.13%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2183, Train Acc=11.64%, Val Loss=2.4742, Val Acc=9.05%, Grad Norm=6.2224\n",
      "Fold 2, Epoch 2: Train Loss=2.2115, Train Acc=11.77%, Val Loss=2.5091, Val Acc=11.01%, Grad Norm=4.9891\n",
      "Fold 2, Epoch 3: Train Loss=2.2021, Train Acc=12.13%, Val Loss=2.3546, Val Acc=11.07%, Grad Norm=3.3142\n",
      "Fold 2, Epoch 4: Train Loss=2.1972, Train Acc=12.45%, Val Loss=2.3405, Val Acc=8.72%, Grad Norm=2.4418\n",
      "Fold 2, Epoch 5: Train Loss=2.1907, Train Acc=13.36%, Val Loss=2.2792, Val Acc=8.46%, Grad Norm=1.9949\n",
      "Fold 2, Epoch 6: Train Loss=2.1825, Train Acc=13.80%, Val Loss=2.2406, Val Acc=9.16%, Grad Norm=1.7148\n",
      "Fold 2, Epoch 7: Train Loss=2.1772, Train Acc=14.28%, Val Loss=2.2403, Val Acc=9.88%, Grad Norm=1.5416\n",
      "Fold 2, Epoch 8: Train Loss=2.1698, Train Acc=14.93%, Val Loss=2.2303, Val Acc=10.92%, Grad Norm=1.4485\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc (Test=10km/h) = 11.09%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2154, Train Acc=11.88%, Val Loss=2.5389, Val Acc=13.65%, Grad Norm=6.3569\n",
      "Fold 3, Epoch 2: Train Loss=2.2100, Train Acc=11.73%, Val Loss=2.4990, Val Acc=8.32%, Grad Norm=5.3154\n",
      "Fold 3, Epoch 3: Train Loss=2.2032, Train Acc=11.86%, Val Loss=2.4106, Val Acc=5.78%, Grad Norm=3.8837\n",
      "Fold 3, Epoch 4: Train Loss=2.1980, Train Acc=12.47%, Val Loss=2.3300, Val Acc=8.21%, Grad Norm=2.7548\n",
      "Fold 3, Epoch 5: Train Loss=2.1939, Train Acc=12.73%, Val Loss=2.3032, Val Acc=13.35%, Grad Norm=2.0639\n",
      "Fold 3, Epoch 6: Train Loss=2.1896, Train Acc=13.35%, Val Loss=2.3238, Val Acc=8.43%, Grad Norm=1.7429\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc (Test=10km/h) = 11.11%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2174, Train Acc=11.56%, Val Loss=2.4760, Val Acc=11.12%, Grad Norm=6.3918\n",
      "Fold 4, Epoch 2: Train Loss=2.2120, Train Acc=11.73%, Val Loss=2.7657, Val Acc=11.13%, Grad Norm=5.2913\n",
      "Fold 4, Epoch 3: Train Loss=2.2045, Train Acc=12.08%, Val Loss=2.2744, Val Acc=11.05%, Grad Norm=3.8893\n",
      "Fold 4, Epoch 4: Train Loss=2.1983, Train Acc=12.54%, Val Loss=2.2925, Val Acc=10.84%, Grad Norm=2.7455\n",
      "Fold 4, Epoch 5: Train Loss=2.1936, Train Acc=12.78%, Val Loss=2.2657, Val Acc=10.28%, Grad Norm=2.1386\n",
      "Fold 4, Epoch 6: Train Loss=2.1877, Train Acc=13.51%, Val Loss=2.2206, Val Acc=9.20%, Grad Norm=1.7381\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc (Test=10km/h) = 11.15%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2171, Train Acc=11.67%, Val Loss=2.8380, Val Acc=8.32%, Grad Norm=6.2548\n",
      "Fold 5, Epoch 2: Train Loss=2.2096, Train Acc=11.96%, Val Loss=2.3805, Val Acc=8.45%, Grad Norm=4.8893\n",
      "Fold 5, Epoch 3: Train Loss=2.2011, Train Acc=11.96%, Val Loss=2.3876, Val Acc=8.25%, Grad Norm=3.0537\n",
      "Fold 5, Epoch 4: Train Loss=2.1967, Train Acc=12.23%, Val Loss=2.2933, Val Acc=8.44%, Grad Norm=2.1947\n",
      "Fold 5, Epoch 5: Train Loss=2.1937, Train Acc=12.50%, Val Loss=2.3179, Val Acc=12.32%, Grad Norm=1.7914\n",
      "Fold 5, Epoch 6: Train Loss=2.1904, Train Acc=12.83%, Val Loss=2.2889, Val Acc=8.53%, Grad Norm=1.6379\n",
      "Fold 5, Epoch 7: Train Loss=2.1832, Train Acc=13.79%, Val Loss=2.2266, Val Acc=10.51%, Grad Norm=1.5606\n",
      "Fold 5, Epoch 8: Train Loss=2.1750, Train Acc=14.64%, Val Loss=2.2362, Val Acc=11.03%, Grad Norm=1.4522\n",
      "Fold 5, Epoch 9: Train Loss=2.1689, Train Acc=15.03%, Val Loss=2.2154, Val Acc=14.33%, Grad Norm=1.3562\n",
      "Fold 5, Epoch 10: Train Loss=2.1649, Train Acc=15.39%, Val Loss=2.1984, Val Acc=13.35%, Grad Norm=1.2825\n",
      "Fold 5, Epoch 11: Train Loss=2.1594, Train Acc=15.62%, Val Loss=2.2028, Val Acc=13.05%, Grad Norm=1.2704\n",
      "Fold 5, Epoch 12: Train Loss=2.1561, Train Acc=15.93%, Val Loss=2.1935, Val Acc=13.65%, Grad Norm=1.3095\n",
      "Fold 5, Epoch 13: Train Loss=2.1547, Train Acc=15.92%, Val Loss=2.1942, Val Acc=13.00%, Grad Norm=1.3411\n",
      "Fold 5, Epoch 14: Train Loss=2.1528, Train Acc=16.17%, Val Loss=2.1888, Val Acc=13.57%, Grad Norm=1.3797\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc (Test=10km/h) = 11.10%\n",
      "\n",
      "SNR -10 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-23_22-36-39_LTE-V_XFR_SpeedSplit_FileBlock_Train20kmh_Test10kmh_SNR-10dB_fdTrain655_fdTest655_group288_ResNet1D\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-15 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "[INFO] 共找到 72 个 .mat 文件（含子目录）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 可用 TX 类别数（train/test 交集）: 9\n",
      "[INFO] Train blocks: 360, Test blocks: 360, sample_len: 288, group_size: 288\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2180, Train Acc=11.66%, Val Loss=2.2602, Val Acc=12.22%, Grad Norm=6.3012\n",
      "Fold 1, Epoch 2: Train Loss=2.2130, Train Acc=11.66%, Val Loss=2.2622, Val Acc=12.50%, Grad Norm=5.1990\n",
      "Fold 1, Epoch 3: Train Loss=2.2063, Train Acc=11.93%, Val Loss=2.2356, Val Acc=5.94%, Grad Norm=3.9721\n",
      "Fold 1, Epoch 4: Train Loss=2.2022, Train Acc=12.03%, Val Loss=2.2371, Val Acc=10.45%, Grad Norm=2.9768\n",
      "Fold 1, Epoch 5: Train Loss=2.1999, Train Acc=12.28%, Val Loss=2.2257, Val Acc=10.30%, Grad Norm=2.3912\n",
      "Fold 1, Epoch 6: Train Loss=2.1984, Train Acc=12.34%, Val Loss=2.2536, Val Acc=10.28%, Grad Norm=1.9961\n",
      "Fold 1, Epoch 7: Train Loss=2.1974, Train Acc=12.50%, Val Loss=2.2050, Val Acc=12.91%, Grad Norm=1.6826\n",
      "Fold 1, Epoch 8: Train Loss=2.1961, Train Acc=12.69%, Val Loss=2.2110, Val Acc=6.26%, Grad Norm=1.4071\n",
      "Fold 1, Epoch 9: Train Loss=2.1952, Train Acc=12.63%, Val Loss=2.2634, Val Acc=4.32%, Grad Norm=1.1950\n",
      "Fold 1, Epoch 10: Train Loss=2.1946, Train Acc=12.73%, Val Loss=2.2225, Val Acc=5.46%, Grad Norm=1.0486\n",
      "Fold 1, Epoch 11: Train Loss=2.1931, Train Acc=13.02%, Val Loss=2.2417, Val Acc=5.57%, Grad Norm=0.9009\n",
      "Fold 1, Epoch 12: Train Loss=2.1920, Train Acc=13.14%, Val Loss=2.2318, Val Acc=4.93%, Grad Norm=0.9880\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc (Test=10km/h) = 11.21%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2207, Train Acc=11.17%, Val Loss=2.2914, Val Acc=9.65%, Grad Norm=6.3988\n",
      "Fold 2, Epoch 2: Train Loss=2.2141, Train Acc=11.25%, Val Loss=2.2248, Val Acc=8.29%, Grad Norm=5.4500\n",
      "Fold 2, Epoch 3: Train Loss=2.2088, Train Acc=11.46%, Val Loss=2.3353, Val Acc=9.47%, Grad Norm=4.1798\n",
      "Fold 2, Epoch 4: Train Loss=2.2044, Train Acc=11.38%, Val Loss=2.2334, Val Acc=11.18%, Grad Norm=3.0934\n",
      "Fold 2, Epoch 5: Train Loss=2.2012, Train Acc=11.63%, Val Loss=2.2187, Val Acc=15.32%, Grad Norm=2.5314\n",
      "Fold 2, Epoch 6: Train Loss=2.2001, Train Acc=11.90%, Val Loss=2.2418, Val Acc=8.33%, Grad Norm=2.1675\n",
      "Fold 2, Epoch 7: Train Loss=2.1990, Train Acc=11.62%, Val Loss=2.2312, Val Acc=10.21%, Grad Norm=1.7966\n",
      "Fold 2, Epoch 8: Train Loss=2.1983, Train Acc=11.82%, Val Loss=2.2572, Val Acc=6.94%, Grad Norm=1.4386\n",
      "Fold 2, Epoch 9: Train Loss=2.1974, Train Acc=11.91%, Val Loss=2.2160, Val Acc=9.14%, Grad Norm=1.2298\n",
      "Fold 2, Epoch 10: Train Loss=2.1963, Train Acc=11.99%, Val Loss=2.2534, Val Acc=8.29%, Grad Norm=1.0636\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc (Test=10km/h) = 11.15%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2163, Train Acc=11.65%, Val Loss=2.3102, Val Acc=7.46%, Grad Norm=6.2531\n",
      "Fold 3, Epoch 2: Train Loss=2.2130, Train Acc=11.66%, Val Loss=2.2755, Val Acc=5.73%, Grad Norm=5.1881\n",
      "Fold 3, Epoch 3: Train Loss=2.2056, Train Acc=11.84%, Val Loss=2.2907, Val Acc=9.48%, Grad Norm=3.9712\n",
      "Fold 3, Epoch 4: Train Loss=2.2019, Train Acc=11.84%, Val Loss=2.2870, Val Acc=8.35%, Grad Norm=2.9795\n",
      "Fold 3, Epoch 5: Train Loss=2.2002, Train Acc=12.15%, Val Loss=2.2486, Val Acc=7.38%, Grad Norm=2.4911\n",
      "Fold 3, Epoch 6: Train Loss=2.1988, Train Acc=12.19%, Val Loss=2.2648, Val Acc=7.97%, Grad Norm=2.0534\n",
      "Fold 3, Epoch 7: Train Loss=2.1977, Train Acc=12.30%, Val Loss=2.2446, Val Acc=8.10%, Grad Norm=1.7289\n",
      "Fold 3, Epoch 8: Train Loss=2.1962, Train Acc=12.33%, Val Loss=2.2284, Val Acc=8.93%, Grad Norm=1.4249\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc (Test=10km/h) = 11.04%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2198, Train Acc=11.60%, Val Loss=2.2978, Val Acc=6.94%, Grad Norm=6.3221\n",
      "Fold 4, Epoch 2: Train Loss=2.2129, Train Acc=11.68%, Val Loss=2.2807, Val Acc=13.93%, Grad Norm=5.4070\n",
      "Fold 4, Epoch 3: Train Loss=2.2079, Train Acc=11.86%, Val Loss=2.2515, Val Acc=8.88%, Grad Norm=4.1966\n",
      "Fold 4, Epoch 4: Train Loss=2.2028, Train Acc=11.67%, Val Loss=2.2156, Val Acc=12.26%, Grad Norm=3.1202\n",
      "Fold 4, Epoch 5: Train Loss=2.2008, Train Acc=11.99%, Val Loss=2.2295, Val Acc=8.33%, Grad Norm=2.4889\n",
      "Fold 4, Epoch 6: Train Loss=2.1997, Train Acc=11.85%, Val Loss=2.2592, Val Acc=13.26%, Grad Norm=2.0920\n",
      "Fold 4, Epoch 7: Train Loss=2.1982, Train Acc=12.21%, Val Loss=2.2431, Val Acc=13.45%, Grad Norm=1.7474\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc (Test=10km/h) = 11.10%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2197, Train Acc=11.51%, Val Loss=2.3054, Val Acc=11.11%, Grad Norm=6.4855\n",
      "Fold 5, Epoch 2: Train Loss=2.2144, Train Acc=11.37%, Val Loss=2.1918, Val Acc=12.50%, Grad Norm=5.4618\n",
      "Fold 5, Epoch 3: Train Loss=2.2072, Train Acc=11.63%, Val Loss=2.2639, Val Acc=12.50%, Grad Norm=4.2091\n",
      "Fold 5, Epoch 4: Train Loss=2.2033, Train Acc=11.57%, Val Loss=2.2301, Val Acc=7.09%, Grad Norm=3.0918\n",
      "Fold 5, Epoch 5: Train Loss=2.2019, Train Acc=11.66%, Val Loss=2.2169, Val Acc=11.46%, Grad Norm=2.4956\n",
      "Fold 5, Epoch 6: Train Loss=2.1997, Train Acc=11.65%, Val Loss=2.2549, Val Acc=8.67%, Grad Norm=2.0910\n",
      "Fold 5, Epoch 7: Train Loss=2.1985, Train Acc=12.17%, Val Loss=2.2353, Val Acc=11.56%, Grad Norm=1.7752\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc (Test=10km/h) = 11.06%\n",
      "\n",
      "SNR -15 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-23_22-36-39_LTE-V_XFR_SpeedSplit_FileBlock_Train20kmh_Test10kmh_SNR-15dB_fdTrain655_fdTest655_group288_ResNet1D\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-20 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "[INFO] 共找到 72 个 .mat 文件（含子目录）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 可用 TX 类别数（train/test 交集）: 9\n",
      "[INFO] Train blocks: 360, Test blocks: 360, sample_len: 288, group_size: 288\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2177, Train Acc=11.59%, Val Loss=2.3405, Val Acc=15.28%, Grad Norm=6.2750\n",
      "Fold 1, Epoch 2: Train Loss=2.2124, Train Acc=11.83%, Val Loss=2.2460, Val Acc=5.71%, Grad Norm=5.3683\n",
      "Fold 1, Epoch 3: Train Loss=2.2068, Train Acc=11.99%, Val Loss=2.2566, Val Acc=10.83%, Grad Norm=4.1481\n",
      "Fold 1, Epoch 4: Train Loss=2.2016, Train Acc=12.13%, Val Loss=2.2779, Val Acc=4.52%, Grad Norm=3.0140\n",
      "Fold 1, Epoch 5: Train Loss=2.1998, Train Acc=12.38%, Val Loss=2.2871, Val Acc=9.78%, Grad Norm=2.3988\n",
      "Fold 1, Epoch 6: Train Loss=2.1987, Train Acc=12.41%, Val Loss=2.2539, Val Acc=5.56%, Grad Norm=2.0264\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc (Test=10km/h) = 11.11%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2204, Train Acc=11.45%, Val Loss=2.2477, Val Acc=9.00%, Grad Norm=6.4373\n",
      "Fold 2, Epoch 2: Train Loss=2.2143, Train Acc=11.52%, Val Loss=2.3016, Val Acc=9.72%, Grad Norm=5.4991\n",
      "Fold 2, Epoch 3: Train Loss=2.2085, Train Acc=11.58%, Val Loss=2.2280, Val Acc=8.80%, Grad Norm=4.2193\n",
      "Fold 2, Epoch 4: Train Loss=2.2044, Train Acc=11.47%, Val Loss=2.2272, Val Acc=9.79%, Grad Norm=3.0772\n",
      "Fold 2, Epoch 5: Train Loss=2.2016, Train Acc=11.78%, Val Loss=2.2411, Val Acc=8.33%, Grad Norm=2.5225\n",
      "Fold 2, Epoch 6: Train Loss=2.2007, Train Acc=11.43%, Val Loss=2.2133, Val Acc=6.97%, Grad Norm=2.1387\n",
      "Fold 2, Epoch 7: Train Loss=2.1995, Train Acc=11.55%, Val Loss=2.2170, Val Acc=8.47%, Grad Norm=1.7861\n",
      "Fold 2, Epoch 8: Train Loss=2.1983, Train Acc=11.70%, Val Loss=2.2016, Val Acc=13.40%, Grad Norm=1.5008\n",
      "Fold 2, Epoch 9: Train Loss=2.1975, Train Acc=11.93%, Val Loss=2.2173, Val Acc=9.58%, Grad Norm=1.2749\n",
      "Fold 2, Epoch 10: Train Loss=2.1969, Train Acc=12.04%, Val Loss=2.2076, Val Acc=7.30%, Grad Norm=1.0220\n",
      "Fold 2, Epoch 11: Train Loss=2.1949, Train Acc=12.12%, Val Loss=2.2163, Val Acc=8.22%, Grad Norm=0.9534\n",
      "Fold 2, Epoch 12: Train Loss=2.1942, Train Acc=12.38%, Val Loss=2.2236, Val Acc=8.88%, Grad Norm=0.9950\n",
      "Fold 2, Epoch 13: Train Loss=2.1937, Train Acc=12.55%, Val Loss=2.2169, Val Acc=8.37%, Grad Norm=1.0173\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc (Test=10km/h) = 11.11%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2183, Train Acc=11.67%, Val Loss=2.3157, Val Acc=5.59%, Grad Norm=6.2979\n",
      "Fold 3, Epoch 2: Train Loss=2.2120, Train Acc=11.61%, Val Loss=2.2167, Val Acc=18.06%, Grad Norm=5.2540\n",
      "Fold 3, Epoch 3: Train Loss=2.2056, Train Acc=11.82%, Val Loss=2.2910, Val Acc=5.58%, Grad Norm=3.9454\n",
      "Fold 3, Epoch 4: Train Loss=2.2015, Train Acc=11.87%, Val Loss=2.2709, Val Acc=6.76%, Grad Norm=2.9512\n",
      "Fold 3, Epoch 5: Train Loss=2.1998, Train Acc=11.98%, Val Loss=2.2707, Val Acc=6.09%, Grad Norm=2.4637\n",
      "Fold 3, Epoch 6: Train Loss=2.1983, Train Acc=11.96%, Val Loss=2.2319, Val Acc=8.42%, Grad Norm=2.0892\n",
      "Fold 3, Epoch 7: Train Loss=2.1974, Train Acc=12.31%, Val Loss=2.2537, Val Acc=5.53%, Grad Norm=1.7774\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc (Test=10km/h) = 11.11%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2192, Train Acc=11.48%, Val Loss=2.2648, Val Acc=7.59%, Grad Norm=6.3310\n",
      "Fold 4, Epoch 2: Train Loss=2.2136, Train Acc=11.54%, Val Loss=2.2490, Val Acc=6.95%, Grad Norm=5.4069\n",
      "Fold 4, Epoch 3: Train Loss=2.2077, Train Acc=11.69%, Val Loss=2.2898, Val Acc=6.94%, Grad Norm=4.1304\n",
      "Fold 4, Epoch 4: Train Loss=2.2028, Train Acc=11.90%, Val Loss=2.3058, Val Acc=9.16%, Grad Norm=3.1007\n",
      "Fold 4, Epoch 5: Train Loss=2.2010, Train Acc=11.71%, Val Loss=2.2690, Val Acc=6.92%, Grad Norm=2.4892\n",
      "Fold 4, Epoch 6: Train Loss=2.1992, Train Acc=12.03%, Val Loss=2.2483, Val Acc=6.94%, Grad Norm=2.0789\n",
      "Fold 4, Epoch 7: Train Loss=2.1979, Train Acc=12.18%, Val Loss=2.2651, Val Acc=7.04%, Grad Norm=1.7711\n",
      "Fold 4, Epoch 8: Train Loss=2.1973, Train Acc=12.25%, Val Loss=2.2325, Val Acc=6.93%, Grad Norm=1.4947\n",
      "Fold 4, Epoch 9: Train Loss=2.1966, Train Acc=12.34%, Val Loss=2.2456, Val Acc=7.01%, Grad Norm=1.2704\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc (Test=10km/h) = 11.11%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2196, Train Acc=11.47%, Val Loss=2.2946, Val Acc=12.19%, Grad Norm=6.4053\n",
      "Fold 5, Epoch 2: Train Loss=2.2145, Train Acc=11.51%, Val Loss=2.2484, Val Acc=7.11%, Grad Norm=5.4218\n",
      "Fold 5, Epoch 3: Train Loss=2.2076, Train Acc=11.73%, Val Loss=2.2105, Val Acc=16.33%, Grad Norm=4.1851\n",
      "Fold 5, Epoch 4: Train Loss=2.2031, Train Acc=11.79%, Val Loss=2.2281, Val Acc=6.93%, Grad Norm=3.1056\n",
      "Fold 5, Epoch 5: Train Loss=2.2014, Train Acc=11.71%, Val Loss=2.2687, Val Acc=11.93%, Grad Norm=2.4906\n",
      "Fold 5, Epoch 6: Train Loss=2.1998, Train Acc=11.94%, Val Loss=2.2174, Val Acc=12.48%, Grad Norm=2.0948\n",
      "Fold 5, Epoch 7: Train Loss=2.1985, Train Acc=11.97%, Val Loss=2.2136, Val Acc=11.20%, Grad Norm=1.7822\n",
      "Fold 5, Epoch 8: Train Loss=2.1974, Train Acc=12.17%, Val Loss=2.1917, Val Acc=12.62%, Grad Norm=1.5011\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc (Test=10km/h) = 11.13%\n",
      "\n",
      "SNR -20 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-23_22-36-39_LTE-V_XFR_SpeedSplit_FileBlock_Train20kmh_Test10kmh_SNR-20dB_fdTrain655_fdTest655_group288_ResNet1D\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-25 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "[INFO] 共找到 72 个 .mat 文件（含子目录）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 可用 TX 类别数（train/test 交集）: 9\n",
      "[INFO] Train blocks: 360, Test blocks: 360, sample_len: 288, group_size: 288\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2173, Train Acc=11.65%, Val Loss=2.2436, Val Acc=10.49%, Grad Norm=6.2864\n",
      "Fold 1, Epoch 2: Train Loss=2.2117, Train Acc=11.99%, Val Loss=2.2064, Val Acc=16.55%, Grad Norm=5.2668\n",
      "Fold 1, Epoch 3: Train Loss=2.2057, Train Acc=12.04%, Val Loss=2.2755, Val Acc=7.32%, Grad Norm=4.0209\n",
      "Fold 1, Epoch 4: Train Loss=2.2015, Train Acc=12.12%, Val Loss=2.2384, Val Acc=4.31%, Grad Norm=2.9904\n",
      "Fold 1, Epoch 5: Train Loss=2.1994, Train Acc=12.11%, Val Loss=2.2495, Val Acc=4.17%, Grad Norm=2.4298\n",
      "Fold 1, Epoch 6: Train Loss=2.1980, Train Acc=12.56%, Val Loss=2.2393, Val Acc=5.43%, Grad Norm=2.0526\n",
      "Fold 1, Epoch 7: Train Loss=2.1967, Train Acc=12.37%, Val Loss=2.2273, Val Acc=10.44%, Grad Norm=1.7053\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc (Test=10km/h) = 11.10%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2193, Train Acc=11.41%, Val Loss=2.2880, Val Acc=10.01%, Grad Norm=6.2372\n",
      "Fold 2, Epoch 2: Train Loss=2.2136, Train Acc=11.33%, Val Loss=2.3222, Val Acc=7.97%, Grad Norm=5.3094\n",
      "Fold 2, Epoch 3: Train Loss=2.2085, Train Acc=11.39%, Val Loss=2.2461, Val Acc=7.00%, Grad Norm=4.0376\n",
      "Fold 2, Epoch 4: Train Loss=2.2035, Train Acc=11.60%, Val Loss=2.2686, Val Acc=7.47%, Grad Norm=3.0119\n",
      "Fold 2, Epoch 5: Train Loss=2.2011, Train Acc=11.78%, Val Loss=2.2328, Val Acc=9.63%, Grad Norm=2.4824\n",
      "Fold 2, Epoch 6: Train Loss=2.1997, Train Acc=11.80%, Val Loss=2.2397, Val Acc=9.04%, Grad Norm=2.0334\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc (Test=10km/h) = 11.14%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2199, Train Acc=11.44%, Val Loss=2.2906, Val Acc=6.20%, Grad Norm=6.2495\n",
      "Fold 3, Epoch 2: Train Loss=2.2131, Train Acc=11.69%, Val Loss=2.3015, Val Acc=6.95%, Grad Norm=5.3544\n",
      "Fold 3, Epoch 3: Train Loss=2.2059, Train Acc=11.67%, Val Loss=2.2751, Val Acc=7.53%, Grad Norm=4.1578\n",
      "Fold 3, Epoch 4: Train Loss=2.2017, Train Acc=11.85%, Val Loss=2.2143, Val Acc=11.22%, Grad Norm=3.0463\n",
      "Fold 3, Epoch 5: Train Loss=2.1994, Train Acc=11.86%, Val Loss=2.2100, Val Acc=11.90%, Grad Norm=2.5113\n",
      "Fold 3, Epoch 6: Train Loss=2.1982, Train Acc=12.18%, Val Loss=2.2234, Val Acc=7.33%, Grad Norm=2.1185\n",
      "Fold 3, Epoch 7: Train Loss=2.1972, Train Acc=12.09%, Val Loss=2.2402, Val Acc=5.94%, Grad Norm=1.7506\n",
      "Fold 3, Epoch 8: Train Loss=2.1964, Train Acc=12.32%, Val Loss=2.2201, Val Acc=7.44%, Grad Norm=1.4834\n",
      "Fold 3, Epoch 9: Train Loss=2.1956, Train Acc=12.30%, Val Loss=2.2163, Val Acc=5.74%, Grad Norm=1.2405\n",
      "Fold 3, Epoch 10: Train Loss=2.1949, Train Acc=12.34%, Val Loss=2.2365, Val Acc=7.42%, Grad Norm=1.0526\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc (Test=10km/h) = 11.09%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2191, Train Acc=11.41%, Val Loss=2.2661, Val Acc=7.10%, Grad Norm=6.3265\n",
      "Fold 4, Epoch 2: Train Loss=2.2131, Train Acc=11.64%, Val Loss=2.3331, Val Acc=10.62%, Grad Norm=5.3431\n",
      "Fold 4, Epoch 3: Train Loss=2.2065, Train Acc=11.66%, Val Loss=2.2455, Val Acc=9.41%, Grad Norm=3.8981\n",
      "Fold 4, Epoch 4: Train Loss=2.2021, Train Acc=11.96%, Val Loss=2.2601, Val Acc=8.07%, Grad Norm=2.8748\n",
      "Fold 4, Epoch 5: Train Loss=2.2005, Train Acc=11.97%, Val Loss=2.2036, Val Acc=10.87%, Grad Norm=2.4277\n",
      "Fold 4, Epoch 6: Train Loss=2.1990, Train Acc=11.96%, Val Loss=2.2219, Val Acc=7.08%, Grad Norm=2.0326\n",
      "Fold 4, Epoch 7: Train Loss=2.1984, Train Acc=12.11%, Val Loss=2.2332, Val Acc=6.94%, Grad Norm=1.6942\n",
      "Fold 4, Epoch 8: Train Loss=2.1978, Train Acc=12.12%, Val Loss=2.2251, Val Acc=7.46%, Grad Norm=1.3822\n",
      "Fold 4, Epoch 9: Train Loss=2.1966, Train Acc=12.07%, Val Loss=2.2115, Val Acc=6.95%, Grad Norm=1.2153\n",
      "Fold 4, Epoch 10: Train Loss=2.1956, Train Acc=12.14%, Val Loss=2.2312, Val Acc=7.98%, Grad Norm=1.0377\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc (Test=10km/h) = 11.26%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2188, Train Acc=11.42%, Val Loss=2.2715, Val Acc=7.08%, Grad Norm=6.3197\n",
      "Fold 5, Epoch 2: Train Loss=2.2148, Train Acc=11.44%, Val Loss=2.2534, Val Acc=9.51%, Grad Norm=5.4395\n",
      "Fold 5, Epoch 3: Train Loss=2.2071, Train Acc=11.79%, Val Loss=2.2405, Val Acc=9.82%, Grad Norm=4.1155\n",
      "Fold 5, Epoch 4: Train Loss=2.2031, Train Acc=11.68%, Val Loss=2.2522, Val Acc=7.44%, Grad Norm=3.0434\n",
      "Fold 5, Epoch 5: Train Loss=2.2011, Train Acc=11.86%, Val Loss=2.2044, Val Acc=11.14%, Grad Norm=2.4668\n",
      "Fold 5, Epoch 6: Train Loss=2.1999, Train Acc=11.82%, Val Loss=2.2319, Val Acc=11.93%, Grad Norm=2.0697\n",
      "Fold 5, Epoch 7: Train Loss=2.1987, Train Acc=11.92%, Val Loss=2.2274, Val Acc=9.51%, Grad Norm=1.7424\n",
      "Fold 5, Epoch 8: Train Loss=2.1980, Train Acc=11.98%, Val Loss=2.2090, Val Acc=8.38%, Grad Norm=1.4302\n",
      "Fold 5, Epoch 9: Train Loss=2.1969, Train Acc=12.04%, Val Loss=2.1981, Val Acc=10.94%, Grad Norm=1.1962\n",
      "Fold 5, Epoch 10: Train Loss=2.1960, Train Acc=12.20%, Val Loss=2.1993, Val Acc=12.33%, Grad Norm=1.0174\n",
      "Fold 5, Epoch 11: Train Loss=2.1944, Train Acc=12.40%, Val Loss=2.2150, Val Acc=8.32%, Grad Norm=0.8851\n",
      "Fold 5, Epoch 12: Train Loss=2.1938, Train Acc=12.47%, Val Loss=2.2157, Val Acc=8.16%, Grad Norm=0.9493\n",
      "Fold 5, Epoch 13: Train Loss=2.1932, Train Acc=12.77%, Val Loss=2.2237, Val Acc=7.82%, Grad Norm=0.9588\n",
      "Fold 5, Epoch 14: Train Loss=2.1926, Train Acc=12.77%, Val Loss=2.2080, Val Acc=9.91%, Grad Norm=1.0101\n",
      "Fold 5, Epoch 15: Train Loss=2.1918, Train Acc=13.03%, Val Loss=2.2123, Val Acc=8.29%, Grad Norm=1.0545\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc (Test=10km/h) = 11.12%\n",
      "\n",
      "SNR -25 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-23_22-36-39_LTE-V_XFR_SpeedSplit_FileBlock_Train20kmh_Test10kmh_SNR-25dB_fdTrain655_fdTest655_group288_ResNet1D\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-30 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "[INFO] 共找到 72 个 .mat 文件（含子目录）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 可用 TX 类别数（train/test 交集）: 9\n",
      "[INFO] Train blocks: 360, Test blocks: 360, sample_len: 288, group_size: 288\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2168, Train Acc=11.73%, Val Loss=2.2321, Val Acc=12.34%, Grad Norm=6.3906\n",
      "Fold 1, Epoch 2: Train Loss=2.2130, Train Acc=11.80%, Val Loss=2.2695, Val Acc=11.06%, Grad Norm=5.3767\n",
      "Fold 1, Epoch 3: Train Loss=2.2056, Train Acc=11.98%, Val Loss=2.2741, Val Acc=9.68%, Grad Norm=4.1307\n",
      "Fold 1, Epoch 4: Train Loss=2.2017, Train Acc=12.13%, Val Loss=2.2580, Val Acc=4.95%, Grad Norm=3.0719\n",
      "Fold 1, Epoch 5: Train Loss=2.2001, Train Acc=12.19%, Val Loss=2.2895, Val Acc=4.48%, Grad Norm=2.4946\n",
      "Fold 1, Epoch 6: Train Loss=2.1984, Train Acc=12.30%, Val Loss=2.2604, Val Acc=8.09%, Grad Norm=2.0922\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc (Test=10km/h) = 11.17%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2183, Train Acc=11.48%, Val Loss=2.2626, Val Acc=13.99%, Grad Norm=6.3555\n",
      "Fold 2, Epoch 2: Train Loss=2.2137, Train Acc=11.33%, Val Loss=2.2235, Val Acc=10.13%, Grad Norm=5.3487\n",
      "Fold 2, Epoch 3: Train Loss=2.2080, Train Acc=11.48%, Val Loss=2.2615, Val Acc=9.02%, Grad Norm=3.9640\n",
      "Fold 2, Epoch 4: Train Loss=2.2038, Train Acc=11.57%, Val Loss=2.2576, Val Acc=7.06%, Grad Norm=2.9690\n",
      "Fold 2, Epoch 5: Train Loss=2.2014, Train Acc=11.56%, Val Loss=2.2519, Val Acc=7.37%, Grad Norm=2.4121\n",
      "Fold 2, Epoch 6: Train Loss=2.1999, Train Acc=11.81%, Val Loss=2.2644, Val Acc=8.27%, Grad Norm=2.0339\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc (Test=10km/h) = 10.91%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2176, Train Acc=11.55%, Val Loss=2.3160, Val Acc=8.52%, Grad Norm=6.2838\n",
      "Fold 3, Epoch 2: Train Loss=2.2117, Train Acc=11.83%, Val Loss=2.2522, Val Acc=13.94%, Grad Norm=5.2922\n",
      "Fold 3, Epoch 3: Train Loss=2.2052, Train Acc=11.86%, Val Loss=2.3032, Val Acc=6.96%, Grad Norm=4.0053\n",
      "Fold 3, Epoch 4: Train Loss=2.2016, Train Acc=11.98%, Val Loss=2.1945, Val Acc=17.26%, Grad Norm=2.9538\n",
      "Fold 3, Epoch 5: Train Loss=2.1994, Train Acc=12.11%, Val Loss=2.2271, Val Acc=8.24%, Grad Norm=2.4175\n",
      "Fold 3, Epoch 6: Train Loss=2.1987, Train Acc=12.17%, Val Loss=2.2784, Val Acc=5.55%, Grad Norm=2.0340\n",
      "Fold 3, Epoch 7: Train Loss=2.1976, Train Acc=11.98%, Val Loss=2.2190, Val Acc=8.32%, Grad Norm=1.7023\n",
      "Fold 3, Epoch 8: Train Loss=2.1964, Train Acc=12.26%, Val Loss=2.2499, Val Acc=7.64%, Grad Norm=1.4431\n",
      "Fold 3, Epoch 9: Train Loss=2.1955, Train Acc=12.35%, Val Loss=2.2380, Val Acc=7.34%, Grad Norm=1.2166\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc (Test=10km/h) = 11.14%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2186, Train Acc=11.62%, Val Loss=2.2403, Val Acc=7.22%, Grad Norm=6.3586\n",
      "Fold 4, Epoch 2: Train Loss=2.2140, Train Acc=11.42%, Val Loss=2.2457, Val Acc=11.08%, Grad Norm=5.3972\n",
      "Fold 4, Epoch 3: Train Loss=2.2077, Train Acc=11.64%, Val Loss=2.2556, Val Acc=11.41%, Grad Norm=3.9813\n",
      "Fold 4, Epoch 4: Train Loss=2.2030, Train Acc=11.90%, Val Loss=2.2677, Val Acc=10.99%, Grad Norm=2.9583\n",
      "Fold 4, Epoch 5: Train Loss=2.2006, Train Acc=11.91%, Val Loss=2.2309, Val Acc=7.98%, Grad Norm=2.5011\n",
      "Fold 4, Epoch 6: Train Loss=2.1995, Train Acc=12.06%, Val Loss=2.2311, Val Acc=8.83%, Grad Norm=2.0657\n",
      "Fold 4, Epoch 7: Train Loss=2.1984, Train Acc=12.13%, Val Loss=2.2095, Val Acc=8.53%, Grad Norm=1.7355\n",
      "Fold 4, Epoch 8: Train Loss=2.1977, Train Acc=12.20%, Val Loss=2.1962, Val Acc=11.10%, Grad Norm=1.3881\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc (Test=10km/h) = 11.22%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2190, Train Acc=11.55%, Val Loss=2.2967, Val Acc=8.33%, Grad Norm=6.3300\n",
      "Fold 5, Epoch 2: Train Loss=2.2142, Train Acc=11.52%, Val Loss=2.2760, Val Acc=8.50%, Grad Norm=5.3860\n",
      "Fold 5, Epoch 3: Train Loss=2.2073, Train Acc=11.57%, Val Loss=2.2545, Val Acc=7.72%, Grad Norm=4.2463\n",
      "Fold 5, Epoch 4: Train Loss=2.2032, Train Acc=11.72%, Val Loss=2.2151, Val Acc=7.04%, Grad Norm=3.1632\n",
      "Fold 5, Epoch 5: Train Loss=2.2010, Train Acc=11.63%, Val Loss=2.2275, Val Acc=9.82%, Grad Norm=2.4969\n",
      "Fold 5, Epoch 6: Train Loss=2.1998, Train Acc=11.83%, Val Loss=2.2317, Val Acc=11.06%, Grad Norm=2.1092\n",
      "Fold 5, Epoch 7: Train Loss=2.1990, Train Acc=11.95%, Val Loss=2.2283, Val Acc=6.94%, Grad Norm=1.7566\n",
      "Fold 5, Epoch 8: Train Loss=2.1979, Train Acc=11.93%, Val Loss=2.2063, Val Acc=10.38%, Grad Norm=1.4398\n",
      "Fold 5, Epoch 9: Train Loss=2.1972, Train Acc=11.93%, Val Loss=2.2543, Val Acc=8.07%, Grad Norm=1.1719\n",
      "Fold 5, Epoch 10: Train Loss=2.1962, Train Acc=11.97%, Val Loss=2.2208, Val Acc=8.13%, Grad Norm=0.9638\n",
      "Fold 5, Epoch 11: Train Loss=2.1948, Train Acc=12.36%, Val Loss=2.2268, Val Acc=7.32%, Grad Norm=0.8933\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc (Test=10km/h) = 11.11%\n",
      "\n",
      "SNR -30 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-23_22-36-39_LTE-V_XFR_SpeedSplit_FileBlock_Train20kmh_Test10kmh_SNR-30dB_fdTrain655_fdTest655_group288_ResNet1D\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-35 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "[INFO] 共找到 72 个 .mat 文件（含子目录）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 可用 TX 类别数（train/test 交集）: 9\n",
      "[INFO] Train blocks: 360, Test blocks: 360, sample_len: 288, group_size: 288\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2187, Train Acc=11.77%, Val Loss=2.2856, Val Acc=9.73%, Grad Norm=6.3156\n",
      "Fold 1, Epoch 2: Train Loss=2.2121, Train Acc=11.98%, Val Loss=2.2202, Val Acc=13.79%, Grad Norm=5.2855\n",
      "Fold 1, Epoch 3: Train Loss=2.2064, Train Acc=11.86%, Val Loss=2.2536, Val Acc=4.50%, Grad Norm=3.9315\n",
      "Fold 1, Epoch 4: Train Loss=2.2009, Train Acc=12.26%, Val Loss=2.2613, Val Acc=5.09%, Grad Norm=2.9303\n",
      "Fold 1, Epoch 5: Train Loss=2.2000, Train Acc=12.33%, Val Loss=2.2209, Val Acc=9.72%, Grad Norm=2.4069\n",
      "Fold 1, Epoch 6: Train Loss=2.1980, Train Acc=12.33%, Val Loss=2.2706, Val Acc=5.52%, Grad Norm=2.0113\n",
      "Fold 1, Epoch 7: Train Loss=2.1977, Train Acc=12.27%, Val Loss=2.2514, Val Acc=5.56%, Grad Norm=1.6394\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc (Test=10km/h) = 11.11%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2203, Train Acc=11.33%, Val Loss=2.2491, Val Acc=10.95%, Grad Norm=6.3118\n",
      "Fold 2, Epoch 2: Train Loss=2.2136, Train Acc=11.54%, Val Loss=2.2645, Val Acc=12.52%, Grad Norm=5.4154\n",
      "Fold 2, Epoch 3: Train Loss=2.2088, Train Acc=11.33%, Val Loss=2.2380, Val Acc=8.56%, Grad Norm=4.1269\n",
      "Fold 2, Epoch 4: Train Loss=2.2040, Train Acc=11.51%, Val Loss=2.2317, Val Acc=10.47%, Grad Norm=3.0290\n",
      "Fold 2, Epoch 5: Train Loss=2.2012, Train Acc=11.79%, Val Loss=2.2406, Val Acc=11.14%, Grad Norm=2.4340\n",
      "Fold 2, Epoch 6: Train Loss=2.1997, Train Acc=11.69%, Val Loss=2.2231, Val Acc=10.90%, Grad Norm=2.0382\n",
      "Fold 2, Epoch 7: Train Loss=2.1987, Train Acc=11.87%, Val Loss=2.2212, Val Acc=9.25%, Grad Norm=1.7155\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc (Test=10km/h) = 11.13%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2177, Train Acc=11.56%, Val Loss=2.2280, Val Acc=11.48%, Grad Norm=6.3651\n",
      "Fold 3, Epoch 2: Train Loss=2.2128, Train Acc=11.72%, Val Loss=2.2529, Val Acc=12.25%, Grad Norm=5.4450\n",
      "Fold 3, Epoch 3: Train Loss=2.2072, Train Acc=11.99%, Val Loss=2.2347, Val Acc=8.51%, Grad Norm=4.1591\n",
      "Fold 3, Epoch 4: Train Loss=2.2022, Train Acc=11.87%, Val Loss=2.2112, Val Acc=9.82%, Grad Norm=3.1853\n",
      "Fold 3, Epoch 5: Train Loss=2.2001, Train Acc=11.99%, Val Loss=2.2177, Val Acc=5.83%, Grad Norm=2.5877\n",
      "Fold 3, Epoch 6: Train Loss=2.1985, Train Acc=12.09%, Val Loss=2.1943, Val Acc=13.88%, Grad Norm=2.1622\n",
      "Fold 3, Epoch 7: Train Loss=2.1973, Train Acc=12.08%, Val Loss=2.2366, Val Acc=8.41%, Grad Norm=1.8091\n",
      "Fold 3, Epoch 8: Train Loss=2.1967, Train Acc=12.24%, Val Loss=2.2472, Val Acc=7.13%, Grad Norm=1.4939\n",
      "Fold 3, Epoch 9: Train Loss=2.1957, Train Acc=12.22%, Val Loss=2.2229, Val Acc=6.11%, Grad Norm=1.2581\n",
      "Fold 3, Epoch 10: Train Loss=2.1951, Train Acc=12.30%, Val Loss=2.2180, Val Acc=8.58%, Grad Norm=1.0726\n",
      "Fold 3, Epoch 11: Train Loss=2.1933, Train Acc=12.53%, Val Loss=2.2199, Val Acc=6.47%, Grad Norm=0.9164\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc (Test=10km/h) = 11.19%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2198, Train Acc=11.58%, Val Loss=2.2344, Val Acc=14.33%, Grad Norm=6.3778\n",
      "Fold 4, Epoch 2: Train Loss=2.2151, Train Acc=11.35%, Val Loss=2.2633, Val Acc=7.30%, Grad Norm=5.3881\n",
      "Fold 4, Epoch 3: Train Loss=2.2075, Train Acc=11.70%, Val Loss=2.2812, Val Acc=7.01%, Grad Norm=3.9917\n",
      "Fold 4, Epoch 4: Train Loss=2.2024, Train Acc=11.87%, Val Loss=2.2923, Val Acc=6.95%, Grad Norm=2.9864\n",
      "Fold 4, Epoch 5: Train Loss=2.2012, Train Acc=11.75%, Val Loss=2.2161, Val Acc=11.35%, Grad Norm=2.4824\n",
      "Fold 4, Epoch 6: Train Loss=2.1993, Train Acc=12.02%, Val Loss=2.2484, Val Acc=7.07%, Grad Norm=2.0722\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc (Test=10km/h) = 11.09%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2184, Train Acc=11.54%, Val Loss=2.3027, Val Acc=7.97%, Grad Norm=6.3308\n",
      "Fold 5, Epoch 2: Train Loss=2.2142, Train Acc=11.45%, Val Loss=2.2395, Val Acc=10.90%, Grad Norm=5.1693\n",
      "Fold 5, Epoch 3: Train Loss=2.2063, Train Acc=11.65%, Val Loss=2.2352, Val Acc=7.55%, Grad Norm=3.7121\n",
      "Fold 5, Epoch 4: Train Loss=2.2018, Train Acc=11.82%, Val Loss=2.2669, Val Acc=10.01%, Grad Norm=2.8329\n",
      "Fold 5, Epoch 5: Train Loss=2.2013, Train Acc=11.76%, Val Loss=2.2246, Val Acc=8.86%, Grad Norm=2.3572\n",
      "Fold 5, Epoch 6: Train Loss=2.2000, Train Acc=11.63%, Val Loss=2.2241, Val Acc=7.01%, Grad Norm=1.9533\n",
      "Fold 5, Epoch 7: Train Loss=2.1984, Train Acc=11.94%, Val Loss=2.2660, Val Acc=7.41%, Grad Norm=1.6622\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc (Test=10km/h) = 11.10%\n",
      "\n",
      "SNR -35 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-23_22-36-39_LTE-V_XFR_SpeedSplit_FileBlock_Train20kmh_Test10kmh_SNR-35dB_fdTrain655_fdTest655_group288_ResNet1D\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-40 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "[INFO] 共找到 72 个 .mat 文件（含子目录）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 可用 TX 类别数（train/test 交集）: 9\n",
      "[INFO] Train blocks: 360, Test blocks: 360, sample_len: 288, group_size: 288\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2184, Train Acc=11.78%, Val Loss=2.2790, Val Acc=6.87%, Grad Norm=6.3169\n",
      "Fold 1, Epoch 2: Train Loss=2.2130, Train Acc=11.73%, Val Loss=2.2414, Val Acc=12.50%, Grad Norm=5.2759\n",
      "Fold 1, Epoch 3: Train Loss=2.2053, Train Acc=12.03%, Val Loss=2.3019, Val Acc=8.54%, Grad Norm=3.9407\n",
      "Fold 1, Epoch 4: Train Loss=2.2016, Train Acc=12.00%, Val Loss=2.2922, Val Acc=11.14%, Grad Norm=2.9865\n",
      "Fold 1, Epoch 5: Train Loss=2.1998, Train Acc=12.14%, Val Loss=2.1953, Val Acc=12.54%, Grad Norm=2.4191\n",
      "Fold 1, Epoch 6: Train Loss=2.1985, Train Acc=12.37%, Val Loss=2.2245, Val Acc=12.42%, Grad Norm=2.0568\n",
      "Fold 1, Epoch 7: Train Loss=2.1977, Train Acc=12.34%, Val Loss=2.2506, Val Acc=10.44%, Grad Norm=1.7232\n",
      "Fold 1, Epoch 8: Train Loss=2.1962, Train Acc=12.51%, Val Loss=2.2195, Val Acc=11.14%, Grad Norm=1.4555\n",
      "Fold 1, Epoch 9: Train Loss=2.1952, Train Acc=12.61%, Val Loss=2.2079, Val Acc=11.17%, Grad Norm=1.1928\n",
      "Fold 1, Epoch 10: Train Loss=2.1951, Train Acc=12.47%, Val Loss=2.2215, Val Acc=7.90%, Grad Norm=0.9957\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc (Test=10km/h) = 11.11%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2197, Train Acc=11.27%, Val Loss=2.2606, Val Acc=11.11%, Grad Norm=6.4073\n",
      "Fold 2, Epoch 2: Train Loss=2.2146, Train Acc=11.41%, Val Loss=2.2592, Val Acc=9.73%, Grad Norm=5.4447\n",
      "Fold 2, Epoch 3: Train Loss=2.2091, Train Acc=11.44%, Val Loss=2.3351, Val Acc=8.78%, Grad Norm=4.1798\n",
      "Fold 2, Epoch 4: Train Loss=2.2034, Train Acc=11.56%, Val Loss=2.2781, Val Acc=8.32%, Grad Norm=3.1005\n",
      "Fold 2, Epoch 5: Train Loss=2.2014, Train Acc=11.70%, Val Loss=2.2239, Val Acc=9.10%, Grad Norm=2.5127\n",
      "Fold 2, Epoch 6: Train Loss=2.1994, Train Acc=11.83%, Val Loss=2.2226, Val Acc=10.98%, Grad Norm=2.1151\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc (Test=10km/h) = 11.11%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2186, Train Acc=11.58%, Val Loss=2.2183, Val Acc=12.32%, Grad Norm=6.3486\n",
      "Fold 3, Epoch 2: Train Loss=2.2125, Train Acc=11.70%, Val Loss=2.2261, Val Acc=9.98%, Grad Norm=5.4026\n",
      "Fold 3, Epoch 3: Train Loss=2.2064, Train Acc=12.04%, Val Loss=2.3220, Val Acc=6.93%, Grad Norm=4.1022\n",
      "Fold 3, Epoch 4: Train Loss=2.2023, Train Acc=11.76%, Val Loss=2.3566, Val Acc=5.99%, Grad Norm=3.0029\n",
      "Fold 3, Epoch 5: Train Loss=2.1998, Train Acc=11.90%, Val Loss=2.2740, Val Acc=5.68%, Grad Norm=2.4722\n",
      "Fold 3, Epoch 6: Train Loss=2.1985, Train Acc=12.24%, Val Loss=2.2544, Val Acc=5.86%, Grad Norm=2.1020\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc (Test=10km/h) = 11.11%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2203, Train Acc=11.55%, Val Loss=2.2429, Val Acc=11.76%, Grad Norm=6.3115\n",
      "Fold 4, Epoch 2: Train Loss=2.2137, Train Acc=11.63%, Val Loss=2.2406, Val Acc=12.23%, Grad Norm=5.2825\n",
      "Fold 4, Epoch 3: Train Loss=2.2076, Train Acc=11.70%, Val Loss=2.2803, Val Acc=7.02%, Grad Norm=4.0013\n",
      "Fold 4, Epoch 4: Train Loss=2.2025, Train Acc=11.85%, Val Loss=2.2893, Val Acc=6.94%, Grad Norm=3.0554\n",
      "Fold 4, Epoch 5: Train Loss=2.2008, Train Acc=11.93%, Val Loss=2.2640, Val Acc=6.94%, Grad Norm=2.4909\n",
      "Fold 4, Epoch 6: Train Loss=2.1996, Train Acc=11.94%, Val Loss=2.2596, Val Acc=7.00%, Grad Norm=2.0697\n",
      "Fold 4, Epoch 7: Train Loss=2.1982, Train Acc=12.06%, Val Loss=2.2628, Val Acc=6.96%, Grad Norm=1.7313\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc (Test=10km/h) = 10.91%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2181, Train Acc=11.56%, Val Loss=2.2902, Val Acc=6.96%, Grad Norm=6.3374\n",
      "Fold 5, Epoch 2: Train Loss=2.2132, Train Acc=11.65%, Val Loss=2.2252, Val Acc=11.63%, Grad Norm=5.3333\n",
      "Fold 5, Epoch 3: Train Loss=2.2076, Train Acc=11.45%, Val Loss=2.2177, Val Acc=12.31%, Grad Norm=3.9592\n",
      "Fold 5, Epoch 4: Train Loss=2.2029, Train Acc=11.84%, Val Loss=2.1930, Val Acc=11.49%, Grad Norm=2.9680\n",
      "Fold 5, Epoch 5: Train Loss=2.2013, Train Acc=11.66%, Val Loss=2.2720, Val Acc=11.11%, Grad Norm=2.4161\n",
      "Fold 5, Epoch 6: Train Loss=2.2003, Train Acc=11.78%, Val Loss=2.2581, Val Acc=10.36%, Grad Norm=2.0289\n",
      "Fold 5, Epoch 7: Train Loss=2.1986, Train Acc=11.97%, Val Loss=2.2360, Val Acc=9.03%, Grad Norm=1.6954\n",
      "Fold 5, Epoch 8: Train Loss=2.1980, Train Acc=11.86%, Val Loss=2.2637, Val Acc=8.31%, Grad Norm=1.4050\n",
      "早停：连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc (Test=10km/h) = 11.08%\n",
      "\n",
      "SNR -40 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-23_22-36-39_LTE-V_XFR_SpeedSplit_FileBlock_Train20kmh_Test10kmh_SNR-40dB_fdTrain655_fdTest655_group288_ResNet1D\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SNR vs 测试准确率曲线已保存到 d:\\Program\\MW-RFF\\MW-RFF\\training_results\\SNR_vs_accuracy_2026-01-23_22-36-39.png\n"
     ]
    }
   ],
   "source": [
    "# ResNet 1D 自动 SNR 循环训练脚本（按 block 整体划分训练/测试）\n",
    "# 改动要点（本版本）：\n",
    "# 1) 训练集固定使用 20 km/h，测试集固定使用 10 km/h（不再 train_test_split）\n",
    "# 2) block 生成方式改为：同一 TX、同一速度、同一文件内顺序抽取补满 block\n",
    "#    - 一个 block 内的帧全部来自同一个文件\n",
    "#    - 每个文件不足一个 block 的剩余帧全部舍弃\n",
    "# 3) 训练/测试各自独立按上述规则生成 blocks；训练集内部仍做 KFold 产生 val\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "# ================= 参数设置 =================\n",
    "data_path = \"E:/rf_datasets/\"  # 数据根目录（可以包含子目录）\n",
    "fs = 5e6\n",
    "fc = 5.9e9\n",
    "\n",
    "# 固定速度划分：训练=20km/h，测试=10km/h\n",
    "TRAIN_SPEED_KMH = 20\n",
    "TEST_SPEED_KMH = 10\n",
    "DOPPLER_SPEED_KMH = 120\n",
    "\n",
    "\n",
    "apply_doppler = True   # 若你的原始数据已经包含真实移动造成的多普勒，建议设为 False\n",
    "apply_awgn = True\n",
    "\n",
    "# 模型超参数\n",
    "batch_size = 64\n",
    "num_epochs = 300\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-3\n",
    "in_planes = 64\n",
    "dropout = 0.5\n",
    "patience = 5\n",
    "n_splits = 5\n",
    "\n",
    "# block 参数：一个 block 内抽满 group_size 帧（来自同一TX、同一速度、同一文件顺序抽）\n",
    "GROUP_SIZE = 288\n",
    "\n",
    "# ================= 多普勒和AWGN处理函数 =================\n",
    "def compute_doppler_shift(v_kmh, fc_hz):\n",
    "    c = 3e8\n",
    "    v_ms = v_kmh / 3.6\n",
    "    return (v_ms / c) * fc_hz\n",
    "\n",
    "def apply_doppler_shift(signal, fd, fs):\n",
    "    t = np.arange(signal.shape[-1]) / fs\n",
    "    doppler_phase = np.exp(1j * 2 * np.pi * fd * t)\n",
    "    return signal * doppler_phase\n",
    "\n",
    "def add_awgn(signal, snr_db):\n",
    "    signal_power = np.mean(np.abs(signal) ** 2)\n",
    "    noise_power = signal_power / (10 ** (snr_db / 10))\n",
    "    noise_real = np.random.randn(*signal.shape)\n",
    "    noise_imag = np.random.randn(*signal.shape)\n",
    "    noise = np.sqrt(noise_power / 2) * (noise_real + 1j * noise_imag)\n",
    "    return signal + noise\n",
    "\n",
    "# ================= 速度解析（尽量鲁棒） =================\n",
    "def infer_speed_kmh(file_path, rfDataset=None):\n",
    "    \"\"\"\n",
    "    优先从 mat/h5 内字段读取；否则从文件名/路径推断。\n",
    "    若你的数据速度字段名不同，可在 keys_to_try 里增补。\n",
    "    \"\"\"\n",
    "    # 1) 尝试从文件内部读取\n",
    "    if rfDataset is not None:\n",
    "        keys_to_try = [\"velocity\", \"speed\", \"veh_speed\", \"VehSpeed\", \"v\", \"V\", \"Vel\"]\n",
    "        for k in keys_to_try:\n",
    "            if k in rfDataset:\n",
    "                try:\n",
    "                    val = np.array(rfDataset[k][:]).squeeze()\n",
    "                    val = float(val)\n",
    "                    # 可能是 m/s，也可能是 km/h；优先匹配 10/20\n",
    "                    cand = [val, val * 3.6]\n",
    "                    cand_int = [int(round(x)) for x in cand]\n",
    "                    if TRAIN_SPEED_KMH in cand_int or TEST_SPEED_KMH in cand_int:\n",
    "                        return TRAIN_SPEED_KMH if TRAIN_SPEED_KMH in cand_int else TEST_SPEED_KMH\n",
    "                    # 否则返回更合理的（> 30 认为是 km/h）\n",
    "                    if val > 30:\n",
    "                        return int(round(val))\n",
    "                    return int(round(val * 3.6))\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    # 2) 从路径/文件名推断\n",
    "    s = (file_path or \"\").replace(\"\\\\\", \"/\")\n",
    "    patterns = [\n",
    "        r\"(\\d+(?:\\.\\d+)?)\\s*(?:km\\/h|kmh|kmph)\",\n",
    "        r\"(?:speed|vel|velocity|v)\\D{0,3}(\\d+(?:\\.\\d+)?)\",\n",
    "        r\"(\\d+(?:\\.\\d+)?)\\s*km\\b\",\n",
    "    ]\n",
    "    hits = []\n",
    "    for p in patterns:\n",
    "        for m in re.finditer(p, s, flags=re.IGNORECASE):\n",
    "            try:\n",
    "                hits.append(float(m.group(1)))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    for h in hits:\n",
    "        hi = int(round(h))\n",
    "        if hi in (TRAIN_SPEED_KMH, TEST_SPEED_KMH):\n",
    "            return hi\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"[ERROR] 无法从文件推断速度：{file_path}\\n\"\n",
    "        f\"请在 infer_speed_kmh() 中补充你的速度字段名，或确保路径/文件名包含 10/20km/h 信息。\"\n",
    "    )\n",
    "\n",
    "# ================= 单文件顺序抽样构建 block（每 block 属于单文件） =================\n",
    "def build_blocks_from_single_files(file_samples_list, group_size):\n",
    "    \"\"\"\n",
    "    file_samples_list: list of np.ndarray, each shape (num_samples, sample_len, 2)\n",
    "    返回 blocks: list of np.ndarray, each shape (sample_len, group_size, 2)  # 已做 transpose(XFR)\n",
    "    规则：\n",
    "      - 对每个文件，顺序切块：每块 group_size 帧\n",
    "      - 不足 group_size 的尾部丢弃\n",
    "      - 一个 block 内所有帧来自同一个文件\n",
    "    \"\"\"\n",
    "    if len(file_samples_list) == 0:\n",
    "        return []\n",
    "\n",
    "    # 检查 sample_len 一致（更稳健）\n",
    "    sample_len = file_samples_list[0].shape[1]\n",
    "    for a in file_samples_list:\n",
    "        if a.ndim != 3 or a.shape[1] != sample_len or a.shape[2] != 2:\n",
    "            raise RuntimeError(\"[ERROR] 文件样本维度不一致或通道数不是2，无法构建 block。\")\n",
    "\n",
    "    blocks = []\n",
    "    for arr in file_samples_list:\n",
    "        n = arr.shape[0]\n",
    "        n_full = n // group_size\n",
    "        if n_full <= 0:\n",
    "            continue\n",
    "        for b in range(n_full):\n",
    "            start = b * group_size\n",
    "            end = start + group_size\n",
    "            big_block = arr[start:end]  # (group_size, sample_len, 2)\n",
    "\n",
    "            # XFR 翻转：每条“新样本”对应同一采样点 across frames\n",
    "            big_block = np.transpose(big_block, (1, 0, 2))  # (sample_len, group_size, 2)\n",
    "            blocks.append(big_block)\n",
    "\n",
    "    return blocks\n",
    "\n",
    "# ================= 数据加载（按 TX+速度 分组；train=20, test=10；单文件成块） =================\n",
    "def load_and_preprocess_blocks_by_speed(\n",
    "    data_root,\n",
    "    train_speed_kmh,\n",
    "    test_speed_kmh,\n",
    "    group_size,\n",
    "    apply_doppler=False,\n",
    "    apply_awgn=False,\n",
    "    snr_db=20,\n",
    "    fs=5e6,\n",
    "    fc=5.9e9,\n",
    "):\n",
    "    \"\"\"\n",
    "    返回：\n",
    "      X_train_blocks: (num_train_blocks, sample_len, group_size, 2)\n",
    "      y_train_blocks: (num_train_blocks,)\n",
    "      X_test_blocks : (num_test_blocks,  sample_len, group_size, 2)\n",
    "      y_test_blocks : (num_test_blocks,)\n",
    "      label_to_idx  : dict(tx_id -> class_index)\n",
    "    \"\"\"\n",
    "    mat_files = glob.glob(os.path.join(data_root, \"**\", \"*.mat\"), recursive=True)\n",
    "    if len(mat_files) == 0:\n",
    "        raise RuntimeError(f\"[ERROR] 在 {data_root} 下未找到 .mat 文件\")\n",
    "\n",
    "    print(f\"[INFO] 共找到 {len(mat_files)} 个 .mat 文件（含子目录）\")\n",
    "\n",
    "    # dict: (tx_id, speed_kmh) -> list of file arrays (num_samples, sample_len, 2)\n",
    "    bucket = {}\n",
    "    tx_set_train = set()\n",
    "    tx_set_test = set()\n",
    "\n",
    "    for file in tqdm(mat_files, desc=\"读取数据\"):\n",
    "        with h5py.File(file, \"r\") as f:\n",
    "            if \"rfDataset\" not in f:\n",
    "                continue\n",
    "            rfDataset = f[\"rfDataset\"]\n",
    "\n",
    "            # 速度（只保留 train/test 两个速度）\n",
    "            try:\n",
    "                spd = infer_speed_kmh(file, rfDataset=rfDataset)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if spd not in (train_speed_kmh, test_speed_kmh):\n",
    "                continue\n",
    "\n",
    "            # TX ID\n",
    "            if \"txID\" not in rfDataset:\n",
    "                continue\n",
    "            txID_uint16 = rfDataset[\"txID\"][:].flatten()\n",
    "            tx_id = \"\".join(chr(int(c)) for c in txID_uint16 if int(c) != 0)\n",
    "            if tx_id == \"\":\n",
    "                continue\n",
    "\n",
    "            # DMRS\n",
    "            if \"dmrs\" not in rfDataset:\n",
    "                continue\n",
    "            dmrs_struct = rfDataset[\"dmrs\"][:]\n",
    "            try:\n",
    "                dmrs_complex = dmrs_struct[\"real\"] + 1j * dmrs_struct[\"imag\"]\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # 该速度对应 fd\n",
    "            fd = compute_doppler_shift(DOPPLER_SPEED_KMH, fc)\n",
    "\n",
    "            processed = []\n",
    "            for i in range(dmrs_complex.shape[0]):\n",
    "                sig = dmrs_complex[i, :]\n",
    "\n",
    "                # step1: 功率归一化\n",
    "                sig = sig / (np.sqrt(np.mean(np.abs(sig) ** 2)) + 1e-12)\n",
    "\n",
    "                # step2: Doppler（只改变相位）\n",
    "                if apply_doppler:\n",
    "                    sig = apply_doppler_shift(sig, fd, fs)\n",
    "\n",
    "                # step3: AWGN（严格按 SNR）\n",
    "                if apply_awgn:\n",
    "                    sig = add_awgn(sig, snr_db)\n",
    "\n",
    "                iq = np.stack((sig.real, sig.imag), axis=-1)  # (sample_len, 2)\n",
    "                processed.append(iq)\n",
    "\n",
    "            processed = np.asarray(processed)  # (num_samples, sample_len, 2)\n",
    "            bucket.setdefault((tx_id, spd), []).append(processed)\n",
    "\n",
    "            if spd == train_speed_kmh:\n",
    "                tx_set_train.add(tx_id)\n",
    "            else:\n",
    "                tx_set_test.add(tx_id)\n",
    "\n",
    "    # 只保留 train/test 都存在的 TX（避免测试出现训练未见类）\n",
    "    common_txs = sorted(list(tx_set_train & tx_set_test))\n",
    "    if len(common_txs) == 0:\n",
    "        raise RuntimeError(\"[ERROR] 未找到同时包含训练速度与测试速度的 TX。请检查文件速度解析与数据目录。\")\n",
    "\n",
    "    label_to_idx = {tx: i for i, tx in enumerate(common_txs)}\n",
    "    print(f\"[INFO] 可用 TX 类别数（train/test 交集）: {len(common_txs)}\")\n",
    "\n",
    "    # 为每个 TX 分别构建 train blocks / test blocks（单文件成块）\n",
    "    X_train_list, y_train_list = [], []\n",
    "    X_test_list, y_test_list = [], []\n",
    "\n",
    "    for tx in common_txs:\n",
    "        train_files = bucket.get((tx, train_speed_kmh), [])\n",
    "        test_files = bucket.get((tx, test_speed_kmh), [])\n",
    "\n",
    "        if len(train_files) == 0 or len(test_files) == 0:\n",
    "            continue\n",
    "\n",
    "        # 单文件顺序切 block\n",
    "        train_blocks_tx = build_blocks_from_single_files(train_files, group_size)\n",
    "        test_blocks_tx = build_blocks_from_single_files(test_files, group_size)\n",
    "\n",
    "        if len(train_blocks_tx) == 0 or len(test_blocks_tx) == 0:\n",
    "            print(f\"[WARN] TX={tx} 无法生成足够的 blocks（train_blocks={len(train_blocks_tx)}, test_blocks={len(test_blocks_tx)}），跳过\")\n",
    "            continue\n",
    "\n",
    "        y_idx = label_to_idx[tx]\n",
    "\n",
    "        X_train_list.extend(train_blocks_tx)\n",
    "        y_train_list.extend([y_idx] * len(train_blocks_tx))\n",
    "\n",
    "        X_test_list.extend(test_blocks_tx)\n",
    "        y_test_list.extend([y_idx] * len(test_blocks_tx))\n",
    "\n",
    "    if len(X_train_list) == 0 or len(X_test_list) == 0:\n",
    "        raise RuntimeError(\"[ERROR] 最终未生成 train/test blocks，请检查 group_size 或样本量。\")\n",
    "\n",
    "    X_train_blocks = np.stack(X_train_list, axis=0)  # (n_train_blocks, sample_len, group_size, 2)\n",
    "    y_train_blocks = np.asarray(y_train_list, dtype=np.int64)\n",
    "\n",
    "    X_test_blocks = np.stack(X_test_list, axis=0)    # (n_test_blocks, sample_len, group_size, 2)\n",
    "    y_test_blocks = np.asarray(y_test_list, dtype=np.int64)\n",
    "\n",
    "    print(\n",
    "        f\"[INFO] Train blocks: {X_train_blocks.shape[0]}, \"\n",
    "        f\"Test blocks: {X_test_blocks.shape[0]}, \"\n",
    "        f\"sample_len: {X_train_blocks.shape[1]}, group_size: {X_train_blocks.shape[2]}\"\n",
    "    )\n",
    "    return X_train_blocks, y_train_blocks, X_test_blocks, y_test_blocks, label_to_idx\n",
    "\n",
    "# ================= 1D ResNet18（增加 dropout 和 in_planes） =================\n",
    "class BasicBlock1D(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "class ResNet18_1D(nn.Module):\n",
    "    def __init__(self, num_classes=10, in_planes=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.conv1 = nn.Conv1d(2, in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1, dropout=dropout)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2, dropout=dropout)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2, dropout=dropout)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2, dropout=dropout)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride, dropout):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes)\n",
    "            )\n",
    "        layers = [BasicBlock1D(self.in_planes, planes, stride, downsample, dropout)]\n",
    "        self.in_planes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock1D(self.in_planes, planes, dropout=dropout))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (B, sample_len, 2) -> (B, 2, sample_len)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ================= 辅助函数 =================\n",
    "def compute_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm += (p.grad.data.norm(2).item()) ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def moving_average(x, w=5):\n",
    "    x = np.array(x)\n",
    "    if len(x) == 0:\n",
    "        return np.array([])\n",
    "    w = max(1, min(w, len(x)))\n",
    "    return np.convolve(x, np.ones(w), \"valid\") / w\n",
    "\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    acc = 100 * correct / total if total > 0 else 0.0\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=range(num_classes))\n",
    "    return acc, cm\n",
    "\n",
    "def plot_training_curves(fold_results, save_folder):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i, res in enumerate(fold_results):\n",
    "        plt.plot(moving_average(res[\"train_loss\"]), label=f\"Fold{i+1} Train Loss\")\n",
    "        plt.plot(moving_average(res[\"val_loss\"]), label=f\"Fold{i+1} Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"训练和验证 Loss 曲线\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(save_folder, \"loss_curves.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_grad_norms(avg_grad_norms, save_folder):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(range(1, len(avg_grad_norms) + 1), avg_grad_norms)\n",
    "    plt.xlabel(\"Fold\")\n",
    "    plt.ylabel(\"平均梯度范数\")\n",
    "    plt.title(\"各 Fold 平均梯度范数\")\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(save_folder, \"avg_grad_norms.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(cm, save_path=None):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "    plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel(\"Reference\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# ================= 主训练函数（train=20km/h, test=10km/h） =================\n",
    "def train_for_snr(SNR_dB, save_folder, results_file, group_size=288):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] 使用设备: {device}\")\n",
    "\n",
    "    # 1) 直接按速度生成 train/test blocks（每 block 来自单文件）\n",
    "    X_train_blocks, y_train_blocks, X_test_blocks, y_test_blocks, label_to_idx = load_and_preprocess_blocks_by_speed(\n",
    "        data_root=data_path,\n",
    "        train_speed_kmh=TRAIN_SPEED_KMH,\n",
    "        test_speed_kmh=TEST_SPEED_KMH,\n",
    "        group_size=group_size,\n",
    "        apply_doppler=apply_doppler,\n",
    "        apply_awgn=apply_awgn,\n",
    "        snr_db=SNR_dB,\n",
    "        fs=fs,\n",
    "        fc=fc,\n",
    "    )\n",
    "    num_classes = len(label_to_idx)\n",
    "\n",
    "    # 2) 展开测试 blocks 用于最终评估\n",
    "    X_test = X_test_blocks.reshape(-1, X_test_blocks.shape[2], X_test_blocks.shape[3])\n",
    "    y_test = np.repeat(y_test_blocks, X_test_blocks.shape[1])\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                 torch.tensor(y_test, dtype=torch.long))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 3) KFold 只在训练 blocks 内做（产生 val）\n",
    "    num_train_blocks = X_train_blocks.shape[0]\n",
    "    if num_train_blocks < 2:\n",
    "        raise RuntimeError(f\"[ERROR] 训练 blocks 数太少：{num_train_blocks}，无法做 KFold。\")\n",
    "    effective_splits = min(n_splits, num_train_blocks)\n",
    "    if effective_splits < 2:\n",
    "        effective_splits = 2\n",
    "\n",
    "    kfold = KFold(n_splits=effective_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    fold_test_accs = []\n",
    "\n",
    "    train_block_indices = np.arange(num_train_blocks)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_block_indices)):\n",
    "        print(f\"\\n====== Fold {fold+1}/{effective_splits} ======\")\n",
    "\n",
    "        # 展开训练 blocks\n",
    "        X_train = X_train_blocks[train_idx].reshape(-1, X_train_blocks.shape[2], X_train_blocks.shape[3])\n",
    "        y_train = np.repeat(y_train_blocks[train_idx], X_train_blocks.shape[1])\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                      torch.tensor(y_train, dtype=torch.long))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # 展开验证 blocks\n",
    "        X_val = X_train_blocks[val_idx].reshape(-1, X_train_blocks.shape[2], X_train_blocks.shape[3])\n",
    "        y_val = np.repeat(y_train_blocks[val_idx], X_train_blocks.shape[1])\n",
    "        val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                                    torch.tensor(y_val, dtype=torch.long))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # 模型、损失函数、优化器\n",
    "        model = ResNet18_1D(num_classes=num_classes, in_planes=in_planes, dropout=dropout).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        best_val_acc = -1.0\n",
    "        patience_counter = 0\n",
    "        best_model_wts = None\n",
    "        train_losses, val_losses, grad_norms = [], [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "            batch_grad_norms = []\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                batch_grad_norms.append(compute_grad_norm(model))\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            train_loss = running_loss / max(1, len(train_loader))\n",
    "            train_acc = 100 * correct_train / max(1, total_train)\n",
    "            avg_grad_norm = float(np.mean(batch_grad_norms)) if len(batch_grad_norms) else 0.0\n",
    "            train_losses.append(train_loss)\n",
    "            grad_norms.append(avg_grad_norm)\n",
    "\n",
    "            # 验证\n",
    "            model.eval()\n",
    "            running_val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "            all_val_labels, all_val_preds = [], []\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    loss_val = criterion(val_outputs, val_labels)\n",
    "                    running_val_loss += loss_val.item()\n",
    "                    _, val_pred = torch.max(val_outputs, 1)\n",
    "                    total_val += val_labels.size(0)\n",
    "                    correct_val += (val_pred == val_labels).sum().item()\n",
    "                    all_val_labels.extend(val_labels.cpu().numpy())\n",
    "                    all_val_preds.extend(val_pred.cpu().numpy())\n",
    "\n",
    "            val_loss = running_val_loss / max(1, len(val_loader))\n",
    "            val_acc = 100 * correct_val / max(1, total_val)\n",
    "            val_losses.append(val_loss)\n",
    "            val_cm = confusion_matrix(all_val_labels, all_val_preds, labels=range(num_classes))\n",
    "\n",
    "            log_msg = (\n",
    "                f\"Fold {fold+1}, Epoch {epoch+1}: \"\n",
    "                f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, \"\n",
    "                f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%, Grad Norm={avg_grad_norm:.4f}\"\n",
    "            )\n",
    "            print(log_msg)\n",
    "            with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(log_msg + \"\\n\")\n",
    "\n",
    "            # 早停\n",
    "            if val_acc > best_val_acc + 0.01:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_model_wts = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    msg = f\"早停：连续 {patience} 个 epoch 验证集未提升\"\n",
    "                    print(msg)\n",
    "                    with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                        f.write(msg + \"\\n\")\n",
    "                    break\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        if best_model_wts is None:\n",
    "            best_model_wts = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        model.load_state_dict(best_model_wts, strict=True)\n",
    "        model.to(device)\n",
    "\n",
    "        # 测试（10km/h）\n",
    "        test_acc, test_cm = evaluate_model(model, test_loader, device, num_classes)\n",
    "        fold_test_accs.append(test_acc)\n",
    "\n",
    "        fold_results.append({\n",
    "            \"train_loss\": train_losses,\n",
    "            \"val_loss\": val_losses,\n",
    "            \"grad_norms\": grad_norms,\n",
    "            \"val_cm\": val_cm,\n",
    "            \"test_cm\": test_cm,\n",
    "        })\n",
    "\n",
    "        plot_confusion_matrix(val_cm, save_path=os.path.join(save_folder, f\"confusion_matrix_val_fold{fold+1}.png\"))\n",
    "        plot_confusion_matrix(test_cm, save_path=os.path.join(save_folder, f\"confusion_matrix_test_fold{fold+1}.png\"))\n",
    "        torch.save(best_model_wts, os.path.join(save_folder, f\"best_model_fold{fold+1}.pth\"))\n",
    "\n",
    "        msg = f\"Fold {fold+1} Test Acc (Test={TEST_SPEED_KMH}km/h) = {test_acc:.2f}%\"\n",
    "        print(msg + \"\\n\")\n",
    "        with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "\n",
    "    plot_training_curves(fold_results, save_folder)\n",
    "    plot_grad_norms([float(np.mean(fr[\"grad_norms\"])) for fr in fold_results], save_folder)\n",
    "\n",
    "    return float(np.mean(fold_test_accs)) if len(fold_test_accs) else 0.0\n",
    "\n",
    "# ================= SNR 循环训练 + 绘制 SNR 曲线 =================\n",
    "if __name__ == \"__main__\":\n",
    "    snr_list = list(range(20, -45, -5))\n",
    "    snr_accs = []\n",
    "\n",
    "    global_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    script_name = \"LTE-V_XFR_SpeedSplit_FileBlock\"\n",
    "\n",
    "    fd_train = int(compute_doppler_shift(DOPPLER_SPEED_KMH, fc))\n",
    "    fd_test = int(compute_doppler_shift(DOPPLER_SPEED_KMH, fc))\n",
    "\n",
    "    for snr_db in snr_list:\n",
    "        print(f\"\\n\\n================== 当前实验 SNR={snr_db} dB ==================\\n\")\n",
    "\n",
    "        folder_name = (\n",
    "            f\"{global_timestamp}_{script_name}_\"\n",
    "            f\"Train{TRAIN_SPEED_KMH}kmh_Test{TEST_SPEED_KMH}kmh_\"\n",
    "            f\"SNR{snr_db}dB_fdTrain{fd_train}_fdTest{fd_test}_\"\n",
    "            f\"group{GROUP_SIZE}_ResNet1D\"\n",
    "        )\n",
    "        save_folder = os.path.join(os.getcwd(), \"training_results\", folder_name)\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "        results_file = os.path.join(save_folder, \"results.txt\")\n",
    "        with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\n================ SNR={snr_db} dB ================\\n\")\n",
    "            f.write(f\"Train speed: {TRAIN_SPEED_KMH} km/h, Test speed: {TEST_SPEED_KMH} km/h\\n\")\n",
    "            f.write(f\"apply_doppler={apply_doppler}, apply_awgn={apply_awgn}\\n\")\n",
    "\n",
    "        test_acc = train_for_snr(snr_db, save_folder, results_file, group_size=GROUP_SIZE)\n",
    "        snr_accs.append(test_acc)\n",
    "        print(f\"SNR {snr_db:>3} dB → results in: {save_folder}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(snr_list, snr_accs, marker=\"o\", linestyle=\"-\")\n",
    "    plt.xlabel(\"SNR (dB)\")\n",
    "    plt.ylabel(\"测试集准确率 (%)\")\n",
    "    plt.title(f\"SNR vs 测试集准确率（Train={TRAIN_SPEED_KMH}km/h, Test={TEST_SPEED_KMH}km/h）\")\n",
    "    plt.grid(True)\n",
    "    snr_curve_path = os.path.join(os.getcwd(), \"training_results\", f\"SNR_vs_accuracy_{global_timestamp}.png\")\n",
    "    plt.savefig(snr_curve_path)\n",
    "    plt.show()\n",
    "    print(f\"[INFO] SNR vs 测试准确率曲线已保存到 {snr_curve_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea38696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================== 当前实验 SNR=20 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 787, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 787\n",
      "[INFO] 训练 block 数: 590, 测试 block 数: 197\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=1.7451, Train Acc=34.53%, Val Loss=1.6837, Val Acc=41.55%, Grad Norm=6.1710\n",
      "Fold 1, Epoch 2: Train Loss=1.0896, Train Acc=60.41%, Val Loss=1.4775, Val Acc=49.59%, Grad Norm=4.2222\n",
      "Fold 1, Epoch 3: Train Loss=0.9063, Train Acc=67.70%, Val Loss=1.4764, Val Acc=53.44%, Grad Norm=3.6987\n",
      "Fold 1, Epoch 4: Train Loss=0.8063, Train Acc=71.63%, Val Loss=1.4411, Val Acc=54.57%, Grad Norm=3.6484\n",
      "Fold 1, Epoch 5: Train Loss=0.7352, Train Acc=74.27%, Val Loss=1.4403, Val Acc=55.48%, Grad Norm=3.7239\n",
      "Fold 1, Epoch 6: Train Loss=0.6757, Train Acc=76.39%, Val Loss=1.4196, Val Acc=56.81%, Grad Norm=3.8669\n",
      "Fold 1, Epoch 7: Train Loss=0.6215, Train Acc=78.48%, Val Loss=1.4425, Val Acc=57.76%, Grad Norm=4.0722\n",
      "Fold 1, Epoch 8: Train Loss=0.5777, Train Acc=80.05%, Val Loss=1.4332, Val Acc=57.30%, Grad Norm=4.2244\n",
      "Fold 1, Epoch 9: Train Loss=0.5353, Train Acc=81.59%, Val Loss=1.3378, Val Acc=59.43%, Grad Norm=4.4424\n",
      "Fold 1, Epoch 10: Train Loss=0.4935, Train Acc=83.03%, Val Loss=1.4366, Val Acc=59.58%, Grad Norm=4.6315\n",
      "Fold 1, Epoch 11: Train Loss=0.4161, Train Acc=85.75%, Val Loss=1.4856, Val Acc=60.84%, Grad Norm=4.8671\n",
      "Fold 1, Epoch 12: Train Loss=0.3834, Train Acc=87.02%, Val Loss=1.4784, Val Acc=60.38%, Grad Norm=5.2884\n",
      "Fold 1, Epoch 13: Train Loss=0.3566, Train Acc=87.85%, Val Loss=1.5008, Val Acc=60.37%, Grad Norm=5.5725\n",
      "Fold 1, Epoch 14: Train Loss=0.3323, Train Acc=88.80%, Val Loss=1.5024, Val Acc=61.50%, Grad Norm=5.8161\n",
      "Fold 1, Epoch 15: Train Loss=0.3098, Train Acc=89.49%, Val Loss=1.4770, Val Acc=61.55%, Grad Norm=6.0895\n",
      "Fold 1, Epoch 16: Train Loss=0.2922, Train Acc=90.07%, Val Loss=1.5219, Val Acc=61.13%, Grad Norm=6.3124\n",
      "Fold 1, Epoch 17: Train Loss=0.2727, Train Acc=90.73%, Val Loss=1.5544, Val Acc=60.67%, Grad Norm=6.4964\n",
      "Fold 1, Epoch 18: Train Loss=0.2550, Train Acc=91.36%, Val Loss=1.5953, Val Acc=60.09%, Grad Norm=6.6847\n",
      "Fold 1, Epoch 19: Train Loss=0.2419, Train Acc=91.88%, Val Loss=1.5797, Val Acc=60.98%, Grad Norm=6.8380\n",
      "Fold 1, Epoch 20: Train Loss=0.2256, Train Acc=92.43%, Val Loss=1.6184, Val Acc=60.91%, Grad Norm=6.9342\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=65.21%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=1.7604, Train Acc=34.04%, Val Loss=1.6027, Val Acc=44.17%, Grad Norm=6.1815\n",
      "Fold 2, Epoch 2: Train Loss=1.1643, Train Acc=57.55%, Val Loss=1.3866, Val Acc=54.01%, Grad Norm=4.1820\n",
      "Fold 2, Epoch 3: Train Loss=0.9796, Train Acc=64.72%, Val Loss=1.3299, Val Acc=60.36%, Grad Norm=3.6948\n",
      "Fold 2, Epoch 4: Train Loss=0.8756, Train Acc=68.72%, Val Loss=1.4032, Val Acc=61.71%, Grad Norm=3.6488\n",
      "Fold 2, Epoch 5: Train Loss=0.7998, Train Acc=71.75%, Val Loss=1.2596, Val Acc=61.82%, Grad Norm=3.7291\n",
      "Fold 2, Epoch 6: Train Loss=0.7377, Train Acc=74.02%, Val Loss=1.1680, Val Acc=63.54%, Grad Norm=3.8896\n",
      "Fold 2, Epoch 7: Train Loss=0.6816, Train Acc=76.25%, Val Loss=1.1813, Val Acc=64.71%, Grad Norm=4.1125\n",
      "Fold 2, Epoch 8: Train Loss=0.6278, Train Acc=78.18%, Val Loss=1.2105, Val Acc=64.69%, Grad Norm=4.3473\n",
      "Fold 2, Epoch 9: Train Loss=0.5837, Train Acc=79.77%, Val Loss=1.1264, Val Acc=66.84%, Grad Norm=4.5570\n",
      "Fold 2, Epoch 10: Train Loss=0.5353, Train Acc=81.50%, Val Loss=1.0812, Val Acc=66.67%, Grad Norm=4.7979\n",
      "Fold 2, Epoch 11: Train Loss=0.4532, Train Acc=84.48%, Val Loss=1.1571, Val Acc=66.66%, Grad Norm=5.0799\n",
      "Fold 2, Epoch 12: Train Loss=0.4163, Train Acc=85.72%, Val Loss=1.1925, Val Acc=67.74%, Grad Norm=5.4983\n",
      "Fold 2, Epoch 13: Train Loss=0.3878, Train Acc=86.80%, Val Loss=1.1881, Val Acc=67.03%, Grad Norm=5.7895\n",
      "Fold 2, Epoch 14: Train Loss=0.3614, Train Acc=87.79%, Val Loss=1.1517, Val Acc=66.57%, Grad Norm=6.0945\n",
      "Fold 2, Epoch 15: Train Loss=0.3376, Train Acc=88.50%, Val Loss=1.2605, Val Acc=65.71%, Grad Norm=6.3258\n",
      "Fold 2, Epoch 16: Train Loss=0.3164, Train Acc=89.23%, Val Loss=1.2447, Val Acc=66.24%, Grad Norm=6.5614\n",
      "Fold 2, Epoch 17: Train Loss=0.3001, Train Acc=89.73%, Val Loss=1.1896, Val Acc=67.23%, Grad Norm=6.7710\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=64.11%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=1.7199, Train Acc=35.58%, Val Loss=1.6747, Val Acc=43.44%, Grad Norm=5.9789\n",
      "Fold 3, Epoch 2: Train Loss=1.1012, Train Acc=59.89%, Val Loss=1.4911, Val Acc=51.31%, Grad Norm=4.0388\n",
      "Fold 3, Epoch 3: Train Loss=0.9251, Train Acc=66.86%, Val Loss=1.4651, Val Acc=51.39%, Grad Norm=3.6153\n",
      "Fold 3, Epoch 4: Train Loss=0.8275, Train Acc=70.51%, Val Loss=1.4231, Val Acc=56.06%, Grad Norm=3.5263\n",
      "Fold 3, Epoch 5: Train Loss=0.7539, Train Acc=73.44%, Val Loss=1.3437, Val Acc=57.33%, Grad Norm=3.6649\n",
      "Fold 3, Epoch 6: Train Loss=0.6895, Train Acc=75.72%, Val Loss=1.3799, Val Acc=58.17%, Grad Norm=3.8502\n",
      "Fold 3, Epoch 7: Train Loss=0.6293, Train Acc=77.99%, Val Loss=1.3092, Val Acc=58.77%, Grad Norm=4.1031\n",
      "Fold 3, Epoch 8: Train Loss=0.5768, Train Acc=80.03%, Val Loss=1.3481, Val Acc=59.21%, Grad Norm=4.2979\n",
      "Fold 3, Epoch 9: Train Loss=0.5276, Train Acc=81.75%, Val Loss=1.2796, Val Acc=60.79%, Grad Norm=4.5156\n",
      "Fold 3, Epoch 10: Train Loss=0.4864, Train Acc=83.18%, Val Loss=1.3082, Val Acc=60.08%, Grad Norm=4.7029\n",
      "Fold 3, Epoch 11: Train Loss=0.4071, Train Acc=86.05%, Val Loss=1.3289, Val Acc=61.61%, Grad Norm=4.8936\n",
      "Fold 3, Epoch 12: Train Loss=0.3729, Train Acc=87.20%, Val Loss=1.2788, Val Acc=62.47%, Grad Norm=5.2994\n",
      "Fold 3, Epoch 13: Train Loss=0.3475, Train Acc=88.16%, Val Loss=1.3869, Val Acc=61.52%, Grad Norm=5.5918\n",
      "Fold 3, Epoch 14: Train Loss=0.3256, Train Acc=88.95%, Val Loss=1.4283, Val Acc=61.29%, Grad Norm=5.8248\n",
      "Fold 3, Epoch 15: Train Loss=0.3037, Train Acc=89.77%, Val Loss=1.3568, Val Acc=62.74%, Grad Norm=6.0871\n",
      "Fold 3, Epoch 16: Train Loss=0.2820, Train Acc=90.42%, Val Loss=1.4009, Val Acc=62.00%, Grad Norm=6.2872\n",
      "Fold 3, Epoch 17: Train Loss=0.2652, Train Acc=91.06%, Val Loss=1.4274, Val Acc=62.42%, Grad Norm=6.4776\n",
      "Fold 3, Epoch 18: Train Loss=0.2470, Train Acc=91.72%, Val Loss=1.4168, Val Acc=62.42%, Grad Norm=6.6122\n",
      "Fold 3, Epoch 19: Train Loss=0.2309, Train Acc=92.16%, Val Loss=1.4369, Val Acc=63.00%, Grad Norm=6.7497\n",
      "Fold 3, Epoch 20: Train Loss=0.2164, Train Acc=92.72%, Val Loss=1.4673, Val Acc=62.63%, Grad Norm=6.8573\n",
      "Fold 3, Epoch 21: Train Loss=0.1774, Train Acc=94.19%, Val Loss=1.4667, Val Acc=62.96%, Grad Norm=6.6350\n",
      "Fold 3, Epoch 22: Train Loss=0.1625, Train Acc=94.64%, Val Loss=1.5053, Val Acc=62.49%, Grad Norm=6.7552\n",
      "Fold 3, Epoch 23: Train Loss=0.1503, Train Acc=95.11%, Val Loss=1.5441, Val Acc=62.18%, Grad Norm=6.7826\n",
      "Fold 3, Epoch 24: Train Loss=0.1418, Train Acc=95.40%, Val Loss=1.5490, Val Acc=61.99%, Grad Norm=6.8929\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=66.15%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=1.7688, Train Acc=33.57%, Val Loss=1.4069, Val Acc=48.95%, Grad Norm=5.9592\n",
      "Fold 4, Epoch 2: Train Loss=1.1438, Train Acc=58.38%, Val Loss=1.3131, Val Acc=56.59%, Grad Norm=4.0786\n",
      "Fold 4, Epoch 3: Train Loss=0.9504, Train Acc=66.21%, Val Loss=1.2101, Val Acc=59.16%, Grad Norm=3.7722\n",
      "Fold 4, Epoch 4: Train Loss=0.8419, Train Acc=70.27%, Val Loss=1.2113, Val Acc=61.09%, Grad Norm=3.7204\n",
      "Fold 4, Epoch 5: Train Loss=0.7634, Train Acc=73.35%, Val Loss=1.2516, Val Acc=59.44%, Grad Norm=3.8022\n",
      "Fold 4, Epoch 6: Train Loss=0.7015, Train Acc=75.55%, Val Loss=1.1963, Val Acc=61.27%, Grad Norm=3.9133\n",
      "Fold 4, Epoch 7: Train Loss=0.6503, Train Acc=77.47%, Val Loss=1.1223, Val Acc=62.66%, Grad Norm=4.1036\n",
      "Fold 4, Epoch 8: Train Loss=0.5996, Train Acc=79.44%, Val Loss=1.2223, Val Acc=62.07%, Grad Norm=4.3284\n",
      "Fold 4, Epoch 9: Train Loss=0.5553, Train Acc=80.94%, Val Loss=1.0888, Val Acc=65.01%, Grad Norm=4.5245\n",
      "Fold 4, Epoch 10: Train Loss=0.5160, Train Acc=82.31%, Val Loss=1.1841, Val Acc=63.77%, Grad Norm=4.7028\n",
      "Fold 4, Epoch 11: Train Loss=0.4356, Train Acc=85.13%, Val Loss=1.1417, Val Acc=64.24%, Grad Norm=4.9529\n",
      "Fold 4, Epoch 12: Train Loss=0.3978, Train Acc=86.47%, Val Loss=1.1470, Val Acc=65.55%, Grad Norm=5.3644\n",
      "Fold 4, Epoch 13: Train Loss=0.3695, Train Acc=87.37%, Val Loss=1.2232, Val Acc=64.22%, Grad Norm=5.6997\n",
      "Fold 4, Epoch 14: Train Loss=0.3460, Train Acc=88.26%, Val Loss=1.2392, Val Acc=64.94%, Grad Norm=6.0121\n",
      "Fold 4, Epoch 15: Train Loss=0.3256, Train Acc=88.98%, Val Loss=1.2816, Val Acc=64.37%, Grad Norm=6.2333\n",
      "Fold 4, Epoch 16: Train Loss=0.3035, Train Acc=89.77%, Val Loss=1.2583, Val Acc=63.94%, Grad Norm=6.4070\n",
      "Fold 4, Epoch 17: Train Loss=0.2867, Train Acc=90.40%, Val Loss=1.3072, Val Acc=64.33%, Grad Norm=6.6070\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=63.09%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=1.7528, Train Acc=34.22%, Val Loss=1.7772, Val Acc=40.56%, Grad Norm=6.0279\n",
      "Fold 5, Epoch 2: Train Loss=1.1040, Train Acc=60.17%, Val Loss=1.1934, Val Acc=58.15%, Grad Norm=4.1940\n",
      "Fold 5, Epoch 3: Train Loss=0.9222, Train Acc=67.12%, Val Loss=1.1625, Val Acc=59.26%, Grad Norm=3.7361\n",
      "Fold 5, Epoch 4: Train Loss=0.8184, Train Acc=71.01%, Val Loss=1.1115, Val Acc=61.70%, Grad Norm=3.6867\n",
      "Fold 5, Epoch 5: Train Loss=0.7423, Train Acc=73.99%, Val Loss=1.1513, Val Acc=60.62%, Grad Norm=3.7477\n",
      "Fold 5, Epoch 6: Train Loss=0.6825, Train Acc=76.17%, Val Loss=1.2109, Val Acc=61.13%, Grad Norm=3.8839\n",
      "Fold 5, Epoch 7: Train Loss=0.6332, Train Acc=77.95%, Val Loss=1.2537, Val Acc=60.95%, Grad Norm=4.0580\n",
      "Fold 5, Epoch 8: Train Loss=0.5828, Train Acc=79.71%, Val Loss=1.1791, Val Acc=63.56%, Grad Norm=4.2535\n",
      "Fold 5, Epoch 9: Train Loss=0.5393, Train Acc=81.44%, Val Loss=1.2063, Val Acc=63.80%, Grad Norm=4.4698\n",
      "Fold 5, Epoch 10: Train Loss=0.4974, Train Acc=82.81%, Val Loss=1.1768, Val Acc=64.55%, Grad Norm=4.6577\n",
      "Fold 5, Epoch 11: Train Loss=0.4222, Train Acc=85.51%, Val Loss=1.2297, Val Acc=64.64%, Grad Norm=4.9309\n",
      "Fold 5, Epoch 12: Train Loss=0.3849, Train Acc=86.91%, Val Loss=1.2238, Val Acc=64.69%, Grad Norm=5.2998\n",
      "Fold 5, Epoch 13: Train Loss=0.3596, Train Acc=87.81%, Val Loss=1.2939, Val Acc=64.51%, Grad Norm=5.6496\n",
      "Fold 5, Epoch 14: Train Loss=0.3327, Train Acc=88.67%, Val Loss=1.3265, Val Acc=64.46%, Grad Norm=5.9009\n",
      "Fold 5, Epoch 15: Train Loss=0.3140, Train Acc=89.27%, Val Loss=1.2721, Val Acc=65.24%, Grad Norm=6.1368\n",
      "Fold 5, Epoch 16: Train Loss=0.2926, Train Acc=90.14%, Val Loss=1.3529, Val Acc=64.58%, Grad Norm=6.3399\n",
      "Fold 5, Epoch 17: Train Loss=0.2728, Train Acc=90.81%, Val Loss=1.3018, Val Acc=65.37%, Grad Norm=6.5416\n",
      "Fold 5, Epoch 18: Train Loss=0.2551, Train Acc=91.34%, Val Loss=1.3593, Val Acc=64.38%, Grad Norm=6.6690\n",
      "Fold 5, Epoch 19: Train Loss=0.2418, Train Acc=91.84%, Val Loss=1.3535, Val Acc=65.18%, Grad Norm=6.8190\n",
      "Fold 5, Epoch 20: Train Loss=0.2265, Train Acc=92.44%, Val Loss=1.3904, Val Acc=63.97%, Grad Norm=6.9415\n",
      "Fold 5, Epoch 21: Train Loss=0.1842, Train Acc=93.89%, Val Loss=1.4204, Val Acc=64.35%, Grad Norm=6.7049\n",
      "Fold 5, Epoch 22: Train Loss=0.1686, Train Acc=94.50%, Val Loss=1.4456, Val Acc=64.80%, Grad Norm=6.8395\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=65.57%\n",
      "\n",
      "SNR  20 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-25_10-47-30_LTE-V_XFR_FileBlock_SNR20dB_fd655_group864_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=15 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:11<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 787, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 787\n",
      "[INFO] 训练 block 数: 590, 测试 block 数: 197\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=1.7541, Train Acc=34.23%, Val Loss=1.6305, Val Acc=42.32%, Grad Norm=6.2703\n",
      "Fold 1, Epoch 2: Train Loss=1.1187, Train Acc=59.54%, Val Loss=1.4773, Val Acc=49.50%, Grad Norm=4.1017\n",
      "Fold 1, Epoch 3: Train Loss=0.9278, Train Acc=66.80%, Val Loss=1.4500, Val Acc=53.65%, Grad Norm=3.6662\n",
      "Fold 1, Epoch 4: Train Loss=0.8270, Train Acc=70.75%, Val Loss=1.3933, Val Acc=54.72%, Grad Norm=3.5927\n",
      "Fold 1, Epoch 5: Train Loss=0.7500, Train Acc=73.69%, Val Loss=1.4854, Val Acc=56.04%, Grad Norm=3.6768\n",
      "Fold 1, Epoch 6: Train Loss=0.6946, Train Acc=75.78%, Val Loss=1.3222, Val Acc=57.31%, Grad Norm=3.7710\n",
      "Fold 1, Epoch 7: Train Loss=0.6453, Train Acc=77.49%, Val Loss=1.4243, Val Acc=57.71%, Grad Norm=3.9244\n",
      "Fold 1, Epoch 8: Train Loss=0.6025, Train Acc=79.05%, Val Loss=1.4356, Val Acc=58.33%, Grad Norm=4.1273\n",
      "Fold 1, Epoch 9: Train Loss=0.5585, Train Acc=80.75%, Val Loss=1.4770, Val Acc=59.26%, Grad Norm=4.3057\n",
      "Fold 1, Epoch 10: Train Loss=0.5228, Train Acc=81.91%, Val Loss=1.3845, Val Acc=60.00%, Grad Norm=4.5557\n",
      "Fold 1, Epoch 11: Train Loss=0.4450, Train Acc=84.84%, Val Loss=1.5335, Val Acc=59.91%, Grad Norm=4.8329\n",
      "Fold 1, Epoch 12: Train Loss=0.4103, Train Acc=85.99%, Val Loss=1.5293, Val Acc=61.29%, Grad Norm=5.2173\n",
      "Fold 1, Epoch 13: Train Loss=0.3842, Train Acc=86.92%, Val Loss=1.4693, Val Acc=60.73%, Grad Norm=5.5630\n",
      "Fold 1, Epoch 14: Train Loss=0.3590, Train Acc=87.87%, Val Loss=1.5638, Val Acc=60.50%, Grad Norm=5.8554\n",
      "Fold 1, Epoch 15: Train Loss=0.3394, Train Acc=88.52%, Val Loss=1.5751, Val Acc=59.96%, Grad Norm=6.1163\n",
      "Fold 1, Epoch 16: Train Loss=0.3195, Train Acc=89.18%, Val Loss=1.4910, Val Acc=60.75%, Grad Norm=6.2928\n",
      "Fold 1, Epoch 17: Train Loss=0.2995, Train Acc=89.78%, Val Loss=1.6051, Val Acc=60.78%, Grad Norm=6.5293\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=65.44%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=1.7344, Train Acc=34.98%, Val Loss=1.5585, Val Acc=45.03%, Grad Norm=6.1466\n",
      "Fold 2, Epoch 2: Train Loss=1.1565, Train Acc=57.83%, Val Loss=1.3149, Val Acc=57.57%, Grad Norm=4.1234\n",
      "Fold 2, Epoch 3: Train Loss=0.9799, Train Acc=64.76%, Val Loss=1.3771, Val Acc=58.60%, Grad Norm=3.7054\n",
      "Fold 2, Epoch 4: Train Loss=0.8760, Train Acc=68.86%, Val Loss=1.2687, Val Acc=61.91%, Grad Norm=3.6412\n",
      "Fold 2, Epoch 5: Train Loss=0.7995, Train Acc=71.71%, Val Loss=1.1556, Val Acc=65.04%, Grad Norm=3.7273\n",
      "Fold 2, Epoch 6: Train Loss=0.7383, Train Acc=73.97%, Val Loss=1.2027, Val Acc=64.03%, Grad Norm=3.9151\n",
      "Fold 2, Epoch 7: Train Loss=0.6846, Train Acc=76.15%, Val Loss=1.1772, Val Acc=64.98%, Grad Norm=4.1105\n",
      "Fold 2, Epoch 8: Train Loss=0.6312, Train Acc=77.98%, Val Loss=1.2141, Val Acc=64.81%, Grad Norm=4.3069\n",
      "Fold 2, Epoch 9: Train Loss=0.5878, Train Acc=79.70%, Val Loss=1.1939, Val Acc=65.26%, Grad Norm=4.5563\n",
      "Fold 2, Epoch 10: Train Loss=0.5430, Train Acc=81.22%, Val Loss=1.1938, Val Acc=65.86%, Grad Norm=4.7703\n",
      "Fold 2, Epoch 11: Train Loss=0.4633, Train Acc=84.06%, Val Loss=1.1308, Val Acc=66.64%, Grad Norm=5.0987\n",
      "Fold 2, Epoch 12: Train Loss=0.4273, Train Acc=85.42%, Val Loss=1.2182, Val Acc=67.03%, Grad Norm=5.4744\n",
      "Fold 2, Epoch 13: Train Loss=0.3997, Train Acc=86.34%, Val Loss=1.2052, Val Acc=67.43%, Grad Norm=5.7895\n",
      "Fold 2, Epoch 14: Train Loss=0.3740, Train Acc=87.30%, Val Loss=1.1672, Val Acc=67.42%, Grad Norm=6.0915\n",
      "Fold 2, Epoch 15: Train Loss=0.3512, Train Acc=88.03%, Val Loss=1.1125, Val Acc=67.24%, Grad Norm=6.3489\n",
      "Fold 2, Epoch 16: Train Loss=0.3286, Train Acc=88.83%, Val Loss=1.2201, Val Acc=66.93%, Grad Norm=6.5735\n",
      "Fold 2, Epoch 17: Train Loss=0.3113, Train Acc=89.38%, Val Loss=1.2651, Val Acc=66.54%, Grad Norm=6.7943\n",
      "Fold 2, Epoch 18: Train Loss=0.2913, Train Acc=90.15%, Val Loss=1.2203, Val Acc=66.45%, Grad Norm=6.9569\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=63.88%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=1.7375, Train Acc=35.19%, Val Loss=1.9250, Val Acc=39.79%, Grad Norm=6.1990\n",
      "Fold 3, Epoch 2: Train Loss=1.1033, Train Acc=60.22%, Val Loss=1.5522, Val Acc=49.98%, Grad Norm=4.1185\n",
      "Fold 3, Epoch 3: Train Loss=0.9266, Train Acc=66.90%, Val Loss=1.5314, Val Acc=52.05%, Grad Norm=3.6403\n",
      "Fold 3, Epoch 4: Train Loss=0.8272, Train Acc=70.65%, Val Loss=1.3708, Val Acc=56.04%, Grad Norm=3.5837\n",
      "Fold 3, Epoch 5: Train Loss=0.7552, Train Acc=73.34%, Val Loss=1.3891, Val Acc=55.60%, Grad Norm=3.6536\n",
      "Fold 3, Epoch 6: Train Loss=0.6950, Train Acc=75.56%, Val Loss=1.3216, Val Acc=58.41%, Grad Norm=3.8368\n",
      "Fold 3, Epoch 7: Train Loss=0.6440, Train Acc=77.50%, Val Loss=1.3791, Val Acc=59.27%, Grad Norm=3.9737\n",
      "Fold 3, Epoch 8: Train Loss=0.5935, Train Acc=79.28%, Val Loss=1.4076, Val Acc=57.54%, Grad Norm=4.2197\n",
      "Fold 3, Epoch 9: Train Loss=0.5509, Train Acc=80.91%, Val Loss=1.3858, Val Acc=59.52%, Grad Norm=4.4454\n",
      "Fold 3, Epoch 10: Train Loss=0.5084, Train Acc=82.45%, Val Loss=1.4093, Val Acc=59.74%, Grad Norm=4.6228\n",
      "Fold 3, Epoch 11: Train Loss=0.4297, Train Acc=85.25%, Val Loss=1.3429, Val Acc=60.94%, Grad Norm=4.8631\n",
      "Fold 3, Epoch 12: Train Loss=0.3984, Train Acc=86.36%, Val Loss=1.3675, Val Acc=60.58%, Grad Norm=5.2655\n",
      "Fold 3, Epoch 13: Train Loss=0.3712, Train Acc=87.24%, Val Loss=1.3993, Val Acc=60.88%, Grad Norm=5.5638\n",
      "Fold 3, Epoch 14: Train Loss=0.3474, Train Acc=88.16%, Val Loss=1.4560, Val Acc=59.12%, Grad Norm=5.8596\n",
      "Fold 3, Epoch 15: Train Loss=0.3273, Train Acc=88.94%, Val Loss=1.4322, Val Acc=60.84%, Grad Norm=6.1155\n",
      "Fold 3, Epoch 16: Train Loss=0.3079, Train Acc=89.49%, Val Loss=1.4240, Val Acc=61.64%, Grad Norm=6.3102\n",
      "Fold 3, Epoch 17: Train Loss=0.2884, Train Acc=90.21%, Val Loss=1.4608, Val Acc=60.74%, Grad Norm=6.4959\n",
      "Fold 3, Epoch 18: Train Loss=0.2739, Train Acc=90.72%, Val Loss=1.4632, Val Acc=60.98%, Grad Norm=6.6846\n",
      "Fold 3, Epoch 19: Train Loss=0.2544, Train Acc=91.50%, Val Loss=1.4865, Val Acc=61.65%, Grad Norm=6.8481\n",
      "Fold 3, Epoch 20: Train Loss=0.2382, Train Acc=91.99%, Val Loss=1.5686, Val Acc=60.84%, Grad Norm=6.9623\n",
      "Fold 3, Epoch 21: Train Loss=0.1960, Train Acc=93.47%, Val Loss=1.5974, Val Acc=60.57%, Grad Norm=6.7509\n",
      "Fold 3, Epoch 22: Train Loss=0.1812, Train Acc=94.03%, Val Loss=1.5506, Val Acc=61.42%, Grad Norm=6.9682\n",
      "Fold 3, Epoch 23: Train Loss=0.1687, Train Acc=94.49%, Val Loss=1.5973, Val Acc=60.92%, Grad Norm=7.0676\n",
      "Fold 3, Epoch 24: Train Loss=0.1610, Train Acc=94.72%, Val Loss=1.6264, Val Acc=60.58%, Grad Norm=7.1637\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=65.70%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=1.7826, Train Acc=32.94%, Val Loss=1.5568, Val Acc=44.30%, Grad Norm=6.0140\n",
      "Fold 4, Epoch 2: Train Loss=1.1665, Train Acc=57.65%, Val Loss=1.2977, Val Acc=56.25%, Grad Norm=4.1159\n",
      "Fold 4, Epoch 3: Train Loss=0.9731, Train Acc=65.25%, Val Loss=1.2598, Val Acc=57.50%, Grad Norm=3.6992\n",
      "Fold 4, Epoch 4: Train Loss=0.8638, Train Acc=69.50%, Val Loss=1.2154, Val Acc=59.04%, Grad Norm=3.6607\n",
      "Fold 4, Epoch 5: Train Loss=0.7894, Train Acc=72.27%, Val Loss=1.1347, Val Acc=62.76%, Grad Norm=3.7108\n",
      "Fold 4, Epoch 6: Train Loss=0.7268, Train Acc=74.68%, Val Loss=1.2234, Val Acc=60.53%, Grad Norm=3.8850\n",
      "Fold 4, Epoch 7: Train Loss=0.6709, Train Acc=76.58%, Val Loss=1.1690, Val Acc=61.83%, Grad Norm=4.0680\n",
      "Fold 4, Epoch 8: Train Loss=0.6205, Train Acc=78.69%, Val Loss=1.1277, Val Acc=63.33%, Grad Norm=4.2778\n",
      "Fold 4, Epoch 9: Train Loss=0.5793, Train Acc=80.09%, Val Loss=1.2015, Val Acc=62.38%, Grad Norm=4.5099\n",
      "Fold 4, Epoch 10: Train Loss=0.5354, Train Acc=81.69%, Val Loss=1.1272, Val Acc=63.83%, Grad Norm=4.7115\n",
      "Fold 4, Epoch 11: Train Loss=0.4590, Train Acc=84.37%, Val Loss=1.1197, Val Acc=64.91%, Grad Norm=4.9562\n",
      "Fold 4, Epoch 12: Train Loss=0.4230, Train Acc=85.72%, Val Loss=1.1646, Val Acc=64.04%, Grad Norm=5.4042\n",
      "Fold 4, Epoch 13: Train Loss=0.3931, Train Acc=86.71%, Val Loss=1.1535, Val Acc=65.36%, Grad Norm=5.7896\n",
      "Fold 4, Epoch 14: Train Loss=0.3690, Train Acc=87.46%, Val Loss=1.2308, Val Acc=63.81%, Grad Norm=6.0453\n",
      "Fold 4, Epoch 15: Train Loss=0.3465, Train Acc=88.25%, Val Loss=1.2392, Val Acc=64.01%, Grad Norm=6.3490\n",
      "Fold 4, Epoch 16: Train Loss=0.3237, Train Acc=88.97%, Val Loss=1.2943, Val Acc=63.84%, Grad Norm=6.5640\n",
      "Fold 4, Epoch 17: Train Loss=0.3043, Train Acc=89.74%, Val Loss=1.2853, Val Acc=63.95%, Grad Norm=6.7748\n",
      "Fold 4, Epoch 18: Train Loss=0.2872, Train Acc=90.33%, Val Loss=1.2625, Val Acc=64.66%, Grad Norm=6.9948\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=62.70%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=1.7861, Train Acc=32.81%, Val Loss=1.6559, Val Acc=43.96%, Grad Norm=6.0654\n",
      "Fold 5, Epoch 2: Train Loss=1.1487, Train Acc=58.54%, Val Loss=1.2670, Val Acc=55.47%, Grad Norm=4.2962\n",
      "Fold 5, Epoch 3: Train Loss=0.9438, Train Acc=66.40%, Val Loss=1.1812, Val Acc=58.60%, Grad Norm=3.7939\n",
      "Fold 5, Epoch 4: Train Loss=0.8422, Train Acc=70.27%, Val Loss=1.1700, Val Acc=60.00%, Grad Norm=3.6882\n",
      "Fold 5, Epoch 5: Train Loss=0.7645, Train Acc=73.20%, Val Loss=1.1502, Val Acc=62.01%, Grad Norm=3.7588\n",
      "Fold 5, Epoch 6: Train Loss=0.7017, Train Acc=75.55%, Val Loss=1.1605, Val Acc=61.33%, Grad Norm=3.8981\n",
      "Fold 5, Epoch 7: Train Loss=0.6512, Train Acc=77.28%, Val Loss=1.1406, Val Acc=63.45%, Grad Norm=4.0618\n",
      "Fold 5, Epoch 8: Train Loss=0.5990, Train Acc=79.35%, Val Loss=1.1798, Val Acc=62.23%, Grad Norm=4.2510\n",
      "Fold 5, Epoch 9: Train Loss=0.5558, Train Acc=80.73%, Val Loss=1.1536, Val Acc=64.45%, Grad Norm=4.4753\n",
      "Fold 5, Epoch 10: Train Loss=0.5162, Train Acc=82.26%, Val Loss=1.2218, Val Acc=63.09%, Grad Norm=4.6917\n",
      "Fold 5, Epoch 11: Train Loss=0.4356, Train Acc=85.07%, Val Loss=1.1708, Val Acc=64.81%, Grad Norm=4.9309\n",
      "Fold 5, Epoch 12: Train Loss=0.4023, Train Acc=86.19%, Val Loss=1.1944, Val Acc=65.77%, Grad Norm=5.3296\n",
      "Fold 5, Epoch 13: Train Loss=0.3732, Train Acc=87.28%, Val Loss=1.2197, Val Acc=65.21%, Grad Norm=5.6473\n",
      "Fold 5, Epoch 14: Train Loss=0.3485, Train Acc=88.07%, Val Loss=1.2577, Val Acc=65.36%, Grad Norm=5.9422\n",
      "Fold 5, Epoch 15: Train Loss=0.3279, Train Acc=88.79%, Val Loss=1.2486, Val Acc=65.14%, Grad Norm=6.1862\n",
      "Fold 5, Epoch 16: Train Loss=0.3086, Train Acc=89.49%, Val Loss=1.2405, Val Acc=65.46%, Grad Norm=6.4195\n",
      "Fold 5, Epoch 17: Train Loss=0.2861, Train Acc=90.25%, Val Loss=1.3215, Val Acc=64.27%, Grad Norm=6.5897\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=63.77%\n",
      "\n",
      "SNR  15 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-25_11-39-22_LTE-V_XFR_FileBlock_SNR15dB_fd655_group864_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=10 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 787, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 787\n",
      "[INFO] 训练 block 数: 590, 测试 block 数: 197\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=1.7996, Train Acc=32.48%, Val Loss=1.8136, Val Acc=38.32%, Grad Norm=6.2168\n",
      "Fold 1, Epoch 2: Train Loss=1.1602, Train Acc=57.78%, Val Loss=1.5525, Val Acc=48.77%, Grad Norm=4.2907\n",
      "Fold 1, Epoch 3: Train Loss=0.9592, Train Acc=65.71%, Val Loss=1.4253, Val Acc=53.89%, Grad Norm=3.7646\n",
      "Fold 1, Epoch 4: Train Loss=0.8550, Train Acc=69.60%, Val Loss=1.3068, Val Acc=56.93%, Grad Norm=3.6494\n",
      "Fold 1, Epoch 5: Train Loss=0.7890, Train Acc=72.25%, Val Loss=1.3248, Val Acc=56.62%, Grad Norm=3.6782\n",
      "Fold 1, Epoch 6: Train Loss=0.7328, Train Acc=74.35%, Val Loss=1.3783, Val Acc=57.29%, Grad Norm=3.7592\n",
      "Fold 1, Epoch 7: Train Loss=0.6805, Train Acc=76.23%, Val Loss=1.4033, Val Acc=57.66%, Grad Norm=3.9205\n",
      "Fold 1, Epoch 8: Train Loss=0.6392, Train Acc=77.70%, Val Loss=1.3370, Val Acc=59.28%, Grad Norm=4.1049\n",
      "Fold 1, Epoch 9: Train Loss=0.6031, Train Acc=79.13%, Val Loss=1.4203, Val Acc=58.24%, Grad Norm=4.3086\n",
      "Fold 1, Epoch 10: Train Loss=0.5639, Train Acc=80.49%, Val Loss=1.3972, Val Acc=59.42%, Grad Norm=4.5352\n",
      "Fold 1, Epoch 11: Train Loss=0.4914, Train Acc=83.12%, Val Loss=1.4140, Val Acc=59.99%, Grad Norm=4.8148\n",
      "Fold 1, Epoch 12: Train Loss=0.4556, Train Acc=84.44%, Val Loss=1.4500, Val Acc=59.54%, Grad Norm=5.2336\n",
      "Fold 1, Epoch 13: Train Loss=0.4274, Train Acc=85.31%, Val Loss=1.4689, Val Acc=60.10%, Grad Norm=5.6123\n",
      "Fold 1, Epoch 14: Train Loss=0.4039, Train Acc=86.21%, Val Loss=1.4403, Val Acc=60.13%, Grad Norm=5.9402\n",
      "Fold 1, Epoch 15: Train Loss=0.3828, Train Acc=87.01%, Val Loss=1.4891, Val Acc=59.79%, Grad Norm=6.2111\n",
      "Fold 1, Epoch 16: Train Loss=0.3595, Train Acc=87.76%, Val Loss=1.5360, Val Acc=59.53%, Grad Norm=6.4396\n",
      "Fold 1, Epoch 17: Train Loss=0.3417, Train Acc=88.34%, Val Loss=1.5243, Val Acc=59.58%, Grad Norm=6.7181\n",
      "Fold 1, Epoch 18: Train Loss=0.3205, Train Acc=89.06%, Val Loss=1.5099, Val Acc=60.53%, Grad Norm=6.9256\n",
      "Fold 1, Epoch 19: Train Loss=0.3046, Train Acc=89.70%, Val Loss=1.6119, Val Acc=59.40%, Grad Norm=7.1258\n",
      "Fold 1, Epoch 20: Train Loss=0.2873, Train Acc=90.32%, Val Loss=1.6471, Val Acc=59.82%, Grad Norm=7.2705\n",
      "Fold 1, Epoch 21: Train Loss=0.2406, Train Acc=91.99%, Val Loss=1.6332, Val Acc=60.44%, Grad Norm=7.1272\n",
      "Fold 1, Epoch 22: Train Loss=0.2237, Train Acc=92.56%, Val Loss=1.6284, Val Acc=60.55%, Grad Norm=7.4198\n",
      "Fold 1, Epoch 23: Train Loss=0.2103, Train Acc=93.02%, Val Loss=1.6501, Val Acc=60.19%, Grad Norm=7.5328\n",
      "Fold 1, Epoch 24: Train Loss=0.1995, Train Acc=93.44%, Val Loss=1.6676, Val Acc=59.77%, Grad Norm=7.7154\n",
      "Fold 1, Epoch 25: Train Loss=0.1904, Train Acc=93.59%, Val Loss=1.6478, Val Acc=60.22%, Grad Norm=7.8774\n",
      "Fold 1, Epoch 26: Train Loss=0.1815, Train Acc=93.98%, Val Loss=1.6826, Val Acc=60.34%, Grad Norm=7.9601\n",
      "Fold 1, Epoch 27: Train Loss=0.1706, Train Acc=94.39%, Val Loss=1.7212, Val Acc=60.68%, Grad Norm=7.9887\n",
      "Fold 1, Epoch 28: Train Loss=0.1613, Train Acc=94.65%, Val Loss=1.7054, Val Acc=59.95%, Grad Norm=8.0097\n",
      "Fold 1, Epoch 29: Train Loss=0.1565, Train Acc=94.87%, Val Loss=1.7274, Val Acc=60.16%, Grad Norm=8.1802\n",
      "Fold 1, Epoch 30: Train Loss=0.1505, Train Acc=95.10%, Val Loss=1.7584, Val Acc=60.12%, Grad Norm=8.1872\n",
      "Fold 1, Epoch 31: Train Loss=0.1301, Train Acc=95.77%, Val Loss=1.7604, Val Acc=60.36%, Grad Norm=7.8648\n",
      "Fold 1, Epoch 32: Train Loss=0.1196, Train Acc=96.12%, Val Loss=1.7346, Val Acc=60.43%, Grad Norm=7.7488\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=64.14%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=1.7918, Train Acc=32.83%, Val Loss=1.6780, Val Acc=42.96%, Grad Norm=5.8588\n",
      "Fold 2, Epoch 2: Train Loss=1.2084, Train Acc=56.13%, Val Loss=1.3883, Val Acc=55.31%, Grad Norm=4.0379\n",
      "Fold 2, Epoch 3: Train Loss=1.0292, Train Acc=62.98%, Val Loss=1.3179, Val Acc=59.44%, Grad Norm=3.6374\n",
      "Fold 2, Epoch 4: Train Loss=0.9279, Train Acc=66.69%, Val Loss=1.3581, Val Acc=59.86%, Grad Norm=3.5397\n",
      "Fold 2, Epoch 5: Train Loss=0.8540, Train Acc=69.55%, Val Loss=1.1679, Val Acc=63.72%, Grad Norm=3.6170\n",
      "Fold 2, Epoch 6: Train Loss=0.7964, Train Acc=71.85%, Val Loss=1.3054, Val Acc=62.69%, Grad Norm=3.7743\n",
      "Fold 2, Epoch 7: Train Loss=0.7438, Train Acc=73.87%, Val Loss=1.1942, Val Acc=62.99%, Grad Norm=3.9536\n",
      "Fold 2, Epoch 8: Train Loss=0.6973, Train Acc=75.65%, Val Loss=1.2420, Val Acc=63.21%, Grad Norm=4.1748\n",
      "Fold 2, Epoch 9: Train Loss=0.6531, Train Acc=77.23%, Val Loss=1.2400, Val Acc=64.18%, Grad Norm=4.4138\n",
      "Fold 2, Epoch 10: Train Loss=0.6121, Train Acc=78.78%, Val Loss=1.2033, Val Acc=64.01%, Grad Norm=4.6582\n",
      "Fold 2, Epoch 11: Train Loss=0.5316, Train Acc=81.59%, Val Loss=1.1589, Val Acc=65.94%, Grad Norm=4.9788\n",
      "Fold 2, Epoch 12: Train Loss=0.4938, Train Acc=83.09%, Val Loss=1.1830, Val Acc=65.30%, Grad Norm=5.4193\n",
      "Fold 2, Epoch 13: Train Loss=0.4643, Train Acc=84.10%, Val Loss=1.1904, Val Acc=65.66%, Grad Norm=5.7571\n",
      "Fold 2, Epoch 14: Train Loss=0.4368, Train Acc=85.16%, Val Loss=1.2200, Val Acc=65.47%, Grad Norm=6.1098\n",
      "Fold 2, Epoch 15: Train Loss=0.4124, Train Acc=85.94%, Val Loss=1.2186, Val Acc=65.92%, Grad Norm=6.3777\n",
      "Fold 2, Epoch 16: Train Loss=0.3904, Train Acc=86.74%, Val Loss=1.2828, Val Acc=64.99%, Grad Norm=6.7059\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=61.14%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=1.7904, Train Acc=32.68%, Val Loss=1.8938, Val Acc=37.82%, Grad Norm=6.0379\n",
      "Fold 3, Epoch 2: Train Loss=1.1541, Train Acc=58.23%, Val Loss=1.5263, Val Acc=48.67%, Grad Norm=4.1010\n",
      "Fold 3, Epoch 3: Train Loss=0.9698, Train Acc=65.27%, Val Loss=1.4921, Val Acc=51.22%, Grad Norm=3.6336\n",
      "Fold 3, Epoch 4: Train Loss=0.8714, Train Acc=68.87%, Val Loss=1.3910, Val Acc=54.99%, Grad Norm=3.5424\n",
      "Fold 3, Epoch 5: Train Loss=0.8037, Train Acc=71.51%, Val Loss=1.3824, Val Acc=54.95%, Grad Norm=3.6147\n",
      "Fold 3, Epoch 6: Train Loss=0.7448, Train Acc=73.83%, Val Loss=1.3620, Val Acc=56.04%, Grad Norm=3.7305\n",
      "Fold 3, Epoch 7: Train Loss=0.6933, Train Acc=75.62%, Val Loss=1.3978, Val Acc=56.30%, Grad Norm=3.9631\n",
      "Fold 3, Epoch 8: Train Loss=0.6400, Train Acc=77.69%, Val Loss=1.3793, Val Acc=56.56%, Grad Norm=4.2012\n",
      "Fold 3, Epoch 9: Train Loss=0.5977, Train Acc=79.19%, Val Loss=1.4139, Val Acc=57.79%, Grad Norm=4.4098\n",
      "Fold 3, Epoch 10: Train Loss=0.5576, Train Acc=80.71%, Val Loss=1.3749, Val Acc=58.12%, Grad Norm=4.5829\n",
      "Fold 3, Epoch 11: Train Loss=0.4825, Train Acc=83.30%, Val Loss=1.3989, Val Acc=58.55%, Grad Norm=4.9069\n",
      "Fold 3, Epoch 12: Train Loss=0.4485, Train Acc=84.58%, Val Loss=1.3605, Val Acc=59.63%, Grad Norm=5.2548\n",
      "Fold 3, Epoch 13: Train Loss=0.4218, Train Acc=85.53%, Val Loss=1.3720, Val Acc=60.18%, Grad Norm=5.6232\n",
      "Fold 3, Epoch 14: Train Loss=0.3989, Train Acc=86.32%, Val Loss=1.3979, Val Acc=60.19%, Grad Norm=5.8995\n",
      "Fold 3, Epoch 15: Train Loss=0.3766, Train Acc=87.19%, Val Loss=1.4117, Val Acc=59.60%, Grad Norm=6.1832\n",
      "Fold 3, Epoch 16: Train Loss=0.3560, Train Acc=87.85%, Val Loss=1.4740, Val Acc=59.63%, Grad Norm=6.4453\n",
      "Fold 3, Epoch 17: Train Loss=0.3362, Train Acc=88.55%, Val Loss=1.5133, Val Acc=59.05%, Grad Norm=6.6464\n",
      "Fold 3, Epoch 18: Train Loss=0.3194, Train Acc=89.14%, Val Loss=1.4806, Val Acc=59.65%, Grad Norm=6.8776\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=64.32%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=1.8257, Train Acc=31.09%, Val Loss=1.6561, Val Acc=43.03%, Grad Norm=6.0156\n",
      "Fold 4, Epoch 2: Train Loss=1.1900, Train Acc=56.74%, Val Loss=1.3023, Val Acc=56.16%, Grad Norm=4.1166\n",
      "Fold 4, Epoch 3: Train Loss=1.0027, Train Acc=63.92%, Val Loss=1.3325, Val Acc=55.88%, Grad Norm=3.7094\n",
      "Fold 4, Epoch 4: Train Loss=0.9002, Train Acc=68.15%, Val Loss=1.2513, Val Acc=57.69%, Grad Norm=3.6387\n",
      "Fold 4, Epoch 5: Train Loss=0.8242, Train Acc=71.03%, Val Loss=1.0965, Val Acc=62.26%, Grad Norm=3.7249\n",
      "Fold 4, Epoch 6: Train Loss=0.7627, Train Acc=73.26%, Val Loss=1.1481, Val Acc=60.78%, Grad Norm=3.8300\n",
      "Fold 4, Epoch 7: Train Loss=0.7134, Train Acc=75.04%, Val Loss=1.1490, Val Acc=60.76%, Grad Norm=4.0033\n",
      "Fold 4, Epoch 8: Train Loss=0.6701, Train Acc=76.74%, Val Loss=1.1508, Val Acc=61.61%, Grad Norm=4.1994\n",
      "Fold 4, Epoch 9: Train Loss=0.6243, Train Acc=78.41%, Val Loss=1.1635, Val Acc=61.27%, Grad Norm=4.4629\n",
      "Fold 4, Epoch 10: Train Loss=0.5868, Train Acc=79.69%, Val Loss=1.2118, Val Acc=61.87%, Grad Norm=4.6595\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=60.06%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=1.7706, Train Acc=33.48%, Val Loss=1.5215, Val Acc=45.46%, Grad Norm=6.0575\n",
      "Fold 5, Epoch 2: Train Loss=1.1644, Train Acc=57.98%, Val Loss=1.2775, Val Acc=54.49%, Grad Norm=4.1027\n",
      "Fold 5, Epoch 3: Train Loss=0.9776, Train Acc=65.11%, Val Loss=1.1970, Val Acc=57.82%, Grad Norm=3.6914\n",
      "Fold 5, Epoch 4: Train Loss=0.8695, Train Acc=69.06%, Val Loss=1.1697, Val Acc=59.52%, Grad Norm=3.6195\n",
      "Fold 5, Epoch 5: Train Loss=0.7996, Train Acc=71.80%, Val Loss=1.1850, Val Acc=59.42%, Grad Norm=3.6495\n",
      "Fold 5, Epoch 6: Train Loss=0.7403, Train Acc=73.94%, Val Loss=1.1566, Val Acc=60.36%, Grad Norm=3.7705\n",
      "Fold 5, Epoch 7: Train Loss=0.6927, Train Acc=75.86%, Val Loss=1.1684, Val Acc=61.53%, Grad Norm=3.9593\n",
      "Fold 5, Epoch 8: Train Loss=0.6451, Train Acc=77.58%, Val Loss=1.1445, Val Acc=62.60%, Grad Norm=4.1945\n",
      "Fold 5, Epoch 9: Train Loss=0.6056, Train Acc=79.01%, Val Loss=1.2070, Val Acc=61.33%, Grad Norm=4.3403\n",
      "Fold 5, Epoch 10: Train Loss=0.5678, Train Acc=80.40%, Val Loss=1.1836, Val Acc=62.97%, Grad Norm=4.5905\n",
      "Fold 5, Epoch 11: Train Loss=0.4924, Train Acc=83.03%, Val Loss=1.2231, Val Acc=63.11%, Grad Norm=4.8650\n",
      "Fold 5, Epoch 12: Train Loss=0.4567, Train Acc=84.33%, Val Loss=1.2332, Val Acc=63.20%, Grad Norm=5.2582\n",
      "Fold 5, Epoch 13: Train Loss=0.4314, Train Acc=85.26%, Val Loss=1.2607, Val Acc=63.29%, Grad Norm=5.6764\n",
      "Fold 5, Epoch 14: Train Loss=0.4071, Train Acc=86.05%, Val Loss=1.2571, Val Acc=63.45%, Grad Norm=5.9498\n",
      "Fold 5, Epoch 15: Train Loss=0.3871, Train Acc=86.76%, Val Loss=1.2762, Val Acc=63.33%, Grad Norm=6.2377\n",
      "Fold 5, Epoch 16: Train Loss=0.3646, Train Acc=87.60%, Val Loss=1.2634, Val Acc=64.11%, Grad Norm=6.5209\n",
      "Fold 5, Epoch 17: Train Loss=0.3446, Train Acc=88.26%, Val Loss=1.2668, Val Acc=63.92%, Grad Norm=6.7660\n",
      "Fold 5, Epoch 18: Train Loss=0.3257, Train Acc=88.91%, Val Loss=1.2565, Val Acc=64.57%, Grad Norm=6.9443\n",
      "Fold 5, Epoch 19: Train Loss=0.3068, Train Acc=89.56%, Val Loss=1.2969, Val Acc=64.79%, Grad Norm=7.1205\n",
      "Fold 5, Epoch 20: Train Loss=0.2881, Train Acc=90.27%, Val Loss=1.3612, Val Acc=63.93%, Grad Norm=7.3070\n",
      "Fold 5, Epoch 21: Train Loss=0.2434, Train Acc=91.82%, Val Loss=1.3934, Val Acc=63.81%, Grad Norm=7.2346\n",
      "Fold 5, Epoch 22: Train Loss=0.2262, Train Acc=92.45%, Val Loss=1.4131, Val Acc=64.13%, Grad Norm=7.4407\n",
      "Fold 5, Epoch 23: Train Loss=0.2137, Train Acc=92.82%, Val Loss=1.4404, Val Acc=63.50%, Grad Norm=7.6915\n",
      "Fold 5, Epoch 24: Train Loss=0.2017, Train Acc=93.30%, Val Loss=1.4398, Val Acc=64.35%, Grad Norm=7.7434\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=63.70%\n",
      "\n",
      "SNR  10 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-25_12-27-53_LTE-V_XFR_FileBlock_SNR10dB_fd655_group864_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=5 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 787, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 787\n",
      "[INFO] 训练 block 数: 590, 测试 block 数: 197\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=1.8631, Train Acc=29.84%, Val Loss=1.8564, Val Acc=36.20%, Grad Norm=6.1498\n",
      "Fold 1, Epoch 2: Train Loss=1.2532, Train Acc=54.20%, Val Loss=1.5764, Val Acc=46.43%, Grad Norm=4.1268\n",
      "Fold 1, Epoch 3: Train Loss=1.0599, Train Acc=61.57%, Val Loss=1.4467, Val Acc=49.97%, Grad Norm=3.5686\n",
      "Fold 1, Epoch 4: Train Loss=0.9628, Train Acc=65.38%, Val Loss=1.4236, Val Acc=51.45%, Grad Norm=3.5031\n",
      "Fold 1, Epoch 5: Train Loss=0.8922, Train Acc=68.29%, Val Loss=1.3891, Val Acc=53.48%, Grad Norm=3.5204\n",
      "Fold 1, Epoch 6: Train Loss=0.8413, Train Acc=70.30%, Val Loss=1.3314, Val Acc=54.69%, Grad Norm=3.6038\n",
      "Fold 1, Epoch 7: Train Loss=0.7991, Train Acc=71.99%, Val Loss=1.4325, Val Acc=53.61%, Grad Norm=3.7269\n",
      "Fold 1, Epoch 8: Train Loss=0.7635, Train Acc=73.31%, Val Loss=1.4191, Val Acc=54.29%, Grad Norm=3.8836\n",
      "Fold 1, Epoch 9: Train Loss=0.7276, Train Acc=74.72%, Val Loss=1.3648, Val Acc=54.90%, Grad Norm=4.0778\n",
      "Fold 1, Epoch 10: Train Loss=0.6966, Train Acc=75.78%, Val Loss=1.4219, Val Acc=55.63%, Grad Norm=4.2748\n",
      "Fold 1, Epoch 11: Train Loss=0.6275, Train Acc=78.20%, Val Loss=1.3620, Val Acc=56.66%, Grad Norm=4.6243\n",
      "Fold 1, Epoch 12: Train Loss=0.5970, Train Acc=79.30%, Val Loss=1.4043, Val Acc=56.68%, Grad Norm=5.0689\n",
      "Fold 1, Epoch 13: Train Loss=0.5722, Train Acc=80.19%, Val Loss=1.4196, Val Acc=56.34%, Grad Norm=5.4346\n",
      "Fold 1, Epoch 14: Train Loss=0.5497, Train Acc=81.10%, Val Loss=1.4310, Val Acc=57.08%, Grad Norm=5.7699\n",
      "Fold 1, Epoch 15: Train Loss=0.5260, Train Acc=81.89%, Val Loss=1.5072, Val Acc=56.38%, Grad Norm=6.1336\n",
      "Fold 1, Epoch 16: Train Loss=0.5079, Train Acc=82.54%, Val Loss=1.4305, Val Acc=56.96%, Grad Norm=6.4636\n",
      "Fold 1, Epoch 17: Train Loss=0.4838, Train Acc=83.36%, Val Loss=1.4396, Val Acc=57.54%, Grad Norm=6.7586\n",
      "Fold 1, Epoch 18: Train Loss=0.4631, Train Acc=84.12%, Val Loss=1.5457, Val Acc=56.29%, Grad Norm=7.0749\n",
      "Fold 1, Epoch 19: Train Loss=0.4467, Train Acc=84.69%, Val Loss=1.5455, Val Acc=56.96%, Grad Norm=7.3193\n",
      "Fold 1, Epoch 20: Train Loss=0.4274, Train Acc=85.35%, Val Loss=1.4788, Val Acc=56.91%, Grad Norm=7.6370\n",
      "Fold 1, Epoch 21: Train Loss=0.3780, Train Acc=87.13%, Val Loss=1.5636, Val Acc=57.16%, Grad Norm=7.8000\n",
      "Fold 1, Epoch 22: Train Loss=0.3544, Train Acc=87.96%, Val Loss=1.5798, Val Acc=56.89%, Grad Norm=8.1674\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=61.86%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=1.9043, Train Acc=28.18%, Val Loss=1.8477, Val Acc=35.46%, Grad Norm=6.0405\n",
      "Fold 2, Epoch 2: Train Loss=1.3270, Train Acc=51.37%, Val Loss=1.4377, Val Acc=48.96%, Grad Norm=4.0920\n",
      "Fold 2, Epoch 3: Train Loss=1.1284, Train Acc=58.86%, Val Loss=1.3601, Val Acc=53.82%, Grad Norm=3.5457\n",
      "Fold 2, Epoch 4: Train Loss=1.0330, Train Acc=62.68%, Val Loss=1.3306, Val Acc=56.37%, Grad Norm=3.4331\n",
      "Fold 2, Epoch 5: Train Loss=0.9643, Train Acc=65.45%, Val Loss=1.2531, Val Acc=58.20%, Grad Norm=3.4722\n",
      "Fold 2, Epoch 6: Train Loss=0.9071, Train Acc=67.75%, Val Loss=1.3286, Val Acc=57.57%, Grad Norm=3.5790\n",
      "Fold 2, Epoch 7: Train Loss=0.8639, Train Acc=69.25%, Val Loss=1.2777, Val Acc=60.38%, Grad Norm=3.7390\n",
      "Fold 2, Epoch 8: Train Loss=0.8208, Train Acc=70.94%, Val Loss=1.2670, Val Acc=61.57%, Grad Norm=3.9472\n",
      "Fold 2, Epoch 9: Train Loss=0.7841, Train Acc=72.38%, Val Loss=1.3181, Val Acc=61.24%, Grad Norm=4.1490\n",
      "Fold 2, Epoch 10: Train Loss=0.7475, Train Acc=73.85%, Val Loss=1.3160, Val Acc=61.78%, Grad Norm=4.3388\n",
      "Fold 2, Epoch 11: Train Loss=0.6745, Train Acc=76.45%, Val Loss=1.3356, Val Acc=62.13%, Grad Norm=4.7127\n",
      "Fold 2, Epoch 12: Train Loss=0.6421, Train Acc=77.67%, Val Loss=1.3707, Val Acc=61.28%, Grad Norm=5.1967\n",
      "Fold 2, Epoch 13: Train Loss=0.6135, Train Acc=78.76%, Val Loss=1.3529, Val Acc=62.73%, Grad Norm=5.6027\n",
      "Fold 2, Epoch 14: Train Loss=0.5902, Train Acc=79.56%, Val Loss=1.4004, Val Acc=61.65%, Grad Norm=5.9822\n",
      "Fold 2, Epoch 15: Train Loss=0.5638, Train Acc=80.54%, Val Loss=1.3464, Val Acc=62.16%, Grad Norm=6.3331\n",
      "Fold 2, Epoch 16: Train Loss=0.5406, Train Acc=81.28%, Val Loss=1.3303, Val Acc=62.31%, Grad Norm=6.6724\n",
      "Fold 2, Epoch 17: Train Loss=0.5174, Train Acc=82.13%, Val Loss=1.3945, Val Acc=63.15%, Grad Norm=7.0180\n",
      "Fold 2, Epoch 18: Train Loss=0.4971, Train Acc=82.88%, Val Loss=1.3407, Val Acc=62.42%, Grad Norm=7.3094\n",
      "Fold 2, Epoch 19: Train Loss=0.4770, Train Acc=83.55%, Val Loss=1.3858, Val Acc=62.77%, Grad Norm=7.5992\n",
      "Fold 2, Epoch 20: Train Loss=0.4562, Train Acc=84.41%, Val Loss=1.3428, Val Acc=63.90%, Grad Norm=7.8613\n",
      "Fold 2, Epoch 21: Train Loss=0.4018, Train Acc=86.31%, Val Loss=1.4347, Val Acc=63.50%, Grad Norm=8.0539\n",
      "Fold 2, Epoch 22: Train Loss=0.3778, Train Acc=87.15%, Val Loss=1.4143, Val Acc=63.35%, Grad Norm=8.4461\n",
      "Fold 2, Epoch 23: Train Loss=0.3613, Train Acc=87.74%, Val Loss=1.4558, Val Acc=62.72%, Grad Norm=8.6834\n",
      "Fold 2, Epoch 24: Train Loss=0.3467, Train Acc=88.22%, Val Loss=1.4736, Val Acc=63.17%, Grad Norm=8.9580\n",
      "Fold 2, Epoch 25: Train Loss=0.3303, Train Acc=88.85%, Val Loss=1.5687, Val Acc=62.06%, Grad Norm=9.2091\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=59.02%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=1.8539, Train Acc=30.27%, Val Loss=1.8213, Val Acc=36.57%, Grad Norm=5.8020\n",
      "Fold 3, Epoch 2: Train Loss=1.2624, Train Acc=53.90%, Val Loss=1.5820, Val Acc=44.02%, Grad Norm=4.0656\n",
      "Fold 3, Epoch 3: Train Loss=1.0659, Train Acc=61.36%, Val Loss=1.4944, Val Acc=49.67%, Grad Norm=3.5244\n",
      "Fold 3, Epoch 4: Train Loss=0.9670, Train Acc=65.27%, Val Loss=1.4146, Val Acc=52.13%, Grad Norm=3.3778\n",
      "Fold 3, Epoch 5: Train Loss=0.9059, Train Acc=67.50%, Val Loss=1.4475, Val Acc=52.93%, Grad Norm=3.3973\n",
      "Fold 3, Epoch 6: Train Loss=0.8548, Train Acc=69.47%, Val Loss=1.3946, Val Acc=53.55%, Grad Norm=3.5090\n",
      "Fold 3, Epoch 7: Train Loss=0.8123, Train Acc=71.19%, Val Loss=1.3985, Val Acc=55.50%, Grad Norm=3.6908\n",
      "Fold 3, Epoch 8: Train Loss=0.7707, Train Acc=72.86%, Val Loss=1.3604, Val Acc=55.08%, Grad Norm=3.8572\n",
      "Fold 3, Epoch 9: Train Loss=0.7362, Train Acc=74.11%, Val Loss=1.4048, Val Acc=55.05%, Grad Norm=4.0694\n",
      "Fold 3, Epoch 10: Train Loss=0.7016, Train Acc=75.40%, Val Loss=1.4064, Val Acc=54.84%, Grad Norm=4.2926\n",
      "Fold 3, Epoch 11: Train Loss=0.6334, Train Acc=78.00%, Val Loss=1.3532, Val Acc=57.20%, Grad Norm=4.6688\n",
      "Fold 3, Epoch 12: Train Loss=0.5990, Train Acc=79.18%, Val Loss=1.4494, Val Acc=55.83%, Grad Norm=5.1160\n",
      "Fold 3, Epoch 13: Train Loss=0.5706, Train Acc=80.17%, Val Loss=1.3803, Val Acc=57.84%, Grad Norm=5.5491\n",
      "Fold 3, Epoch 14: Train Loss=0.5484, Train Acc=81.06%, Val Loss=1.3988, Val Acc=57.91%, Grad Norm=5.8891\n",
      "Fold 3, Epoch 15: Train Loss=0.5250, Train Acc=81.87%, Val Loss=1.3719, Val Acc=58.01%, Grad Norm=6.2102\n",
      "Fold 3, Epoch 16: Train Loss=0.5030, Train Acc=82.58%, Val Loss=1.4222, Val Acc=57.38%, Grad Norm=6.5421\n",
      "Fold 3, Epoch 17: Train Loss=0.4836, Train Acc=83.40%, Val Loss=1.5037, Val Acc=57.28%, Grad Norm=6.8504\n",
      "Fold 3, Epoch 18: Train Loss=0.4585, Train Acc=84.24%, Val Loss=1.4963, Val Acc=57.37%, Grad Norm=7.1161\n",
      "Fold 3, Epoch 19: Train Loss=0.4421, Train Acc=84.84%, Val Loss=1.4927, Val Acc=57.35%, Grad Norm=7.4034\n",
      "Fold 3, Epoch 20: Train Loss=0.4198, Train Acc=85.51%, Val Loss=1.5772, Val Acc=56.43%, Grad Norm=7.7010\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=59.89%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=1.9165, Train Acc=27.45%, Val Loss=1.7168, Val Acc=38.59%, Grad Norm=6.0530\n",
      "Fold 4, Epoch 2: Train Loss=1.3186, Train Acc=51.77%, Val Loss=1.3186, Val Acc=51.87%, Grad Norm=4.1662\n",
      "Fold 4, Epoch 3: Train Loss=1.1120, Train Acc=59.77%, Val Loss=1.2289, Val Acc=56.43%, Grad Norm=3.6310\n",
      "Fold 4, Epoch 4: Train Loss=1.0055, Train Acc=63.98%, Val Loss=1.2686, Val Acc=56.26%, Grad Norm=3.4902\n",
      "Fold 4, Epoch 5: Train Loss=0.9373, Train Acc=66.75%, Val Loss=1.2515, Val Acc=56.70%, Grad Norm=3.5305\n",
      "Fold 4, Epoch 6: Train Loss=0.8828, Train Acc=68.80%, Val Loss=1.2463, Val Acc=57.83%, Grad Norm=3.6249\n",
      "Fold 4, Epoch 7: Train Loss=0.8391, Train Acc=70.47%, Val Loss=1.2266, Val Acc=58.23%, Grad Norm=3.7394\n",
      "Fold 4, Epoch 8: Train Loss=0.7989, Train Acc=72.10%, Val Loss=1.3467, Val Acc=57.12%, Grad Norm=3.9459\n",
      "Fold 4, Epoch 9: Train Loss=0.7630, Train Acc=73.38%, Val Loss=1.2382, Val Acc=59.16%, Grad Norm=4.1425\n",
      "Fold 4, Epoch 10: Train Loss=0.7323, Train Acc=74.46%, Val Loss=1.1755, Val Acc=60.42%, Grad Norm=4.3584\n",
      "Fold 4, Epoch 11: Train Loss=0.6595, Train Acc=77.15%, Val Loss=1.2444, Val Acc=59.11%, Grad Norm=4.7622\n",
      "Fold 4, Epoch 12: Train Loss=0.6276, Train Acc=78.37%, Val Loss=1.2122, Val Acc=61.26%, Grad Norm=5.1807\n",
      "Fold 4, Epoch 13: Train Loss=0.6010, Train Acc=79.27%, Val Loss=1.2558, Val Acc=59.51%, Grad Norm=5.5920\n",
      "Fold 4, Epoch 14: Train Loss=0.5773, Train Acc=80.09%, Val Loss=1.2268, Val Acc=60.41%, Grad Norm=5.9540\n",
      "Fold 4, Epoch 15: Train Loss=0.5531, Train Acc=80.95%, Val Loss=1.2691, Val Acc=60.40%, Grad Norm=6.3133\n",
      "Fold 4, Epoch 16: Train Loss=0.5311, Train Acc=81.78%, Val Loss=1.3195, Val Acc=59.45%, Grad Norm=6.5979\n",
      "Fold 4, Epoch 17: Train Loss=0.5114, Train Acc=82.44%, Val Loss=1.3108, Val Acc=59.97%, Grad Norm=6.9239\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=57.95%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=1.9042, Train Acc=27.55%, Val Loss=1.7666, Val Acc=36.63%, Grad Norm=6.0570\n",
      "Fold 5, Epoch 2: Train Loss=1.2769, Train Acc=53.57%, Val Loss=1.3611, Val Acc=50.21%, Grad Norm=4.2789\n",
      "Fold 5, Epoch 3: Train Loss=1.0740, Train Acc=61.36%, Val Loss=1.3017, Val Acc=54.05%, Grad Norm=3.6350\n",
      "Fold 5, Epoch 4: Train Loss=0.9780, Train Acc=64.97%, Val Loss=1.2449, Val Acc=55.28%, Grad Norm=3.4959\n",
      "Fold 5, Epoch 5: Train Loss=0.9112, Train Acc=67.37%, Val Loss=1.2402, Val Acc=55.20%, Grad Norm=3.5282\n",
      "Fold 5, Epoch 6: Train Loss=0.8584, Train Acc=69.65%, Val Loss=1.2097, Val Acc=58.18%, Grad Norm=3.6089\n",
      "Fold 5, Epoch 7: Train Loss=0.8144, Train Acc=71.30%, Val Loss=1.2112, Val Acc=58.08%, Grad Norm=3.7444\n",
      "Fold 5, Epoch 8: Train Loss=0.7762, Train Acc=72.72%, Val Loss=1.2276, Val Acc=57.67%, Grad Norm=3.9535\n",
      "Fold 5, Epoch 9: Train Loss=0.7363, Train Acc=74.28%, Val Loss=1.1925, Val Acc=59.31%, Grad Norm=4.1395\n",
      "Fold 5, Epoch 10: Train Loss=0.7060, Train Acc=75.36%, Val Loss=1.2206, Val Acc=58.46%, Grad Norm=4.3193\n",
      "Fold 5, Epoch 11: Train Loss=0.6362, Train Acc=77.90%, Val Loss=1.2141, Val Acc=60.12%, Grad Norm=4.6439\n",
      "Fold 5, Epoch 12: Train Loss=0.6035, Train Acc=79.07%, Val Loss=1.1816, Val Acc=61.33%, Grad Norm=5.0870\n",
      "Fold 5, Epoch 13: Train Loss=0.5769, Train Acc=80.15%, Val Loss=1.2072, Val Acc=61.04%, Grad Norm=5.4547\n",
      "Fold 5, Epoch 14: Train Loss=0.5561, Train Acc=80.78%, Val Loss=1.2609, Val Acc=60.03%, Grad Norm=5.8235\n",
      "Fold 5, Epoch 15: Train Loss=0.5356, Train Acc=81.53%, Val Loss=1.2637, Val Acc=60.60%, Grad Norm=6.1756\n",
      "Fold 5, Epoch 16: Train Loss=0.5127, Train Acc=82.40%, Val Loss=1.2877, Val Acc=60.50%, Grad Norm=6.4652\n",
      "Fold 5, Epoch 17: Train Loss=0.4930, Train Acc=83.05%, Val Loss=1.2762, Val Acc=60.97%, Grad Norm=6.7747\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=59.83%\n",
      "\n",
      "SNR   5 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-25_13-19-00_LTE-V_XFR_FileBlock_SNR5dB_fd655_group864_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=0 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 787, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 787\n",
      "[INFO] 训练 block 数: 590, 测试 block 数: 197\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.0620, Train Acc=20.91%, Val Loss=2.4676, Val Acc=25.18%, Grad Norm=5.9452\n",
      "Fold 1, Epoch 2: Train Loss=1.5262, Train Acc=43.53%, Val Loss=1.7246, Val Acc=37.81%, Grad Norm=4.2140\n",
      "Fold 1, Epoch 3: Train Loss=1.2933, Train Acc=52.75%, Val Loss=1.5688, Val Acc=44.40%, Grad Norm=3.6129\n",
      "Fold 1, Epoch 4: Train Loss=1.1867, Train Acc=57.10%, Val Loss=1.4665, Val Acc=46.81%, Grad Norm=3.3992\n",
      "Fold 1, Epoch 5: Train Loss=1.1199, Train Acc=59.71%, Val Loss=1.4341, Val Acc=48.87%, Grad Norm=3.3799\n",
      "Fold 1, Epoch 6: Train Loss=1.0756, Train Acc=61.48%, Val Loss=1.4277, Val Acc=49.57%, Grad Norm=3.4435\n",
      "Fold 1, Epoch 7: Train Loss=1.0376, Train Acc=63.02%, Val Loss=1.4238, Val Acc=49.17%, Grad Norm=3.4812\n",
      "Fold 1, Epoch 8: Train Loss=1.0097, Train Acc=64.14%, Val Loss=1.3681, Val Acc=50.83%, Grad Norm=3.6187\n",
      "Fold 1, Epoch 9: Train Loss=0.9799, Train Acc=65.18%, Val Loss=1.4860, Val Acc=48.94%, Grad Norm=3.7663\n",
      "Fold 1, Epoch 10: Train Loss=0.9544, Train Acc=66.13%, Val Loss=1.4140, Val Acc=50.43%, Grad Norm=3.9226\n",
      "Fold 1, Epoch 11: Train Loss=0.8954, Train Acc=68.33%, Val Loss=1.4481, Val Acc=51.02%, Grad Norm=4.2203\n",
      "Fold 1, Epoch 12: Train Loss=0.8710, Train Acc=69.32%, Val Loss=1.3981, Val Acc=52.21%, Grad Norm=4.6092\n",
      "Fold 1, Epoch 13: Train Loss=0.8476, Train Acc=70.34%, Val Loss=1.3984, Val Acc=52.23%, Grad Norm=4.9867\n",
      "Fold 1, Epoch 14: Train Loss=0.8298, Train Acc=70.88%, Val Loss=1.3791, Val Acc=52.60%, Grad Norm=5.3319\n",
      "Fold 1, Epoch 15: Train Loss=0.8117, Train Acc=71.60%, Val Loss=1.4304, Val Acc=52.14%, Grad Norm=5.6442\n",
      "Fold 1, Epoch 16: Train Loss=0.7950, Train Acc=72.22%, Val Loss=1.4231, Val Acc=52.33%, Grad Norm=5.9981\n",
      "Fold 1, Epoch 17: Train Loss=0.7769, Train Acc=72.91%, Val Loss=1.4401, Val Acc=51.89%, Grad Norm=6.3771\n",
      "Fold 1, Epoch 18: Train Loss=0.7591, Train Acc=73.63%, Val Loss=1.4854, Val Acc=51.20%, Grad Norm=6.6584\n",
      "Fold 1, Epoch 19: Train Loss=0.7398, Train Acc=74.41%, Val Loss=1.4853, Val Acc=51.61%, Grad Norm=7.0060\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=54.21%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.0437, Train Acc=21.68%, Val Loss=2.0478, Val Acc=26.79%, Grad Norm=6.1166\n",
      "Fold 2, Epoch 2: Train Loss=1.5489, Train Acc=42.75%, Val Loss=1.5846, Val Acc=42.03%, Grad Norm=4.1635\n",
      "Fold 2, Epoch 3: Train Loss=1.3524, Train Acc=50.62%, Val Loss=1.4071, Val Acc=49.16%, Grad Norm=3.5106\n",
      "Fold 2, Epoch 4: Train Loss=1.2449, Train Acc=54.75%, Val Loss=1.4238, Val Acc=50.25%, Grad Norm=3.3342\n",
      "Fold 2, Epoch 5: Train Loss=1.1811, Train Acc=57.19%, Val Loss=1.3736, Val Acc=52.05%, Grad Norm=3.2649\n",
      "Fold 2, Epoch 6: Train Loss=1.1349, Train Acc=59.03%, Val Loss=1.3504, Val Acc=53.21%, Grad Norm=3.3051\n",
      "Fold 2, Epoch 7: Train Loss=1.1011, Train Acc=60.42%, Val Loss=1.3122, Val Acc=53.90%, Grad Norm=3.3939\n",
      "Fold 2, Epoch 8: Train Loss=1.0713, Train Acc=61.59%, Val Loss=1.3404, Val Acc=53.99%, Grad Norm=3.4658\n",
      "Fold 2, Epoch 9: Train Loss=1.0459, Train Acc=62.57%, Val Loss=1.2954, Val Acc=56.14%, Grad Norm=3.6227\n",
      "Fold 2, Epoch 10: Train Loss=1.0189, Train Acc=63.62%, Val Loss=1.3803, Val Acc=54.39%, Grad Norm=3.7907\n",
      "Fold 2, Epoch 11: Train Loss=0.9640, Train Acc=65.77%, Val Loss=1.3409, Val Acc=55.39%, Grad Norm=4.1269\n",
      "Fold 2, Epoch 12: Train Loss=0.9394, Train Acc=66.75%, Val Loss=1.3525, Val Acc=55.28%, Grad Norm=4.5321\n",
      "Fold 2, Epoch 13: Train Loss=0.9175, Train Acc=67.63%, Val Loss=1.3365, Val Acc=56.43%, Grad Norm=4.9101\n",
      "Fold 2, Epoch 14: Train Loss=0.8996, Train Acc=68.31%, Val Loss=1.3752, Val Acc=57.13%, Grad Norm=5.2665\n",
      "Fold 2, Epoch 15: Train Loss=0.8807, Train Acc=69.05%, Val Loss=1.3838, Val Acc=56.55%, Grad Norm=5.6051\n",
      "Fold 2, Epoch 16: Train Loss=0.8617, Train Acc=69.92%, Val Loss=1.3538, Val Acc=57.08%, Grad Norm=5.9545\n",
      "Fold 2, Epoch 17: Train Loss=0.8451, Train Acc=70.45%, Val Loss=1.3695, Val Acc=56.72%, Grad Norm=6.2705\n",
      "Fold 2, Epoch 18: Train Loss=0.8275, Train Acc=70.91%, Val Loss=1.3780, Val Acc=56.94%, Grad Norm=6.6184\n",
      "Fold 2, Epoch 19: Train Loss=0.8093, Train Acc=71.84%, Val Loss=1.3675, Val Acc=57.82%, Grad Norm=6.9632\n",
      "Fold 2, Epoch 20: Train Loss=0.7883, Train Acc=72.66%, Val Loss=1.4063, Val Acc=56.40%, Grad Norm=7.3357\n",
      "Fold 2, Epoch 21: Train Loss=0.7373, Train Acc=74.30%, Val Loss=1.4027, Val Acc=57.21%, Grad Norm=7.8313\n",
      "Fold 2, Epoch 22: Train Loss=0.7129, Train Acc=75.38%, Val Loss=1.4290, Val Acc=56.42%, Grad Norm=8.3554\n",
      "Fold 2, Epoch 23: Train Loss=0.6939, Train Acc=76.07%, Val Loss=1.4741, Val Acc=56.86%, Grad Norm=8.8295\n",
      "Fold 2, Epoch 24: Train Loss=0.6787, Train Acc=76.60%, Val Loss=1.4656, Val Acc=56.79%, Grad Norm=9.2112\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=52.96%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.0841, Train Acc=19.91%, Val Loss=2.5318, Val Acc=21.76%, Grad Norm=5.9909\n",
      "Fold 3, Epoch 2: Train Loss=1.5594, Train Acc=42.83%, Val Loss=1.8534, Val Acc=36.49%, Grad Norm=4.3858\n",
      "Fold 3, Epoch 3: Train Loss=1.2862, Train Acc=53.24%, Val Loss=1.7657, Val Acc=38.23%, Grad Norm=3.6199\n",
      "Fold 3, Epoch 4: Train Loss=1.1862, Train Acc=57.04%, Val Loss=1.6600, Val Acc=43.13%, Grad Norm=3.3176\n",
      "Fold 3, Epoch 5: Train Loss=1.1240, Train Acc=59.41%, Val Loss=1.5917, Val Acc=44.95%, Grad Norm=3.2654\n",
      "Fold 3, Epoch 6: Train Loss=1.0821, Train Acc=61.14%, Val Loss=1.5324, Val Acc=46.08%, Grad Norm=3.2997\n",
      "Fold 3, Epoch 7: Train Loss=1.0462, Train Acc=62.51%, Val Loss=1.5158, Val Acc=47.19%, Grad Norm=3.3686\n",
      "Fold 3, Epoch 8: Train Loss=1.0166, Train Acc=63.65%, Val Loss=1.5363, Val Acc=47.67%, Grad Norm=3.4723\n",
      "Fold 3, Epoch 9: Train Loss=0.9879, Train Acc=64.76%, Val Loss=1.5225, Val Acc=48.03%, Grad Norm=3.6012\n",
      "Fold 3, Epoch 10: Train Loss=0.9637, Train Acc=65.78%, Val Loss=1.5543, Val Acc=47.56%, Grad Norm=3.7403\n",
      "Fold 3, Epoch 11: Train Loss=0.9070, Train Acc=67.82%, Val Loss=1.4780, Val Acc=49.85%, Grad Norm=4.0777\n",
      "Fold 3, Epoch 12: Train Loss=0.8823, Train Acc=68.83%, Val Loss=1.4918, Val Acc=49.88%, Grad Norm=4.4692\n",
      "Fold 3, Epoch 13: Train Loss=0.8631, Train Acc=69.54%, Val Loss=1.4368, Val Acc=51.25%, Grad Norm=4.8377\n",
      "Fold 3, Epoch 14: Train Loss=0.8430, Train Acc=70.32%, Val Loss=1.4871, Val Acc=50.25%, Grad Norm=5.1666\n",
      "Fold 3, Epoch 15: Train Loss=0.8259, Train Acc=71.01%, Val Loss=1.4578, Val Acc=50.84%, Grad Norm=5.4876\n",
      "Fold 3, Epoch 16: Train Loss=0.8090, Train Acc=71.73%, Val Loss=1.5176, Val Acc=50.48%, Grad Norm=5.8429\n",
      "Fold 3, Epoch 17: Train Loss=0.7926, Train Acc=72.28%, Val Loss=1.5046, Val Acc=50.63%, Grad Norm=6.1654\n",
      "Fold 3, Epoch 18: Train Loss=0.7746, Train Acc=73.07%, Val Loss=1.4985, Val Acc=50.45%, Grad Norm=6.4605\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=54.77%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.0694, Train Acc=20.47%, Val Loss=2.1413, Val Acc=23.30%, Grad Norm=5.6141\n",
      "Fold 4, Epoch 2: Train Loss=1.5784, Train Acc=41.42%, Val Loss=1.6826, Val Acc=38.54%, Grad Norm=4.1422\n",
      "Fold 4, Epoch 3: Train Loss=1.3441, Train Acc=50.91%, Val Loss=1.4751, Val Acc=46.80%, Grad Norm=3.5222\n",
      "Fold 4, Epoch 4: Train Loss=1.2344, Train Acc=55.24%, Val Loss=1.3831, Val Acc=49.99%, Grad Norm=3.3519\n",
      "Fold 4, Epoch 5: Train Loss=1.1692, Train Acc=57.86%, Val Loss=1.3842, Val Acc=51.21%, Grad Norm=3.2677\n",
      "Fold 4, Epoch 6: Train Loss=1.1254, Train Acc=59.43%, Val Loss=1.2963, Val Acc=52.05%, Grad Norm=3.3011\n",
      "Fold 4, Epoch 7: Train Loss=1.0889, Train Acc=61.01%, Val Loss=1.3394, Val Acc=52.38%, Grad Norm=3.3998\n",
      "Fold 4, Epoch 8: Train Loss=1.0589, Train Acc=62.25%, Val Loss=1.2919, Val Acc=54.05%, Grad Norm=3.5024\n",
      "Fold 4, Epoch 9: Train Loss=1.0295, Train Acc=63.39%, Val Loss=1.3333, Val Acc=53.84%, Grad Norm=3.6790\n",
      "Fold 4, Epoch 10: Train Loss=1.0047, Train Acc=64.34%, Val Loss=1.3025, Val Acc=54.10%, Grad Norm=3.8581\n",
      "Fold 4, Epoch 11: Train Loss=0.9479, Train Acc=66.54%, Val Loss=1.2609, Val Acc=55.56%, Grad Norm=4.2071\n",
      "Fold 4, Epoch 12: Train Loss=0.9213, Train Acc=67.43%, Val Loss=1.2640, Val Acc=55.67%, Grad Norm=4.5997\n",
      "Fold 4, Epoch 13: Train Loss=0.9016, Train Acc=68.24%, Val Loss=1.2616, Val Acc=55.70%, Grad Norm=4.9444\n",
      "Fold 4, Epoch 14: Train Loss=0.8810, Train Acc=69.03%, Val Loss=1.2913, Val Acc=55.29%, Grad Norm=5.3070\n",
      "Fold 4, Epoch 15: Train Loss=0.8644, Train Acc=69.71%, Val Loss=1.2942, Val Acc=55.61%, Grad Norm=5.6558\n",
      "Fold 4, Epoch 16: Train Loss=0.8460, Train Acc=70.40%, Val Loss=1.3102, Val Acc=55.56%, Grad Norm=6.0249\n",
      "Fold 4, Epoch 17: Train Loss=0.8254, Train Acc=71.18%, Val Loss=1.3019, Val Acc=55.68%, Grad Norm=6.3755\n",
      "Fold 4, Epoch 18: Train Loss=0.8078, Train Acc=71.84%, Val Loss=1.3070, Val Acc=55.43%, Grad Norm=6.6999\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=52.54%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.0378, Train Acc=21.80%, Val Loss=2.3448, Val Acc=23.31%, Grad Norm=5.9327\n",
      "Fold 5, Epoch 2: Train Loss=1.5359, Train Acc=43.26%, Val Loss=1.5892, Val Acc=42.62%, Grad Norm=4.2690\n",
      "Fold 5, Epoch 3: Train Loss=1.2998, Train Acc=52.77%, Val Loss=1.4247, Val Acc=47.56%, Grad Norm=3.6088\n",
      "Fold 5, Epoch 4: Train Loss=1.2009, Train Acc=56.61%, Val Loss=1.3489, Val Acc=50.30%, Grad Norm=3.3913\n",
      "Fold 5, Epoch 5: Train Loss=1.1403, Train Acc=58.96%, Val Loss=1.3016, Val Acc=52.00%, Grad Norm=3.3238\n",
      "Fold 5, Epoch 6: Train Loss=1.0936, Train Acc=60.73%, Val Loss=1.2933, Val Acc=52.86%, Grad Norm=3.4162\n",
      "Fold 5, Epoch 7: Train Loss=1.0569, Train Acc=62.31%, Val Loss=1.3145, Val Acc=52.90%, Grad Norm=3.4650\n",
      "Fold 5, Epoch 8: Train Loss=1.0269, Train Acc=63.43%, Val Loss=1.2889, Val Acc=53.49%, Grad Norm=3.5855\n",
      "Fold 5, Epoch 9: Train Loss=1.0005, Train Acc=64.41%, Val Loss=1.2750, Val Acc=53.74%, Grad Norm=3.6867\n",
      "Fold 5, Epoch 10: Train Loss=0.9745, Train Acc=65.36%, Val Loss=1.3033, Val Acc=52.69%, Grad Norm=3.8921\n",
      "Fold 5, Epoch 11: Train Loss=0.9182, Train Acc=67.50%, Val Loss=1.2692, Val Acc=55.25%, Grad Norm=4.1580\n",
      "Fold 5, Epoch 12: Train Loss=0.8932, Train Acc=68.46%, Val Loss=1.2392, Val Acc=55.87%, Grad Norm=4.5364\n",
      "Fold 5, Epoch 13: Train Loss=0.8709, Train Acc=69.18%, Val Loss=1.2775, Val Acc=55.16%, Grad Norm=4.9208\n",
      "Fold 5, Epoch 14: Train Loss=0.8521, Train Acc=70.06%, Val Loss=1.2757, Val Acc=55.23%, Grad Norm=5.2859\n",
      "Fold 5, Epoch 15: Train Loss=0.8323, Train Acc=70.73%, Val Loss=1.3158, Val Acc=54.62%, Grad Norm=5.5883\n",
      "Fold 5, Epoch 16: Train Loss=0.8161, Train Acc=71.51%, Val Loss=1.2833, Val Acc=56.00%, Grad Norm=5.9276\n",
      "Fold 5, Epoch 17: Train Loss=0.8010, Train Acc=71.92%, Val Loss=1.2961, Val Acc=56.21%, Grad Norm=6.2365\n",
      "Fold 5, Epoch 18: Train Loss=0.7833, Train Acc=72.63%, Val Loss=1.2970, Val Acc=55.68%, Grad Norm=6.5812\n",
      "Fold 5, Epoch 19: Train Loss=0.7654, Train Acc=73.33%, Val Loss=1.2941, Val Acc=56.42%, Grad Norm=6.8950\n",
      "Fold 5, Epoch 20: Train Loss=0.7465, Train Acc=73.95%, Val Loss=1.3182, Val Acc=55.67%, Grad Norm=7.2128\n",
      "Fold 5, Epoch 21: Train Loss=0.6976, Train Acc=75.86%, Val Loss=1.3453, Val Acc=56.13%, Grad Norm=7.6855\n",
      "Fold 5, Epoch 22: Train Loss=0.6735, Train Acc=76.72%, Val Loss=1.3506, Val Acc=56.05%, Grad Norm=8.2288\n",
      "Fold 5, Epoch 23: Train Loss=0.6549, Train Acc=77.33%, Val Loss=1.3728, Val Acc=56.02%, Grad Norm=8.6847\n",
      "Fold 5, Epoch 24: Train Loss=0.6407, Train Acc=77.86%, Val Loss=1.3677, Val Acc=56.21%, Grad Norm=9.0818\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=54.73%\n",
      "\n",
      "SNR   0 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-25_14-02-46_LTE-V_XFR_FileBlock_SNR0dB_fd655_group864_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-5 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 787, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 787\n",
      "[INFO] 训练 block 数: 590, 测试 block 数: 197\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.1968, Train Acc=13.54%, Val Loss=2.6909, Val Acc=13.95%, Grad Norm=6.1420\n",
      "Fold 1, Epoch 2: Train Loss=1.9710, Train Acc=25.72%, Val Loss=1.9973, Val Acc=25.03%, Grad Norm=4.2601\n",
      "Fold 1, Epoch 3: Train Loss=1.7149, Train Acc=36.35%, Val Loss=1.7912, Val Acc=32.25%, Grad Norm=3.6300\n",
      "Fold 1, Epoch 4: Train Loss=1.5785, Train Acc=41.92%, Val Loss=1.7207, Val Acc=34.13%, Grad Norm=3.4013\n",
      "Fold 1, Epoch 5: Train Loss=1.4993, Train Acc=45.18%, Val Loss=1.6600, Val Acc=36.80%, Grad Norm=3.2823\n",
      "Fold 1, Epoch 6: Train Loss=1.4489, Train Acc=47.31%, Val Loss=1.6274, Val Acc=38.46%, Grad Norm=3.2870\n",
      "Fold 1, Epoch 7: Train Loss=1.4115, Train Acc=48.79%, Val Loss=1.6219, Val Acc=39.14%, Grad Norm=3.3148\n",
      "Fold 1, Epoch 8: Train Loss=1.3823, Train Acc=50.08%, Val Loss=1.5887, Val Acc=40.31%, Grad Norm=3.4106\n",
      "Fold 1, Epoch 9: Train Loss=1.3571, Train Acc=51.06%, Val Loss=1.6157, Val Acc=40.66%, Grad Norm=3.5446\n",
      "Fold 1, Epoch 10: Train Loss=1.3353, Train Acc=51.93%, Val Loss=1.5863, Val Acc=41.43%, Grad Norm=3.6317\n",
      "Fold 1, Epoch 11: Train Loss=1.2863, Train Acc=53.75%, Val Loss=1.5724, Val Acc=42.49%, Grad Norm=3.8657\n",
      "Fold 1, Epoch 12: Train Loss=1.2609, Train Acc=54.83%, Val Loss=1.5673, Val Acc=42.56%, Grad Norm=4.2044\n",
      "Fold 1, Epoch 13: Train Loss=1.2464, Train Acc=55.34%, Val Loss=1.5782, Val Acc=42.55%, Grad Norm=4.4835\n",
      "Fold 1, Epoch 14: Train Loss=1.2284, Train Acc=56.29%, Val Loss=1.5916, Val Acc=42.73%, Grad Norm=4.7700\n",
      "Fold 1, Epoch 15: Train Loss=1.2161, Train Acc=56.63%, Val Loss=1.5731, Val Acc=42.90%, Grad Norm=5.0632\n",
      "Fold 1, Epoch 16: Train Loss=1.2012, Train Acc=57.30%, Val Loss=1.5725, Val Acc=42.91%, Grad Norm=5.3433\n",
      "Fold 1, Epoch 17: Train Loss=1.1880, Train Acc=57.78%, Val Loss=1.6089, Val Acc=42.19%, Grad Norm=5.6240\n",
      "Fold 1, Epoch 18: Train Loss=1.1748, Train Acc=58.36%, Val Loss=1.5553, Val Acc=43.49%, Grad Norm=5.9254\n",
      "Fold 1, Epoch 19: Train Loss=1.1592, Train Acc=59.02%, Val Loss=1.5870, Val Acc=42.98%, Grad Norm=6.2074\n",
      "Fold 1, Epoch 20: Train Loss=1.1446, Train Acc=59.57%, Val Loss=1.5919, Val Acc=42.96%, Grad Norm=6.5365\n",
      "Fold 1, Epoch 21: Train Loss=1.1026, Train Acc=61.18%, Val Loss=1.5825, Val Acc=43.34%, Grad Norm=7.0419\n",
      "Fold 1, Epoch 22: Train Loss=1.0829, Train Acc=62.06%, Val Loss=1.6040, Val Acc=42.97%, Grad Norm=7.4954\n",
      "Fold 1, Epoch 23: Train Loss=1.0637, Train Acc=62.73%, Val Loss=1.6210, Val Acc=42.41%, Grad Norm=7.9824\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=43.99%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2022, Train Acc=13.13%, Val Loss=2.6186, Val Acc=13.14%, Grad Norm=5.8461\n",
      "Fold 2, Epoch 2: Train Loss=2.0144, Train Acc=23.61%, Val Loss=2.0492, Val Acc=24.20%, Grad Norm=4.2550\n",
      "Fold 2, Epoch 3: Train Loss=1.7574, Train Acc=34.99%, Val Loss=1.7910, Val Acc=32.75%, Grad Norm=3.6420\n",
      "Fold 2, Epoch 4: Train Loss=1.6176, Train Acc=40.74%, Val Loss=1.7348, Val Acc=34.76%, Grad Norm=3.3250\n",
      "Fold 2, Epoch 5: Train Loss=1.5363, Train Acc=43.76%, Val Loss=1.6398, Val Acc=38.08%, Grad Norm=3.2296\n",
      "Fold 2, Epoch 6: Train Loss=1.4878, Train Acc=45.60%, Val Loss=1.5975, Val Acc=40.22%, Grad Norm=3.1823\n",
      "Fold 2, Epoch 7: Train Loss=1.4521, Train Acc=47.28%, Val Loss=1.5871, Val Acc=40.95%, Grad Norm=3.2256\n",
      "Fold 2, Epoch 8: Train Loss=1.4235, Train Acc=48.21%, Val Loss=1.5446, Val Acc=42.20%, Grad Norm=3.3005\n",
      "Fold 2, Epoch 9: Train Loss=1.3960, Train Acc=49.57%, Val Loss=1.5248, Val Acc=43.51%, Grad Norm=3.4234\n",
      "Fold 2, Epoch 10: Train Loss=1.3756, Train Acc=50.49%, Val Loss=1.5370, Val Acc=43.78%, Grad Norm=3.5288\n",
      "Fold 2, Epoch 11: Train Loss=1.3263, Train Acc=52.42%, Val Loss=1.5238, Val Acc=44.80%, Grad Norm=3.7501\n",
      "Fold 2, Epoch 12: Train Loss=1.3016, Train Acc=53.39%, Val Loss=1.5109, Val Acc=45.36%, Grad Norm=4.1084\n",
      "Fold 2, Epoch 13: Train Loss=1.2849, Train Acc=54.08%, Val Loss=1.5209, Val Acc=45.17%, Grad Norm=4.3996\n",
      "Fold 2, Epoch 14: Train Loss=1.2726, Train Acc=54.56%, Val Loss=1.5127, Val Acc=45.12%, Grad Norm=4.6875\n",
      "Fold 2, Epoch 15: Train Loss=1.2572, Train Acc=55.17%, Val Loss=1.5220, Val Acc=45.61%, Grad Norm=4.9704\n",
      "Fold 2, Epoch 16: Train Loss=1.2442, Train Acc=55.59%, Val Loss=1.5061, Val Acc=46.07%, Grad Norm=5.2438\n",
      "Fold 2, Epoch 17: Train Loss=1.2305, Train Acc=56.25%, Val Loss=1.5182, Val Acc=45.33%, Grad Norm=5.5643\n",
      "Fold 2, Epoch 18: Train Loss=1.2149, Train Acc=56.85%, Val Loss=1.5513, Val Acc=45.16%, Grad Norm=5.8812\n",
      "Fold 2, Epoch 19: Train Loss=1.2017, Train Acc=57.31%, Val Loss=1.5395, Val Acc=45.63%, Grad Norm=6.1430\n",
      "Fold 2, Epoch 20: Train Loss=1.1910, Train Acc=57.76%, Val Loss=1.5308, Val Acc=45.84%, Grad Norm=6.4914\n",
      "Fold 2, Epoch 21: Train Loss=1.1409, Train Acc=59.82%, Val Loss=1.5298, Val Acc=46.01%, Grad Norm=7.0004\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=42.53%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.1866, Train Acc=14.43%, Val Loss=2.4505, Val Acc=11.87%, Grad Norm=6.1000\n",
      "Fold 3, Epoch 2: Train Loss=1.9249, Train Acc=27.86%, Val Loss=2.0175, Val Acc=25.26%, Grad Norm=4.1547\n",
      "Fold 3, Epoch 3: Train Loss=1.6885, Train Acc=37.86%, Val Loss=1.8060, Val Acc=32.09%, Grad Norm=3.6146\n",
      "Fold 3, Epoch 4: Train Loss=1.5599, Train Acc=42.97%, Val Loss=1.8124, Val Acc=33.44%, Grad Norm=3.3785\n",
      "Fold 3, Epoch 5: Train Loss=1.4835, Train Acc=46.14%, Val Loss=1.7711, Val Acc=34.72%, Grad Norm=3.2607\n",
      "Fold 3, Epoch 6: Train Loss=1.4343, Train Acc=48.05%, Val Loss=1.7045, Val Acc=36.22%, Grad Norm=3.2448\n",
      "Fold 3, Epoch 7: Train Loss=1.3991, Train Acc=49.22%, Val Loss=1.7428, Val Acc=36.05%, Grad Norm=3.2913\n",
      "Fold 3, Epoch 8: Train Loss=1.3703, Train Acc=50.58%, Val Loss=1.6867, Val Acc=37.50%, Grad Norm=3.3700\n",
      "Fold 3, Epoch 9: Train Loss=1.3471, Train Acc=51.43%, Val Loss=1.7111, Val Acc=36.72%, Grad Norm=3.4461\n",
      "Fold 3, Epoch 10: Train Loss=1.3262, Train Acc=52.19%, Val Loss=1.7220, Val Acc=37.42%, Grad Norm=3.5619\n",
      "Fold 3, Epoch 11: Train Loss=1.2759, Train Acc=54.21%, Val Loss=1.6827, Val Acc=38.92%, Grad Norm=3.7851\n",
      "Fold 3, Epoch 12: Train Loss=1.2553, Train Acc=54.96%, Val Loss=1.6849, Val Acc=38.78%, Grad Norm=4.1541\n",
      "Fold 3, Epoch 13: Train Loss=1.2386, Train Acc=55.64%, Val Loss=1.6844, Val Acc=39.58%, Grad Norm=4.4325\n",
      "Fold 3, Epoch 14: Train Loss=1.2242, Train Acc=56.34%, Val Loss=1.6791, Val Acc=39.96%, Grad Norm=4.7226\n",
      "Fold 3, Epoch 15: Train Loss=1.2085, Train Acc=56.94%, Val Loss=1.6833, Val Acc=39.93%, Grad Norm=5.0095\n",
      "Fold 3, Epoch 16: Train Loss=1.1936, Train Acc=57.47%, Val Loss=1.6689, Val Acc=39.91%, Grad Norm=5.2935\n",
      "Fold 3, Epoch 17: Train Loss=1.1804, Train Acc=57.99%, Val Loss=1.7113, Val Acc=39.43%, Grad Norm=5.6516\n",
      "Fold 3, Epoch 18: Train Loss=1.1654, Train Acc=58.78%, Val Loss=1.6809, Val Acc=40.59%, Grad Norm=5.9011\n",
      "Fold 3, Epoch 19: Train Loss=1.1536, Train Acc=59.11%, Val Loss=1.6911, Val Acc=39.82%, Grad Norm=6.1839\n",
      "Fold 3, Epoch 20: Train Loss=1.1381, Train Acc=59.76%, Val Loss=1.6955, Val Acc=40.75%, Grad Norm=6.5072\n",
      "Fold 3, Epoch 21: Train Loss=1.0924, Train Acc=61.64%, Val Loss=1.7046, Val Acc=40.28%, Grad Norm=6.9899\n",
      "Fold 3, Epoch 22: Train Loss=1.0701, Train Acc=62.41%, Val Loss=1.7410, Val Acc=40.09%, Grad Norm=7.5439\n",
      "Fold 3, Epoch 23: Train Loss=1.0574, Train Acc=62.98%, Val Loss=1.7320, Val Acc=40.30%, Grad Norm=8.0125\n",
      "Fold 3, Epoch 24: Train Loss=1.0392, Train Acc=63.57%, Val Loss=1.7403, Val Acc=39.88%, Grad Norm=8.4707\n",
      "Fold 3, Epoch 25: Train Loss=1.0258, Train Acc=64.06%, Val Loss=1.7409, Val Acc=40.55%, Grad Norm=8.8812\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=43.35%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2032, Train Acc=13.09%, Val Loss=2.6954, Val Acc=9.36%, Grad Norm=6.1302\n",
      "Fold 4, Epoch 2: Train Loss=2.0185, Train Acc=23.66%, Val Loss=2.0514, Val Acc=23.24%, Grad Norm=4.1882\n",
      "Fold 4, Epoch 3: Train Loss=1.7666, Train Acc=34.74%, Val Loss=1.8168, Val Acc=31.55%, Grad Norm=3.5929\n",
      "Fold 4, Epoch 4: Train Loss=1.6239, Train Acc=40.20%, Val Loss=1.6829, Val Acc=35.75%, Grad Norm=3.3280\n",
      "Fold 4, Epoch 5: Train Loss=1.5410, Train Acc=43.57%, Val Loss=1.6207, Val Acc=38.24%, Grad Norm=3.2240\n",
      "Fold 4, Epoch 6: Train Loss=1.4879, Train Acc=45.69%, Val Loss=1.6097, Val Acc=38.92%, Grad Norm=3.2271\n",
      "Fold 4, Epoch 7: Train Loss=1.4509, Train Acc=47.15%, Val Loss=1.6241, Val Acc=39.96%, Grad Norm=3.2540\n",
      "Fold 4, Epoch 8: Train Loss=1.4201, Train Acc=48.56%, Val Loss=1.5713, Val Acc=41.48%, Grad Norm=3.3232\n",
      "Fold 4, Epoch 9: Train Loss=1.3930, Train Acc=49.75%, Val Loss=1.5887, Val Acc=40.97%, Grad Norm=3.4674\n",
      "Fold 4, Epoch 10: Train Loss=1.3691, Train Acc=50.76%, Val Loss=1.5694, Val Acc=42.49%, Grad Norm=3.6181\n",
      "Fold 4, Epoch 11: Train Loss=1.3184, Train Acc=52.86%, Val Loss=1.5496, Val Acc=43.19%, Grad Norm=3.8307\n",
      "Fold 4, Epoch 12: Train Loss=1.2959, Train Acc=53.65%, Val Loss=1.5536, Val Acc=43.18%, Grad Norm=4.1493\n",
      "Fold 4, Epoch 13: Train Loss=1.2782, Train Acc=54.31%, Val Loss=1.5567, Val Acc=43.30%, Grad Norm=4.4810\n",
      "Fold 4, Epoch 14: Train Loss=1.2636, Train Acc=55.00%, Val Loss=1.5846, Val Acc=42.75%, Grad Norm=4.7853\n",
      "Fold 4, Epoch 15: Train Loss=1.2467, Train Acc=55.71%, Val Loss=1.5616, Val Acc=43.10%, Grad Norm=5.0773\n",
      "Fold 4, Epoch 16: Train Loss=1.2347, Train Acc=56.19%, Val Loss=1.5552, Val Acc=44.01%, Grad Norm=5.3515\n",
      "Fold 4, Epoch 17: Train Loss=1.2198, Train Acc=56.89%, Val Loss=1.5468, Val Acc=43.35%, Grad Norm=5.6497\n",
      "Fold 4, Epoch 18: Train Loss=1.2069, Train Acc=57.39%, Val Loss=1.5706, Val Acc=43.88%, Grad Norm=5.9210\n",
      "Fold 4, Epoch 19: Train Loss=1.1904, Train Acc=58.09%, Val Loss=1.5447, Val Acc=44.50%, Grad Norm=6.2536\n",
      "Fold 4, Epoch 20: Train Loss=1.1784, Train Acc=58.57%, Val Loss=1.5526, Val Acc=44.68%, Grad Norm=6.5453\n",
      "Fold 4, Epoch 21: Train Loss=1.1331, Train Acc=60.24%, Val Loss=1.6022, Val Acc=43.83%, Grad Norm=7.0400\n",
      "Fold 4, Epoch 22: Train Loss=1.1123, Train Acc=60.99%, Val Loss=1.5914, Val Acc=44.69%, Grad Norm=7.5835\n",
      "Fold 4, Epoch 23: Train Loss=1.0959, Train Acc=61.62%, Val Loss=1.5634, Val Acc=44.80%, Grad Norm=8.0747\n",
      "Fold 4, Epoch 24: Train Loss=1.0819, Train Acc=62.11%, Val Loss=1.6013, Val Acc=43.87%, Grad Norm=8.5010\n",
      "Fold 4, Epoch 25: Train Loss=1.0663, Train Acc=62.84%, Val Loss=1.5771, Val Acc=44.72%, Grad Norm=8.9798\n",
      "Fold 4, Epoch 26: Train Loss=1.0488, Train Acc=63.37%, Val Loss=1.5961, Val Acc=44.27%, Grad Norm=9.4206\n",
      "Fold 4, Epoch 27: Train Loss=1.0380, Train Acc=63.89%, Val Loss=1.6089, Val Acc=44.50%, Grad Norm=9.8797\n",
      "Fold 4, Epoch 28: Train Loss=1.0236, Train Acc=64.42%, Val Loss=1.6331, Val Acc=44.05%, Grad Norm=10.2699\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=42.68%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.1976, Train Acc=13.37%, Val Loss=2.7055, Val Acc=8.90%, Grad Norm=6.1223\n",
      "Fold 5, Epoch 2: Train Loss=1.9975, Train Acc=24.20%, Val Loss=2.0893, Val Acc=22.55%, Grad Norm=4.1427\n",
      "Fold 5, Epoch 3: Train Loss=1.7627, Train Acc=34.63%, Val Loss=1.8723, Val Acc=30.34%, Grad Norm=3.6390\n",
      "Fold 5, Epoch 4: Train Loss=1.6132, Train Acc=41.01%, Val Loss=1.6924, Val Acc=36.02%, Grad Norm=3.4433\n",
      "Fold 5, Epoch 5: Train Loss=1.5201, Train Acc=44.54%, Val Loss=1.5954, Val Acc=39.94%, Grad Norm=3.3431\n",
      "Fold 5, Epoch 6: Train Loss=1.4609, Train Acc=46.96%, Val Loss=1.5653, Val Acc=41.34%, Grad Norm=3.3226\n",
      "Fold 5, Epoch 7: Train Loss=1.4220, Train Acc=48.45%, Val Loss=1.5249, Val Acc=42.86%, Grad Norm=3.3495\n",
      "Fold 5, Epoch 8: Train Loss=1.3919, Train Acc=49.74%, Val Loss=1.5360, Val Acc=43.48%, Grad Norm=3.4181\n",
      "Fold 5, Epoch 9: Train Loss=1.3663, Train Acc=50.90%, Val Loss=1.5632, Val Acc=41.96%, Grad Norm=3.5277\n",
      "Fold 5, Epoch 10: Train Loss=1.3445, Train Acc=51.68%, Val Loss=1.5435, Val Acc=43.00%, Grad Norm=3.6507\n",
      "Fold 5, Epoch 11: Train Loss=1.2962, Train Acc=53.50%, Val Loss=1.5250, Val Acc=43.88%, Grad Norm=3.8469\n",
      "Fold 5, Epoch 12: Train Loss=1.2737, Train Acc=54.41%, Val Loss=1.4992, Val Acc=45.22%, Grad Norm=4.1786\n",
      "Fold 5, Epoch 13: Train Loss=1.2540, Train Acc=55.29%, Val Loss=1.5170, Val Acc=44.66%, Grad Norm=4.4936\n",
      "Fold 5, Epoch 14: Train Loss=1.2397, Train Acc=55.94%, Val Loss=1.5011, Val Acc=45.46%, Grad Norm=4.7895\n",
      "Fold 5, Epoch 15: Train Loss=1.2259, Train Acc=56.40%, Val Loss=1.4968, Val Acc=45.35%, Grad Norm=5.0360\n",
      "Fold 5, Epoch 16: Train Loss=1.2126, Train Acc=57.00%, Val Loss=1.5196, Val Acc=45.45%, Grad Norm=5.3587\n",
      "Fold 5, Epoch 17: Train Loss=1.1977, Train Acc=57.55%, Val Loss=1.5052, Val Acc=45.77%, Grad Norm=5.6399\n",
      "Fold 5, Epoch 18: Train Loss=1.1854, Train Acc=58.01%, Val Loss=1.5283, Val Acc=44.75%, Grad Norm=5.9177\n",
      "Fold 5, Epoch 19: Train Loss=1.1714, Train Acc=58.48%, Val Loss=1.5175, Val Acc=45.23%, Grad Norm=6.2254\n",
      "Fold 5, Epoch 20: Train Loss=1.1543, Train Acc=59.24%, Val Loss=1.5332, Val Acc=45.07%, Grad Norm=6.5824\n",
      "Fold 5, Epoch 21: Train Loss=1.1129, Train Acc=60.78%, Val Loss=1.5340, Val Acc=45.38%, Grad Norm=7.0604\n",
      "Fold 5, Epoch 22: Train Loss=1.0895, Train Acc=61.82%, Val Loss=1.5663, Val Acc=45.08%, Grad Norm=7.6575\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=44.36%\n",
      "\n",
      "SNR  -5 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-25_14-35-14_LTE-V_XFR_FileBlock_SNR-5dB_fd655_group864_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-10 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 787, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 787\n",
      "[INFO] 训练 block 数: 590, 测试 block 数: 197\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2181, Train Acc=11.63%, Val Loss=2.3660, Val Acc=12.68%, Grad Norm=6.2676\n",
      "Fold 1, Epoch 2: Train Loss=2.1972, Train Acc=13.10%, Val Loss=2.2303, Val Acc=12.74%, Grad Norm=4.2633\n",
      "Fold 1, Epoch 3: Train Loss=2.1218, Train Acc=18.69%, Val Loss=2.0963, Val Acc=20.19%, Grad Norm=3.1243\n",
      "Fold 1, Epoch 4: Train Loss=2.0221, Train Acc=23.92%, Val Loss=2.0099, Val Acc=23.82%, Grad Norm=2.9096\n",
      "Fold 1, Epoch 5: Train Loss=1.9613, Train Acc=26.81%, Val Loss=1.9955, Val Acc=24.29%, Grad Norm=2.6868\n",
      "Fold 1, Epoch 6: Train Loss=1.9201, Train Acc=28.69%, Val Loss=1.9939, Val Acc=24.74%, Grad Norm=2.6644\n",
      "Fold 1, Epoch 7: Train Loss=1.8854, Train Acc=30.19%, Val Loss=1.9597, Val Acc=25.59%, Grad Norm=2.7093\n",
      "Fold 1, Epoch 8: Train Loss=1.8586, Train Acc=31.32%, Val Loss=1.9331, Val Acc=26.94%, Grad Norm=2.7951\n",
      "Fold 1, Epoch 9: Train Loss=1.8309, Train Acc=32.82%, Val Loss=1.9130, Val Acc=28.12%, Grad Norm=2.9595\n",
      "Fold 1, Epoch 10: Train Loss=1.8057, Train Acc=33.89%, Val Loss=1.9052, Val Acc=28.36%, Grad Norm=3.0639\n",
      "Fold 1, Epoch 11: Train Loss=1.7610, Train Acc=35.95%, Val Loss=1.8848, Val Acc=29.43%, Grad Norm=3.3067\n",
      "Fold 1, Epoch 12: Train Loss=1.7397, Train Acc=37.01%, Val Loss=1.8882, Val Acc=28.89%, Grad Norm=3.6681\n",
      "Fold 1, Epoch 13: Train Loss=1.7185, Train Acc=37.95%, Val Loss=1.8739, Val Acc=29.87%, Grad Norm=4.0047\n",
      "Fold 1, Epoch 14: Train Loss=1.7052, Train Acc=38.76%, Val Loss=1.8842, Val Acc=29.68%, Grad Norm=4.3414\n",
      "Fold 1, Epoch 15: Train Loss=1.6878, Train Acc=39.23%, Val Loss=1.8816, Val Acc=29.76%, Grad Norm=4.6558\n",
      "Fold 1, Epoch 16: Train Loss=1.6751, Train Acc=39.76%, Val Loss=1.8811, Val Acc=29.57%, Grad Norm=4.9290\n",
      "Fold 1, Epoch 17: Train Loss=1.6586, Train Acc=40.66%, Val Loss=1.8989, Val Acc=29.07%, Grad Norm=5.2312\n",
      "Fold 1, Epoch 18: Train Loss=1.6459, Train Acc=41.08%, Val Loss=1.9050, Val Acc=29.19%, Grad Norm=5.5389\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=30.84%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2186, Train Acc=11.50%, Val Loss=2.2764, Val Acc=12.71%, Grad Norm=6.4745\n",
      "Fold 2, Epoch 2: Train Loss=2.2043, Train Acc=12.42%, Val Loss=2.1941, Val Acc=12.88%, Grad Norm=4.5369\n",
      "Fold 2, Epoch 3: Train Loss=2.1624, Train Acc=16.09%, Val Loss=2.1354, Val Acc=17.03%, Grad Norm=3.1789\n",
      "Fold 2, Epoch 4: Train Loss=2.0752, Train Acc=21.28%, Val Loss=2.0577, Val Acc=20.89%, Grad Norm=2.7888\n",
      "Fold 2, Epoch 5: Train Loss=1.9945, Train Acc=25.57%, Val Loss=2.0400, Val Acc=21.65%, Grad Norm=2.7513\n",
      "Fold 2, Epoch 6: Train Loss=1.9389, Train Acc=28.07%, Val Loss=2.0013, Val Acc=23.78%, Grad Norm=2.6796\n",
      "Fold 2, Epoch 7: Train Loss=1.8962, Train Acc=30.32%, Val Loss=1.9873, Val Acc=23.84%, Grad Norm=2.7565\n",
      "Fold 2, Epoch 8: Train Loss=1.8591, Train Acc=31.84%, Val Loss=1.9366, Val Acc=26.68%, Grad Norm=2.8123\n",
      "Fold 2, Epoch 9: Train Loss=1.8344, Train Acc=32.75%, Val Loss=1.9394, Val Acc=26.43%, Grad Norm=2.8773\n",
      "Fold 2, Epoch 10: Train Loss=1.8137, Train Acc=33.81%, Val Loss=1.9109, Val Acc=27.87%, Grad Norm=2.9697\n",
      "Fold 2, Epoch 11: Train Loss=1.7733, Train Acc=35.58%, Val Loss=1.8969, Val Acc=28.73%, Grad Norm=3.2046\n",
      "Fold 2, Epoch 12: Train Loss=1.7515, Train Acc=36.41%, Val Loss=1.8812, Val Acc=29.55%, Grad Norm=3.5550\n",
      "Fold 2, Epoch 13: Train Loss=1.7364, Train Acc=37.17%, Val Loss=1.9012, Val Acc=28.77%, Grad Norm=3.8479\n",
      "Fold 2, Epoch 14: Train Loss=1.7214, Train Acc=37.84%, Val Loss=1.8760, Val Acc=29.82%, Grad Norm=4.1470\n",
      "Fold 2, Epoch 15: Train Loss=1.7072, Train Acc=38.52%, Val Loss=1.8957, Val Acc=29.16%, Grad Norm=4.4662\n",
      "Fold 2, Epoch 16: Train Loss=1.6926, Train Acc=39.14%, Val Loss=1.9013, Val Acc=29.32%, Grad Norm=4.7529\n",
      "Fold 2, Epoch 17: Train Loss=1.6768, Train Acc=39.83%, Val Loss=1.8841, Val Acc=29.40%, Grad Norm=5.1210\n",
      "Fold 2, Epoch 18: Train Loss=1.6641, Train Acc=40.61%, Val Loss=1.8923, Val Acc=29.74%, Grad Norm=5.4229\n",
      "Fold 2, Epoch 19: Train Loss=1.6481, Train Acc=41.14%, Val Loss=1.8880, Val Acc=30.10%, Grad Norm=5.7353\n",
      "Fold 2, Epoch 20: Train Loss=1.6303, Train Acc=41.72%, Val Loss=1.8945, Val Acc=29.82%, Grad Norm=6.0938\n",
      "Fold 2, Epoch 21: Train Loss=1.5864, Train Acc=43.73%, Val Loss=1.8875, Val Acc=30.85%, Grad Norm=6.5938\n",
      "Fold 2, Epoch 22: Train Loss=1.5627, Train Acc=44.69%, Val Loss=1.8922, Val Acc=30.71%, Grad Norm=7.2361\n",
      "Fold 2, Epoch 23: Train Loss=1.5447, Train Acc=45.39%, Val Loss=1.9021, Val Acc=30.30%, Grad Norm=7.8237\n",
      "Fold 2, Epoch 24: Train Loss=1.5291, Train Acc=45.86%, Val Loss=1.9050, Val Acc=30.68%, Grad Norm=8.3816\n",
      "Fold 2, Epoch 25: Train Loss=1.5155, Train Acc=46.51%, Val Loss=1.9138, Val Acc=30.42%, Grad Norm=8.8727\n",
      "Fold 2, Epoch 26: Train Loss=1.4970, Train Acc=47.27%, Val Loss=1.9258, Val Acc=30.54%, Grad Norm=9.4137\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=28.98%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2177, Train Acc=11.61%, Val Loss=2.2971, Val Acc=10.16%, Grad Norm=6.2586\n",
      "Fold 3, Epoch 2: Train Loss=2.2013, Train Acc=12.81%, Val Loss=2.2770, Val Acc=10.85%, Grad Norm=4.3588\n",
      "Fold 3, Epoch 3: Train Loss=2.1369, Train Acc=17.79%, Val Loss=2.1833, Val Acc=16.03%, Grad Norm=3.2334\n",
      "Fold 3, Epoch 4: Train Loss=2.0364, Train Acc=23.24%, Val Loss=2.1018, Val Acc=18.69%, Grad Norm=2.8572\n",
      "Fold 3, Epoch 5: Train Loss=1.9610, Train Acc=26.95%, Val Loss=2.0705, Val Acc=21.37%, Grad Norm=2.7733\n",
      "Fold 3, Epoch 6: Train Loss=1.9038, Train Acc=29.40%, Val Loss=2.0266, Val Acc=23.54%, Grad Norm=2.7879\n",
      "Fold 3, Epoch 7: Train Loss=1.8642, Train Acc=31.35%, Val Loss=2.0082, Val Acc=24.28%, Grad Norm=2.8125\n",
      "Fold 3, Epoch 8: Train Loss=1.8362, Train Acc=32.48%, Val Loss=2.0035, Val Acc=24.73%, Grad Norm=2.8635\n",
      "Fold 3, Epoch 9: Train Loss=1.8117, Train Acc=33.82%, Val Loss=1.9735, Val Acc=26.32%, Grad Norm=2.9569\n",
      "Fold 3, Epoch 10: Train Loss=1.7923, Train Acc=34.59%, Val Loss=1.9675, Val Acc=26.76%, Grad Norm=3.0546\n",
      "Fold 3, Epoch 11: Train Loss=1.7477, Train Acc=36.54%, Val Loss=1.9700, Val Acc=27.16%, Grad Norm=3.2702\n",
      "Fold 3, Epoch 12: Train Loss=1.7260, Train Acc=37.51%, Val Loss=1.9535, Val Acc=27.67%, Grad Norm=3.6158\n",
      "Fold 3, Epoch 13: Train Loss=1.7126, Train Acc=38.16%, Val Loss=1.9473, Val Acc=27.60%, Grad Norm=3.9194\n",
      "Fold 3, Epoch 14: Train Loss=1.6950, Train Acc=38.95%, Val Loss=1.9396, Val Acc=28.38%, Grad Norm=4.2383\n",
      "Fold 3, Epoch 15: Train Loss=1.6828, Train Acc=39.37%, Val Loss=1.9757, Val Acc=27.24%, Grad Norm=4.5497\n",
      "Fold 3, Epoch 16: Train Loss=1.6666, Train Acc=40.24%, Val Loss=1.9629, Val Acc=27.86%, Grad Norm=4.8852\n",
      "Fold 3, Epoch 17: Train Loss=1.6535, Train Acc=40.72%, Val Loss=1.9621, Val Acc=28.31%, Grad Norm=5.1599\n",
      "Fold 3, Epoch 18: Train Loss=1.6388, Train Acc=41.31%, Val Loss=1.9708, Val Acc=28.16%, Grad Norm=5.4612\n",
      "Fold 3, Epoch 19: Train Loss=1.6254, Train Acc=41.71%, Val Loss=1.9652, Val Acc=28.58%, Grad Norm=5.7758\n",
      "Fold 3, Epoch 20: Train Loss=1.6079, Train Acc=42.61%, Val Loss=1.9626, Val Acc=27.96%, Grad Norm=6.1356\n",
      "Fold 3, Epoch 21: Train Loss=1.5654, Train Acc=44.45%, Val Loss=1.9689, Val Acc=28.38%, Grad Norm=6.5858\n",
      "Fold 3, Epoch 22: Train Loss=1.5399, Train Acc=45.42%, Val Loss=1.9712, Val Acc=28.21%, Grad Norm=7.2506\n",
      "Fold 3, Epoch 23: Train Loss=1.5224, Train Acc=46.17%, Val Loss=1.9724, Val Acc=28.14%, Grad Norm=7.8373\n",
      "Fold 3, Epoch 24: Train Loss=1.5070, Train Acc=46.79%, Val Loss=1.9848, Val Acc=28.28%, Grad Norm=8.3694\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=29.66%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2171, Train Acc=11.80%, Val Loss=2.4276, Val Acc=13.50%, Grad Norm=6.4141\n",
      "Fold 4, Epoch 2: Train Loss=2.2003, Train Acc=12.75%, Val Loss=2.2980, Val Acc=8.62%, Grad Norm=4.4859\n",
      "Fold 4, Epoch 3: Train Loss=2.1405, Train Acc=17.26%, Val Loss=2.1420, Val Acc=17.19%, Grad Norm=3.0825\n",
      "Fold 4, Epoch 4: Train Loss=2.0616, Train Acc=21.80%, Val Loss=2.0977, Val Acc=18.62%, Grad Norm=2.7669\n",
      "Fold 4, Epoch 5: Train Loss=1.9840, Train Acc=25.95%, Val Loss=2.0118, Val Acc=24.44%, Grad Norm=2.7482\n",
      "Fold 4, Epoch 6: Train Loss=1.9295, Train Acc=28.54%, Val Loss=2.0070, Val Acc=25.12%, Grad Norm=2.6784\n",
      "Fold 4, Epoch 7: Train Loss=1.8969, Train Acc=29.97%, Val Loss=1.9708, Val Acc=26.08%, Grad Norm=2.6438\n",
      "Fold 4, Epoch 8: Train Loss=1.8701, Train Acc=31.17%, Val Loss=1.9370, Val Acc=27.77%, Grad Norm=2.7000\n",
      "Fold 4, Epoch 9: Train Loss=1.8479, Train Acc=32.19%, Val Loss=1.9245, Val Acc=28.12%, Grad Norm=2.8199\n",
      "Fold 4, Epoch 10: Train Loss=1.8193, Train Acc=33.60%, Val Loss=1.9176, Val Acc=28.75%, Grad Norm=2.9581\n",
      "Fold 4, Epoch 11: Train Loss=1.7747, Train Acc=35.37%, Val Loss=1.9125, Val Acc=28.77%, Grad Norm=3.1998\n",
      "Fold 4, Epoch 12: Train Loss=1.7549, Train Acc=36.15%, Val Loss=1.9107, Val Acc=28.76%, Grad Norm=3.5452\n",
      "Fold 4, Epoch 13: Train Loss=1.7354, Train Acc=37.24%, Val Loss=1.9030, Val Acc=29.15%, Grad Norm=3.8637\n",
      "Fold 4, Epoch 14: Train Loss=1.7214, Train Acc=37.85%, Val Loss=1.8944, Val Acc=28.88%, Grad Norm=4.1736\n",
      "Fold 4, Epoch 15: Train Loss=1.7069, Train Acc=38.40%, Val Loss=1.8897, Val Acc=29.80%, Grad Norm=4.4731\n",
      "Fold 4, Epoch 16: Train Loss=1.6933, Train Acc=39.07%, Val Loss=1.9056, Val Acc=29.02%, Grad Norm=4.7664\n",
      "Fold 4, Epoch 17: Train Loss=1.6805, Train Acc=39.47%, Val Loss=1.8978, Val Acc=29.90%, Grad Norm=5.0485\n",
      "Fold 4, Epoch 18: Train Loss=1.6651, Train Acc=40.16%, Val Loss=1.8950, Val Acc=29.86%, Grad Norm=5.3611\n",
      "Fold 4, Epoch 19: Train Loss=1.6484, Train Acc=40.87%, Val Loss=1.8994, Val Acc=29.65%, Grad Norm=5.7047\n",
      "Fold 4, Epoch 20: Train Loss=1.6331, Train Acc=41.56%, Val Loss=1.8993, Val Acc=30.13%, Grad Norm=6.0877\n",
      "Fold 4, Epoch 21: Train Loss=1.5906, Train Acc=43.36%, Val Loss=1.9195, Val Acc=29.52%, Grad Norm=6.5443\n",
      "Fold 4, Epoch 22: Train Loss=1.5675, Train Acc=44.29%, Val Loss=1.9146, Val Acc=29.95%, Grad Norm=7.2294\n",
      "Fold 4, Epoch 23: Train Loss=1.5489, Train Acc=45.03%, Val Loss=1.9184, Val Acc=29.99%, Grad Norm=7.8137\n",
      "Fold 4, Epoch 24: Train Loss=1.5310, Train Acc=45.90%, Val Loss=1.9307, Val Acc=30.02%, Grad Norm=8.3581\n",
      "Fold 4, Epoch 25: Train Loss=1.5138, Train Acc=46.47%, Val Loss=1.9169, Val Acc=30.08%, Grad Norm=8.9859\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=29.23%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2169, Train Acc=11.73%, Val Loss=2.5444, Val Acc=6.78%, Grad Norm=6.2233\n",
      "Fold 5, Epoch 2: Train Loss=2.2051, Train Acc=12.13%, Val Loss=2.2879, Val Acc=8.75%, Grad Norm=4.2804\n",
      "Fold 5, Epoch 3: Train Loss=2.1544, Train Acc=16.32%, Val Loss=2.2132, Val Acc=12.30%, Grad Norm=3.0512\n",
      "Fold 5, Epoch 4: Train Loss=2.0700, Train Acc=21.28%, Val Loss=2.0870, Val Acc=19.54%, Grad Norm=2.7682\n",
      "Fold 5, Epoch 5: Train Loss=1.9981, Train Acc=24.85%, Val Loss=2.0650, Val Acc=21.39%, Grad Norm=2.6487\n",
      "Fold 5, Epoch 6: Train Loss=1.9384, Train Acc=28.20%, Val Loss=1.9647, Val Acc=25.91%, Grad Norm=2.7506\n",
      "Fold 5, Epoch 7: Train Loss=1.8918, Train Acc=30.28%, Val Loss=1.9362, Val Acc=27.06%, Grad Norm=2.8124\n",
      "Fold 5, Epoch 8: Train Loss=1.8579, Train Acc=31.97%, Val Loss=1.8821, Val Acc=30.03%, Grad Norm=2.8683\n",
      "Fold 5, Epoch 9: Train Loss=1.8309, Train Acc=33.09%, Val Loss=1.8494, Val Acc=31.54%, Grad Norm=2.9556\n",
      "Fold 5, Epoch 10: Train Loss=1.8089, Train Acc=34.09%, Val Loss=1.8548, Val Acc=30.85%, Grad Norm=3.0564\n",
      "Fold 5, Epoch 11: Train Loss=1.7633, Train Acc=36.04%, Val Loss=1.8346, Val Acc=31.77%, Grad Norm=3.3004\n",
      "Fold 5, Epoch 12: Train Loss=1.7402, Train Acc=36.99%, Val Loss=1.8448, Val Acc=31.13%, Grad Norm=3.6441\n",
      "Fold 5, Epoch 13: Train Loss=1.7266, Train Acc=37.48%, Val Loss=1.8406, Val Acc=31.46%, Grad Norm=3.9691\n",
      "Fold 5, Epoch 14: Train Loss=1.7115, Train Acc=38.31%, Val Loss=1.8265, Val Acc=32.22%, Grad Norm=4.2447\n",
      "Fold 5, Epoch 15: Train Loss=1.6985, Train Acc=38.79%, Val Loss=1.8478, Val Acc=31.22%, Grad Norm=4.5439\n",
      "Fold 5, Epoch 16: Train Loss=1.6847, Train Acc=39.31%, Val Loss=1.8334, Val Acc=32.17%, Grad Norm=4.8449\n",
      "Fold 5, Epoch 17: Train Loss=1.6699, Train Acc=40.00%, Val Loss=1.8358, Val Acc=31.86%, Grad Norm=5.1201\n",
      "Fold 5, Epoch 18: Train Loss=1.6556, Train Acc=40.58%, Val Loss=1.8461, Val Acc=31.45%, Grad Norm=5.4620\n",
      "Fold 5, Epoch 19: Train Loss=1.6411, Train Acc=41.17%, Val Loss=1.8460, Val Acc=30.98%, Grad Norm=5.8093\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=29.18%\n",
      "\n",
      "SNR -10 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-25_15-13-06_LTE-V_XFR_FileBlock_SNR-10dB_fd655_group864_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-15 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 787, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 787\n",
      "[INFO] 训练 block 数: 590, 测试 block 数: 197\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2195, Train Acc=11.42%, Val Loss=2.2265, Val Acc=11.70%, Grad Norm=6.2534\n",
      "Fold 1, Epoch 2: Train Loss=2.2094, Train Acc=11.70%, Val Loss=2.2542, Val Acc=10.98%, Grad Norm=4.2935\n",
      "Fold 1, Epoch 3: Train Loss=2.2022, Train Acc=11.81%, Val Loss=2.2207, Val Acc=11.07%, Grad Norm=2.8719\n",
      "Fold 1, Epoch 4: Train Loss=2.1968, Train Acc=12.55%, Val Loss=2.2206, Val Acc=11.16%, Grad Norm=2.1811\n",
      "Fold 1, Epoch 5: Train Loss=2.1872, Train Acc=13.95%, Val Loss=2.2129, Val Acc=10.94%, Grad Norm=1.7747\n",
      "Fold 1, Epoch 6: Train Loss=2.1739, Train Acc=15.43%, Val Loss=2.2026, Val Acc=12.03%, Grad Norm=1.5910\n",
      "Fold 1, Epoch 7: Train Loss=2.1625, Train Acc=16.33%, Val Loss=2.2066, Val Acc=12.45%, Grad Norm=1.4927\n",
      "Fold 1, Epoch 8: Train Loss=2.1523, Train Acc=17.22%, Val Loss=2.1921, Val Acc=13.77%, Grad Norm=1.4679\n",
      "Fold 1, Epoch 9: Train Loss=2.1439, Train Acc=17.89%, Val Loss=2.1696, Val Acc=15.15%, Grad Norm=1.4981\n",
      "Fold 1, Epoch 10: Train Loss=2.1343, Train Acc=18.73%, Val Loss=2.1732, Val Acc=15.06%, Grad Norm=1.5691\n",
      "Fold 1, Epoch 11: Train Loss=2.1151, Train Acc=20.08%, Val Loss=2.1611, Val Acc=16.37%, Grad Norm=1.8030\n",
      "Fold 1, Epoch 12: Train Loss=2.1034, Train Acc=20.78%, Val Loss=2.1655, Val Acc=16.77%, Grad Norm=2.1009\n",
      "Fold 1, Epoch 13: Train Loss=2.0944, Train Acc=21.25%, Val Loss=2.1589, Val Acc=16.86%, Grad Norm=2.3389\n",
      "Fold 1, Epoch 14: Train Loss=2.0853, Train Acc=21.79%, Val Loss=2.1654, Val Acc=16.48%, Grad Norm=2.5993\n",
      "Fold 1, Epoch 15: Train Loss=2.0775, Train Acc=22.41%, Val Loss=2.1607, Val Acc=16.56%, Grad Norm=2.8434\n",
      "Fold 1, Epoch 16: Train Loss=2.0681, Train Acc=22.94%, Val Loss=2.1634, Val Acc=16.60%, Grad Norm=3.1093\n",
      "Fold 1, Epoch 17: Train Loss=2.0603, Train Acc=23.49%, Val Loss=2.1703, Val Acc=16.69%, Grad Norm=3.3763\n",
      "Fold 1, Epoch 18: Train Loss=2.0496, Train Acc=24.01%, Val Loss=2.1638, Val Acc=16.88%, Grad Norm=3.6832\n",
      "Fold 1, Epoch 19: Train Loss=2.0395, Train Acc=24.60%, Val Loss=2.1662, Val Acc=17.18%, Grad Norm=3.9970\n",
      "Fold 1, Epoch 20: Train Loss=2.0302, Train Acc=25.10%, Val Loss=2.1739, Val Acc=16.70%, Grad Norm=4.3359\n",
      "Fold 1, Epoch 21: Train Loss=1.9983, Train Acc=26.71%, Val Loss=2.1673, Val Acc=17.33%, Grad Norm=4.8955\n",
      "Fold 1, Epoch 22: Train Loss=1.9807, Train Acc=27.65%, Val Loss=2.1745, Val Acc=17.56%, Grad Norm=5.5734\n",
      "Fold 1, Epoch 23: Train Loss=1.9663, Train Acc=28.33%, Val Loss=2.1878, Val Acc=16.90%, Grad Norm=6.2202\n",
      "Fold 1, Epoch 24: Train Loss=1.9497, Train Acc=29.16%, Val Loss=2.1919, Val Acc=17.16%, Grad Norm=6.8873\n",
      "Fold 1, Epoch 25: Train Loss=1.9337, Train Acc=29.85%, Val Loss=2.1982, Val Acc=17.13%, Grad Norm=7.5621\n",
      "Fold 1, Epoch 26: Train Loss=1.9178, Train Acc=30.87%, Val Loss=2.2061, Val Acc=17.00%, Grad Norm=8.2667\n",
      "Fold 1, Epoch 27: Train Loss=1.9001, Train Acc=31.55%, Val Loss=2.2098, Val Acc=17.42%, Grad Norm=9.0313\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=17.55%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2201, Train Acc=11.25%, Val Loss=2.2290, Val Acc=12.66%, Grad Norm=6.2832\n",
      "Fold 2, Epoch 2: Train Loss=2.2106, Train Acc=11.52%, Val Loss=2.2044, Val Acc=12.52%, Grad Norm=4.5130\n",
      "Fold 2, Epoch 3: Train Loss=2.2033, Train Acc=11.75%, Val Loss=2.2133, Val Acc=12.49%, Grad Norm=2.9928\n",
      "Fold 2, Epoch 4: Train Loss=2.1972, Train Acc=12.45%, Val Loss=2.2235, Val Acc=10.19%, Grad Norm=2.2520\n",
      "Fold 2, Epoch 5: Train Loss=2.1857, Train Acc=14.18%, Val Loss=2.2256, Val Acc=11.58%, Grad Norm=1.8390\n",
      "Fold 2, Epoch 6: Train Loss=2.1729, Train Acc=15.44%, Val Loss=2.2073, Val Acc=12.52%, Grad Norm=1.5823\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=13.45%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2195, Train Acc=11.33%, Val Loss=2.2547, Val Acc=9.62%, Grad Norm=6.3592\n",
      "Fold 3, Epoch 2: Train Loss=2.2107, Train Acc=11.40%, Val Loss=2.2106, Val Acc=14.29%, Grad Norm=4.5822\n",
      "Fold 3, Epoch 3: Train Loss=2.2030, Train Acc=11.64%, Val Loss=2.2054, Val Acc=14.18%, Grad Norm=2.9139\n",
      "Fold 3, Epoch 4: Train Loss=2.1986, Train Acc=12.09%, Val Loss=2.2138, Val Acc=10.71%, Grad Norm=2.2574\n",
      "Fold 3, Epoch 5: Train Loss=2.1896, Train Acc=13.63%, Val Loss=2.1986, Val Acc=12.49%, Grad Norm=1.8285\n",
      "Fold 3, Epoch 6: Train Loss=2.1747, Train Acc=15.31%, Val Loss=2.2005, Val Acc=13.74%, Grad Norm=1.6378\n",
      "Fold 3, Epoch 7: Train Loss=2.1613, Train Acc=16.62%, Val Loss=2.1959, Val Acc=13.40%, Grad Norm=1.5256\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=14.47%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2185, Train Acc=11.57%, Val Loss=2.3035, Val Acc=7.68%, Grad Norm=6.3254\n",
      "Fold 4, Epoch 2: Train Loss=2.2096, Train Acc=11.39%, Val Loss=2.2330, Val Acc=12.22%, Grad Norm=4.4551\n",
      "Fold 4, Epoch 3: Train Loss=2.2014, Train Acc=11.96%, Val Loss=2.2617, Val Acc=9.94%, Grad Norm=2.9955\n",
      "Fold 4, Epoch 4: Train Loss=2.1946, Train Acc=12.96%, Val Loss=2.2256, Val Acc=10.74%, Grad Norm=2.3751\n",
      "Fold 4, Epoch 5: Train Loss=2.1828, Train Acc=14.41%, Val Loss=2.2190, Val Acc=11.92%, Grad Norm=1.8221\n",
      "Fold 4, Epoch 6: Train Loss=2.1696, Train Acc=15.85%, Val Loss=2.2038, Val Acc=11.86%, Grad Norm=1.6086\n",
      "Fold 4, Epoch 7: Train Loss=2.1588, Train Acc=16.87%, Val Loss=2.2100, Val Acc=11.94%, Grad Norm=1.5178\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=13.45%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2204, Train Acc=11.50%, Val Loss=2.2759, Val Acc=9.24%, Grad Norm=6.3192\n",
      "Fold 5, Epoch 2: Train Loss=2.2104, Train Acc=11.46%, Val Loss=2.2095, Val Acc=9.37%, Grad Norm=4.3442\n",
      "Fold 5, Epoch 3: Train Loss=2.2026, Train Acc=11.79%, Val Loss=2.2516, Val Acc=11.30%, Grad Norm=2.8976\n",
      "Fold 5, Epoch 4: Train Loss=2.1993, Train Acc=12.36%, Val Loss=2.2157, Val Acc=9.91%, Grad Norm=2.2826\n",
      "Fold 5, Epoch 5: Train Loss=2.1926, Train Acc=13.45%, Val Loss=2.2009, Val Acc=12.07%, Grad Norm=1.8273\n",
      "Fold 5, Epoch 6: Train Loss=2.1782, Train Acc=15.15%, Val Loss=2.2079, Val Acc=11.47%, Grad Norm=1.6158\n",
      "Fold 5, Epoch 7: Train Loss=2.1658, Train Acc=16.14%, Val Loss=2.1938, Val Acc=12.92%, Grad Norm=1.4551\n",
      "Fold 5, Epoch 8: Train Loss=2.1569, Train Acc=16.89%, Val Loss=2.1755, Val Acc=14.46%, Grad Norm=1.3947\n",
      "Fold 5, Epoch 9: Train Loss=2.1487, Train Acc=17.51%, Val Loss=2.1933, Val Acc=13.45%, Grad Norm=1.3953\n",
      "Fold 5, Epoch 10: Train Loss=2.1437, Train Acc=17.81%, Val Loss=2.1736, Val Acc=14.77%, Grad Norm=1.4161\n",
      "Fold 5, Epoch 11: Train Loss=2.1253, Train Acc=19.10%, Val Loss=2.1645, Val Acc=15.00%, Grad Norm=1.6438\n",
      "Fold 5, Epoch 12: Train Loss=2.1152, Train Acc=19.75%, Val Loss=2.1562, Val Acc=16.21%, Grad Norm=1.9356\n",
      "Fold 5, Epoch 13: Train Loss=2.1062, Train Acc=20.42%, Val Loss=2.1634, Val Acc=15.90%, Grad Norm=2.1767\n",
      "Fold 5, Epoch 14: Train Loss=2.0988, Train Acc=21.10%, Val Loss=2.1600, Val Acc=15.86%, Grad Norm=2.4116\n",
      "Fold 5, Epoch 15: Train Loss=2.0910, Train Acc=21.52%, Val Loss=2.1638, Val Acc=16.08%, Grad Norm=2.6449\n",
      "Fold 5, Epoch 16: Train Loss=2.0841, Train Acc=21.88%, Val Loss=2.1553, Val Acc=16.81%, Grad Norm=2.8751\n",
      "Fold 5, Epoch 17: Train Loss=2.0749, Train Acc=22.49%, Val Loss=2.1564, Val Acc=16.92%, Grad Norm=3.1506\n",
      "Fold 5, Epoch 18: Train Loss=2.0659, Train Acc=23.14%, Val Loss=2.1490, Val Acc=17.52%, Grad Norm=3.4186\n",
      "Fold 5, Epoch 19: Train Loss=2.0562, Train Acc=23.53%, Val Loss=2.1603, Val Acc=17.26%, Grad Norm=3.7291\n",
      "Fold 5, Epoch 20: Train Loss=2.0452, Train Acc=24.47%, Val Loss=2.1656, Val Acc=17.08%, Grad Norm=4.0419\n",
      "Fold 5, Epoch 21: Train Loss=2.0151, Train Acc=26.07%, Val Loss=2.1576, Val Acc=17.69%, Grad Norm=4.5755\n",
      "Fold 5, Epoch 22: Train Loss=1.9988, Train Acc=26.77%, Val Loss=2.1521, Val Acc=18.31%, Grad Norm=5.2309\n",
      "Fold 5, Epoch 23: Train Loss=1.9867, Train Acc=27.31%, Val Loss=2.1610, Val Acc=18.00%, Grad Norm=5.8426\n",
      "Fold 5, Epoch 24: Train Loss=1.9702, Train Acc=28.18%, Val Loss=2.1729, Val Acc=17.80%, Grad Norm=6.4568\n",
      "Fold 5, Epoch 25: Train Loss=1.9552, Train Acc=28.87%, Val Loss=2.1752, Val Acc=17.96%, Grad Norm=7.1147\n",
      "Fold 5, Epoch 26: Train Loss=1.9387, Train Acc=29.76%, Val Loss=2.1897, Val Acc=17.54%, Grad Norm=7.7799\n",
      "Fold 5, Epoch 27: Train Loss=1.9241, Train Acc=30.44%, Val Loss=2.1776, Val Acc=18.50%, Grad Norm=8.5262\n",
      "Fold 5, Epoch 28: Train Loss=1.9062, Train Acc=31.44%, Val Loss=2.1965, Val Acc=17.62%, Grad Norm=9.2626\n",
      "Fold 5, Epoch 29: Train Loss=1.8870, Train Acc=32.20%, Val Loss=2.2066, Val Acc=17.94%, Grad Norm=10.0597\n",
      "Fold 5, Epoch 30: Train Loss=1.8684, Train Acc=32.93%, Val Loss=2.2241, Val Acc=17.48%, Grad Norm=10.9563\n",
      "Fold 5, Epoch 31: Train Loss=1.8241, Train Acc=35.15%, Val Loss=2.2275, Val Acc=17.60%, Grad Norm=11.8128\n",
      "Fold 5, Epoch 32: Train Loss=1.8021, Train Acc=36.27%, Val Loss=2.2387, Val Acc=17.50%, Grad Norm=12.8368\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=17.06%\n",
      "\n",
      "SNR -15 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-25_15-49-18_LTE-V_XFR_FileBlock_SNR-15dB_fd655_group864_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-20 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 787, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 787\n",
      "[INFO] 训练 block 数: 590, 测试 block 数: 197\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2207, Train Acc=11.36%, Val Loss=2.3182, Val Acc=9.33%, Grad Norm=6.3087\n",
      "Fold 1, Epoch 2: Train Loss=2.2111, Train Acc=11.41%, Val Loss=2.2521, Val Acc=8.47%, Grad Norm=4.5577\n",
      "Fold 1, Epoch 3: Train Loss=2.2047, Train Acc=11.41%, Val Loss=2.2247, Val Acc=8.98%, Grad Norm=3.0287\n",
      "Fold 1, Epoch 4: Train Loss=2.2011, Train Acc=11.75%, Val Loss=2.2327, Val Acc=7.58%, Grad Norm=2.2993\n",
      "Fold 1, Epoch 5: Train Loss=2.1993, Train Acc=11.86%, Val Loss=2.2247, Val Acc=11.42%, Grad Norm=1.7915\n",
      "Fold 1, Epoch 6: Train Loss=2.1973, Train Acc=12.08%, Val Loss=2.2264, Val Acc=9.02%, Grad Norm=1.3677\n",
      "Fold 1, Epoch 7: Train Loss=2.1959, Train Acc=12.13%, Val Loss=2.2102, Val Acc=9.83%, Grad Norm=1.0827\n",
      "Fold 1, Epoch 8: Train Loss=2.1944, Train Acc=12.62%, Val Loss=2.2096, Val Acc=9.02%, Grad Norm=0.8829\n",
      "Fold 1, Epoch 9: Train Loss=2.1935, Train Acc=12.70%, Val Loss=2.2125, Val Acc=10.74%, Grad Norm=0.7370\n",
      "Fold 1, Epoch 10: Train Loss=2.1930, Train Acc=12.78%, Val Loss=2.2097, Val Acc=10.22%, Grad Norm=0.6578\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.65%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2201, Train Acc=11.45%, Val Loss=2.2035, Val Acc=13.54%, Grad Norm=6.3290\n",
      "Fold 2, Epoch 2: Train Loss=2.2112, Train Acc=11.33%, Val Loss=2.2084, Val Acc=11.15%, Grad Norm=4.4391\n",
      "Fold 2, Epoch 3: Train Loss=2.2040, Train Acc=11.55%, Val Loss=2.2178, Val Acc=9.99%, Grad Norm=2.9547\n",
      "Fold 2, Epoch 4: Train Loss=2.2020, Train Acc=11.43%, Val Loss=2.2073, Val Acc=8.65%, Grad Norm=2.2600\n",
      "Fold 2, Epoch 5: Train Loss=2.1996, Train Acc=11.77%, Val Loss=2.2302, Val Acc=11.01%, Grad Norm=1.7286\n",
      "Fold 2, Epoch 6: Train Loss=2.1981, Train Acc=11.84%, Val Loss=2.2127, Val Acc=8.82%, Grad Norm=1.3207\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2204, Train Acc=11.35%, Val Loss=2.2361, Val Acc=9.41%, Grad Norm=6.3387\n",
      "Fold 3, Epoch 2: Train Loss=2.2106, Train Acc=11.48%, Val Loss=2.2238, Val Acc=12.74%, Grad Norm=4.5594\n",
      "Fold 3, Epoch 3: Train Loss=2.2038, Train Acc=11.54%, Val Loss=2.2317, Val Acc=9.30%, Grad Norm=3.0109\n",
      "Fold 3, Epoch 4: Train Loss=2.2009, Train Acc=11.66%, Val Loss=2.2202, Val Acc=7.15%, Grad Norm=2.2540\n",
      "Fold 3, Epoch 5: Train Loss=2.1994, Train Acc=11.80%, Val Loss=2.2191, Val Acc=6.78%, Grad Norm=1.7090\n",
      "Fold 3, Epoch 6: Train Loss=2.1977, Train Acc=11.99%, Val Loss=2.2315, Val Acc=6.79%, Grad Norm=1.3364\n",
      "Fold 3, Epoch 7: Train Loss=2.1961, Train Acc=12.16%, Val Loss=2.2065, Val Acc=10.76%, Grad Norm=1.0374\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.23%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2198, Train Acc=11.34%, Val Loss=2.2406, Val Acc=10.94%, Grad Norm=6.3048\n",
      "Fold 4, Epoch 2: Train Loss=2.2099, Train Acc=11.49%, Val Loss=2.2550, Val Acc=10.18%, Grad Norm=4.5116\n",
      "Fold 4, Epoch 3: Train Loss=2.2025, Train Acc=11.72%, Val Loss=2.2541, Val Acc=10.76%, Grad Norm=3.0386\n",
      "Fold 4, Epoch 4: Train Loss=2.2002, Train Acc=11.77%, Val Loss=2.2277, Val Acc=11.53%, Grad Norm=2.2826\n",
      "Fold 4, Epoch 5: Train Loss=2.1983, Train Acc=11.98%, Val Loss=2.2389, Val Acc=9.51%, Grad Norm=1.7322\n",
      "Fold 4, Epoch 6: Train Loss=2.1965, Train Acc=12.17%, Val Loss=2.2318, Val Acc=7.59%, Grad Norm=1.3397\n",
      "Fold 4, Epoch 7: Train Loss=2.1948, Train Acc=12.50%, Val Loss=2.2321, Val Acc=9.70%, Grad Norm=1.0359\n",
      "Fold 4, Epoch 8: Train Loss=2.1936, Train Acc=12.61%, Val Loss=2.2144, Val Acc=10.25%, Grad Norm=0.8397\n",
      "Fold 4, Epoch 9: Train Loss=2.1930, Train Acc=12.79%, Val Loss=2.2170, Val Acc=8.64%, Grad Norm=0.6726\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.37%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2203, Train Acc=11.43%, Val Loss=2.1990, Val Acc=12.42%, Grad Norm=6.4594\n",
      "Fold 5, Epoch 2: Train Loss=2.2112, Train Acc=11.47%, Val Loss=2.2632, Val Acc=7.15%, Grad Norm=4.5231\n",
      "Fold 5, Epoch 3: Train Loss=2.2039, Train Acc=11.64%, Val Loss=2.2193, Val Acc=6.97%, Grad Norm=3.0044\n",
      "Fold 5, Epoch 4: Train Loss=2.2017, Train Acc=11.70%, Val Loss=2.2308, Val Acc=9.05%, Grad Norm=2.2675\n",
      "Fold 5, Epoch 5: Train Loss=2.1995, Train Acc=11.92%, Val Loss=2.2061, Val Acc=11.90%, Grad Norm=1.7382\n",
      "Fold 5, Epoch 6: Train Loss=2.1975, Train Acc=12.33%, Val Loss=2.2154, Val Acc=7.21%, Grad Norm=1.3201\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.20%\n",
      "\n",
      "SNR -20 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-25_16-14-57_LTE-V_XFR_FileBlock_SNR-20dB_fd655_group864_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-25 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 787, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 787\n",
      "[INFO] 训练 block 数: 590, 测试 block 数: 197\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2198, Train Acc=11.49%, Val Loss=2.2168, Val Acc=8.55%, Grad Norm=6.3786\n",
      "Fold 1, Epoch 2: Train Loss=2.2112, Train Acc=11.45%, Val Loss=2.2448, Val Acc=12.70%, Grad Norm=4.5043\n",
      "Fold 1, Epoch 3: Train Loss=2.2043, Train Acc=11.54%, Val Loss=2.2245, Val Acc=7.74%, Grad Norm=3.0407\n",
      "Fold 1, Epoch 4: Train Loss=2.2024, Train Acc=11.60%, Val Loss=2.2211, Val Acc=11.00%, Grad Norm=2.3191\n",
      "Fold 1, Epoch 5: Train Loss=2.2001, Train Acc=11.59%, Val Loss=2.1983, Val Acc=11.31%, Grad Norm=1.7427\n",
      "Fold 1, Epoch 6: Train Loss=2.1987, Train Acc=11.67%, Val Loss=2.2115, Val Acc=10.06%, Grad Norm=1.3198\n",
      "Fold 1, Epoch 7: Train Loss=2.1974, Train Acc=11.91%, Val Loss=2.2072, Val Acc=7.61%, Grad Norm=1.0061\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.12%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2197, Train Acc=11.42%, Val Loss=2.2233, Val Acc=12.55%, Grad Norm=6.3171\n",
      "Fold 2, Epoch 2: Train Loss=2.2111, Train Acc=11.43%, Val Loss=2.2209, Val Acc=8.87%, Grad Norm=4.5427\n",
      "Fold 2, Epoch 3: Train Loss=2.2042, Train Acc=11.49%, Val Loss=2.2387, Val Acc=8.88%, Grad Norm=3.0138\n",
      "Fold 2, Epoch 4: Train Loss=2.2016, Train Acc=11.49%, Val Loss=2.2071, Val Acc=10.14%, Grad Norm=2.2750\n",
      "Fold 2, Epoch 5: Train Loss=2.2005, Train Acc=11.68%, Val Loss=2.2061, Val Acc=12.44%, Grad Norm=1.7376\n",
      "Fold 2, Epoch 6: Train Loss=2.1990, Train Acc=11.61%, Val Loss=2.2157, Val Acc=8.47%, Grad Norm=1.3074\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2200, Train Acc=11.39%, Val Loss=2.2857, Val Acc=9.05%, Grad Norm=6.1851\n",
      "Fold 3, Epoch 2: Train Loss=2.2100, Train Acc=11.40%, Val Loss=2.2681, Val Acc=9.29%, Grad Norm=4.4344\n",
      "Fold 3, Epoch 3: Train Loss=2.2040, Train Acc=11.54%, Val Loss=2.2480, Val Acc=9.41%, Grad Norm=2.9139\n",
      "Fold 3, Epoch 4: Train Loss=2.2011, Train Acc=11.67%, Val Loss=2.2352, Val Acc=14.09%, Grad Norm=2.2240\n",
      "Fold 3, Epoch 5: Train Loss=2.1997, Train Acc=11.67%, Val Loss=2.2439, Val Acc=8.08%, Grad Norm=1.6783\n",
      "Fold 3, Epoch 6: Train Loss=2.1980, Train Acc=11.89%, Val Loss=2.2316, Val Acc=9.35%, Grad Norm=1.2745\n",
      "Fold 3, Epoch 7: Train Loss=2.1971, Train Acc=12.13%, Val Loss=2.2132, Val Acc=8.93%, Grad Norm=0.9609\n",
      "Fold 3, Epoch 8: Train Loss=2.1961, Train Acc=12.15%, Val Loss=2.2171, Val Acc=8.59%, Grad Norm=0.7501\n",
      "Fold 3, Epoch 9: Train Loss=2.1959, Train Acc=12.08%, Val Loss=2.2101, Val Acc=8.02%, Grad Norm=0.5843\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=10.87%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2194, Train Acc=11.29%, Val Loss=2.2932, Val Acc=9.71%, Grad Norm=6.2934\n",
      "Fold 4, Epoch 2: Train Loss=2.2105, Train Acc=11.41%, Val Loss=2.2861, Val Acc=8.47%, Grad Norm=4.4822\n",
      "Fold 4, Epoch 3: Train Loss=2.2034, Train Acc=11.59%, Val Loss=2.2307, Val Acc=10.34%, Grad Norm=2.9200\n",
      "Fold 4, Epoch 4: Train Loss=2.2002, Train Acc=11.69%, Val Loss=2.2051, Val Acc=11.23%, Grad Norm=2.2156\n",
      "Fold 4, Epoch 5: Train Loss=2.1987, Train Acc=11.83%, Val Loss=2.2301, Val Acc=7.64%, Grad Norm=1.7156\n",
      "Fold 4, Epoch 6: Train Loss=2.1975, Train Acc=11.77%, Val Loss=2.2309, Val Acc=8.45%, Grad Norm=1.3061\n",
      "Fold 4, Epoch 7: Train Loss=2.1959, Train Acc=12.23%, Val Loss=2.2145, Val Acc=9.05%, Grad Norm=0.9930\n",
      "Fold 4, Epoch 8: Train Loss=2.1953, Train Acc=12.15%, Val Loss=2.2217, Val Acc=9.32%, Grad Norm=0.7618\n",
      "Fold 4, Epoch 9: Train Loss=2.1949, Train Acc=12.26%, Val Loss=2.2206, Val Acc=7.98%, Grad Norm=0.6090\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.02%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2207, Train Acc=11.38%, Val Loss=2.2564, Val Acc=6.83%, Grad Norm=6.3052\n",
      "Fold 5, Epoch 2: Train Loss=2.2115, Train Acc=11.37%, Val Loss=2.2390, Val Acc=11.59%, Grad Norm=4.4126\n",
      "Fold 5, Epoch 3: Train Loss=2.2036, Train Acc=11.79%, Val Loss=2.2124, Val Acc=11.94%, Grad Norm=2.9365\n",
      "Fold 5, Epoch 4: Train Loss=2.2014, Train Acc=11.74%, Val Loss=2.2218, Val Acc=6.99%, Grad Norm=2.2532\n",
      "Fold 5, Epoch 5: Train Loss=2.2001, Train Acc=11.80%, Val Loss=2.1986, Val Acc=13.17%, Grad Norm=1.7073\n",
      "Fold 5, Epoch 6: Train Loss=2.1986, Train Acc=12.01%, Val Loss=2.2047, Val Acc=11.03%, Grad Norm=1.2801\n",
      "Fold 5, Epoch 7: Train Loss=2.1977, Train Acc=12.09%, Val Loss=2.1975, Val Acc=11.56%, Grad Norm=0.9818\n",
      "Fold 5, Epoch 8: Train Loss=2.1972, Train Acc=11.93%, Val Loss=2.2009, Val Acc=10.26%, Grad Norm=0.7372\n",
      "Fold 5, Epoch 9: Train Loss=2.1964, Train Acc=12.15%, Val Loss=2.2113, Val Acc=7.33%, Grad Norm=0.6126\n",
      "Fold 5, Epoch 10: Train Loss=2.1962, Train Acc=12.07%, Val Loss=2.2076, Val Acc=7.89%, Grad Norm=0.4731\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.03%\n",
      "\n",
      "SNR -25 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-25_16-27-30_LTE-V_XFR_FileBlock_SNR-25dB_fd655_group864_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-30 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 787, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 787\n",
      "[INFO] 训练 block 数: 590, 测试 block 数: 197\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2206, Train Acc=11.30%, Val Loss=2.2526, Val Acc=12.20%, Grad Norm=6.3418\n",
      "Fold 1, Epoch 2: Train Loss=2.2103, Train Acc=11.43%, Val Loss=2.2681, Val Acc=6.78%, Grad Norm=4.4615\n",
      "Fold 1, Epoch 3: Train Loss=2.2039, Train Acc=11.59%, Val Loss=2.2111, Val Acc=10.54%, Grad Norm=2.9425\n",
      "Fold 1, Epoch 4: Train Loss=2.2018, Train Acc=11.55%, Val Loss=2.2601, Val Acc=6.89%, Grad Norm=2.2497\n",
      "Fold 1, Epoch 5: Train Loss=2.2003, Train Acc=11.64%, Val Loss=2.2196, Val Acc=7.64%, Grad Norm=1.7148\n",
      "Fold 1, Epoch 6: Train Loss=2.1985, Train Acc=11.71%, Val Loss=2.2063, Val Acc=9.96%, Grad Norm=1.3236\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.17%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2200, Train Acc=11.35%, Val Loss=2.2278, Val Acc=9.54%, Grad Norm=6.3448\n",
      "Fold 2, Epoch 2: Train Loss=2.2112, Train Acc=11.34%, Val Loss=2.2161, Val Acc=8.86%, Grad Norm=4.4684\n",
      "Fold 2, Epoch 3: Train Loss=2.2043, Train Acc=11.52%, Val Loss=2.2145, Val Acc=11.02%, Grad Norm=2.9416\n",
      "Fold 2, Epoch 4: Train Loss=2.2015, Train Acc=11.46%, Val Loss=2.2001, Val Acc=13.01%, Grad Norm=2.1804\n",
      "Fold 2, Epoch 5: Train Loss=2.2001, Train Acc=11.69%, Val Loss=2.1994, Val Acc=11.25%, Grad Norm=1.6952\n",
      "Fold 2, Epoch 6: Train Loss=2.1991, Train Acc=11.72%, Val Loss=2.2097, Val Acc=10.03%, Grad Norm=1.2824\n",
      "Fold 2, Epoch 7: Train Loss=2.1977, Train Acc=11.88%, Val Loss=2.2060, Val Acc=8.87%, Grad Norm=0.9848\n",
      "Fold 2, Epoch 8: Train Loss=2.1972, Train Acc=11.84%, Val Loss=2.2049, Val Acc=8.60%, Grad Norm=0.7401\n",
      "Fold 2, Epoch 9: Train Loss=2.1967, Train Acc=11.93%, Val Loss=2.2045, Val Acc=8.64%, Grad Norm=0.5748\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.10%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2211, Train Acc=11.46%, Val Loss=2.2788, Val Acc=9.82%, Grad Norm=6.3294\n",
      "Fold 3, Epoch 2: Train Loss=2.2111, Train Acc=11.53%, Val Loss=2.2142, Val Acc=10.53%, Grad Norm=4.5147\n",
      "Fold 3, Epoch 3: Train Loss=2.2039, Train Acc=11.54%, Val Loss=2.2478, Val Acc=10.21%, Grad Norm=3.0439\n",
      "Fold 3, Epoch 4: Train Loss=2.2015, Train Acc=11.66%, Val Loss=2.2159, Val Acc=9.39%, Grad Norm=2.2931\n",
      "Fold 3, Epoch 5: Train Loss=2.1999, Train Acc=11.66%, Val Loss=2.2095, Val Acc=11.75%, Grad Norm=1.7877\n",
      "Fold 3, Epoch 6: Train Loss=2.1985, Train Acc=11.99%, Val Loss=2.2055, Val Acc=9.95%, Grad Norm=1.3417\n",
      "Fold 3, Epoch 7: Train Loss=2.1973, Train Acc=11.90%, Val Loss=2.2071, Val Acc=9.11%, Grad Norm=0.9963\n",
      "Fold 3, Epoch 8: Train Loss=2.1969, Train Acc=12.02%, Val Loss=2.2216, Val Acc=6.97%, Grad Norm=0.7952\n",
      "Fold 3, Epoch 9: Train Loss=2.1963, Train Acc=11.96%, Val Loss=2.2108, Val Acc=7.57%, Grad Norm=0.6010\n",
      "Fold 3, Epoch 10: Train Loss=2.1960, Train Acc=12.16%, Val Loss=2.2151, Val Acc=6.79%, Grad Norm=0.4756\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.18%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2190, Train Acc=11.54%, Val Loss=2.3494, Val Acc=7.63%, Grad Norm=6.4034\n",
      "Fold 4, Epoch 2: Train Loss=2.2100, Train Acc=11.57%, Val Loss=2.2097, Val Acc=13.56%, Grad Norm=4.5364\n",
      "Fold 4, Epoch 3: Train Loss=2.2036, Train Acc=11.68%, Val Loss=2.2332, Val Acc=8.90%, Grad Norm=3.0693\n",
      "Fold 4, Epoch 4: Train Loss=2.2010, Train Acc=11.66%, Val Loss=2.2404, Val Acc=8.06%, Grad Norm=2.2802\n",
      "Fold 4, Epoch 5: Train Loss=2.1996, Train Acc=11.45%, Val Loss=2.2294, Val Acc=9.06%, Grad Norm=1.7630\n",
      "Fold 4, Epoch 6: Train Loss=2.1977, Train Acc=11.66%, Val Loss=2.2199, Val Acc=9.46%, Grad Norm=1.3377\n",
      "Fold 4, Epoch 7: Train Loss=2.1966, Train Acc=11.79%, Val Loss=2.2218, Val Acc=8.72%, Grad Norm=1.0141\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.14%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2206, Train Acc=11.26%, Val Loss=2.2475, Val Acc=8.55%, Grad Norm=6.2548\n",
      "Fold 5, Epoch 2: Train Loss=2.2106, Train Acc=11.41%, Val Loss=2.2449, Val Acc=12.53%, Grad Norm=4.4803\n",
      "Fold 5, Epoch 3: Train Loss=2.2041, Train Acc=11.54%, Val Loss=2.2198, Val Acc=8.47%, Grad Norm=2.9592\n",
      "Fold 5, Epoch 4: Train Loss=2.2019, Train Acc=11.63%, Val Loss=2.2194, Val Acc=12.69%, Grad Norm=2.2552\n",
      "Fold 5, Epoch 5: Train Loss=2.2003, Train Acc=11.72%, Val Loss=2.2160, Val Acc=8.33%, Grad Norm=1.7350\n",
      "Fold 5, Epoch 6: Train Loss=2.1985, Train Acc=11.96%, Val Loss=2.2037, Val Acc=9.53%, Grad Norm=1.3535\n",
      "Fold 5, Epoch 7: Train Loss=2.1978, Train Acc=12.04%, Val Loss=2.2114, Val Acc=8.22%, Grad Norm=1.0434\n",
      "Fold 5, Epoch 8: Train Loss=2.1971, Train Acc=11.98%, Val Loss=2.2092, Val Acc=7.13%, Grad Norm=0.7582\n",
      "Fold 5, Epoch 9: Train Loss=2.1966, Train Acc=12.01%, Val Loss=2.2102, Val Acc=7.50%, Grad Norm=0.5911\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.24%\n",
      "\n",
      "SNR -30 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-25_16-40-56_LTE-V_XFR_FileBlock_SNR-30dB_fd655_group864_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-35 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 787, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 787\n",
      "[INFO] 训练 block 数: 590, 测试 block 数: 197\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2197, Train Acc=11.55%, Val Loss=2.2784, Val Acc=7.65%, Grad Norm=6.3505\n",
      "Fold 1, Epoch 2: Train Loss=2.2117, Train Acc=11.37%, Val Loss=2.2232, Val Acc=11.67%, Grad Norm=4.5233\n",
      "Fold 1, Epoch 3: Train Loss=2.2045, Train Acc=11.38%, Val Loss=2.2274, Val Acc=6.79%, Grad Norm=3.0756\n",
      "Fold 1, Epoch 4: Train Loss=2.2016, Train Acc=11.64%, Val Loss=2.2567, Val Acc=6.78%, Grad Norm=2.3266\n",
      "Fold 1, Epoch 5: Train Loss=2.2001, Train Acc=11.69%, Val Loss=2.2000, Val Acc=11.08%, Grad Norm=1.7929\n",
      "Fold 1, Epoch 6: Train Loss=2.1992, Train Acc=11.60%, Val Loss=2.2129, Val Acc=10.52%, Grad Norm=1.3442\n",
      "Fold 1, Epoch 7: Train Loss=2.1977, Train Acc=11.63%, Val Loss=2.2163, Val Acc=7.06%, Grad Norm=1.0586\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.19%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2202, Train Acc=11.35%, Val Loss=2.2150, Val Acc=11.42%, Grad Norm=6.2580\n",
      "Fold 2, Epoch 2: Train Loss=2.2110, Train Acc=11.31%, Val Loss=2.1988, Val Acc=10.44%, Grad Norm=4.4023\n",
      "Fold 2, Epoch 3: Train Loss=2.2044, Train Acc=11.43%, Val Loss=2.2289, Val Acc=10.40%, Grad Norm=2.9675\n",
      "Fold 2, Epoch 4: Train Loss=2.2018, Train Acc=11.52%, Val Loss=2.2172, Val Acc=12.43%, Grad Norm=2.2587\n",
      "Fold 2, Epoch 5: Train Loss=2.2004, Train Acc=11.71%, Val Loss=2.2230, Val Acc=8.68%, Grad Norm=1.6916\n",
      "Fold 2, Epoch 6: Train Loss=2.1991, Train Acc=11.51%, Val Loss=2.1995, Val Acc=9.57%, Grad Norm=1.2577\n",
      "Fold 2, Epoch 7: Train Loss=2.1978, Train Acc=11.80%, Val Loss=2.2164, Val Acc=8.56%, Grad Norm=0.9514\n",
      "Fold 2, Epoch 8: Train Loss=2.1974, Train Acc=11.73%, Val Loss=2.2154, Val Acc=8.46%, Grad Norm=0.7373\n",
      "Fold 2, Epoch 9: Train Loss=2.1971, Train Acc=11.79%, Val Loss=2.2066, Val Acc=9.45%, Grad Norm=0.5647\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.08%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2199, Train Acc=11.41%, Val Loss=2.2631, Val Acc=6.92%, Grad Norm=6.3450\n",
      "Fold 3, Epoch 2: Train Loss=2.2107, Train Acc=11.27%, Val Loss=2.2344, Val Acc=7.17%, Grad Norm=4.3989\n",
      "Fold 3, Epoch 3: Train Loss=2.2043, Train Acc=11.53%, Val Loss=2.2113, Val Acc=9.44%, Grad Norm=2.8858\n",
      "Fold 3, Epoch 4: Train Loss=2.2019, Train Acc=11.59%, Val Loss=2.2260, Val Acc=6.78%, Grad Norm=2.1917\n",
      "Fold 3, Epoch 5: Train Loss=2.1998, Train Acc=11.81%, Val Loss=2.2146, Val Acc=9.21%, Grad Norm=1.6868\n",
      "Fold 3, Epoch 6: Train Loss=2.1986, Train Acc=11.89%, Val Loss=2.2078, Val Acc=9.94%, Grad Norm=1.2488\n",
      "Fold 3, Epoch 7: Train Loss=2.1976, Train Acc=11.93%, Val Loss=2.2235, Val Acc=6.87%, Grad Norm=0.9376\n",
      "Fold 3, Epoch 8: Train Loss=2.1969, Train Acc=11.96%, Val Loss=2.2029, Val Acc=7.60%, Grad Norm=0.7231\n",
      "Fold 3, Epoch 9: Train Loss=2.1964, Train Acc=12.09%, Val Loss=2.2138, Val Acc=6.78%, Grad Norm=0.5675\n",
      "Fold 3, Epoch 10: Train Loss=2.1962, Train Acc=12.12%, Val Loss=2.2169, Val Acc=6.80%, Grad Norm=0.4422\n",
      "Fold 3, Epoch 11: Train Loss=2.1958, Train Acc=12.21%, Val Loss=2.2103, Val Acc=6.84%, Grad Norm=0.3963\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.12%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2191, Train Acc=11.40%, Val Loss=2.2550, Val Acc=7.63%, Grad Norm=6.3266\n",
      "Fold 4, Epoch 2: Train Loss=2.2099, Train Acc=11.44%, Val Loss=2.2480, Val Acc=10.90%, Grad Norm=4.5737\n",
      "Fold 4, Epoch 3: Train Loss=2.2027, Train Acc=11.84%, Val Loss=2.2598, Val Acc=10.94%, Grad Norm=3.0061\n",
      "Fold 4, Epoch 4: Train Loss=2.2002, Train Acc=11.92%, Val Loss=2.2036, Val Acc=11.43%, Grad Norm=2.2391\n",
      "Fold 4, Epoch 5: Train Loss=2.1990, Train Acc=11.58%, Val Loss=2.2316, Val Acc=7.63%, Grad Norm=1.6699\n",
      "Fold 4, Epoch 6: Train Loss=2.1975, Train Acc=11.88%, Val Loss=2.2217, Val Acc=8.46%, Grad Norm=1.2744\n",
      "Fold 4, Epoch 7: Train Loss=2.1966, Train Acc=11.86%, Val Loss=2.2257, Val Acc=7.62%, Grad Norm=0.9338\n",
      "Fold 4, Epoch 8: Train Loss=2.1958, Train Acc=11.97%, Val Loss=2.2188, Val Acc=7.95%, Grad Norm=0.6898\n",
      "Fold 4, Epoch 9: Train Loss=2.1955, Train Acc=11.96%, Val Loss=2.2144, Val Acc=7.70%, Grad Norm=0.5475\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=10.75%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2200, Train Acc=11.37%, Val Loss=2.3009, Val Acc=6.79%, Grad Norm=6.2709\n",
      "Fold 5, Epoch 2: Train Loss=2.2103, Train Acc=11.37%, Val Loss=2.2484, Val Acc=9.31%, Grad Norm=4.2643\n",
      "Fold 5, Epoch 3: Train Loss=2.2041, Train Acc=11.51%, Val Loss=2.2408, Val Acc=8.39%, Grad Norm=2.8556\n",
      "Fold 5, Epoch 4: Train Loss=2.2018, Train Acc=11.65%, Val Loss=2.2106, Val Acc=10.48%, Grad Norm=2.1859\n",
      "Fold 5, Epoch 5: Train Loss=2.2001, Train Acc=11.63%, Val Loss=2.2196, Val Acc=6.78%, Grad Norm=1.6572\n",
      "Fold 5, Epoch 6: Train Loss=2.1991, Train Acc=11.81%, Val Loss=2.2101, Val Acc=7.91%, Grad Norm=1.2693\n",
      "Fold 5, Epoch 7: Train Loss=2.1977, Train Acc=11.84%, Val Loss=2.2040, Val Acc=7.54%, Grad Norm=0.9298\n",
      "Fold 5, Epoch 8: Train Loss=2.1971, Train Acc=12.03%, Val Loss=2.2101, Val Acc=6.91%, Grad Norm=0.7200\n",
      "Fold 5, Epoch 9: Train Loss=2.1965, Train Acc=12.23%, Val Loss=2.2066, Val Acc=9.37%, Grad Norm=0.5743\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.26%\n",
      "\n",
      "SNR -35 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-25_16-54-25_LTE-V_XFR_FileBlock_SNR-35dB_fd655_group864_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-40 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 787, 每 block 样本数: 256, 每样本长度: 256\n",
      "[INFO] 总 block 数: 787\n",
      "[INFO] 训练 block 数: 590, 测试 block 数: 197\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2204, Train Acc=11.30%, Val Loss=2.3610, Val Acc=6.78%, Grad Norm=6.3675\n",
      "Fold 1, Epoch 2: Train Loss=2.2110, Train Acc=11.43%, Val Loss=2.2688, Val Acc=11.85%, Grad Norm=4.4338\n",
      "Fold 1, Epoch 3: Train Loss=2.2039, Train Acc=11.50%, Val Loss=2.2503, Val Acc=8.43%, Grad Norm=2.9286\n",
      "Fold 1, Epoch 4: Train Loss=2.2019, Train Acc=11.47%, Val Loss=2.2242, Val Acc=8.52%, Grad Norm=2.2243\n",
      "Fold 1, Epoch 5: Train Loss=2.2002, Train Acc=11.51%, Val Loss=2.2168, Val Acc=8.52%, Grad Norm=1.6829\n",
      "Fold 1, Epoch 6: Train Loss=2.1990, Train Acc=11.80%, Val Loss=2.2035, Val Acc=8.94%, Grad Norm=1.2621\n",
      "Fold 1, Epoch 7: Train Loss=2.1978, Train Acc=11.70%, Val Loss=2.2056, Val Acc=8.02%, Grad Norm=0.9489\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.13%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2203, Train Acc=11.30%, Val Loss=2.2741, Val Acc=9.13%, Grad Norm=6.3515\n",
      "Fold 2, Epoch 2: Train Loss=2.2111, Train Acc=11.20%, Val Loss=2.2203, Val Acc=12.70%, Grad Norm=4.4816\n",
      "Fold 2, Epoch 3: Train Loss=2.2043, Train Acc=11.56%, Val Loss=2.2158, Val Acc=8.58%, Grad Norm=2.8994\n",
      "Fold 2, Epoch 4: Train Loss=2.2021, Train Acc=11.53%, Val Loss=2.2052, Val Acc=13.55%, Grad Norm=2.1738\n",
      "Fold 2, Epoch 5: Train Loss=2.2001, Train Acc=11.56%, Val Loss=2.2158, Val Acc=8.47%, Grad Norm=1.6554\n",
      "Fold 2, Epoch 6: Train Loss=2.1990, Train Acc=11.51%, Val Loss=2.2145, Val Acc=8.47%, Grad Norm=1.2653\n",
      "Fold 2, Epoch 7: Train Loss=2.1980, Train Acc=11.71%, Val Loss=2.2109, Val Acc=9.85%, Grad Norm=0.9156\n",
      "Fold 2, Epoch 8: Train Loss=2.1975, Train Acc=11.74%, Val Loss=2.2051, Val Acc=8.47%, Grad Norm=0.6940\n",
      "Fold 2, Epoch 9: Train Loss=2.1969, Train Acc=11.72%, Val Loss=2.2080, Val Acc=9.39%, Grad Norm=0.5440\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.03%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2195, Train Acc=11.42%, Val Loss=2.2194, Val Acc=9.80%, Grad Norm=6.3644\n",
      "Fold 3, Epoch 2: Train Loss=2.2112, Train Acc=11.49%, Val Loss=2.1930, Val Acc=12.87%, Grad Norm=4.4903\n",
      "Fold 3, Epoch 3: Train Loss=2.2048, Train Acc=11.52%, Val Loss=2.2096, Val Acc=12.61%, Grad Norm=2.9964\n",
      "Fold 3, Epoch 4: Train Loss=2.2014, Train Acc=11.81%, Val Loss=2.2044, Val Acc=10.39%, Grad Norm=2.2720\n",
      "Fold 3, Epoch 5: Train Loss=2.1998, Train Acc=11.70%, Val Loss=2.2121, Val Acc=7.88%, Grad Norm=1.7073\n",
      "Fold 3, Epoch 6: Train Loss=2.1984, Train Acc=11.92%, Val Loss=2.2278, Val Acc=6.79%, Grad Norm=1.2713\n",
      "Fold 3, Epoch 7: Train Loss=2.1976, Train Acc=11.86%, Val Loss=2.2065, Val Acc=7.60%, Grad Norm=0.9448\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.08%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2203, Train Acc=11.33%, Val Loss=2.2226, Val Acc=13.56%, Grad Norm=6.3932\n",
      "Fold 4, Epoch 2: Train Loss=2.2107, Train Acc=11.49%, Val Loss=2.2549, Val Acc=8.09%, Grad Norm=4.6737\n",
      "Fold 4, Epoch 3: Train Loss=2.2035, Train Acc=11.70%, Val Loss=2.2666, Val Acc=7.62%, Grad Norm=3.0927\n",
      "Fold 4, Epoch 4: Train Loss=2.2004, Train Acc=11.73%, Val Loss=2.2309, Val Acc=11.28%, Grad Norm=2.3114\n",
      "Fold 4, Epoch 5: Train Loss=2.1994, Train Acc=11.68%, Val Loss=2.2591, Val Acc=8.39%, Grad Norm=1.7818\n",
      "Fold 4, Epoch 6: Train Loss=2.1976, Train Acc=11.86%, Val Loss=2.2003, Val Acc=9.71%, Grad Norm=1.3472\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.08%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2214, Train Acc=11.07%, Val Loss=2.2281, Val Acc=10.90%, Grad Norm=6.2867\n",
      "Fold 5, Epoch 2: Train Loss=2.2111, Train Acc=11.36%, Val Loss=2.2001, Val Acc=11.74%, Grad Norm=4.4019\n",
      "Fold 5, Epoch 3: Train Loss=2.2036, Train Acc=11.62%, Val Loss=2.2386, Val Acc=11.83%, Grad Norm=2.9610\n",
      "Fold 5, Epoch 4: Train Loss=2.2020, Train Acc=11.76%, Val Loss=2.2032, Val Acc=11.89%, Grad Norm=2.2759\n",
      "Fold 5, Epoch 5: Train Loss=2.2003, Train Acc=11.78%, Val Loss=2.2141, Val Acc=10.69%, Grad Norm=1.7498\n",
      "Fold 5, Epoch 6: Train Loss=2.1989, Train Acc=11.83%, Val Loss=2.2210, Val Acc=8.47%, Grad Norm=1.2882\n",
      "Fold 5, Epoch 7: Train Loss=2.1977, Train Acc=11.88%, Val Loss=2.2127, Val Acc=8.45%, Grad Norm=0.9793\n",
      "Fold 5, Epoch 8: Train Loss=2.1969, Train Acc=12.04%, Val Loss=2.2031, Val Acc=8.64%, Grad Norm=0.7476\n",
      "Fold 5, Epoch 9: Train Loss=2.1967, Train Acc=12.10%, Val Loss=2.2069, Val Acc=7.25%, Grad Norm=0.5640\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.01%\n",
      "\n",
      "SNR -40 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-25_17-09-13_LTE-V_XFR_FileBlock_SNR-40dB_fd655_group864_ResNet\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqoAAAHTCAYAAADvQDr+AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAaFpJREFUeJzt3XlYVOX7BvB7Zhj2HURQUcE1c8Ed930ptzTLNUuzTctKM7P6qvwsK0uszKw0NRcys8zcd3PXxBVxA9kUFJB9mWGW9/cHMDmCMCjMGeD+XBeXzjlnznnmieD2Pee8RyaEECAiIiIisjByqQsgIiIiIioOgyoRERERWSQGVSIiIiKySAyqRERERGSRGFSJiIiIyCIxqBIRERGRRWJQJSIiIiKLxKBKRERERBaJQZWIqBqIj483en358mWJKiEiMh2DKhFRBcjLyyvT9jdu3MD169eNlmm12hLfM2HCBOzfvx+JiYn4888/H7rd5cuX4e/vjyNHjgAAYmJi0LZtWyxcuLBMNRaaM2cOvvjiCwDAvHnzEBQU9Ej70el06N+/PzZu3Fjm9+bm5iIoKAinT59+pGMTUeXAoEpEVM5u3rwJPz8/rF+/HgBw/vx57Nq1y+jr6NGjRu+ZP38+5s2bZ3h96NAhBAQE4M6dOw89zqFDh3Dp0iUcPHgQzz33HDZv3lzsdsHBwejZsye6desGAKhXrx6+++47xMXFGbYRQkCtVkOtVhuW7dy5EyEhIUb70mq1WLp0Ka5cuQIAaNWqFebPn4/Vq1cXe+zIyEjcuHED0dHRiI6ORmRkJGJiYgAAISEhOHz4MNq2bfvQzwgA2dnZ0Gg0Rsvs7OywZs0a/PTTT0bLCz9HTk5OifskokpCEBGZSV5envj4449FnTp1hJ2dnRg4cKCIjY01rAcgOnXqZPSeHj16iB49egghhHjxxRcFAAFAyOVy4e/vL/73v/+J3Nxcc36MUmm1WjF79mwhk8nEkiVLxLhx40Tt2rVFx44dRceOHYW/v79o06aN0Ov1IiEhQeTl5YmXX35ZvPjii4Z9ZGVlGbZXqVTFHsff318sWbJECCHEu+++K1q2bCm0Wq3RNhcvXhQKhcLQt9K+pk6danjv4sWLhVKpFFu2bDEsW7t2rQAgzp49a1i2fPlyERERIfR6vVCr1UKtVhvW9enTRzg5OQmFQiGsra2Fo6OjGDZsmLh3757w8fERDg4OwsXFRTg7OwsAYt68eUU+Z48ePYRcLhfOzs7Cw8NDeHh4CBcXF8Pf7/9ydHQUcrncqJdEVHkxqBKR2cyaNUvUrl1b/P7772Lr1q3C399f9OzZ07C+MCzdH4IeDKoNGjQQ//77rzh8+LD45JNPhI2NjZg0aZK5P4pJtm/fLpKSksSkSZPE3LlzDctXrVplCKAAxJkzZ4oEVSGEuHv3rggMDBTXr18XsbGxIjExUaSmphq+mjZtKhYuXChSU1PF7du3xcWLF41Cok6nEx07dhQ1a9Y02u+lS5cEABEVFVXqZ/j000+FjY2N2Ldvn9Dr9aJly5alht2PPvqoyH7q1q0rfvzxRyGEEHq9XgwdOlS0a9dO5OTkCCGE+PXXX4W3t7fIysoq8l61Wi3i4+ONlo0YMUKMHj1a6HQ6odPpjD7zsWPHRF5eXqmfjYgsn5WZBm6JiLBy5Up88MEHGDlyJABArVZj5MiRiI6ORv369Q3bLVmyBCtXrix2H7a2tmjXrh0AoFu3bkhLS8OSJUuwbNkyWFtbV/hnMIVWq4VcLsfTTz8NAJDJZACARYsWISwsDD169AAAQ702NjbF7sfLywsnTpxAWFgYGjduXOw277//Pt5//33D63PnziEgIAAA8NVXX+HUqVPw8vJCWlqaYZvMzEwAQEZGhmG5VquFtbU1nJ2djfb/4YcfIiEhAenp6Vi7di0uXrwIX19fXLx4EQCwe/dujB49GqmpqdBoNNBoNHBwcDDax61btxAbG4vevXsDAD755BMcO3YMZ8+ehZ2dHfR6PebPn48PPvigyHsBYNOmTXj11VexdOlSvPjii1izZg1CQ0Oxa9cuxMbGol+/fvjhhx/QpUsXTJo0CTt27MDZs2fh7+9fbM+IqPJgUCUis9DpdEhLS0NiYqJh2YABA3DkyBF4eHgYltWuXRu//vorvvzyS6PlD9O+fXuo1WokJyejVq1aFVJ7WX3wwQe4ePEifv31V6PPkJOTgxs3bhiCamGAfdDt27exY8cOKJVKuLi4YNiwYcjMzISNjQ2USqVhu6FDh6JZs2b4/PPPodfrkZubCzs7OwDApUuXMGfOHHTp0gXHjx83+oeAXq8HAHTt2hVyef6tChqNBh999BE+/PDDIvUsWbIEt2/fRkBAAJydnSGXy+Hq6goAhmDp4uICvV4PtVoNe3t7o/dv3rwZLVu2RMOGDQEA7777Llq1aoV69erBw8MDWq0WmZmZmD9/PubPn4/U1FT89ttvhn/QjB07FlqtFqtWrUJqaipmzJiBCRMmYMOGDQgJCUFAQAA6deqEoKAgnDlzBmfOnGFIJaoieDMVEZmFQqHAwIEDsWjRIsydOxcZGRlwdHRE165d4eTkZNjupZdeAgCsWLHCpP3euXMHMpnMpFALAH5+fvjss8+Mlj3//POGUAQAV69eRf/+/eHi4gIvLy9MmTKlTHfxT506FXfu3EFgYCCSkpIMy62srIw+68PcuXMHy5cvx2effYaPPvoIcrkctra2UKvV0Ov10Gg0yM3NRe3atZGQkAAAkMvlcHBwMARPtVqNCRMmICgoCDVq1DDczBQdHY3du3cDAI4ePWpYFhsbi1dfffWhNUVHR8PV1RVvv/02YmJiIJPJIJPJMGTIEMPxrays4ODggKysLMP79Ho9li1bBpVKhfHjx2P8+PE4fvw4OnbsCIVCgeTkZGzbtg2+vr5ITk5GcnIyfHx8ioyOT5gwAQcOHECzZs0wYMAAuLq6YuHChWjdujV++eUX2NvbY86cOThw4IAhEBNR5cegSkRms2rVKvTp0wf/93//h3r16mHhwoWG0b1Cnp6eGDt2LJYtWwadTvfQfQkh8O+//+Krr77CU0899dDT5w96/vnnsW3bNsNrjUaDPXv2YPTo0YZlo0ePRlpaGjZv3owlS5Zg06ZNCA4ONvlz+vn54dChQ3jhhRdQo0YNo3W2tralvr9t27Y4ffo0Zs+ebQhsR48ehZOTExQKBaytrdGvXz/UqVMH0dHRxe6jXbt2+Omnn6BQKJCUlIT69esbvgYMGAAgf0T1/uXjxo0rdl/R0dHo0qULLl68CC8vL/j6+iI1NRWpqanYsGEDACAlJQVJSUmIiYkxOn1fOENAt27dMHDgQJw/fx5RUVGl9uDBoJqUlITnnnsOHTp0wKZNm3DlyhUMGzYMa9euRe/evfHNN99g37596NatG44dO1bq/omocmBQJSKz8fDwwK5du7B//340a9YMs2bNwsiRIyGEMNpu2rRpiImJwdatW4vs4/Lly5DJZJDL5ejQoQPq1q1r8ugrkB9CT548iXv37gEAjhw5Ap1Oh0GDBhm2iYqKQt++fdG7d2+MGjUK27dvx8CBA8v0Wd3d3fH2228D+O9Ue3JyMlxcXIyWlcbKKv8Krfbt2+PKlSto164d/u///g9r165FkyZNEB4eXuL71Wq1ySOqv/zyC7Kzs43eL4RAt27dsGHDBsNlBYWn/l1dXQ2h1M3NDZ6enqhbt67hkobTp09j1qxZsLe3R2BgIMaPHw9vb2/Y2NhAp9NBp9PB1dUVTz31FGJjYw37vP/hBHq9Hj///DOaN28OW1tbHDp0CB06dEB0dDTat2+PuXPnIjs7Gx988AH8/PwwcuRI9OjRA99//71J/SUiy8agSkRm17t3bxw9ehSzZs3C5s2bi0xW36pVK3Tv3h1Lliwp8t4GDRrg3Llzhput5syZAx8fH5OP3bp1azRs2BA7duwAAGzbtg1Dhw41hDAAmDJlChYuXIiBAwdi7ty5AGC4QaksXnnlFcyaNcswN2lCQoLhOtr75ys1hYODA5o2bQobGxvUqFEDfn5+aNu2LZKTk4s8KOB+qampRUZUO3fuDADo3r270fJ69eoZRkgLHT16FLdu3TKa61Sv1yMtLQ1paWmGYJuamop79+4hPj7e8A8Pf39/LFy40DB/6/1UKhUUCgXS0tKwc+dO1K1b17DP+6811mg0+Omnn/B///d/WLduHRQKBby9vfHUU0/hs88+Q1xcHIKCgtC6dWscPHgQX375JVasWGG4DpiIKjcGVSIyi+3btyMgIADp6ekA8m8kWrBgAZydnXHu3Lki27/99ts4cOAAbty4YbTc1tYWAQEBmDhxItq3b49PPvmkzLWMGjXKcPp/27ZtGDVqlNH6zz77DKdOnUKfPn1w6tQpdOjQodjQXJKkpCRs2bIFTZs2xbPPPovevXvj+PHjqFu3Ljp16oQPP/zQcGmDKde/3n/XfiE/Pz+4u7vj+PHjRdYJIaBSqdCnTx+cO3fOEALT0tIwZswYvPDCC0bL0tPTkZ2dbXQJBAAsX74cXbt2RaNGjQzLbt26ZQi3L7/8MgAYgm69evUM4dXT0xNvvvlmsZ8nMzMTer0egwcPxqxZs5CYmIjBgwdj8ODBhtHuws+xf/9+TJ48GQAwZMgQ7Nu3D4sXL4aHhwcCAgIwYsQIHDt2DG+99RYA4MUXX0SjRo2KPCSAiCofBlUiMgtXV1dcuHAB58+fNyzLzs6GSqVCvXr1imw/bNgw1KtXr8gz6u83Z84cHDlyBIcPHy5TLaNGjcLu3bsRHh6OpKQko9P6t27dwjvvvIPmzZtj5syZ2LVrF8aOHVumywsAYPXq1ahRowbGjRuHESNGwMvLCzExMXjiiScQERGBUaNGGUZVSwuq69atQ0BAgNF2Go0GOp0OPXv2xM6dOwEAd+/eNay/d+8e7Ozs4OXlhYCAAMPNTzKZDL/++ivWrl1rtEwmk0GpVMLR0REqlQpA/qNWN2zYgEmTJhnVc//oZ+EIbHp6OrKysqDRaODo6FjkM7zyyiuQyWTYv38/gPzRZU9PT2zbtg1ffPEFvLy8sG3bNmzbts3oxrh33nkHTk5OsLKyKlLvtWvXMHPmTMOlIIXL5XI5bGxs8PPPP5v834uILBODKhGZRceOHREQEIDJkyfjjz/+wN69e/H888/D3d3d6I77QgqFAlOmTClxn4MHD0abNm3KPKr65JNPwtfXF++99x6eeeYZoxt3XF1dsWbNGrz77rs4fPgw/v77bxw7dqxM0x3l5OTg66+/xjvvvGPY91dffYUWLVpg9+7dGDlyJC5evAh7e3sIIRAYGFjsfnQ6Ha5evYpJkybho48+Qnx8PNLS0rB8+XL4+PggOjraMDp87949zJ49GwsWLACQP5op8h/qAiEEbt++jRYtWuDZZ5/FhAkT8PLLL2PatGmoX78+Dh8+bLRt4Q1f8+fPh729fZER54cRQkCr1Rb7+NLly5dDCIE+ffpAo9EgNDQUbdq0KXWfX3/9NbKzs6HVao1qFEKgSZMm+PLLL4ss1+l0UKlUhtFeIqq8GFSJyCysrKywfft2tGvXDlOmTMHo0aMhl8tx8OBBuLm5FfueV155pcicnA+aM2cO9u7di9OnT5epnlGjRmHnzp1FTnU7Ojpi+/btCA8Px5AhQzBhwgS0bt0aS5cuNXnfn3/+OdLS0gynq8PCwvDLL7/ggw8+QFBQENq0aYPx48cbjZAWN8PBwYMHkZeXh5CQEMTHx8PPzw8qlQq9e/fGmjVr4Ovri+HDh8PDwwOzZs3C8ePHERsba7SP8PBwzJ49G02aNMGTTz5pGEkFgG+++QbTpk0z3Dj2888/G6a70uv1uHbtGiZOnGj030AIUez0VIUjmUqlEu7u7kY13P/ZunfvDi8vL/zyyy/F/gOl8NiFbG1tYW9vD4VCUXrjCxSOqN4/5ywRVVLmewgWEVHVp9frRZ8+fQyPdU1OThb+/v4iMDBQ6PV6IYQQV65cEdbW1mLz5s0iIyNDTJo0Sbi7u4tp06YZ7Wv58uVi8+bNQoj8x6mePHmy2GP+9ddfQiaTCQBi27ZtQggh4uPjRbNmzQQAUbNmTbF69WrD9uPHjxcTJkwwvI6MjBTjxo0TVlZWQi6XizNnzggh8h9HmpGRYXSsxYsXizp16oiEhIQiX/Hx8SIuLk5ERkYavadfv35Gx3/99ddFvXr1RG5urhBCiCNHjoh69eqJnJwcMXfuXCGTycShQ4dK7XXDhg3FF198Uep2RFR58clURETlSCaTYc+ePYYbgjQaDfr06YOPPvrIMJLZtGlTnD59Gq1atQIA5ObmYvz48Zg9e7bRvgpHZIH8x6l6eXkVe8xhw4Zh69atiI6ONkyz5ePjg6effhpvvPEGXnrpJaPrRtVqtdGsA/7+/li3bh2+/PJLHD161HCHv1wuL/KAArVabbjz3lR79uwxev3SSy9h9OjRhksM2rZti/3798POzg7Xr1/H3Llz0bVr11L3q1KpDNfTElHVJBPigQkMiYiIiIgsAK9RJSIiIiKLxKBKRERERBaJQZWIiIiILBKDKhERERFZpCp1179er0d8fDycnJwMd9cSERERkeUQQiAzMxO1atWCXF7ymGmVCqrx8fHw9fWVugwiIiIiKkVcXBzq1KlT4jZVKqgWzvcXFxcHZ2fnCj+eRqPBnj170L9/fz4BxYzYd2mw79Jg36XBvkuDfZeGufuekZEBX1/fIvM0F6dKBdXC0/3Ozs5mC6r29vZwdnbm/1BmxL5Lg32XBvsuDfZdGuy7NKTquymXafJmKiIiIiKySAyqRERERGSRGFSJiIiIyCIxqBIRERGRRWJQJSIiIiKLxKBKRERERBaJQZWIiIiILBKDKhERERFZJAZVIiIiIrJIDKpERERE1ZROL3AqKgWhyTKcikqBTi+kLslIlXqEKhERERGZZldYAoK2hiMhXQVAgTU3zsDHxRZzhzTDwOY+UpcHgCOqRERERNXOrrAEvLHubEFI/c+ddBXeWHcWu8ISJKrMGIMqERERUTWi0wsEbQ1HcSf5C5cFbQ23iMsAeOqfiIiI6DHo9AKno1KQmKmCl5MtOvi5QyGXSV0WAEAIgSy1Fmk5GqTlaJCak4fTUfeKjKQavQdAQroKp6NS0KmBh/mKLQaDKhEREdEjMr7OM19FXeep0ugMYTM/eOYhteB1eq4Gqdn5r9Nz8/9MK9hO+4gjo4mZDw+z5sKgSkRERPQICq/zfDAGFl7nuWx8m2LDqlanR1quxihsFobK1Jy8gnV5SM3W/Pf3nDyoNPpHrtVWKYebvTVc7JSQy2QIT8go9T1eTraPfLzywqBKREREVEamXOc5feMF7LiUgLRcrVEQzVRpH/m4VnIZXO2VcLW3hqtd/p9u9kq4OeSHULeC1y72hX+3hqu9ErZKhVHtXb84gDvpqmLrlwHwdsm/hEFqDKpEREREZaDS6LDhdGyJ13kCQE6eDn9fePjd8862VnBzsDaETrfCAFoQMov709HGCjLZ413/qpDLMHdIM7yx7ixkgFFYLdzz3CHNLOI6WwZVIiIiohLcSVfhbGwqQmPyvy7Hp0OjM+26z2cCaqFroxpFQqiLnVLSIDiwuQ+WjW9T5PpabwubR5VBlYiIiKiARqfHlYQMnI1JRWhsGs7GpOJ2Wm6R7ZxtlchQaUrd36j2dSW/c/5hBjb3Qb9m3jgRkYg9R06hf7eO6NTQyyJGUgsxqBIREVG1lZKdVxBKU3E2JhUXbqUVuWlJLgOe8HFG23puaFPXDW3rucHHxRbdFh6sFNd5lkQhl6GjnzvuXRHoaEHTahViUCUiIqJqQacXuJGYibMxaQiNScXZ2FREJWcX2c7FTok2dV3zg2k9N7Sq4woHm6KRqbJc51mZMagSERFRlZSh0uB87H+h9HxsGjLVRe+4b+jliLYFI6Vt6rnB39MBchMCZmW5zrMyY1AlIiIiyen0AqeiUhCaLINHVEqZr5UUQiD6Xo7hhqdzsam4djcT4oHz8g7WCgTUdUWbuvmhtI2vG1zslY9cd+F1npb6ZKrKjkGViIiIJGX8dCcF1tw4U+rTnXLzdLhwKw1nC64tPRubhpTsvCLb1XW3N4yUtqnriiY1nWClkJdr/Qq5zGJvmKrsGFSJiIhIMqY83WnAk96IT1fln8IvOI0fHp9R5NGg1lZytKztcl8wdUMNJxvzfRgqdwyqREREJAlTnu70zm/n4WKrxN1MdZFtajrboF09d7QuuPHpyVousLYq39FSkhaDKhEREUnidFRKqU93Umn0UGnUUMhleLKWs+Ha0rb13FDLxfaxn9JElo1BlYiIiCSRmFFySC00rXdDvNGzIeysFaVvTFUKgyoRERGZVYZKgz9Db2H5kZsmbd+pgSdDajXFoEpERERmEXY7HetOxmDL+XjkanSlbl9Znu5EFYdBlYiIiCqMSqPD1gvxWHcqFhfi0gzLG3k5YnxgPTjZWmHGxgsA+HQnKopBlYiIiMpdVHI21p+Mwe+ht5CeqwEAKBUyDGzug/Ed66KDn7vhRih7awWf7kTFYlAlIiKicqHV6bHvyl2sOxmLoxHJhuW1Xe0wtmNdPN/Ot9h5TQuf7nQiIhF7jpxC/24dy/xkKqqaGFSJiIjosdxJV+HX07HY8G8s7mbkz3cqkwG9mnhhfGBd9GhceuhUyGXo6OeOe1cEOvIRpFSAQZWIiIjKTK8XOBaZjHUnY7DvSiJ0BU+J8nS0xvPtfDGmQ134uttLXCVVdgyqREREZLLU7DxsCr2F9adiEH0vx7C8g587xgfWw8Anvfl0KCo3DKpERERUIiEEzsWlYd3JGGy7mIA8rR4A4GhjhRFtamNcx3po4u0kcZVUFTGoEhERUbGy1VpsOR+PdSdjEJ6QYVjezMcZ4wPrYVhALTjYMEpQxeF3FxERERm5fjcT607GYPPZ28hUawEA1lZyDG7pg/GB9dDa19UwtRRRRWJQJSIiIuRp9dh1+Q7WnYzB6agUw/L6HvYY17EeRratAzcHawkrpOqIQZWIiKgai0vJwa+nY7HxTBySs/IA5E8V1fcJL4wPrIcuDTwh51RRJBHJg+qsWbMQHh6OrVu3AgDCwsIwceJEREREYPLkyVi4cCFPLxAREZUjnV7gn+uJWHcyFgevJUIUPLu0prMNRrevizEd6sLbxVbaIokgcVC9ePEivv/+e1y4kP+MX7VajSFDhmDAgAHYsGEDpk2bhtWrV2PixIlSlklERFQp6PQCp6NSkJipgpeTLTo8MHF+cpYav/0bh5BTsbidlmtY3rWhJ8YH1kWfJ2pCqeDUUmQ5JAuqer0er776Kt599134+/sDAHbu3In09HQEBwfD3t4eCxYswNSpUxlUiYiISrErLAFBW8ORkK4yLPNxscWcwc3g7mCNdadisSssARpd/vCpi50Sz7Wtg7Ed68K/hqNUZROVSLKg+sMPP+DSpUt49dVX8ffff2PgwIG4cOECAgMDYW+f/ySLli1bIjw8/KH7UKvVUKvVhtcZGflTZ2g0Gmg0mor9AAXHuf9PMg/2XRrsuzTYd2lUtr7vvnwXb224APHA8oR0Fd5Yf9ZoWcs6zhjb3heDWnjDVqkAYDmfs7L1vaowd9/LchyZEOLB7+sKl5WVBT8/P3h7e2PEiBE4fPgwsrOz0a1bN6hUKixdutSwbY0aNXD9+nW4ubkV2c+8efMQFBRUZHlISIgh7BIREVVlegEEnVUgLQ8AHnZPh0BgDYGu3nr4cvCUJJaTk4OxY8ciPT0dzs7OJW4ryYjqn3/+iezsbBw8eBCenp7QarVo0aIFVq5cWeQ0v62tLXJycooNqrNnz8b06dMNrzMyMuDr64v+/fuX+sHLg0ajwd69e9GvXz8olcoKPx7lY9+lwb5Lg32XRmXq+6moFKSdPFPKVjK8OaQ9Ovq5m6WmR1WZ+l6VmLvvhWfATSFJUL116xYCAwPh6emZX4SVFVq2bImrV68iKSnJaNvMzExYWxc/b5uNjQ1sbGyKLFcqlWb9Bjf38Sgf+y4N9l0a7Ls0KkPf7+VoTd7O0j9LocrQ96rIXH0vyzEkubWvTp06yM3NNVoWExODr7/+GidOnDAsi4qKglqthru7Zf8LkIiISCpOtqb90vdy4nRTVPlIElQHDRqE8PBw/PDDD7h16xa+/fZbXLhwASNGjEBGRgZWrVoFAFiwYAH69u0LhUIhRZlEREQWLfZeDj7fcaXEbWTIv/u/g4Wf9icqjiSn/j08PLBjxw689957mD59Onx8fLBx40b4+vpixYoVGDNmDGbOnAm5XI5Dhw5JUSIREZFFOxaRjKkhZ5GWo4GzrRUyVFrIAKM7/wtvrZo7pJnRfKpElYVk01N16dLF6DR/oaFDhyIyMhKhoaEIDAyEh4eHBNURERFZJiEEVh2Lxqc7rkCnF2jl64qfXmiLc7GpReZR9XaxxdwhzTCwuY+EFRM9OskfoVocb29vDBo0SOoyiIiILIpKo8PHf4VhU+gtAMCzberg0+HNYatUYGBzH/Rr5l3ik6mIKhuLDKpERERk7G6GCq+tDcX5uDTIZcBHg5phUpf6kMn+C6IKuQydGvBMJFUdDKpEREQW7lxsKl5bG4rETDVc7JRYOrYNujbylLosogrHoEpERGTBNoXewod/XkKeTo/GNR2xfEI71PNwkLosIrNgUCUiIrJAWp0eC3ZcxcpjUQCA/s1qInhUABxt+Kubqg9+txMREVmY1Ow8vPnrWRyLuAcAeLtPI7zdpxHkvDGKqhkGVSIiIgty7U4mXllzBrEpObC3ViD4+VacXoqqLQZVIiIiC7Er7A6mbzyPnDwdfN3tsHxCOzT1dpa6LCLJMKgSERFJTK8XWHIgAov3XQcAdG7ggaVj28DNwVriyoikxaBKREQkoWy1FjM2XsCuy3cAABO71MdHTz8BK4Vc4sqIpMegSkREJJHYezl4Zc0ZXLubCWuFHJ8Mb47n2/lKXRaRxWBQJSIiksCxiGRMDTmLtBwNajjZ4McX2qJNXTepyyKyKAyqREREZiSEwOrj0fhk+xXo9AKtfF3x4/i28Haxlbo0IovDoEpERGQmaq0OH28Ow++htwAAI9rUxoLhLWCrVEhcGZFlYlAlIiIyg8QMFV5bF4pzsWmQy4CPBjXDpC71IZNxEn+ih2FQJSIiqmDn49Lw2tozuJuhhoudEt+NbY1ujWpIXRaRxWNQJSIiqkB/hN7C7M2XkKfVo5GXI5ZPaIf6ng5Sl0VUKTCoEhERVQCtTo/Pdl7Fz0ejAAD9mtXE4lEBcLThr14iU/H/FiIionKWlpOHN0PO4WhEMgBgWp9GeKdPI8jlvB6VqCwYVImIiMrR9buZmPzLGcSm5MDeWoFFz7XCUy18pC6LqFJiUCUiIionuy/fwfTfziM7T4c6bnZYPqEdnvBxlrosokqLQZWIiOgx6fUCSw5EYPG+6wCATv4eWDquDdwdrCWujKhyY1AlIiJ6DNlqLWZsvIBdl+8AAF7qXB8fDXoCSoVc4sqIKj8GVSIiokcUey8Hr649g6t3MmGtkOOTZ5rj+fa+UpdFVGUwqBIRET2C4xHJmBJyFmk5GtRwssEP49uibT03qcsiqlIYVImIiMpACIFfjkdj/vYr0OkFWtVxwY8vtIO3i63UpRFVOQyqREREJlJrdfjfX2HYeOYWAGBE69pYMKIFbJUKiSsjqpoYVImIiEyQmKHCa+tCcS42DXIZ8OHTT+Dlrn6QyTiJP1FFYVAlIiK6j04vcCoqBaHJMnhEpaBTQy9cup2O19aewd0MNVzslPhubGt0a1RD6lKJqjwGVSIiogK7whIQtDUcCekqAAqsuXEGLnZKZKu10OoFGnk5YvmEdqjv6SB1qUTVAoMqERER8kPqG+vOQjywPD1XAwBoWccFIa8EwtGGvzqJzIWzERMRUbWn0wsEbQ0vElLvl5Sphh1vmiIyKwZVIiKq9k5HpRSc7n+4hHQVTkelmKkiIgIYVImIiJCYWXJILet2RFQ+GFSJiKja83IybbJ+U7cjovLBoEpERNVeBz93eDvbPHS9DICPiy06+LmbrygiYlAlIiJSyGVo6u1c7LrC6fznDmkGhZyT+xOZE4MqERFVe0duJOHQ9SQAgLu9tdE6bxdbLBvfBgOb+0hRGlG1xsngiIioWkvP0WDm7xcBABM61cPcIU/iREQi9hw5hf7dOqJTQy+OpBJJhEGViIiqtTl/h+FOhgr+ng6Y/dQTUMhl6OjnjntXBDr6uTOkEkmIp/6JiKja2nYxHlvOx0Mhl2HR861gZ80J/YksCYMqERFVS3czVPj4rzAAwNSeDdC6rpvEFRHRgxhUiYio2hFCYNYfF5GWo0Hz2s54q08jqUsiomIwqBIRUbWz/lQsDl1LgrWVHIufD4BSwV+HRJaI/2cSEVG1Ep2cjU+3XwEAzBrYFI1qOklcERE9DIMqERFVG1qdHtM3nkeuRofODTwwsXN9qUsiohIwqBIRUbXx4+GbOBubBicbK3z5XCvIOfUUkUVjUCUiomoh7HY6Fu+9DgCYN/RJ1Ha1k7giIioNgyoREVV5Ko0O0zeeh1YvMPBJb4xoU1vqkojIBAyqRERU5S3acw3X72bB09EGnw5vDpmMp/yJKgMGVSIiqtJO3ryHFUejAABfPNsCHo42EldERKZiUCUioiorU6XBjI0XIAQwur0v+jxRU+qSiKgMGFSJiKjK+r+t4bidlgtfdzt8PLiZ1OUQURkxqBIRUZW05/Id/B56CzIZsOi5ADjaWEldEhGVkWRBddq0aZDJZIavhg0bAgDCwsLQvn17uLm5YebMmRBCSFUiERFVUslZasz+8xIA4NXu/ujg5y5xRUT0KCQLqmfOnMH27duRmpqK1NRUnDt3Dmq1GkOGDEHbtm1x5swZhIeHY/Xq1VKVSERElZAQArP/vIR72Xlo6u2E6f0aS10SET0iSYKqVqvF5cuX0b17d7i6usLV1RVOTk7YuXMn0tPTERwcjAYNGmDBggX4+eefpSiRiIgqqd9Db2Fv+F0oFTIsHhUAGyuF1CUR0SOS5IKdS5cuQa/XIyAgALdv30aPHj3w008/4cKFCwgMDIS9vT0AoGXLlggPD3/oftRqNdRqteF1RkYGAECj0UCj0VTshyg4zv1/knmw79Jg36XBvpfNrdRcBG29DAB4u3dDNPS0e6Tese/SYN+lYe6+l+U4MiHBRaDr16/H4sWLsWTJEnh6euLdd9+FVqvFk08+CZVKhaVLlxq2rVGjBq5fvw43N7ci+5k3bx6CgoKKLA8JCTGEXSIiqh70AvjusgKRmTL4Owm89aQOcs7rT2RxcnJyMHbsWKSnp8PZ2bnEbSUJqg+KjY2Fn5+f4Qar4OBgwzpfX1+cPHkStWsXfdxdcSOqvr6+SE5OLvWDlweNRoO9e/eiX79+UCqVFX48yse+S4N9lwb7brqfj0Xj813XYW+twNapnVDX/dEHLNh3abDv0jB33zMyMuDp6WlSULWIuTq8vLyg1+vh7e2NsLAwo3WZmZmwtrYu9n02NjawsSn6hBGlUmnWb3BzH4/yse/SYN+lwb6X7NqdTATvjQAA/G9wMzSo6VIu+2XfpcG+S8NcfS/LMSS5mWrmzJkICQkxvD5x4gTkcjlatGiBEydOGJZHRUVBrVbD3Z3TihARUfHytHq889t55On06NPUC6Pb+0pdEhGVE0lGVFu1aoWPP/4YNWvWhE6nw1tvvYUJEyagf//+yMjIwKpVqzBx4kQsWLAAffv2hULBOzaJiKh43+y/jisJGXCzV+KzZ1tAJuOFqURVhSRBdfz48bh8+TKeffZZKBQKjB8/HgsWLICVlRVWrFiBMWPGYObMmZDL5Th06JAUJRIRUSUQGpOCZYciAQALhreAl5OtxBURUXmS7BrVzz77DJ999lmR5UOHDkVkZCRCQ0MRGBgIDw8PCaojIiJLl63WYvrGC9ALYETr2niqhY/UJRFRObOIm6ke5O3tjUGDBkldBhERWbBPd1xBzL0c1HKxxbxhT0pdDhFVAMkeoUpERPSoDl5LRMipWADAV8+1grMt7xAnqooYVImIqFJJzc7D+5suAgAmdqmPzg09Ja6IiCoKgyoREVUaQgh8/FcYkjLVaOjliFkDm0pdEhFVIAZVIiKqNLacj8f2Swmwksuw+PkA2Co5fSFRVcagSkRElUJCei7+tyX/6YXT+jRCizrl8/QpIrJcDKpERGTx9HqBmb9fRKZKi1a+rpjSs4HUJRGRGTCoEhGRxVtzIhpHI5Jhq5Rj8fOtYKXgry+i6oD/pxMRkUWLSMzCZzuvAgA+fPoJ+NdwlLgiIjIXBlUiIrJYGp0eMzaeh1qrR7dGnhjfsZ7UJRGRGTGoEhGRxVp6MAIXbqXD2dYKX45sBblcJnVJRGRGDKpERGSRLsSlYcmBCADA/Geaw9vFVuKKiMjcGFSJiMjiqDQ6vLvxPHR6gcEtfTAsoLbUJRGRBBhUiYjI4ny+8ypuJmXDy8kGnzzTXOpyiEgiDKpERGRRjt5Ixurj0QCAhSNbwtXeWtqCiEgyDKpERGQx0nM1mLnpAgBgfGBd9GziJXFFRCQlBlUiIrIY8/6+jIR0Fep72OPDp5+QuhwikhiDKhERWYQdlxKw+dxtyGVA8KgA2FtbSV0SEUmMQZWIiCSXmKHCh5svAQCm9GyINnXdJK6IiCwBgyoREUlKCIFZf1xEWo4GT9ZyxrQ+jaQuiYgsBIMqERFJ6tfTcTh4LQnWVnIsHhUAayv+aiKifCZdAKTT6fDrr79i48aNCA8Ph16vh0wmg5ubG55++mlMnDgRfn5+FV0rERFVMTH3svHJ9nAAwPsDmqBxTSeJKyIiS1LqP1sPHz6MNm3aIDQ0FHPmzEFERARu3ryJyMhI7Ny5E/Xr18czzzyDjz/+GHq93hw1ExFRFaDTC0zfeAE5eTp09HPHpC4c8CAiYyUG1TVr1mDGjBn4448/sHjxYrRr185ofY0aNTBp0iScOXMGGo0GgwcPrtBiiYio6vjxcCRCY1LhaGOFRc+3glwuk7okIrIwJZ7679evH0aOHAl7e/sSd6JUKvHFF18gIiKiXIsjIqKq6XJ8OhbvvQ4AmDukGeq4lfx7hoiqpxKDqo+Pz0PXHTlyBN999x3u3bsHNzc3vPrqq+jXr1+5F0hERFWLSqPD9N8uQKMT6N+sJka2rSN1SURkoUq9RvXWrVuYO3cutmzZYnQN6sSJEzF06FCsWLECL7zwAsaOHVuhhRIRUdWweO91XLubCU9Ha3w2ogVkMp7yJ6LilXrXf506dRAUFISLFy/i008/haOjI8aOHYupU6fi448/hkwmgxACb731ljnqJSKiSuzUzXv46chNAMBnI1rCw9FG4oqIyJKZND1Vbm4ubt++DT8/P+j1eqxcuRIqlQorV65Er169KrpGIiKqAjJVGsz4/QKEAJ5vVwf9mtWUuiQisnAmBdWePXviiSeeQP369ZGcnIxt27bh+vXrOH78OObNmwcfHx+MGTMGzs7OFV0vERFVUvO3heNWai7quNnhf4ObSV0OEVUCJgVVIUSR1zKZDD179kTPnj1x584d/PHHH5g4cWKFFElERJXb3vC72HjmFmQyYNFzreBkq5S6JCKqBEwKqocOHcKhQ4eQkpKC1q1bY/78+VAq//sh4+3tzZBKREQGOr3A6agUJGaqYGslx4ebLwEAXunmj47+HhJXR0SVRYlBdcOGDfD29kbPnj3x9NNPl7ij9PR0LFq0CP/3f/9XrgUSEVHlsissAUFbw5GQrjJaXsvFFtP7NZaoKiKqjEqcnqpjx46YPn06PvnkE6jV6odud/jwYXTr1g0NGzYs9wKJiKjy2BWWgDfWnS0SUgEgPl2FQ9cSJaiKiCqrEkdU/fz8cOzYMXz++edo3rw5nnrqKXTs2BE1a9ZEVlYWIiMjsWXLFiiVSqxfvx4tWrQwV91ERGRhdHqBoK3hEA9ZLwMQtDUc/Zp5Q8HHpRKRCUqd8N/Ozg5BQUE4f/48unfvjmvXrmHTpk04cuQIlEolli9fjv379zOkEhFVc6ejUoodSS0kACSkq3A6KsV8RRFRpWbSzVQA4ODggJEjR2LkyJEVWQ8REVVSiZkPD6mPsh0RUakjqkRERKbwcrIt1+2IiBhUiYioXHTwc4ePy8NDqAyAj4stOvi5m68oIqrUGFSJiKhcKOQy/G9Q8U+cKrx1au6QZryRiohMxqBKRETlRlfwJMMHo6i3iy2WjW+Dgc19zF8UEVVaJt9MRUREVBKtTo/Fe68DAKb1aYRAfw8kZqrg5ZR/up8jqURUVgyqRERULv48exs3k7Ph7mCNV7r7w9GGv2KI6PHw1D8RET02tVaHr/flj6ZO6dmAIZWIygWDKhERPbaQU7GIT1fB29kW4wPrSV0OEVURDKpERPRYstVaLD0YASD/2lRbpULiioioqnjsoJqXl4cFCxaURy1ERFQJrT4ejeSsPNTzsMdz7epIXQ4RVSEmBdUmTZogJycHP/zwg2HZtGnTsGPHDgDAhg0bKqY6IiKyaOk5GvzwTyQAYHq/xlAqeKKOiMqPSVe7CyEQGRmJ+fPnw8nJCf7+/tixYweCgoJgbW0NhYKneYiIqqOfjkQiU6VFk5pOGNKyltTlEFEVY9I/fV1dXdGiRQscOHAA27dvh0ajwffff4/o6GgAgEzGufGIiKqbpEw1Vh6NBgDM6N8Ycs6TSkTlrEznaBISEhASEgIvLy9MmzYNixcvBpA/4kpERNXL0oMRyNXo0MrXFf2a1ZS6HCKqgkye6E6v1+Ott97CU089hS5duqBWrVr46KOPAHBElYiourmVmoOQU7EAgPcHNOHvASKqECUG1aioKAwdOhTZ2dkIDw/HqVOn8Mwzz8DT0xPPPvssxo4dCwcHB0RERKB79+5Qq9U4deqUuWonIiKJfLv/BvJ0enRu4IEuDT2lLoeIqqgSg6qXlxemTJmCJUuW4Omnn8YLL7yAli1bYufOnfD29kb37t3Rs2dPvPvuu5gxYwbUarW56iYiIolEJmVhU+gtAMB7A5pIXA0RVWUlXqPq4OCAN954A46Ojjhz5gzu3LmDFStWYPr06bhx4waUSiWGDRsGFxcXDBs2DM8///wjFTFw4ECsXr0aAPDPP//giSeegKenJ4KDgx9pf0REVHGC916HXgB9n6iJNnXdpC6HiKowk2+m8vLyws8//4yPPvoITZo0wQcffICePXs+dgHr16/H7t27AQBJSUkYOnQoxowZgxMnTmD9+vU4ePDgYx+DiIjKx+X4dGy/mACZLP9OfyKiimTSzVS5ubkAgNmzZyMjIwOZmZn49NNPcfr0aQCPfjNVSkoKZsyYgSZN8k8drV+/HrVq1cL//vc/yGQyzJkzBz///DN69er1SPsnIqLytWjPdQDAkJa18ISPs8TVEFFVZ1JQTUpKwq1bt/Dnn3/i6NGjqFGjBlq3bo0XX3wRa9asgUajeaSDz5gxA8OHDzcE4QsXLqBXr16G4NuhQwd88MEHD32/Wq02ui42IyMDAKDRaB65prIoPIY5jkX/Yd+lwb5Lw5L6HhqTigNXE6GQy/BWLz+LqKmiWFLfqxP2XRrm7ntZjiMTJkyCevToUXTt2hV5eXmwtrYGkD8jwM2bN9GpUyd06tQJFy5cKFORBw8exIsvvojLly/jrbfeQs+ePbF161YEBgZi5syZAIDs7GzUqlUL6enpxe5j3rx5CAoKKrI8JCQE9vb2ZaqHiIgeTghgyWUFIjNl6OSlx+gGeqlLIqJKKicnB2PHjkV6ejqcnUs+M2PSiGrXrl0BwBBSAcDPzw9+fn4AUOaQqlKp8Nprr2HZsmVwcnL6rxgrK9jY2Bhe29raIicn56H7mT17NqZPn254nZGRAV9fX/Tv37/UD14eNBoN9u7di379+kGpVFb48Sgf+y4N9l0altL3IxHJiDx5FtZWcnwxoTt8XGwlq8UcLKXv1Q37Lg1z973wDLgpTJ7wHwD+/vtvpKWlwcqq+Lf5+fmhU6dOpe5n/vz5aN++PQYNGmS03N3dHUlJSYbXmZmZRuH4QTY2NkbBtpBSqTTrN7i5j0f52HdpsO/SkLLvQggs3hcJAHghsB7qejqV8o6qg9/v0mDfpWGuvpflGGUKqvPnz0fz5s0BANu3b8egQYOwe/duDBgwAEII/PPPPwgLC4ODg0OJ+wkJCUFSUhJcXV0B5A8Bb9y4EQDQuXNnw3bnzp1D7dq1y1IiERGVs92X7+DS7XQ4WCswpWcDqcshomrEpKDauXNn/PDDD5DJZFi1ahWA/BudVq1ahV69ehmWbdu2DXp96dctHTlyBFqt1vD6vffeQ2BgIF566SX4+vpi37596NGjBxYuXIgBAwY8yuciIqJyoNMLfFVwp//LXf3g4Vj0LBYRUUUxKajGxMRg7NixiIqKwpw5cwAA8fHxmDNnDqKjow3L2rVrZ3TN6cPUqVPH6LWjoyM8PT3h6emJxYsX4+mnn4ajoyNcXV0NDwIgIiLz23L+NiISs+Bip8Tk7v5Sl0NE1UyJQTU6OhpyuRy1a9fGsWPH0LJlS8OpeKVSidq1a8Pa2tqwzM3t0Z5Qcn8Yff311zFgwABcvXoV3bp1g6Oj4yPtk4iIHk+eVo/F+/JHU1/v0QDOtrxmkIjMq8SgeuDAAbzzzjvw9fWFUqmEo6MjJkyYAL1ej+XLl+OFF15ASEgIXnvttXIt6v4ZBYiISBq/nYlDXEouajjZ4MXO9aQuh4iqoRIfoTpp0iTs3bsXcrkcL7zwAmrWrIm2bduiffv2SE9PR6NGjXD69Gm4urritddeQ3JysrnqJiKiCpSbp8OS/TcAAG/1bgh76zLde0tEVC5KDKoA0LZtW6xbtw4ZGRkYPHgwwsPDER4ejhs3buD27dvIycnBvn374OTkhH79+pmjZiIiqmBrTkQjMVONOm52GN2+rtTlEFE1Veo/kW/fvo2JEyfizJkzyMvLwzvvvIP169cbzYGVl5eH7t27448//qjQYomIqOJlqDRY9k/+vKnv9G0Ma6tSxzSIiCpEqT99rKyskJaWhsjISNja2kKtVuPLL79EXFwc4uLiEBsbCx8fH/z555/w9+cdoUREld2KI1FIy9GgoZcjhrfmXNZEJB2TLjpKSUnBkCFDYGNjA51Oh5ycHKNHm6akpOD777+Hn58fnnrqqQorloiIKlZKdh5+PnITADCjX2Mo5DKJKyKi6qzUEVWtVgt/f39cvXoV27dvx3PPPYe///4bH3/8MWJjY5GUlIRXX30Vd+7cwd27d81RMxERVZBlhyKQnadD89rOGNjcW+pyiKiaK3VE1d3dHXPnzgWQP1H/3Llz8dprr2HcuHEYMGAAevXqVeFFEhFRxUtIz8UvJ2IAAO/1bwKZjKOpRCStUoOqk5MThg0bZrTM29sb+/fvr7CiiIjI/JYciECeVo8O9d3Ro3ENqcshIir91D8ACCGwbt26h67XaDTo3r07tFptuRVGRETmE52cjY3/xgEA3hvA0VQisgwmBVWZTIa3334bAJCbm4tatWoBAHx9fQEACoUCx44dg0KhqKAyiYioIn297zq0eoGeTWqgg5+71OUQEQEwMagCgL29PQDA1tYW1tbWAAAXF5f8ncjzd8N/gRMRVT7X7mRiy4V4APnXphIRWYpSr1H9+OOPodVqodFoEBwcDADIzMxEcHAw0tLSEBwcDCFEhRdKREQVY9GeaxACGNTCB81ru0hdDhGRQYlB9fz589i2bRu2b9+On376CWFhYRBCQKPR4NKlS8jNzcWlS5fMVSsREZWz83Fp2BN+F3IZ8G6/xlKXQ0RkpMSgGhAQgDNnzsDKygouLi5YuXIlAOCff/7BqlWr0Lp1a6xatQoAsGbNmoqvloiIytVXu68BAEa0qYOGXo4SV0NEZKzEa1QzMzPRv39//PHHHwDy7/7X6XQAAL1eD5lMZrSMlwAQEVUexyOScTQiGUqFDG/3aSR1OURERZQ4opqdnY2AgAC89957iImJgZVV/uZCCCiVSgghjJZZWVkZQisREVkuIQS+3JM/mjq2Q134uttLXBERUVEljqh6e3sjODgYN2/exObNm9G2bVu4ublh6dKlSElJQWpqKlJSUpCSkoLk5GTExcWZq24iInoM+68k4lxsGmyVckzt3VDqcoiIilXqXf9A/rRTbdq0wZ49e/DHH39Ar9dDo9HA09OzousjIqJyptcLfFUwmjqxix+8nGwlroiIqHgmBdVr166hX79+GD16NBYuXAgAGDFiBCIiIvDaa69hwoQJcHJyqtBCiYiofGy7lICrdzLhZGuF17r7S10OEdFDlTrh/9WrV9G9e3dMmjTJEFIB4M8//8SSJUuwa9cu1K5dG++++26FFkpERI9Po9MjuGA09bXu/nC1t5a4IiKihyt1RLVu3br47rvv8NxzzxVZ16NHD/To0QP79u3DuXPnKqRAIiIqP3+E3kL0vRx4OFhjYhc/qcshIipRqUHV3t6+2JB6v759+6Jv377lVhQREZU/lUaHb/bfAABM6dUQDjYmXf1FRCSZUk/9A0BoaGiRZTk5ORBC4P333wcALFmyBDt37izf6oiIqNysPxWLhHQVfFxsMa5jXanLISIqlUlBdcyYMQCAv//+GyqVCosWLcLMmTMRGxuLv//+27AuJSWl4iolIqJHlqXW4vuDEQCAt/s0gq1SIXFFRESlMymo2tjYAACGDx8OIQRWrlwJa2tr2NjYwNraGmlpabh48SKGDx9eocUSEdGjWXU0Cvey8+Dn6YBn29aRuhwiIpOYFFRlMln+xnI57OzsYG1tDZlMZniE6qJFi/DGG2/A3p5PNiEisjRpOXn46chNAMC7/RpDqTDpRz8RkeRKvZL+/rv5CwPrg/bs2YMjR46UX1VERFRufjx8E5kqLZp6O2FwCx+pyyEiMlmJQTUoKAi//vorFAoFRowYAb1ejxEjRiAqKgpZWVno1asXcnJy8NFHH+H48ePIzMzEkCFDzFU7ERGVIjFThVXHogAAMwc0gVxe/IADEZElKvH8z9ChQ3H48GHI5XIMGjQIMpkMgwYNgouLC/z8/PDDDz/gzp07+Pbbb/HBBx8gKCjIXHUTEZEJlh6IgEqjR5u6rujd1EvqcoiIyqTEoNq6dWt4eXlBJpPh5ZdfNvzp7u6O5s2bY+XKlfD390fv3r1x8uRJnDlzxlx1ExFRKeJSchByOhYA8N6AJg+9fIuIyFKVGFTPnz9f7ByqDzpx4gQ2bNhQbkUREdHj+2b/DWh0Al0beqJzA0+pyyEiKrMSg+qZM2cwYMAAREREICQkBHq9HiEhIUhNTcXVq1dx/fp1yGQy/Pzzz3j//fehUqnMVTcREZUgIjETf569BSB/NJWIqDIqMahOnjwZ165dw6hRozB+/Hg0btwY+/btQ69eveDg4IDo6GgAQP369fHkk09i1apV5qiZiIhKsXjvDegF0L9ZTQT4ukpdDhHRIyl1Mj0PDw+sWrUKv//+O+7evYvatWsbXr/wwgtQq9UAgEGDBmH37t0VXjAREZUs7HY6tl9KgEwGzOjP0VQiqrxKnUe10LPPPou2bdsiNjbWaPnevXsBAOPGjcPEiRPLtzoiIiqzr/ZcAwA8E1AbTbydJK6GiOjRmRxUgfxT/PXr1zdaVrduXajVari5uZVnXURE9AhOR6Xg0LUkWMlleKdvI6nLISJ6LCY9R2/Hjh3IzMzE22+/XWSdXq/HM888A1dXV0yZMqXcCyQiItMIIfDl7qsAgOfb+6Keh4PEFRERPZ5Sg2pwcDBeeuklhIWFYcuWLUV3IJfj999/x3PPPYdffvkFWVlZFVIoERGV7J/rSfg3OhXWVnJM683RVCKq/Eo99e/h4YF///0X9erVg1xunGsjIiKwb98+rF69GomJidi6dSscHR0rrFgiIiqeXi/w5e78a1Nf7FQP3i62EldERPT4Sg2qt2/fxi+//AIASElJweTJkxEbG4vLly9Dq9WiS5cumD17NoYMGVIkyBIRkXnsunwHl+Mz4GCtwBs9G0pdDhFRuSgxqAohEBMTA2tra9ja2sLKygodO3ZE586d0aNHDzRo0AAAsHbtWmRkZMDV1dUcNRMR0X10eoFFBXf6T+7mD3cHa4krIiIqHyUGVZlMhh9//NHw+o8//sArr7wCBwcHZGRkGJYfOHAA1tbWGDVqVMVVSkRExdp87jYik7Lhaq/E5G5+UpdDRFRuyjQ9VSGlUgk7OzsA+aOuQgjIZDIGVSIiM1NrdVi89zoAYErPBnCyVUpcERFR+TH5olIhBHQ6HQBAo9EgJycHeXl50Gg0iIyMxJ9//om8vLwKK5SIiIr67d843E7LRU1nG0zoVF/qcoiIypXJI6p5eXnIyckBAOzatQtWVvlvnTFjBoQQmDNnDoQQFVMlEREVkZOnxbf7IwAAb/ZuBFulQuKKiIjKl0kjqiqVCjt27MDdu3eRkJAAPz8/JCYmIjExERMnTsTGjRshhICNjU1F10tERAV+OR6D5Cw1fN3tMKqdr9TlEBGVu1JHVJOTkzFw4EA0aNAAw4cPxwsvvIDIyEjDeplMBoVCgdmzZ+PJJ5/EwIEDK7RgIiIC0nM1+OGf/J/F7/ZtDGsrTg9IRFVPqUE1ISEB/fv3x4IFCwAAbm5uiIqKKrJdXFwcfH35L3oiInP4+chNpOdq0MjLEcMCaktdDhFRhSg1qLZo0QItWrQwvB43blyx2zGkEhGZR3KWGiuO5g8YzOjfBAq5TOKKiIgqRpnPFT3zzDMVUAYREZlq2aFI5OTp0LKOCwY8WVPqcoiIKkyJI6pnz57F22+/DaWy9Hn5ZDIZBg4ciJkzZ5ZbcUREZCw+LRdrT8YAAGYOaAKZjKOpRFR1lRhU69Wrh5kzZ5p0N398fDxef/11TJ06Ffb29uVWIBER/WfJgRvI0+oR6O+Org09pS6HiKhClRhUPTw8MHToUKxatQrr16+HXG58pYBWq4VGo8GRI0eg1WqhVqsN86uaIi0tDdeuXUPjxo3h5ub2aJ+AiKiaiErOxsYztwBwNJWIqgeTUmWPHj1Qv379IkFVr9dDq9UCANRqNZ599llYW1ubdODff/8dr7zyCnx9fXHz5k2sXr0azz33HMLCwjBx4kRERERg8uTJWLhwIX8YExEBWLz3OnR6gd5NvdC2nrvU5RARVbhSg+ratWsfOpqqVquxZ88enDlzBqNHj8aIESOwcOHCUg+anp6OKVOm4PDhw2jZsiVWr16NmTNnYujQoRgyZAgGDBiADRs2YNq0aVi9ejUmTpz46J+QiKgKuHonE1svxgMAZvRvLHE1RETmUWpQjY2NRcuWLdGnTx+oVCo4OztDofjvMX03b97EgAED8MUXX2Dy5MkmHTQjIwNff/01WrZsCQBo06YN7t27h507dyI9PR3BwcGwt7fHggULMHXqVAZVIqqWdHqBU1EpCE2W4Ze/wyEEMLilD56s5SJ1aUREZmHSqf+mTZvi3r17GD9+PKysrGBlZQVPT0/UqlULXbp0wddff40XXnjB5IP6+voa5mPVaDRYvHgxhg8fjgsXLiAwMNBwM1bLli0RHh7+0P2o1Wqo1WrD64yMDMM+NRqNyfU8qsJjmONY9B/2XRrsu3ntvnwXn+y4ijsZagAKAOkAgDa+LvxvYAb8fpcG+y4Nc/e9LMeRCSFESRt8+umnsLW1xciRIw2PSxVCICkpCeHh4Th79ixWrVqF7t274/vvv0etWrVMPviFCxfQu3dvWFtb48qVK5g/fz5UKhWWLl1q2KZGjRq4fv16sTdbzZs3D0FBQUWWh4SEcOYBIqq0LtyTYeX1wsut7r9GP//H9aTGerTyKPFHNxGRxcrJycHYsWORnp4OZ2fnErctNah+//33+P7774vcJKXVaqFSqXDixAnI5XLMmDED+/fvx40bN0y+oUoIgbNnz+Ldd9+Fl5cXGjRoAI1Gg+DgYMM2vr6+OHnyJGrXLvqIwOJGVH19fZGcnFzqBy8PGo0Ge/fuRb9+/Uyaa5bKB/suDfbdPHR6gZ6LDheMpBYlA+DtYoOD07vziVQViN/v0mDfpWHuvmdkZMDT09OkoFrqqf+XX34ZHh4eGDVqlGHnjo6O0Ol0WLBgAfLy8uDj44OVK1fi5MmTJodUIP8hAW3btsUvv/yCBg0a4LPPPkNYWJjRNpmZmQ/dp42NTbFzvCqVSrN+g5v7eJSPfZcG+16xzkTee2hIBfLHVBPS1Th3KxOdGniYr7Bqit/v0mDfpWGuvpflGKU+QvXNN9/EDz/8ACEEcnNzMWDAAHz11Ve4c+cOLl26hMaNG2PatGmIi4tDYGCgSQf9559/jJ5gZW1tDZlMhieeeAInTpwwLI+KioJarYa7O6dhIaLqITFTVa7bERFVZiUG1ZycHMTFxWHjxo2QyWSYPXs2PD098d5778HX1xebNm3C5cuXkZOTg0aNGmHZsmUmHbRx48b46aef8NNPPyEuLg4ffvgh+vfvj6effhoZGRlYtWoVAGDBggXo27ev0SwDRERVmZeTbbluR0RUmZV46t/e3h67du0yvJ47dy6USqXRnKp169bFihUrMG7cOLRu3dqkg/r4+GDTpk1455138N5772HAgAFYs2YNrKyssGLFCowZMwYzZ86EXC7HoUOHHu2TERFVQh383OHjYouE9OJHTPOvUbVFBz+eaSKiqs/0550CJT7mtFevXmU6cL9+/XD58uUiy4cOHYrIyEiEhoYiMDAQHh68BouIqg+FXIY5g5vhjfVni6wrvHVq7pBmvJGKiKqFMgVVc/H29sagQYOkLoOISBK2yuIvd/J2scXcIc0wsLmPmSsiIpKGRQZVIqLqSgiBr/ddBwC80s0PPRp5YM+RU+jfrSM6NfTiSCoRVSsMqkREFuTgtURcuJUOO6UCr/VoABcbOe5dEejo586QSkTVTqnTUxERkXnkj6beAABM6FwPno5F54kmIqpOGFSJiCzEgauJuHgrHfbWCrzazV/qcoiIJMegSkRkAYxGUzvVhwdHU4mIGFSJiCzB/iuJuHS7YDS1O0dTiYgABlUiIskJIfD1/vw7/V/sXB/uDtYSV0REZBkYVImIJLbvSiLCbmfAwVqBV3htKhGRAYMqEZGE7p83laOpRETGGFSJiCS0N/wuLsdzNJWIqDgMqkREErn/Tv+XutSHG0dTiYiMMKgSEUlkT/hdhCdkwNHGCpO7cjSViOhBDKpERBIQQuCbwtHUzhxNJSIqDoMqEZEEdl++bzS1m5/U5RARWSQGVSIiM9PrBb7Znz+aOrFLfbjaczSViKg4DKpERGa2J/wOriRkwMnGCi935WgqEdHDMKgSEZmRXv/fnf4cTSUiKhmDKhGRGe2+fAdX72QWjKbyTn8iopIwqBIRmYnRtald/eBir5S4IiIiy8agSkRkJrsKR1NtrfByF16bSkRUGgZVIiIz0Ov/mzd1UheOphIRmYJBlYjIDHaG3cG1u/mjqZN4pz8RkUkYVImIKlj+tanXAQAvd/WDix1HU4mITMGgSkRUwXaEJeD63Sw42VphIq9NJSIyGYMqEVEFuv/a1Mld/TmaSkRUBgyqREQVaPulBNxIzIKzrRVe6lJf6nKIiCoVBlUiogqi0wt8WzBv6sscTSUiKjMGVSKiCnL/aOrErvWlLoeIqNJhUCUiqgD3j6ZO7uYPZ1uOphIRlRWDKhFRBdh2MR4RiVlwsVPy2lQiokfEoEpEVM6MRlO7+nE0lYjoETGoEhGVs20X4xGZlM3RVCKix8SgSkRUjnR6gW8KRlNf6eYHJ46mEhE9MgZVIqJytPVCPG4mZcPVXokXO9eXuhwiokqNQZWIqJzcf23qK938OZpKRPSYGFSJiMrJ3xdu42YyR1OJiMoLgyoRUTnQ6vRYsj8CQP5oqqONlcQVERFVfgyqRETl4O8L8biZnA03jqYSEZUbBlUiosek1emx5EDBaGp3jqYSEZUXBlUiose05Xw8ogpHUzvVl7ocIqIqg0GViOgx5I+m5t/p/2r3BnDgaCoRUblhUCUiegx/nY9H9L0cuDtYY0KnelKXQ0RUpTCoEhE9IuPRVH+OphIRlTMGVSKiR7T53G3EcDSViKjCMKgSET0CrU6P7w7m3+n/Wnd/2FtzNJWIqLwxqBIRPYI/C0ZTPRys8QJHU4mIKgSDKhFRGWl0enxXMG/qaz04mkpEVFEYVImIymjz2duITcmBp6M1xgdyNJWIqKIwqBIRlYFGp8eSg/l3+r/WvQFHU4mIKhCDKhFRGfx59hbiUnI5mkpEZAYMqkREJtLo9FhScG3q6z0awM5aIXFFRERVG4MqEZGJ/gi9hVupufB0tMG4jhxNJSKqaAyqREQmyNP+N2/q6z38OZpKRGQGkgXVLVu2wN/fH1ZWVggICMCVK1cAAGFhYWjfvj3c3Nwwc+ZMCCGkKpGIyOCPs/mjqTWcbHhtKhGRmUgSVCMjIzFx4kR8/vnnuH37Nho3bozJkydDrVZjyJAhaNu2Lc6cOYPw8HCsXr1aihKJiAzytP/Nm/p6jwawVXI0lYjIHCQJqleuXMHnn3+O559/HjVr1sQbb7yBc+fOYefOnUhPT0dwcDAaNGiABQsW4Oeff5aiRCIig02ht3A7LX80dVzHulKXQ0RUbUgyAeDgwYONXl+7dg2NGjXChQsXEBgYCHt7ewBAy5YtER4e/tD9qNVqqNVqw+uMjAwAgEajgUajqYDKjRUewxzHov+w79Korn3PH03Nnzf11W71oYAeGo3ebMevrn2XGvsuDfZdGubue1mOIxMSXwSal5eHJ598EtOnT0dERARUKhWWLl1qWF+jRg1cv34dbm5uRd47b948BAUFFVkeEhJiCLtERI/j2F0ZNt5UwFkp8L/WOvAeKiKix5OTk4OxY8ciPT0dzs7OJW4reVCdPXs2du7ciX///Rcff/wxNBoNgoODDet9fX1x8uRJ1K5du8h7ixtR9fX1RXJycqkfvDxoNBrs3bsX/fr1g1KprPDjUT72XRrVse95Wj36fn0UCekqfPx0E7zYyfw3UVXHvlsC9l0a7Ls0zN33jIwMeHp6mhRUJX3234EDB7B06VKcPHkSSqUS7u7uCAsLM9omMzMT1tbWxb7fxsYGNjY2RZYrlUqzfoOb+3iUj32XRnXq+2+hMUhIV8HLyQbjO/lBKeFNVNWp75aEfZcG+y4Nc/W9LMeQbHqqqKgojBkzBkuXLkWzZs0AAO3bt8eJEyeMtlGr1XB3d5eqTCKqptRaHb4vmDd1Sk/e6U9EJAVJgmpubi4GDx6MYcOGYfjw4cjKykJWVha6deuGjIwMrFq1CgCwYMEC9O3bFwoFf0EQkXltPHML8ekq1HS2wegOvNOfiEgKkpz637NnD8LDwxEeHo7ly5cblkdFRWHFihUYM2YMZs6cCblcjkOHDklRIhFVY8ajqQ05mkpEJBFJguqwYcMe+sSp+vXrIzIyEqGhoQgMDISHh4eZqyOi6m7jv3FISFfB29kWo9r7Sl0OEVG1JenNVA/j7e2NQYMGSV0GEVVDaq0OSw9GAgCm9OK1qUREUpLsZioiIkv0279xuJORP5r6fDuOphIRSYlBlYiogEqjw/cFo6lTOZpKRCQ5BlUiogKFo6k+LrZ4ntemEhFJjkGViAgFo6mHCu7079UQNlYcTSUikhqDKhERgA2nY3E3Q41aLrZ4vl0dqcshIiIwqBIRFYymFt7pz9FUIiJLwaBKRNXer6djkZiZP5r6HEdTiYgsBoMqEVVrKo0OywpGU6f25mgqEZElYVAlomot5FT+aGptVzs815Z3+hMRWRIGVSKqtlQaHZb9UzhvakNYW/FHIhGRJeFPZSKqttafikVSwWjqyLa8NpWIyNIwqBJRtaTS6PBDwWjqm705mkpEZIn4k5mIqqV1J2MMo6nPtuFoKhGRJWJQJaJqJzdPhx/+uQkAeIujqUREFos/nYmo2ll/KgbJWWrUcbPDs7w2lYjIYjGoElG1kj+amn9t6lu9G0Kp4I9BIiJLxZ/QRFStrDsZg+SsPPi622EEr00lIrJoDKpEVG3k5Gnx4+GC0dRejTiaSkRk4aykLoCIqCLp9AKno1KQmKnC6ah7SM7KQ113ewxvU1vq0oiIqBQMqkRUZe0KS0DQ1nAkpKuMlvdoUoOjqURElQB/UhNRlbQrLAFvrDtbJKQCwLoTMdgVliBBVUREVBYMqkRU5ej0AkFbwyFK2CZoazh0+pK2ICIiqTGoElGVczoqpdiR1EICQEK6CqejUsxXFBERlRmDKhFVOYmZDw+pj7IdERFJg0GViKoUIQSik7NN2tbLybaCqyEiosfBu/6JqMo4efMePt95Fefj0krcTgbA28UWHfzczVIXERE9GgZVIqr0riRkYOGuqzh4LQkAYG+tQM/GNbAz7A4AGN1UJSv4c+6QZlDIZSAiIsvFoEpElVZcSg4W772OzedvQwjASi7DmA518VafhvBysi12HlVvF1vMHdIMA5v7SFg5ERGZgkGViCqdlOw8fHcgAutOxiBPpwcADGrpg/f6N4Gfp4Nhu4HNfdCvmbfhyVReTvmn+zmSSkRUOTCoElGlkZOnxcqjUfjxn5vIVGsBAJ0beOCDp5qiZR3XYt+jkMvQqYGHGaskIqLywqBKRBZPo9Pjt3/j8M3+G0jKVAMAnqzljFkDm6JbI0/IZBwhJSKqihhUichiCSGw49IdfLXnGqIKppzydbfDe/2bYEjLWpDzFD4RUZXGoEpEFul4ZDK+2HkVF26lAwA8HKwxrU8jjOlQF9ZWnAKaiKg6YFAlIosSHp+BL3ZdxT/X/5tq6pVu/niluz8cbfgji4ioOuFPfSKyCHEpOVi05xq2XIg3TDU1rmNdvNm7EWo42UhdHhERSYBBlYgkdS9Lje8O5k81pdHlT80/tFUtzOjfGPU8HEp5NxERVWUMqkQkiWy1Fj8fjcJPh28iq2CqqW6NPDFrYFM0r+0icXVERGQJGFSJyKw0Oj02nI7FN/sjkJyVP9VUi9oumDWwKbo28pS4OiIisiQMqkRkFnq9wI6wBHy1+xqi7+UAAOp52OO9/k0wqIUPp5oiIqIiGFSJqMIdi0jGF7uu4mLBVFOejtZ4u08jjGrPqaaIiOjhGFSJqMKE3U7HF7uu4siNZACAg7UCr3ZvgMnd/ODAqaaIiKgU/E1BROUu9l4OFu29hi3n4wEASoUM4zrWw5u9G8LTkVNNERGRaRhUiajcJGep8d2BCKw/9d9UU88E1ML0fk1Q18Ne4uqIiKiyYVAloseWpdZixZGbWH74JrLzdACA7o1r4P0BTTjVFBERPTIGVapUdHqBU1EpCE2WwSMqBZ0aekHBu8Ur3MP6nqfVY8O/sfh2/w0kZ+UBAFrWccEHA5uic0NONUVERI+HQfURVebApNMLnI5KQWKmCl5Otujg514pat8VloCgreFISFcBUGDNjTPwcbHF3CHNMLC5j9Tllaoq9d3b2RaDWvpg35W7iCmYasrP0wHv9W+Cp1t4Qyaz/M9FRESWj0H1EVTmwGRce77KUPuusAS8se4sxAPL76Sr8Ma6s1g2vo3F11+l+p6hws9HowAAno42eKdvI4xq7wulglNNERFR+eFvlTIq/MV9f+AA/gtMu8ISJKqsdJW1dp1eIGhreJGwBMCwLGhrOHT64raQnqX2XacXyFZrkZylRlxKDm7czcTFW2k4dfMeDl1LxI6L8Zj1x6Vi+17IycYKB9/rgfGB9RhSiYio3HFEtQxKC0wy5Aemfs28Le6Urim1z/v7MjrU9wBk+dvrhYBOL4z+nv/nA+uFgF5//99RzLL8P+/fR9FlD6wvWBZzL7tIyHuw/oR0FT7efAkNvBxhJZfBSiE3/KlUyGAll8NKIftvWcGfCrnMsF6pkBW8Ltz2gWXy/L+X5bT2o3zPCCGg1uqRm6dDrqbgK08H1X1/z9UUvM7TIVejf+B1wfpi3q8q2DZXo0OeVm/y53iYTLUWYbcz0KmBx2Pvi4iI6EEMqmVwOirFpMDUKmg3lAq5IZwIkR8+Cre5/y//bSMeeF24Xjzw+sH3iyLrHravkggAdzLUaPPJ3tI3tlC//htnluPkh10ZlAXhV1EQaItblpunNel7psOne6EXMARJc7NTKmBnrYCdUgFbpRx21grkqHW4mZxd6nsTMx/++YiIiB4Hg2oZmPoLOUutA2D+sFHeFHIZFDIZ5HIU/Cm7b1n+nwr5A+sLlxnWyaCQwWiZ0XqZDAo5DCOVimKWJ2Wqse9KYqn1dm/kCTcHa2h1Alq9HlqdgEYvoNXp/1umF9Do8pfp9AKawu10AjrDewq3Lz7ha/X561R4/BHJQveyNcUut1bIDcExP0T+FyjtlArY3vd3O+uC9UoF7AreY3vfuuLeb2etgI2VvNhR4hOR9zBm+clSa/dysn3sz09ERFQcBtUyMPUX8lcjWyKgrmvBq/wAIJMV/g2GUPDf68ItZUav8eB6E95neOsD60JjUvD6urOl1r7u5Q7o3MATcgu6dEGnF+j6xQHcSVcVewpdBsDbxRarJnYo10suRMGlB/nh9r/wagjBhWG3IATnh938EFwYkMNuZ2DxvuulHuuTZ5qjg5+7UZi0tZLDSsLrPjv4ucPHxbbUvnfwczd3aUREVE0wqJaBqb+4h7epY3HXqPZr5m1S7Z0sLKQC+aOqc4c0wxvrzkIGGNVfWOncIc3KvecyWf7pfCsFYKtUPNI+ejbxwoZ/Y0vt+5gOdS3ue0aqvhMRERXibbplUPiLG7hv5LKApf/irsy1A8DA5j5YNr4NvF2MR7W9XWwtemoq9p2IiOjRSRpUk5OT4efnh+joaMOysLAwtG/fHm5ubpg5c6bhxiBLUZl/cVfm2oH8+o/O6o11k9phQiMd1k1qh6OzeleKutl3IiKispPs1H9ycjIGDx5sFFLVajWGDBmCAQMGYMOGDZg2bRpWr16NiRMnSlVmsQY290G/Zt44EZGIPUdOoX+3jpXmyVSFtVfGJyQB+SOUHf3cce+KQMdKVDf7TkREVHaSBdXRo0dj7NixOHXqlGHZzp07kZ6ejuDgYNjb22PBggWYOnXqQ4OqWq2GWq02vM7IyAAAaDQaaDTF30VdntrUccI9T4E2dZyg12mhr0Q3+rer6wzAGQAqXe2F/23N8d+4vLHvVFbsuzTYd2mw79Iwd9/LchyZkOjcelRUFPz8/CCTyRAVFYX69esjKCgIp06dwo4dOwDk33Xt4eGBlJSUYvcxb948BAUFFVkeEhICe3v7Cq2fiIiIiMouJycHY8eORXp6OpydnUvcVrIRVT8/vyLLMjIyjJbLZDIoFAqkpqbCzc2tyPazZ8/G9OnTjd7v6+uL/v37l/rBy4NGo8HevXvRr18/KJXKCj8e5WPfpcG+S4N9lwb7Lg32XRrm7nvhGXBTWNT0VFZWVrCxsTFaZmtri5ycnGKDqo2NTZHtAUCpVJr1G9zcx6N87Ls02HdpsO/SYN+lwb5Lw1x9L8sxLGp6Knd3dyQlJRkty8zMhLW1tUQVEREREZFULCqotm/fHidOnDC8joqKglqthrs7n3xDREREVN1YVFDt3r07MjIysGrVKgDAggUL0LdvXygUj/ZUICIiIiKqvCzuGtUVK1ZgzJgxmDlzJuRyOQ4dOiR1WUREREQkAcmD6oOzYw0dOhSRkZEIDQ1FYGAgPDw8JKqMiIiIiKQkeVAtjre3NwYNGiR1GUREREQkIYu6RpWIiIiIqJBFjqg+qsLLCMoykezj0Gg0yMnJQUZGBud7MyP2XRrsuzTYd2mw79Jg36Vh7r4X5jRTHo5apYJqZmYmAMDX11fiSoiIiIioJJmZmXBxcSlxG5kwJc5WEnq9HvHx8XBycoJMJqvw4xU+sjUuLs4sj2ylfOy7NNh3abDv0mDfpcG+S8PcfRdCIDMzE7Vq1YJcXvJVqFVqRFUul6NOnTpmP66zszP/h5IA+y4N9l0a7Ls02HdpsO/SMGffSxtJLcSbqYiIiIjIIjGoEhEREZFFYlB9DDY2Npg7dy5sbGykLqVaYd+lwb5Lg32XBvsuDfZdGpbc9yp1MxURERERVR0cUSUiIiIii8SgSkREREQWiUGViIiIiCwSgypVKmlpaTh16hRSU1OlLoWIiIgqGIPqYxo4cCBWr15teP3PP//giSeegKenJ4KDg6UrrAr6/fffUb9+fUyePBl16tTB77//blgXFhaG9u3bw83NDTNnzjTp+cFkmi1btsDf3x9WVlYICAjAlStXDOvY94qVnJwMPz8/REdHGy1n3ysW+2s+xX2Ps/8V72E/1y2x9wyqj2H9+vXYvXu34XVSUhKGDh2KMWPG4MSJE1i/fj0OHjwoYYVVR3p6OqZMmYLDhw/j0qVLWLp0KWbOnAkAUKvVGDJkCNq2bYszZ84gPDzc6B8P9OgiIyMxceJEfP7557h9+zYaN26MyZMnA2DfK1pycjIGDx5cJKSy7xWL/TWf4r7H2f+K97Cf6xbbe0GP5N69e6JmzZqiSZMmYtWqVUIIIRYvXiyaNm0q9Hq9EEKIv/76S4wbN07CKquO2NhYsW7dOsPrCxcuCEdHRyGEEJs3bxZubm4iOztbCCHE+fPnRZcuXSSps6rZunWr+PHHHw2vDxw4IOzs7IQQ7HtF69Onj/jmm28EABEVFWVYzr5XLPbXfIr7Hmf/K97Dfq5bau+tpA7KldWMGTMwfPhw5ObmGpZduHABvXr1gkwmAwB06NABH3zwgVQlVim+vr4YN24cAECj0WDx4sUYPnw4gPy+BwYGwt7eHgDQsmVLhIeHS1ZrVTJ48GCj19euXUOjRo0AsO8Vbfny5fDz88Pbb79ttJx9r1jsr/kU9z3O/le8h/1ct9Te89T/QzzzzDNwdXUt8vXdd9/h4MGD2L9/PxYuXGj0noyMDPj5+RleOzs7Iz4+3tylV2ol9R3I/yHm7e2NXbt24dtvvwVQtO8ymQwKhYI3XJVBaX0HgLy8PCxatAivv/46APa9PJTU9/t7ez/2vWKxv+ZT3Pc4+29e9/9ct9Tec0T1IX788Uej0dJC7u7uaNeuHZYtWwYnJyejdVZWVkaPH7O1tUVOTk6F11qVlNR3IP9feHv27MG7776LyZMnY9OmTUX6DvzXezc3N7PUXdmV1ncAmDt3LhwcHAzXqLLvj8+Uvj+Ifa9Y7K+02H/zuv/n+scff2yRvWdQfYiaNWsWu/yjjz5C+/btMWjQoCLr3N3dkZSUZHidmZkJa2vrCquxKnpY3wvJZDK0bdsWv/zyCxo0aIC0tDS4u7sjLCzMaDv2vmxK6/uBAwewdOlSnDx5EkqlEgDY93JQWt+Lw75XLPZXWuy/+Tz4c91Se89T/2UUEhKCLVu2GE7RhYSEYMqUKZgyZQrat2+PEydOGLY9d+4cateuLWG1Vcc///xjuMsfAKytrSGTySCXy4v0PSoqCmq1usRRKTJdVFQUxowZg6VLl6JZs2aG5ey7NNj3isX+Sov9N4/ifq5bbO+lvpursomLixNRUVGGr2effVZ8+eWXIikpSSQlJQlbW1uxd+9ekZeXJwYOHCjefPNNqUuuEuLj44Wzs7P48ccfRWxsrJgwYYIYOHCgEEIIjUYjatSoIVauXCmEEGLy5Mli8ODBUpZbZeTk5IhmzZqJV155RWRmZhq+9Ho9+24meOCuf/a9YrG/5nf/9zj7X/Ee9nM9Ly/PInvPoPqYXnzxRcP0VEIIsWzZMqFUKoWbm5vw8/MTd+7cka64KmbPnj2iWbNmwsnJSYwcOVIkJiYa1m3ZskXY29sLDw8PUaNGDXH58mUJK606/vrrLwGgyFfhLxX2veI9GFSFYN8rGvtrXg9+j7P/Faukn+uW2HuZEBbw2IEqJioqClevXkW3bt3g6OgodTnVxp07dxAaGorAwEB4eHhIXU61wb5Lg32vWOyvtNh/6Vha7xlUiYiIiMgi8WYqIiIiIrJIDKpEREREZJEYVImIiIjIIjGoEhEREZFFYlAlIqqEvv32W2RkZDzy+4ODg6FWq8uxIiKi8segSkRUyaxZswbbtm2Dg4PDI+8jOzsbU6dOLceqiIjKH4MqEdEjOHnyJNq2bQsnJyf07dsXt2/fBgC89NJLqFu3LnQ6HQDg0KFDkMlkhnUymQwymQzu7u4YPXo0kpKSynTce/fuYf78+diwYQMUCkWR9atXr0bPnj2LHM/Z2RnPPPMMEhMTAQD/+9//EBUVhcOHDz9qC4iIKhyDKhFRGeXk5GDYsGF48803ER4eDicnJ7z11luG9XFxcdiyZUux73399deRmpqKAwcOIDIyEm+//XaZjv3NN99g6tSpJj9/u/B4ly9fhk6nw3vvvWdY99VXX2HevHllOj4RkTkxqBIRldGVK1eQlpaGiRMnwtfXF3PmzDGMoAKAQqHAd999V+x7bWxs4OrqioCAAMyfPx/79+8v07H/+usvjB071uTtC4/n6+uLMWPGIDQ01LCudevWSE5ONoyyEhFZGgZVIqIy8vX1hVwuxyeffAKtVovWrVsbjaAOHjwYhw8fRnh4eIn7sbOzQ05OjsnH1Wq1yM3NhZeXl9Hy+fPnw8vLC40bN8a5c+eKfW9eXh62bNmCli1bGi1v3749wsLCTK6BiMicGFSJiMrIy8sLa9euxVdffYWGDRti7dq1Ruvr16+PIUOGPHRUFQBUKhWWLl2Kzp07m3zcpKQk1KhRw2jZ33//jcWLF2PTpk1YvXo11q1bZ7R+2bJlcHV1hZOTE06ePIlvvvmmyGfhiCoRWSoGVSKiRzBy5EjExMTgpZdewquvvoqZM2carZ82bRrWrl1bZAqpwuDo7OyMq1ev4vvvvzf5mA4ODsjKyjJatnnzZowdOxbdu3dH586d8fLLLxutHzduHM6fP4/Dhw+jfv36mDZtmtH6rKwsODo6mlwDEZE5MagSEZVRfHw8IiMj4eLignnz5mHnzp1YtGgRYmNjDdv06tULfn5+WL16tdF7C4NjQEAAhgwZggYNGph8XGdnZ6Snp0Or1RqWJSQkoG7duobXD+7P2dkZ9evXR8eOHREcHIzffvsNaWlphvWRkZFG7ycisiQMqkREZfTbb79h8uTJhtfdu3eHlZWVUQAE8kdV//77b6NlhcFx/vz5WLJkCVJSUsp07E6dOuGff/4xvPby8kJ8fLzh9f1h+UF6vR4ADDd+ZWVlISIiAi1atChTDURE5sKgSkRURn379sXx48fx66+/4vbt25g3bx58fHzQtGlTo+3GjRsHV1fXYvcxYMAABAQEIDg4uEzHfv311/Hpp58aXg8dOhTr16/H8ePHcerUKSxfvtxoe7VajbS0NFy5cgVz5szBE088AQ8PDwDAokWLMGHCBMM8r0REloZBlYiojFq0aIFVq1Zh7ty5aNKkCQ4ePIgtW7bA2traaDs7Ozu88sorD93PJ598UuZR1Z49e8LLywtLly4FADz77LN47bXXMGzYMLz44osYNmyY0fY//PAD3NzcEBgYCJlMhk2bNgEAQkNDsWnTJqN5VYmILI1MCCGkLoKIiEyXmZmJAQMGYPv27XBzc3ukfQwePBiff/45mjdvXs7VERGVHwZVIqJKSK/XQy5/9JNij/t+IiJzYFAlIiIiIovEf04TERERkUViUCUiIiIii8SgSkREREQWiUGViIiIiCwSgyoRERERWSQGVSIiIiKySAyqRERERGSRGFSJiIiIyCL9P5J523cY9pyYAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SNR vs 测试准确率曲线已保存到 d:\\Program\\MW-RFF\\MW-RFF\\training_results\\SNR_vs_accuracy_2026-01-25_17-09-13.png\n"
     ]
    }
   ],
   "source": [
    "# ResNet 1D 自动 SNR 循环训练脚本（按 block 整体划分训练/测试）\n",
    "# 版本改动：block 生成方式改为“同一TX下，同一文件内顺序抽取补满 block”\n",
    "# - 一个 block 内的帧全部来自同一个文件\n",
    "# - 每个文件不足一个 block 的剩余帧全部丢弃\n",
    "# - 其余训练/验证/测试划分逻辑保持：先按 block 随机划 train/test，再在 train 内做 KFold\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "from data_utilities import *\n",
    "\n",
    "# ================= 参数设置 =================\n",
    "data_path = \"E:/rf_datasets_IQ_raw/\"  # 数据文件夹\n",
    "fs = 1.4e6\n",
    "fc = 5.9e9\n",
    "v = 120\n",
    "apply_doppler = True\n",
    "apply_awgn = True\n",
    "\n",
    "# 模型超参数\n",
    "batch_size = 64\n",
    "num_epochs = 300\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 1e-3\n",
    "in_planes = 64\n",
    "dropout = 0.5\n",
    "patience = 10\n",
    "n_splits = 5\n",
    "\n",
    "# ================= 多普勒和AWGN处理函数 =================\n",
    "def compute_doppler_shift(v, fc):\n",
    "    c = 3e8\n",
    "    v = v / 3.6\n",
    "    return (v / c) * fc\n",
    "\n",
    "def apply_doppler_shift(signal, fd, fs):\n",
    "    t = np.arange(signal.shape[-1]) / fs\n",
    "    doppler_phase = np.exp(1j * 2 * np.pi * fd * t)\n",
    "    return signal * doppler_phase\n",
    "\n",
    "def add_awgn(signal, snr_db):\n",
    "    signal_power = np.mean(np.abs(signal) ** 2)\n",
    "    noise_power = signal_power / (10 ** (snr_db / 10))\n",
    "    noise_real = np.random.randn(*signal.shape)\n",
    "    noise_imag = np.random.randn(*signal.shape)\n",
    "    noise = np.sqrt(noise_power / 2) * (noise_real + 1j * noise_imag)\n",
    "    return signal + noise\n",
    "\n",
    "# ================= 数据加载（按 block 保存，并翻转 block） =================\n",
    "def load_and_preprocess_with_grouping(mat_folder, group_size=288, apply_doppler=False,\n",
    "                                      target_velocity=30, apply_awgn=False, snr_db=20,\n",
    "                                      fs=5e6, fc=5.9e9):\n",
    "    \"\"\"\n",
    "    新逻辑（按“同一文件内”构造 block）：\n",
    "      - 对每个 .mat 文件：顺序取 group_size 帧组成一个 block\n",
    "      - 文件内剩余不足 group_size 的帧全部丢弃\n",
    "      - 每个 block 翻转为 (sample_len, group_size, 2)\n",
    "      - 返回：\n",
    "          X_blocks: (num_blocks, sample_len, group_size, 2)\n",
    "          y_blocks: (num_blocks,)\n",
    "          label_to_idx: {tx_id: class_idx}\n",
    "    \"\"\"\n",
    "    mat_files = glob.glob(os.path.join(mat_folder, '*.mat'))\n",
    "    print(f\"共找到 {len(mat_files)} 个 .mat 文件\")\n",
    "    fd = compute_doppler_shift(target_velocity, fc)\n",
    "    print(f\"目标速度 {target_velocity} km/h，多普勒频移 {fd:.2f} Hz\")\n",
    "\n",
    "    X_blocks_list = []       # 每个元素 big_block: (sample_len, group_size, 2)\n",
    "    y_blocks_str_list = []   # 每个元素 tx_id（字符串）\n",
    "    label_set = set()\n",
    "\n",
    "    for file in tqdm(mat_files, desc='读取数据'):\n",
    "        with h5py.File(file, 'r') as f:\n",
    "            if 'rfDataset' not in f:\n",
    "                continue\n",
    "            rfDataset = f['rfDataset']\n",
    "\n",
    "            if 'dmrs' not in rfDataset or 'txID' not in rfDataset:\n",
    "                continue\n",
    "\n",
    "            dmrs_struct = rfDataset['dmrs'][:]\n",
    "            dmrs_complex = dmrs_struct['real'] + 1j * dmrs_struct['imag']\n",
    "\n",
    "            txID_uint16 = rfDataset['txID'][:].flatten()\n",
    "            tx_id = ''.join(chr(int(c)) for c in txID_uint16 if int(c) != 0)\n",
    "            if tx_id == \"\":\n",
    "                continue\n",
    "\n",
    "            # 处理整文件所有帧\n",
    "            processed_signals = []\n",
    "            for i in range(dmrs_complex.shape[0]):\n",
    "                sig = dmrs_complex[i, :]\n",
    "\n",
    "                # step1: 原始信号功率归一化\n",
    "                sig = sig / (np.sqrt(np.mean(np.abs(sig) ** 2)) + 1e-12)\n",
    "\n",
    "                # step2: Doppler（只改变相位，不改变功率）\n",
    "                if apply_doppler:\n",
    "                    sig = apply_doppler_shift(sig, fd, fs)\n",
    "\n",
    "                # step3: AWGN（严格按照 SNR 产生噪声）\n",
    "                if apply_awgn:\n",
    "                    sig = add_awgn(sig, snr_db)\n",
    "\n",
    "                iq = np.stack((sig.real, sig.imag), axis=-1)  # (sample_len, 2)\n",
    "                processed_signals.append(iq)\n",
    "\n",
    "            processed_signals = np.asarray(processed_signals)  # (num_samples_file, sample_len, 2)\n",
    "            num_samples_file = processed_signals.shape[0]\n",
    "\n",
    "            # 文件内顺序切 block：不足一块的直接丢弃\n",
    "            num_full_blocks = num_samples_file // group_size\n",
    "            if num_full_blocks == 0:\n",
    "                continue\n",
    "\n",
    "            for b in range(num_full_blocks):\n",
    "                start = b * group_size\n",
    "                end = start + group_size\n",
    "                big_block = processed_signals[start:end]  # (group_size, sample_len, 2)\n",
    "\n",
    "                # 翻转 block：每条新“样本”对应同一采样点的 IQ\n",
    "                big_block = np.transpose(big_block, (1, 0, 2))  # (sample_len, group_size, 2)\n",
    "\n",
    "                X_blocks_list.append(big_block)\n",
    "                y_blocks_str_list.append(tx_id)\n",
    "                label_set.add(tx_id)\n",
    "\n",
    "    if len(X_blocks_list) == 0:\n",
    "        raise RuntimeError(\"没有生成任何 block，请检查数据路径或 group_size 设置\")\n",
    "\n",
    "    label_list = sorted(list(label_set))\n",
    "    label_to_idx = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    X_blocks = np.stack(X_blocks_list, axis=0)  # (num_blocks, sample_len, group_size, 2)\n",
    "    y_blocks = np.array([label_to_idx[s] for s in y_blocks_str_list], dtype=np.int64)\n",
    "\n",
    "    print(f\"[INFO] 生成 block 数: {X_blocks.shape[0]}, 每 block 样本数: {X_blocks.shape[2]}, 每样本长度: {X_blocks.shape[1]}\")\n",
    "    return X_blocks, y_blocks, label_to_idx\n",
    "\n",
    "# ================= 1D ResNet18（增加 dropout 和 in_planes） =================\n",
    "class BasicBlock1D(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "class ResNet18_1D(nn.Module):\n",
    "    def __init__(self, num_classes=10, in_planes=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.conv1 = nn.Conv1d(2, in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1, dropout=dropout)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2, dropout=dropout)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2, dropout=dropout)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2, dropout=dropout)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride, dropout):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes)\n",
    "            )\n",
    "        layers = [BasicBlock1D(self.in_planes, planes, stride, downsample, dropout)]\n",
    "        self.in_planes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock1D(self.in_planes, planes, dropout=dropout))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (B, sample_len, 2) -> (B, 2, sample_len)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ================= 辅助函数 =================\n",
    "def compute_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm += (p.grad.data.norm(2).item()) ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def moving_average(x, w=5):\n",
    "    x = np.array(x)\n",
    "    if len(x) == 0:\n",
    "        return np.array([])\n",
    "    if w <= 0:\n",
    "        w = 1\n",
    "    if len(x) < w:\n",
    "        w = len(x)\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    acc = 100 * correct / total if total > 0 else 0.0\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=range(num_classes))\n",
    "    return acc, cm\n",
    "\n",
    "def plot_training_curves(fold_results, save_folder):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i, res in enumerate(fold_results):\n",
    "        plt.plot(moving_average(res['train_loss']), label=f'Fold{i+1} Train Loss')\n",
    "        plt.plot(moving_average(res['val_loss']), label=f'Fold{i+1} Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('训练和验证Loss曲线')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(save_folder, 'loss_curves.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_grad_norms(avg_grad_norms, save_folder):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(range(1, len(avg_grad_norms) + 1), avg_grad_norms)\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('平均梯度范数')\n",
    "    plt.title('各Fold平均梯度范数')\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(save_folder, 'avg_grad_norms.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(cm, save_path=None):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Reference')\n",
    "    plt.xlabel('Predicted')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def check_block_overlap(train_blocks_idx, val_blocks_idx, test_blocks_idx):\n",
    "    train_set = set(train_blocks_idx)\n",
    "    val_set = set(val_blocks_idx)\n",
    "    test_set = set(test_blocks_idx)\n",
    "\n",
    "    overlap_train_val = train_set & val_set\n",
    "    overlap_train_test = train_set & test_set\n",
    "    overlap_val_test = val_set & test_set\n",
    "\n",
    "    if overlap_train_val or overlap_train_test or overlap_val_test:\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] Block 重叠检测失败！\"\n",
    "            f\"\\nTrain-Val overlap: {overlap_train_val}\"\n",
    "            f\"\\nTrain-Test overlap: {overlap_train_test}\"\n",
    "            f\"\\nVal-Test overlap: {overlap_val_test}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\")\n",
    "\n",
    "# ================= 主训练函数（按 block 划分） =================\n",
    "def train_for_snr(SNR_dB, save_folder, results_file, group_size=288):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] 使用设备: {device}\")\n",
    "\n",
    "    # 1) 加载 block（每个 block 来自同一文件）\n",
    "    X_blocks, y_blocks, label_to_idx = load_and_preprocess_with_grouping(\n",
    "        data_path, group_size=group_size, apply_doppler=apply_doppler,\n",
    "        target_velocity=v, apply_awgn=apply_awgn, snr_db=SNR_dB, fs=fs, fc=fc\n",
    "    )\n",
    "    num_blocks = X_blocks.shape[0]\n",
    "    print(f\"[INFO] 总 block 数: {num_blocks}\")\n",
    "\n",
    "    # 2) 按 block 做 train/test 划分（保证同一 block 不会被拆分）\n",
    "    block_idx = np.arange(num_blocks)\n",
    "    train_block_idx, test_block_idx = train_test_split(\n",
    "        block_idx, test_size=0.25, stratify=y_blocks, random_state=42\n",
    "    )\n",
    "\n",
    "    X_train_blocks = X_blocks[train_block_idx]\n",
    "    y_train_blocks = y_blocks[train_block_idx]\n",
    "    X_test_blocks = X_blocks[test_block_idx]\n",
    "    y_test_blocks = y_blocks[test_block_idx]\n",
    "\n",
    "    # 展开测试 block 用于最终评估\n",
    "    X_test = X_test_blocks.reshape(-1, X_test_blocks.shape[2], X_test_blocks.shape[3])\n",
    "    y_test = np.repeat(y_test_blocks, X_test_blocks.shape[1])\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                 torch.tensor(y_test, dtype=torch.long))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"[INFO] 训练 block 数: {len(train_block_idx)}, 测试 block 数: {len(test_block_idx)}\")\n",
    "\n",
    "    # 检查训练集和测试集 block 是否有重叠\n",
    "    check_block_overlap(train_block_idx, [], test_block_idx)\n",
    "\n",
    "    # 3) KFold 按 block 做 fold，而不是样本级\n",
    "    # 注意：kfold.split 传入训练块数量即可\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    fold_test_accs = []\n",
    "\n",
    "    for fold, (train_block_idx_fold, val_block_idx_fold) in enumerate(kfold.split(X_train_blocks)):\n",
    "        print(f\"\\n====== Fold {fold+1}/{n_splits} ======\")\n",
    "\n",
    "        # 展开训练 block\n",
    "        X_train = X_train_blocks[train_block_idx_fold].reshape(-1, X_train_blocks.shape[2], X_train_blocks.shape[3])\n",
    "        y_train = np.repeat(y_train_blocks[train_block_idx_fold], X_train_blocks.shape[1])\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                      torch.tensor(y_train, dtype=torch.long))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # 展开验证 block\n",
    "        X_val = X_train_blocks[val_block_idx_fold].reshape(-1, X_train_blocks.shape[2], X_train_blocks.shape[3])\n",
    "        y_val = np.repeat(y_train_blocks[val_block_idx_fold], X_train_blocks.shape[1])\n",
    "        val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                                    torch.tensor(y_val, dtype=torch.long))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # 模型、损失函数、优化器\n",
    "        model = ResNet18_1D(num_classes=len(label_to_idx), in_planes=in_planes, dropout=dropout).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        best_model_wts = None\n",
    "        train_losses, val_losses, grad_norms = [], [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "            batch_grad_norms = []\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                grad_norm = compute_grad_norm(model)\n",
    "                batch_grad_norms.append(grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            train_loss = running_loss / max(1, len(train_loader))\n",
    "            train_acc = 100 * correct_train / max(1, total_train)\n",
    "            avg_grad_norm = float(np.mean(batch_grad_norms)) if len(batch_grad_norms) else 0.0\n",
    "            train_losses.append(train_loss)\n",
    "            grad_norms.append(avg_grad_norm)\n",
    "\n",
    "            # 验证集\n",
    "            model.eval()\n",
    "            running_val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "            all_val_labels, all_val_preds = [], []\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    loss_val = criterion(val_outputs, val_labels)\n",
    "                    running_val_loss += loss_val.item()\n",
    "                    _, val_predicted = torch.max(val_outputs, 1)\n",
    "                    total_val += val_labels.size(0)\n",
    "                    correct_val += (val_predicted == val_labels).sum().item()\n",
    "                    all_val_labels.extend(val_labels.cpu().numpy())\n",
    "                    all_val_preds.extend(val_predicted.cpu().numpy())\n",
    "\n",
    "            val_loss = running_val_loss / max(1, len(val_loader))\n",
    "            val_acc = 100 * correct_val / max(1, total_val)\n",
    "            val_losses.append(val_loss)\n",
    "            val_cm = confusion_matrix(all_val_labels, all_val_preds, labels=range(len(label_to_idx)))\n",
    "\n",
    "            log_msg = (f\"Fold {fold+1}, Epoch {epoch+1}: \"\n",
    "                       f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, \"\n",
    "                       f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%, Grad Norm={avg_grad_norm:.4f}\")\n",
    "            print(log_msg)\n",
    "            with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(log_msg + \"\\n\")\n",
    "\n",
    "            # 早停\n",
    "            if val_acc > best_val_acc + 0.01:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_model_wts = model.state_dict()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    msg = f\"早停，连续 {patience} 个 epoch 验证集未提升\"\n",
    "                    print(msg)\n",
    "                    with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                        f.write(msg + \"\\n\")\n",
    "                    break\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # 保存最佳模型 & 测试集评估\n",
    "        if best_model_wts is None:\n",
    "            best_model_wts = model.state_dict()\n",
    "\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        test_acc, all_test_cm = evaluate_model(model, test_loader, device, len(label_to_idx))\n",
    "        fold_test_accs.append(test_acc)\n",
    "\n",
    "        fold_results.append({\n",
    "            'train_loss': train_losses,\n",
    "            'val_loss': val_losses,\n",
    "            'grad_norms': grad_norms,\n",
    "            'val_cm': val_cm,\n",
    "            'test_cm': all_test_cm\n",
    "        })\n",
    "\n",
    "        plot_confusion_matrix(val_cm, save_path=os.path.join(save_folder, f\"confusion_matrix_val_fold{fold+1}.png\"))\n",
    "        plot_confusion_matrix(all_test_cm, save_path=os.path.join(save_folder, f\"confusion_matrix_test_fold{fold+1}.png\"))\n",
    "        torch.save(best_model_wts, os.path.join(save_folder, f\"best_model_fold{fold+1}.pth\"))\n",
    "\n",
    "        print(f\"Fold {fold+1} Test Acc={test_acc:.2f}%\\n\")\n",
    "        with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Fold {fold+1} Test Acc={test_acc:.2f}%\\n\")\n",
    "\n",
    "    plot_training_curves(fold_results, save_folder)\n",
    "    plot_grad_norms([float(np.mean(f['grad_norms'])) for f in fold_results], save_folder)\n",
    "\n",
    "    return float(np.mean(fold_test_accs)) if len(fold_test_accs) else 0.0\n",
    "\n",
    "# ================= SNR 循环训练 + 绘制 SNR 曲线 =================\n",
    "if __name__ == \"__main__\":\n",
    "    snr_list = list(range(20, -45, -5))\n",
    "    snr_accs = []\n",
    "\n",
    "    for snr_db in snr_list:\n",
    "        print(f\"\\n\\n================== 当前实验 SNR={snr_db} dB ==================\\n\")\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        script_name = \"LTE-V_XFR_FileBlock\"\n",
    "        folder_name = (f\"{timestamp}_{script_name}_SNR{snr_db}dB_\"\n",
    "                       f\"fd{int(compute_doppler_shift(v, fc))}_\"\n",
    "                       f\"group{256}_ResNet\")\n",
    "        save_folder = os.path.join(os.getcwd(), \"training_results\", folder_name)\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "        results_file = os.path.join(save_folder, \"results.txt\")\n",
    "        with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\n================ SNR={snr_db} dB =================\\n\")\n",
    "\n",
    "        test_acc = train_for_snr(snr_db, save_folder, results_file, group_size=256)\n",
    "        snr_accs.append(test_acc)\n",
    "        print(f\"SNR {snr_db:>3} dB → results in: {save_folder}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(snr_list, snr_accs, marker='o', linestyle='-')\n",
    "    plt.xlabel(\"SNR (dB)\")\n",
    "    plt.ylabel(\"测试集准确率 (%)\")\n",
    "    plt.title(\"SNR vs 测试集准确率\")\n",
    "    plt.grid(True)\n",
    "    snr_curve_path = os.path.join(os.getcwd(), \"training_results\", f\"SNR_vs_accuracy_{timestamp}.png\")\n",
    "    plt.savefig(snr_curve_path)\n",
    "    plt.show()\n",
    "    print(f\"[INFO] SNR vs 测试准确率曲线已保存到 {snr_curve_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f2436a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================== 当前实验 SNR=20 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "统一目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 216, 每 block 样本数: 864, 每样本长度: 288\n",
      "[INFO] 总 block 数: 216, 类别数: 9\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "[INFO] 训练 block 数: 162, 测试 block 数: 54\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.1109, Train Acc=18.50%, Val Loss=3.6757, Val Acc=12.67%, Grad Norm=4.9500\n",
      "Fold 1, Epoch 2: Train Loss=1.8776, Train Acc=28.97%, Val Loss=3.7006, Val Acc=17.65%, Grad Norm=5.3727\n",
      "Fold 1, Epoch 3: Train Loss=1.7266, Train Acc=35.23%, Val Loss=2.7790, Val Acc=20.19%, Grad Norm=4.9417\n",
      "Fold 1, Epoch 4: Train Loss=1.6057, Train Acc=40.43%, Val Loss=2.5842, Val Acc=28.67%, Grad Norm=4.8020\n",
      "Fold 1, Epoch 5: Train Loss=1.4892, Train Acc=45.10%, Val Loss=2.3172, Val Acc=27.43%, Grad Norm=4.6694\n",
      "Fold 1, Epoch 6: Train Loss=1.3907, Train Acc=49.04%, Val Loss=2.4522, Val Acc=29.86%, Grad Norm=4.5898\n",
      "Fold 1, Epoch 7: Train Loss=1.3031, Train Acc=52.11%, Val Loss=2.3035, Val Acc=28.91%, Grad Norm=4.5577\n",
      "Fold 1, Epoch 8: Train Loss=1.2093, Train Acc=55.70%, Val Loss=2.0919, Val Acc=35.56%, Grad Norm=4.5225\n",
      "Fold 1, Epoch 9: Train Loss=1.1360, Train Acc=58.93%, Val Loss=2.1408, Val Acc=37.04%, Grad Norm=4.4865\n",
      "Fold 1, Epoch 10: Train Loss=1.0791, Train Acc=60.81%, Val Loss=2.0307, Val Acc=40.54%, Grad Norm=4.4724\n",
      "Fold 1, Epoch 11: Train Loss=0.9900, Train Acc=64.55%, Val Loss=2.1347, Val Acc=40.62%, Grad Norm=4.4133\n",
      "Fold 1, Epoch 12: Train Loss=0.9502, Train Acc=65.95%, Val Loss=2.1314, Val Acc=40.36%, Grad Norm=4.5728\n",
      "Fold 1, Epoch 13: Train Loss=0.9210, Train Acc=67.22%, Val Loss=2.1857, Val Acc=40.05%, Grad Norm=4.6525\n",
      "Fold 1, Epoch 14: Train Loss=0.8961, Train Acc=68.11%, Val Loss=2.1530, Val Acc=40.99%, Grad Norm=4.7402\n",
      "Fold 1, Epoch 15: Train Loss=0.8652, Train Acc=69.15%, Val Loss=2.0784, Val Acc=41.04%, Grad Norm=4.7951\n",
      "Fold 1, Epoch 16: Train Loss=0.8358, Train Acc=70.26%, Val Loss=2.1648, Val Acc=39.34%, Grad Norm=4.9055\n",
      "Fold 1, Epoch 17: Train Loss=0.8155, Train Acc=71.13%, Val Loss=1.9268, Val Acc=43.06%, Grad Norm=4.9729\n",
      "Fold 1, Epoch 18: Train Loss=0.7920, Train Acc=72.12%, Val Loss=2.1236, Val Acc=41.39%, Grad Norm=5.0437\n",
      "Fold 1, Epoch 19: Train Loss=0.7678, Train Acc=72.84%, Val Loss=2.1543, Val Acc=39.96%, Grad Norm=5.1431\n",
      "Fold 1, Epoch 20: Train Loss=0.7457, Train Acc=73.67%, Val Loss=2.0974, Val Acc=42.19%, Grad Norm=5.1966\n",
      "Fold 1, Epoch 21: Train Loss=0.7084, Train Acc=75.28%, Val Loss=2.0485, Val Acc=42.03%, Grad Norm=5.2479\n",
      "Fold 1, Epoch 22: Train Loss=0.6857, Train Acc=75.94%, Val Loss=2.0946, Val Acc=42.13%, Grad Norm=5.3916\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=52.61%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.0893, Train Acc=19.96%, Val Loss=4.4261, Val Acc=14.53%, Grad Norm=4.9793\n",
      "Fold 2, Epoch 2: Train Loss=1.8046, Train Acc=32.89%, Val Loss=2.9847, Val Acc=16.81%, Grad Norm=5.3390\n",
      "Fold 2, Epoch 3: Train Loss=1.6335, Train Acc=39.77%, Val Loss=3.2200, Val Acc=16.91%, Grad Norm=4.7955\n",
      "Fold 2, Epoch 4: Train Loss=1.5234, Train Acc=44.03%, Val Loss=2.9185, Val Acc=18.80%, Grad Norm=4.5541\n",
      "Fold 2, Epoch 5: Train Loss=1.4256, Train Acc=47.32%, Val Loss=2.5040, Val Acc=25.07%, Grad Norm=4.5092\n",
      "Fold 2, Epoch 6: Train Loss=1.3197, Train Acc=51.70%, Val Loss=2.5974, Val Acc=25.22%, Grad Norm=4.5395\n",
      "Fold 2, Epoch 7: Train Loss=1.2297, Train Acc=55.13%, Val Loss=2.4020, Val Acc=29.62%, Grad Norm=4.4886\n",
      "Fold 2, Epoch 8: Train Loss=1.1496, Train Acc=58.22%, Val Loss=2.5123, Val Acc=34.09%, Grad Norm=4.4552\n",
      "Fold 2, Epoch 9: Train Loss=1.0877, Train Acc=60.52%, Val Loss=2.4442, Val Acc=34.14%, Grad Norm=4.4426\n",
      "Fold 2, Epoch 10: Train Loss=1.0337, Train Acc=62.29%, Val Loss=2.3504, Val Acc=32.64%, Grad Norm=4.4542\n",
      "Fold 2, Epoch 11: Train Loss=0.9369, Train Acc=66.70%, Val Loss=2.4000, Val Acc=34.81%, Grad Norm=4.3362\n",
      "Fold 2, Epoch 12: Train Loss=0.9036, Train Acc=67.64%, Val Loss=2.4032, Val Acc=35.35%, Grad Norm=4.5025\n",
      "Fold 2, Epoch 13: Train Loss=0.8807, Train Acc=68.51%, Val Loss=2.3649, Val Acc=36.12%, Grad Norm=4.6440\n",
      "Fold 2, Epoch 14: Train Loss=0.8528, Train Acc=69.40%, Val Loss=2.3703, Val Acc=37.82%, Grad Norm=4.6631\n",
      "Fold 2, Epoch 15: Train Loss=0.8287, Train Acc=70.10%, Val Loss=2.5354, Val Acc=36.55%, Grad Norm=4.8108\n",
      "Fold 2, Epoch 16: Train Loss=0.8010, Train Acc=71.21%, Val Loss=2.4528, Val Acc=35.90%, Grad Norm=4.8761\n",
      "Fold 2, Epoch 17: Train Loss=0.7823, Train Acc=72.10%, Val Loss=2.4632, Val Acc=35.39%, Grad Norm=4.9813\n",
      "Fold 2, Epoch 18: Train Loss=0.7600, Train Acc=72.80%, Val Loss=2.4577, Val Acc=37.86%, Grad Norm=5.0632\n",
      "Fold 2, Epoch 19: Train Loss=0.7361, Train Acc=73.69%, Val Loss=2.5091, Val Acc=36.71%, Grad Norm=5.1025\n",
      "Fold 2, Epoch 20: Train Loss=0.7230, Train Acc=74.23%, Val Loss=2.4264, Val Acc=37.50%, Grad Norm=5.2406\n",
      "Fold 2, Epoch 21: Train Loss=0.6703, Train Acc=76.15%, Val Loss=2.5116, Val Acc=37.95%, Grad Norm=5.1639\n",
      "Fold 2, Epoch 22: Train Loss=0.6489, Train Acc=77.22%, Val Loss=2.4785, Val Acc=38.08%, Grad Norm=5.3132\n",
      "Fold 2, Epoch 23: Train Loss=0.6429, Train Acc=77.19%, Val Loss=2.5202, Val Acc=37.71%, Grad Norm=5.4970\n",
      "Fold 2, Epoch 24: Train Loss=0.6315, Train Acc=77.82%, Val Loss=2.4721, Val Acc=39.83%, Grad Norm=5.5587\n",
      "Fold 2, Epoch 25: Train Loss=0.6198, Train Acc=78.24%, Val Loss=2.5158, Val Acc=38.63%, Grad Norm=5.7134\n",
      "Fold 2, Epoch 26: Train Loss=0.6061, Train Acc=78.77%, Val Loss=2.5997, Val Acc=39.07%, Grad Norm=5.7902\n",
      "Fold 2, Epoch 27: Train Loss=0.5916, Train Acc=79.02%, Val Loss=2.4747, Val Acc=39.86%, Grad Norm=5.8659\n",
      "Fold 2, Epoch 28: Train Loss=0.5802, Train Acc=79.55%, Val Loss=2.4188, Val Acc=39.71%, Grad Norm=5.9622\n",
      "Fold 2, Epoch 29: Train Loss=0.5741, Train Acc=79.58%, Val Loss=2.5022, Val Acc=38.76%, Grad Norm=6.0729\n",
      "Fold 2, Epoch 30: Train Loss=0.5680, Train Acc=79.92%, Val Loss=2.6483, Val Acc=38.42%, Grad Norm=6.1823\n",
      "Fold 2, Epoch 31: Train Loss=0.5354, Train Acc=81.27%, Val Loss=2.4318, Val Acc=41.08%, Grad Norm=6.1290\n",
      "Fold 2, Epoch 32: Train Loss=0.5236, Train Acc=81.73%, Val Loss=2.5916, Val Acc=39.59%, Grad Norm=6.2489\n",
      "Fold 2, Epoch 33: Train Loss=0.5252, Train Acc=81.69%, Val Loss=2.5266, Val Acc=40.04%, Grad Norm=6.4196\n",
      "Fold 2, Epoch 34: Train Loss=0.5067, Train Acc=82.33%, Val Loss=2.5237, Val Acc=40.66%, Grad Norm=6.4364\n",
      "Fold 2, Epoch 35: Train Loss=0.5094, Train Acc=82.43%, Val Loss=2.5798, Val Acc=39.34%, Grad Norm=6.5496\n",
      "Fold 2, Epoch 36: Train Loss=0.4998, Train Acc=82.49%, Val Loss=2.4906, Val Acc=40.58%, Grad Norm=6.6563\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=53.20%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.0908, Train Acc=19.89%, Val Loss=4.3718, Val Acc=8.70%, Grad Norm=4.7438\n",
      "Fold 3, Epoch 2: Train Loss=1.8377, Train Acc=31.07%, Val Loss=3.5687, Val Acc=20.24%, Grad Norm=5.3784\n",
      "Fold 3, Epoch 3: Train Loss=1.6371, Train Acc=39.33%, Val Loss=3.6544, Val Acc=25.80%, Grad Norm=4.9416\n",
      "Fold 3, Epoch 4: Train Loss=1.5200, Train Acc=43.64%, Val Loss=3.1776, Val Acc=28.87%, Grad Norm=4.6139\n",
      "Fold 3, Epoch 5: Train Loss=1.4115, Train Acc=47.89%, Val Loss=3.0561, Val Acc=30.24%, Grad Norm=4.5036\n",
      "Fold 3, Epoch 6: Train Loss=1.3179, Train Acc=51.25%, Val Loss=3.0886, Val Acc=28.90%, Grad Norm=4.4476\n",
      "Fold 3, Epoch 7: Train Loss=1.2385, Train Acc=54.28%, Val Loss=2.9769, Val Acc=33.26%, Grad Norm=4.3731\n",
      "Fold 3, Epoch 8: Train Loss=1.1731, Train Acc=57.07%, Val Loss=2.8596, Val Acc=32.09%, Grad Norm=4.3632\n",
      "Fold 3, Epoch 9: Train Loss=1.1214, Train Acc=58.81%, Val Loss=2.7642, Val Acc=33.96%, Grad Norm=4.3458\n",
      "Fold 3, Epoch 10: Train Loss=1.0688, Train Acc=61.11%, Val Loss=2.6696, Val Acc=37.90%, Grad Norm=4.2862\n",
      "Fold 3, Epoch 11: Train Loss=0.9984, Train Acc=63.86%, Val Loss=2.7975, Val Acc=36.87%, Grad Norm=4.2143\n",
      "Fold 3, Epoch 12: Train Loss=0.9595, Train Acc=65.24%, Val Loss=2.7651, Val Acc=37.22%, Grad Norm=4.3540\n",
      "Fold 3, Epoch 13: Train Loss=0.9356, Train Acc=65.99%, Val Loss=2.8237, Val Acc=39.41%, Grad Norm=4.4796\n",
      "Fold 3, Epoch 14: Train Loss=0.9120, Train Acc=67.20%, Val Loss=2.7595, Val Acc=39.58%, Grad Norm=4.5977\n",
      "Fold 3, Epoch 15: Train Loss=0.8929, Train Acc=67.71%, Val Loss=2.7954, Val Acc=39.21%, Grad Norm=4.6745\n",
      "Fold 3, Epoch 16: Train Loss=0.8680, Train Acc=68.70%, Val Loss=2.8828, Val Acc=39.94%, Grad Norm=4.7799\n",
      "Fold 3, Epoch 17: Train Loss=0.8401, Train Acc=69.82%, Val Loss=2.7557, Val Acc=39.93%, Grad Norm=4.8780\n",
      "Fold 3, Epoch 18: Train Loss=0.8272, Train Acc=70.13%, Val Loss=2.7789, Val Acc=40.08%, Grad Norm=4.9679\n",
      "Fold 3, Epoch 19: Train Loss=0.8059, Train Acc=70.98%, Val Loss=2.7597, Val Acc=40.91%, Grad Norm=5.0385\n",
      "Fold 3, Epoch 20: Train Loss=0.7823, Train Acc=71.95%, Val Loss=2.7261, Val Acc=41.51%, Grad Norm=5.1189\n",
      "Fold 3, Epoch 21: Train Loss=0.7384, Train Acc=73.64%, Val Loss=2.7495, Val Acc=41.78%, Grad Norm=5.1495\n",
      "Fold 3, Epoch 22: Train Loss=0.7255, Train Acc=74.14%, Val Loss=2.8829, Val Acc=39.37%, Grad Norm=5.3587\n",
      "Fold 3, Epoch 23: Train Loss=0.7057, Train Acc=74.85%, Val Loss=2.6720, Val Acc=42.75%, Grad Norm=5.4613\n",
      "Fold 3, Epoch 24: Train Loss=0.6953, Train Acc=75.42%, Val Loss=2.7085, Val Acc=42.27%, Grad Norm=5.6094\n",
      "Fold 3, Epoch 25: Train Loss=0.6865, Train Acc=75.52%, Val Loss=2.6970, Val Acc=42.59%, Grad Norm=5.7835\n",
      "Fold 3, Epoch 26: Train Loss=0.6730, Train Acc=76.35%, Val Loss=2.7169, Val Acc=42.63%, Grad Norm=5.8858\n",
      "Fold 3, Epoch 27: Train Loss=0.6590, Train Acc=76.83%, Val Loss=2.6814, Val Acc=42.90%, Grad Norm=5.9754\n",
      "Fold 3, Epoch 28: Train Loss=0.6521, Train Acc=77.05%, Val Loss=2.7050, Val Acc=43.09%, Grad Norm=6.1140\n",
      "Fold 3, Epoch 29: Train Loss=0.6370, Train Acc=77.28%, Val Loss=2.7566, Val Acc=42.80%, Grad Norm=6.2097\n",
      "Fold 3, Epoch 30: Train Loss=0.6279, Train Acc=77.69%, Val Loss=2.7130, Val Acc=43.17%, Grad Norm=6.3298\n",
      "Fold 3, Epoch 31: Train Loss=0.6005, Train Acc=78.81%, Val Loss=2.7143, Val Acc=43.31%, Grad Norm=6.3714\n",
      "Fold 3, Epoch 32: Train Loss=0.5903, Train Acc=79.41%, Val Loss=2.6313, Val Acc=44.09%, Grad Norm=6.4283\n",
      "Fold 3, Epoch 33: Train Loss=0.5831, Train Acc=79.64%, Val Loss=2.6568, Val Acc=45.77%, Grad Norm=6.5799\n",
      "Fold 3, Epoch 34: Train Loss=0.5819, Train Acc=79.56%, Val Loss=2.7074, Val Acc=43.68%, Grad Norm=6.6762\n",
      "Fold 3, Epoch 35: Train Loss=0.5737, Train Acc=79.89%, Val Loss=2.7182, Val Acc=43.03%, Grad Norm=6.7441\n",
      "Fold 3, Epoch 36: Train Loss=0.5670, Train Acc=80.40%, Val Loss=2.7571, Val Acc=42.63%, Grad Norm=6.8297\n",
      "Fold 3, Epoch 37: Train Loss=0.5566, Train Acc=80.56%, Val Loss=2.7332, Val Acc=43.95%, Grad Norm=6.9331\n",
      "Fold 3, Epoch 38: Train Loss=0.5510, Train Acc=80.74%, Val Loss=2.7096, Val Acc=43.63%, Grad Norm=6.9999\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=52.57%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1003, Train Acc=20.04%, Val Loss=4.0093, Val Acc=19.47%, Grad Norm=4.8710\n",
      "Fold 4, Epoch 2: Train Loss=1.8903, Train Acc=29.40%, Val Loss=4.4817, Val Acc=18.87%, Grad Norm=5.2180\n",
      "Fold 4, Epoch 3: Train Loss=1.7087, Train Acc=37.14%, Val Loss=4.0599, Val Acc=21.18%, Grad Norm=4.8959\n",
      "Fold 4, Epoch 4: Train Loss=1.5710, Train Acc=42.06%, Val Loss=4.0406, Val Acc=26.06%, Grad Norm=4.5723\n",
      "Fold 4, Epoch 5: Train Loss=1.4670, Train Acc=45.69%, Val Loss=3.7339, Val Acc=27.40%, Grad Norm=4.5198\n",
      "Fold 4, Epoch 6: Train Loss=1.3688, Train Acc=49.71%, Val Loss=3.7928, Val Acc=27.83%, Grad Norm=4.4358\n",
      "Fold 4, Epoch 7: Train Loss=1.2892, Train Acc=52.39%, Val Loss=3.8120, Val Acc=30.52%, Grad Norm=4.3310\n",
      "Fold 4, Epoch 8: Train Loss=1.2217, Train Acc=54.93%, Val Loss=3.8040, Val Acc=35.03%, Grad Norm=4.3436\n",
      "Fold 4, Epoch 9: Train Loss=1.1659, Train Acc=57.33%, Val Loss=3.4108, Val Acc=35.81%, Grad Norm=4.2732\n",
      "Fold 4, Epoch 10: Train Loss=1.1075, Train Acc=59.36%, Val Loss=3.4744, Val Acc=36.55%, Grad Norm=4.2891\n",
      "Fold 4, Epoch 11: Train Loss=1.0288, Train Acc=62.86%, Val Loss=3.6062, Val Acc=35.73%, Grad Norm=4.2954\n",
      "Fold 4, Epoch 12: Train Loss=0.9956, Train Acc=63.96%, Val Loss=3.6623, Val Acc=37.48%, Grad Norm=4.4254\n",
      "Fold 4, Epoch 13: Train Loss=0.9735, Train Acc=64.46%, Val Loss=3.6871, Val Acc=34.85%, Grad Norm=4.5603\n",
      "Fold 4, Epoch 14: Train Loss=0.9468, Train Acc=65.73%, Val Loss=3.8368, Val Acc=36.30%, Grad Norm=4.6231\n",
      "Fold 4, Epoch 15: Train Loss=0.9194, Train Acc=66.65%, Val Loss=3.7386, Val Acc=35.76%, Grad Norm=4.7446\n",
      "Fold 4, Epoch 16: Train Loss=0.9008, Train Acc=67.39%, Val Loss=3.6090, Val Acc=38.98%, Grad Norm=4.8640\n",
      "Fold 4, Epoch 17: Train Loss=0.8739, Train Acc=68.51%, Val Loss=3.6225, Val Acc=39.85%, Grad Norm=4.8489\n",
      "Fold 4, Epoch 18: Train Loss=0.8500, Train Acc=69.27%, Val Loss=3.5844, Val Acc=40.68%, Grad Norm=5.0339\n",
      "Fold 4, Epoch 19: Train Loss=0.8362, Train Acc=69.91%, Val Loss=3.7543, Val Acc=41.13%, Grad Norm=5.1314\n",
      "Fold 4, Epoch 20: Train Loss=0.8134, Train Acc=70.73%, Val Loss=3.5354, Val Acc=40.43%, Grad Norm=5.2118\n",
      "Fold 4, Epoch 21: Train Loss=0.7708, Train Acc=72.21%, Val Loss=3.5657, Val Acc=40.44%, Grad Norm=5.1964\n",
      "Fold 4, Epoch 22: Train Loss=0.7485, Train Acc=73.23%, Val Loss=3.5413, Val Acc=41.09%, Grad Norm=5.3930\n",
      "Fold 4, Epoch 23: Train Loss=0.7407, Train Acc=73.48%, Val Loss=3.6452, Val Acc=41.20%, Grad Norm=5.5329\n",
      "Fold 4, Epoch 24: Train Loss=0.7234, Train Acc=74.21%, Val Loss=3.6205, Val Acc=41.91%, Grad Norm=5.6364\n",
      "Fold 4, Epoch 25: Train Loss=0.7141, Train Acc=74.67%, Val Loss=3.6794, Val Acc=42.04%, Grad Norm=5.7958\n",
      "Fold 4, Epoch 26: Train Loss=0.7022, Train Acc=74.81%, Val Loss=3.5306, Val Acc=41.36%, Grad Norm=5.8914\n",
      "Fold 4, Epoch 27: Train Loss=0.6868, Train Acc=75.61%, Val Loss=3.7154, Val Acc=42.53%, Grad Norm=5.9757\n",
      "Fold 4, Epoch 28: Train Loss=0.6824, Train Acc=75.55%, Val Loss=3.6187, Val Acc=43.95%, Grad Norm=6.1381\n",
      "Fold 4, Epoch 29: Train Loss=0.6710, Train Acc=76.23%, Val Loss=3.7601, Val Acc=41.80%, Grad Norm=6.2312\n",
      "Fold 4, Epoch 30: Train Loss=0.6597, Train Acc=76.48%, Val Loss=3.6070, Val Acc=40.94%, Grad Norm=6.3187\n",
      "Fold 4, Epoch 31: Train Loss=0.6262, Train Acc=77.78%, Val Loss=3.5295, Val Acc=43.13%, Grad Norm=6.3357\n",
      "Fold 4, Epoch 32: Train Loss=0.6200, Train Acc=78.23%, Val Loss=3.6191, Val Acc=42.94%, Grad Norm=6.4813\n",
      "Fold 4, Epoch 33: Train Loss=0.6118, Train Acc=78.58%, Val Loss=3.5604, Val Acc=42.77%, Grad Norm=6.5923\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=48.03%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.0839, Train Acc=20.38%, Val Loss=4.4628, Val Acc=10.05%, Grad Norm=5.0175\n",
      "Fold 5, Epoch 2: Train Loss=1.8408, Train Acc=29.82%, Val Loss=4.0619, Val Acc=11.89%, Grad Norm=5.1586\n",
      "Fold 5, Epoch 3: Train Loss=1.6789, Train Acc=36.78%, Val Loss=3.9552, Val Acc=16.69%, Grad Norm=4.7434\n",
      "Fold 5, Epoch 4: Train Loss=1.5449, Train Acc=42.84%, Val Loss=3.6095, Val Acc=21.60%, Grad Norm=4.6188\n",
      "Fold 5, Epoch 5: Train Loss=1.4431, Train Acc=46.78%, Val Loss=3.1170, Val Acc=26.66%, Grad Norm=4.4819\n",
      "Fold 5, Epoch 6: Train Loss=1.3474, Train Acc=50.20%, Val Loss=3.2193, Val Acc=25.49%, Grad Norm=4.4003\n",
      "Fold 5, Epoch 7: Train Loss=1.2649, Train Acc=53.24%, Val Loss=3.0268, Val Acc=28.90%, Grad Norm=4.3552\n",
      "Fold 5, Epoch 8: Train Loss=1.1962, Train Acc=56.02%, Val Loss=2.8695, Val Acc=31.28%, Grad Norm=4.3667\n",
      "Fold 5, Epoch 9: Train Loss=1.1238, Train Acc=58.76%, Val Loss=2.9914, Val Acc=30.24%, Grad Norm=4.3041\n",
      "Fold 5, Epoch 10: Train Loss=1.0692, Train Acc=60.60%, Val Loss=2.8467, Val Acc=34.79%, Grad Norm=4.2959\n",
      "Fold 5, Epoch 11: Train Loss=0.9885, Train Acc=63.91%, Val Loss=2.5948, Val Acc=33.62%, Grad Norm=4.2473\n",
      "Fold 5, Epoch 12: Train Loss=0.9557, Train Acc=65.10%, Val Loss=2.5998, Val Acc=37.73%, Grad Norm=4.3780\n",
      "Fold 5, Epoch 13: Train Loss=0.9222, Train Acc=66.53%, Val Loss=2.5774, Val Acc=36.73%, Grad Norm=4.4762\n",
      "Fold 5, Epoch 14: Train Loss=0.9043, Train Acc=66.81%, Val Loss=2.5803, Val Acc=39.65%, Grad Norm=4.6180\n",
      "Fold 5, Epoch 15: Train Loss=0.8690, Train Acc=68.61%, Val Loss=2.6368, Val Acc=37.97%, Grad Norm=4.6845\n",
      "Fold 5, Epoch 16: Train Loss=0.8554, Train Acc=68.95%, Val Loss=2.7877, Val Acc=37.58%, Grad Norm=4.8163\n",
      "Fold 5, Epoch 17: Train Loss=0.8339, Train Acc=69.65%, Val Loss=2.6625, Val Acc=38.50%, Grad Norm=4.9048\n",
      "Fold 5, Epoch 18: Train Loss=0.8119, Train Acc=70.60%, Val Loss=2.4871, Val Acc=40.85%, Grad Norm=4.9895\n",
      "Fold 5, Epoch 19: Train Loss=0.7926, Train Acc=71.18%, Val Loss=2.7827, Val Acc=39.62%, Grad Norm=5.0951\n",
      "Fold 5, Epoch 20: Train Loss=0.7696, Train Acc=72.24%, Val Loss=2.6583, Val Acc=40.08%, Grad Norm=5.1314\n",
      "Fold 5, Epoch 21: Train Loss=0.7224, Train Acc=74.11%, Val Loss=2.5744, Val Acc=40.49%, Grad Norm=5.1242\n",
      "Fold 5, Epoch 22: Train Loss=0.7052, Train Acc=74.62%, Val Loss=2.6696, Val Acc=41.70%, Grad Norm=5.3329\n",
      "Fold 5, Epoch 23: Train Loss=0.6968, Train Acc=74.92%, Val Loss=2.7187, Val Acc=40.27%, Grad Norm=5.5013\n",
      "Fold 5, Epoch 24: Train Loss=0.6856, Train Acc=75.69%, Val Loss=2.7342, Val Acc=40.29%, Grad Norm=5.6045\n",
      "Fold 5, Epoch 25: Train Loss=0.6700, Train Acc=76.19%, Val Loss=2.6755, Val Acc=41.97%, Grad Norm=5.7056\n",
      "Fold 5, Epoch 26: Train Loss=0.6602, Train Acc=76.38%, Val Loss=2.7381, Val Acc=42.52%, Grad Norm=5.7733\n",
      "Fold 5, Epoch 27: Train Loss=0.6430, Train Acc=77.14%, Val Loss=2.7414, Val Acc=41.32%, Grad Norm=5.9238\n",
      "Fold 5, Epoch 28: Train Loss=0.6382, Train Acc=77.28%, Val Loss=2.6509, Val Acc=43.01%, Grad Norm=6.0525\n",
      "Fold 5, Epoch 29: Train Loss=0.6272, Train Acc=77.66%, Val Loss=2.8205, Val Acc=41.15%, Grad Norm=6.1549\n",
      "Fold 5, Epoch 30: Train Loss=0.6145, Train Acc=77.97%, Val Loss=2.7079, Val Acc=42.72%, Grad Norm=6.2525\n",
      "Fold 5, Epoch 31: Train Loss=0.5872, Train Acc=79.13%, Val Loss=2.6629, Val Acc=43.49%, Grad Norm=6.2438\n",
      "Fold 5, Epoch 32: Train Loss=0.5823, Train Acc=79.36%, Val Loss=2.6738, Val Acc=42.06%, Grad Norm=6.4115\n",
      "Fold 5, Epoch 33: Train Loss=0.5680, Train Acc=79.89%, Val Loss=2.6409, Val Acc=42.81%, Grad Norm=6.4477\n",
      "Fold 5, Epoch 34: Train Loss=0.5647, Train Acc=80.09%, Val Loss=2.6621, Val Acc=42.96%, Grad Norm=6.6008\n",
      "Fold 5, Epoch 35: Train Loss=0.5605, Train Acc=80.29%, Val Loss=2.6896, Val Acc=43.66%, Grad Norm=6.6757\n",
      "Fold 5, Epoch 36: Train Loss=0.5531, Train Acc=80.52%, Val Loss=2.6006, Val Acc=43.72%, Grad Norm=6.7807\n",
      "Fold 5, Epoch 37: Train Loss=0.5473, Train Acc=80.79%, Val Loss=2.7988, Val Acc=41.69%, Grad Norm=6.8343\n",
      "Fold 5, Epoch 38: Train Loss=0.5391, Train Acc=81.13%, Val Loss=2.5746, Val Acc=44.55%, Grad Norm=6.8801\n",
      "Fold 5, Epoch 39: Train Loss=0.5359, Train Acc=81.12%, Val Loss=2.7223, Val Acc=42.42%, Grad Norm=6.9802\n",
      "Fold 5, Epoch 40: Train Loss=0.5330, Train Acc=81.26%, Val Loss=2.5953, Val Acc=43.70%, Grad Norm=7.0779\n",
      "Fold 5, Epoch 41: Train Loss=0.5181, Train Acc=82.02%, Val Loss=2.7306, Val Acc=42.58%, Grad Norm=7.0536\n",
      "Fold 5, Epoch 42: Train Loss=0.5080, Train Acc=82.36%, Val Loss=2.6798, Val Acc=42.99%, Grad Norm=7.1218\n",
      "Fold 5, Epoch 43: Train Loss=0.5081, Train Acc=82.31%, Val Loss=2.6661, Val Acc=43.04%, Grad Norm=7.2043\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=50.92%\n",
      "\n",
      "SNR  20 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-24_17-50-59_LTE-V_XFR_SingleFileBlock_SNR20dB_fd655_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=15 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "统一目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:09<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 216, 每 block 样本数: 864, 每样本长度: 288\n",
      "[INFO] 总 block 数: 216, 类别数: 9\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "[INFO] 训练 block 数: 162, 测试 block 数: 54\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.0968, Train Acc=19.21%, Val Loss=2.8067, Val Acc=13.46%, Grad Norm=4.9227\n",
      "Fold 1, Epoch 2: Train Loss=1.8613, Train Acc=29.51%, Val Loss=3.0075, Val Acc=18.30%, Grad Norm=5.2463\n",
      "Fold 1, Epoch 3: Train Loss=1.7118, Train Acc=35.95%, Val Loss=2.9892, Val Acc=21.82%, Grad Norm=4.9268\n",
      "Fold 1, Epoch 4: Train Loss=1.5805, Train Acc=41.23%, Val Loss=2.9749, Val Acc=28.66%, Grad Norm=4.7926\n",
      "Fold 1, Epoch 5: Train Loss=1.4848, Train Acc=45.38%, Val Loss=2.5078, Val Acc=24.49%, Grad Norm=4.6430\n",
      "Fold 1, Epoch 6: Train Loss=1.3940, Train Acc=49.07%, Val Loss=2.3728, Val Acc=25.51%, Grad Norm=4.5927\n",
      "Fold 1, Epoch 7: Train Loss=1.3139, Train Acc=52.11%, Val Loss=2.2185, Val Acc=32.67%, Grad Norm=4.4737\n",
      "Fold 1, Epoch 8: Train Loss=1.2401, Train Acc=54.91%, Val Loss=2.3003, Val Acc=32.54%, Grad Norm=4.4988\n",
      "Fold 1, Epoch 9: Train Loss=1.1688, Train Acc=57.68%, Val Loss=2.1763, Val Acc=34.35%, Grad Norm=4.4914\n",
      "Fold 1, Epoch 10: Train Loss=1.0977, Train Acc=60.55%, Val Loss=2.2327, Val Acc=34.60%, Grad Norm=4.4671\n",
      "Fold 1, Epoch 11: Train Loss=1.0101, Train Acc=63.62%, Val Loss=1.9840, Val Acc=39.53%, Grad Norm=4.4142\n",
      "Fold 1, Epoch 12: Train Loss=0.9677, Train Acc=65.49%, Val Loss=2.1024, Val Acc=39.49%, Grad Norm=4.5420\n",
      "Fold 1, Epoch 13: Train Loss=0.9406, Train Acc=66.28%, Val Loss=2.0720, Val Acc=37.87%, Grad Norm=4.6520\n",
      "Fold 1, Epoch 14: Train Loss=0.9079, Train Acc=67.78%, Val Loss=2.1357, Val Acc=38.97%, Grad Norm=4.7484\n",
      "Fold 1, Epoch 15: Train Loss=0.8807, Train Acc=68.66%, Val Loss=2.0504, Val Acc=39.77%, Grad Norm=4.8432\n",
      "Fold 1, Epoch 16: Train Loss=0.8593, Train Acc=69.47%, Val Loss=2.2253, Val Acc=38.37%, Grad Norm=4.9404\n",
      "Fold 1, Epoch 17: Train Loss=0.8345, Train Acc=70.42%, Val Loss=2.1343, Val Acc=39.08%, Grad Norm=5.0180\n",
      "Fold 1, Epoch 18: Train Loss=0.8137, Train Acc=71.18%, Val Loss=2.0738, Val Acc=37.97%, Grad Norm=5.1056\n",
      "Fold 1, Epoch 19: Train Loss=0.7976, Train Acc=71.90%, Val Loss=2.1364, Val Acc=38.65%, Grad Norm=5.1893\n",
      "Fold 1, Epoch 20: Train Loss=0.7766, Train Acc=72.66%, Val Loss=2.1151, Val Acc=41.32%, Grad Norm=5.2554\n",
      "Fold 1, Epoch 21: Train Loss=0.7281, Train Acc=74.61%, Val Loss=2.1653, Val Acc=40.01%, Grad Norm=5.2264\n",
      "Fold 1, Epoch 22: Train Loss=0.7047, Train Acc=75.31%, Val Loss=2.1121, Val Acc=40.78%, Grad Norm=5.3436\n",
      "Fold 1, Epoch 23: Train Loss=0.6954, Train Acc=75.80%, Val Loss=2.1452, Val Acc=40.28%, Grad Norm=5.5032\n",
      "Fold 1, Epoch 24: Train Loss=0.6809, Train Acc=76.25%, Val Loss=2.1625, Val Acc=40.15%, Grad Norm=5.6257\n",
      "Fold 1, Epoch 25: Train Loss=0.6727, Train Acc=76.30%, Val Loss=2.2197, Val Acc=39.11%, Grad Norm=5.7732\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=53.45%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.0897, Train Acc=19.69%, Val Loss=3.6120, Val Acc=18.53%, Grad Norm=4.9623\n",
      "Fold 2, Epoch 2: Train Loss=1.8119, Train Acc=32.83%, Val Loss=3.3907, Val Acc=12.84%, Grad Norm=5.2862\n",
      "Fold 2, Epoch 3: Train Loss=1.6528, Train Acc=39.24%, Val Loss=2.9734, Val Acc=16.39%, Grad Norm=4.7198\n",
      "Fold 2, Epoch 4: Train Loss=1.5435, Train Acc=43.42%, Val Loss=2.6775, Val Acc=20.83%, Grad Norm=4.5395\n",
      "Fold 2, Epoch 5: Train Loss=1.4430, Train Acc=46.92%, Val Loss=2.7196, Val Acc=22.81%, Grad Norm=4.4627\n",
      "Fold 2, Epoch 6: Train Loss=1.3389, Train Acc=50.75%, Val Loss=2.8475, Val Acc=24.74%, Grad Norm=4.4131\n",
      "Fold 2, Epoch 7: Train Loss=1.2434, Train Acc=54.43%, Val Loss=2.7623, Val Acc=28.57%, Grad Norm=4.3818\n",
      "Fold 2, Epoch 8: Train Loss=1.1631, Train Acc=57.67%, Val Loss=2.4915, Val Acc=30.98%, Grad Norm=4.4022\n",
      "Fold 2, Epoch 9: Train Loss=1.1091, Train Acc=59.57%, Val Loss=2.6763, Val Acc=32.03%, Grad Norm=4.3334\n",
      "Fold 2, Epoch 10: Train Loss=1.0440, Train Acc=62.07%, Val Loss=2.4563, Val Acc=32.74%, Grad Norm=4.2852\n",
      "Fold 2, Epoch 11: Train Loss=0.9647, Train Acc=65.25%, Val Loss=2.5982, Val Acc=32.74%, Grad Norm=4.2517\n",
      "Fold 2, Epoch 12: Train Loss=0.9321, Train Acc=66.50%, Val Loss=2.5041, Val Acc=33.24%, Grad Norm=4.3973\n",
      "Fold 2, Epoch 13: Train Loss=0.9069, Train Acc=67.10%, Val Loss=2.4739, Val Acc=34.44%, Grad Norm=4.5515\n",
      "Fold 2, Epoch 14: Train Loss=0.8777, Train Acc=68.33%, Val Loss=2.4176, Val Acc=36.67%, Grad Norm=4.5674\n",
      "Fold 2, Epoch 15: Train Loss=0.8507, Train Acc=69.46%, Val Loss=2.5275, Val Acc=34.45%, Grad Norm=4.6739\n",
      "Fold 2, Epoch 16: Train Loss=0.8326, Train Acc=70.01%, Val Loss=2.4848, Val Acc=36.09%, Grad Norm=4.7932\n",
      "Fold 2, Epoch 17: Train Loss=0.8062, Train Acc=71.09%, Val Loss=2.5888, Val Acc=35.83%, Grad Norm=4.8433\n",
      "Fold 2, Epoch 18: Train Loss=0.7871, Train Acc=71.76%, Val Loss=2.4959, Val Acc=38.03%, Grad Norm=4.9148\n",
      "Fold 2, Epoch 19: Train Loss=0.7699, Train Acc=72.15%, Val Loss=2.4123, Val Acc=37.02%, Grad Norm=5.0213\n",
      "Fold 2, Epoch 20: Train Loss=0.7526, Train Acc=72.96%, Val Loss=2.6037, Val Acc=34.95%, Grad Norm=5.0784\n",
      "Fold 2, Epoch 21: Train Loss=0.7062, Train Acc=74.91%, Val Loss=2.4981, Val Acc=38.35%, Grad Norm=5.0922\n",
      "Fold 2, Epoch 22: Train Loss=0.6889, Train Acc=75.55%, Val Loss=2.5243, Val Acc=37.63%, Grad Norm=5.2199\n",
      "Fold 2, Epoch 23: Train Loss=0.6719, Train Acc=75.99%, Val Loss=2.4995, Val Acc=36.54%, Grad Norm=5.3944\n",
      "Fold 2, Epoch 24: Train Loss=0.6661, Train Acc=76.24%, Val Loss=2.5813, Val Acc=37.88%, Grad Norm=5.5143\n",
      "Fold 2, Epoch 25: Train Loss=0.6496, Train Acc=76.95%, Val Loss=2.4165, Val Acc=39.11%, Grad Norm=5.5779\n",
      "Fold 2, Epoch 26: Train Loss=0.6366, Train Acc=77.43%, Val Loss=2.5668, Val Acc=37.29%, Grad Norm=5.6795\n",
      "Fold 2, Epoch 27: Train Loss=0.6247, Train Acc=78.02%, Val Loss=2.6259, Val Acc=38.55%, Grad Norm=5.7915\n",
      "Fold 2, Epoch 28: Train Loss=0.6105, Train Acc=78.58%, Val Loss=2.4995, Val Acc=39.72%, Grad Norm=5.8458\n",
      "Fold 2, Epoch 29: Train Loss=0.6060, Train Acc=78.63%, Val Loss=2.6109, Val Acc=38.30%, Grad Norm=6.0219\n",
      "Fold 2, Epoch 30: Train Loss=0.5944, Train Acc=79.23%, Val Loss=2.6519, Val Acc=38.31%, Grad Norm=6.0888\n",
      "Fold 2, Epoch 31: Train Loss=0.5677, Train Acc=80.14%, Val Loss=2.5425, Val Acc=38.73%, Grad Norm=6.0925\n",
      "Fold 2, Epoch 32: Train Loss=0.5608, Train Acc=80.57%, Val Loss=2.4768, Val Acc=39.30%, Grad Norm=6.1971\n",
      "Fold 2, Epoch 33: Train Loss=0.5579, Train Acc=80.49%, Val Loss=2.5315, Val Acc=38.93%, Grad Norm=6.3226\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=51.68%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.0818, Train Acc=20.31%, Val Loss=3.7742, Val Acc=14.02%, Grad Norm=4.8075\n",
      "Fold 3, Epoch 2: Train Loss=1.8678, Train Acc=29.52%, Val Loss=3.6994, Val Acc=16.42%, Grad Norm=5.1266\n",
      "Fold 3, Epoch 3: Train Loss=1.6865, Train Acc=37.81%, Val Loss=3.3142, Val Acc=25.62%, Grad Norm=4.8108\n",
      "Fold 3, Epoch 4: Train Loss=1.5730, Train Acc=42.01%, Val Loss=3.4881, Val Acc=20.69%, Grad Norm=4.5340\n",
      "Fold 3, Epoch 5: Train Loss=1.4804, Train Acc=45.49%, Val Loss=3.1512, Val Acc=29.00%, Grad Norm=4.4226\n",
      "Fold 3, Epoch 6: Train Loss=1.3855, Train Acc=48.97%, Val Loss=2.8743, Val Acc=31.89%, Grad Norm=4.4409\n",
      "Fold 3, Epoch 7: Train Loss=1.2860, Train Acc=52.90%, Val Loss=2.9354, Val Acc=31.42%, Grad Norm=4.4068\n",
      "Fold 3, Epoch 8: Train Loss=1.2202, Train Acc=55.33%, Val Loss=2.7315, Val Acc=33.34%, Grad Norm=4.4048\n",
      "Fold 3, Epoch 9: Train Loss=1.1585, Train Acc=58.07%, Val Loss=2.8254, Val Acc=31.00%, Grad Norm=4.3375\n",
      "Fold 3, Epoch 10: Train Loss=1.1000, Train Acc=59.93%, Val Loss=2.7377, Val Acc=36.09%, Grad Norm=4.3534\n",
      "Fold 3, Epoch 11: Train Loss=1.0168, Train Acc=63.48%, Val Loss=2.6529, Val Acc=37.71%, Grad Norm=4.2706\n",
      "Fold 3, Epoch 12: Train Loss=0.9779, Train Acc=64.51%, Val Loss=2.6438, Val Acc=36.76%, Grad Norm=4.4408\n",
      "Fold 3, Epoch 13: Train Loss=0.9535, Train Acc=65.79%, Val Loss=2.7156, Val Acc=38.77%, Grad Norm=4.5451\n",
      "Fold 3, Epoch 14: Train Loss=0.9275, Train Acc=66.66%, Val Loss=2.6662, Val Acc=39.04%, Grad Norm=4.6218\n",
      "Fold 3, Epoch 15: Train Loss=0.9061, Train Acc=67.39%, Val Loss=2.6656, Val Acc=39.46%, Grad Norm=4.7092\n",
      "Fold 3, Epoch 16: Train Loss=0.8809, Train Acc=68.30%, Val Loss=2.9310, Val Acc=37.42%, Grad Norm=4.8016\n",
      "Fold 3, Epoch 17: Train Loss=0.8624, Train Acc=68.84%, Val Loss=2.7628, Val Acc=40.67%, Grad Norm=4.8493\n",
      "Fold 3, Epoch 18: Train Loss=0.8470, Train Acc=69.45%, Val Loss=2.8330, Val Acc=38.35%, Grad Norm=4.9813\n",
      "Fold 3, Epoch 19: Train Loss=0.8236, Train Acc=70.53%, Val Loss=2.7383, Val Acc=41.09%, Grad Norm=5.0369\n",
      "Fold 3, Epoch 20: Train Loss=0.8058, Train Acc=71.00%, Val Loss=2.5620, Val Acc=44.11%, Grad Norm=5.1274\n",
      "Fold 3, Epoch 21: Train Loss=0.7528, Train Acc=73.18%, Val Loss=2.6608, Val Acc=41.20%, Grad Norm=5.1083\n",
      "Fold 3, Epoch 22: Train Loss=0.7458, Train Acc=73.47%, Val Loss=2.7460, Val Acc=40.01%, Grad Norm=5.3136\n",
      "Fold 3, Epoch 23: Train Loss=0.7321, Train Acc=73.91%, Val Loss=2.8142, Val Acc=40.06%, Grad Norm=5.4172\n",
      "Fold 3, Epoch 24: Train Loss=0.7171, Train Acc=74.51%, Val Loss=2.7522, Val Acc=40.93%, Grad Norm=5.5465\n",
      "Fold 3, Epoch 25: Train Loss=0.7065, Train Acc=75.00%, Val Loss=2.7527, Val Acc=40.55%, Grad Norm=5.6639\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=50.06%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1114, Train Acc=19.31%, Val Loss=4.6134, Val Acc=14.29%, Grad Norm=4.7881\n",
      "Fold 4, Epoch 2: Train Loss=1.9198, Train Acc=27.78%, Val Loss=4.0224, Val Acc=22.30%, Grad Norm=5.2269\n",
      "Fold 4, Epoch 3: Train Loss=1.7472, Train Acc=34.92%, Val Loss=3.9016, Val Acc=21.06%, Grad Norm=4.7478\n",
      "Fold 4, Epoch 4: Train Loss=1.6204, Train Acc=39.68%, Val Loss=3.8513, Val Acc=25.60%, Grad Norm=4.5478\n",
      "Fold 4, Epoch 5: Train Loss=1.5293, Train Acc=43.21%, Val Loss=3.5872, Val Acc=26.53%, Grad Norm=4.3509\n",
      "Fold 4, Epoch 6: Train Loss=1.4370, Train Acc=46.74%, Val Loss=3.8731, Val Acc=27.15%, Grad Norm=4.3242\n",
      "Fold 4, Epoch 7: Train Loss=1.3524, Train Acc=49.84%, Val Loss=3.5615, Val Acc=34.74%, Grad Norm=4.2315\n",
      "Fold 4, Epoch 8: Train Loss=1.2828, Train Acc=52.72%, Val Loss=3.6789, Val Acc=31.65%, Grad Norm=4.2196\n",
      "Fold 4, Epoch 9: Train Loss=1.2155, Train Acc=55.19%, Val Loss=3.6467, Val Acc=32.43%, Grad Norm=4.2035\n",
      "Fold 4, Epoch 10: Train Loss=1.1642, Train Acc=57.20%, Val Loss=3.6146, Val Acc=35.94%, Grad Norm=4.2090\n",
      "Fold 4, Epoch 11: Train Loss=1.0878, Train Acc=60.36%, Val Loss=3.6590, Val Acc=36.68%, Grad Norm=4.1750\n",
      "Fold 4, Epoch 12: Train Loss=1.0475, Train Acc=61.58%, Val Loss=3.7272, Val Acc=34.84%, Grad Norm=4.3262\n",
      "Fold 4, Epoch 13: Train Loss=1.0176, Train Acc=62.80%, Val Loss=3.8492, Val Acc=35.39%, Grad Norm=4.4721\n",
      "Fold 4, Epoch 14: Train Loss=0.9937, Train Acc=63.87%, Val Loss=3.6343, Val Acc=39.24%, Grad Norm=4.5842\n",
      "Fold 4, Epoch 15: Train Loss=0.9668, Train Acc=64.76%, Val Loss=3.6559, Val Acc=39.13%, Grad Norm=4.6869\n",
      "Fold 4, Epoch 16: Train Loss=0.9515, Train Acc=65.46%, Val Loss=3.6736, Val Acc=38.03%, Grad Norm=4.7611\n",
      "Fold 4, Epoch 17: Train Loss=0.9288, Train Acc=66.26%, Val Loss=3.6530, Val Acc=40.47%, Grad Norm=4.8508\n",
      "Fold 4, Epoch 18: Train Loss=0.9060, Train Acc=66.96%, Val Loss=3.8736, Val Acc=36.91%, Grad Norm=4.9165\n",
      "Fold 4, Epoch 19: Train Loss=0.8861, Train Acc=67.88%, Val Loss=3.6512, Val Acc=40.47%, Grad Norm=5.0351\n",
      "Fold 4, Epoch 20: Train Loss=0.8691, Train Acc=68.56%, Val Loss=3.7711, Val Acc=37.45%, Grad Norm=5.1141\n",
      "Fold 4, Epoch 21: Train Loss=0.8147, Train Acc=70.69%, Val Loss=3.5303, Val Acc=42.85%, Grad Norm=5.0852\n",
      "Fold 4, Epoch 22: Train Loss=0.8026, Train Acc=71.15%, Val Loss=3.5158, Val Acc=42.13%, Grad Norm=5.3033\n",
      "Fold 4, Epoch 23: Train Loss=0.7904, Train Acc=71.67%, Val Loss=3.5084, Val Acc=42.24%, Grad Norm=5.4789\n",
      "Fold 4, Epoch 24: Train Loss=0.7833, Train Acc=71.83%, Val Loss=3.5198, Val Acc=42.31%, Grad Norm=5.5882\n",
      "Fold 4, Epoch 25: Train Loss=0.7686, Train Acc=72.38%, Val Loss=3.6273, Val Acc=41.74%, Grad Norm=5.6746\n",
      "Fold 4, Epoch 26: Train Loss=0.7567, Train Acc=72.96%, Val Loss=3.6688, Val Acc=41.97%, Grad Norm=5.7919\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=48.27%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.0988, Train Acc=19.75%, Val Loss=3.3601, Val Acc=13.74%, Grad Norm=5.0237\n",
      "Fold 5, Epoch 2: Train Loss=1.8329, Train Acc=31.19%, Val Loss=3.3089, Val Acc=15.43%, Grad Norm=5.2872\n",
      "Fold 5, Epoch 3: Train Loss=1.6829, Train Acc=37.00%, Val Loss=3.4689, Val Acc=17.58%, Grad Norm=4.6911\n",
      "Fold 5, Epoch 4: Train Loss=1.5628, Train Acc=42.07%, Val Loss=2.8260, Val Acc=23.39%, Grad Norm=4.4970\n",
      "Fold 5, Epoch 5: Train Loss=1.4693, Train Acc=45.87%, Val Loss=2.9794, Val Acc=25.12%, Grad Norm=4.3721\n",
      "Fold 5, Epoch 6: Train Loss=1.3853, Train Acc=48.97%, Val Loss=3.1583, Val Acc=25.67%, Grad Norm=4.2618\n",
      "Fold 5, Epoch 7: Train Loss=1.3019, Train Acc=52.06%, Val Loss=3.0772, Val Acc=27.50%, Grad Norm=4.2095\n",
      "Fold 5, Epoch 8: Train Loss=1.2248, Train Acc=54.83%, Val Loss=2.6138, Val Acc=33.40%, Grad Norm=4.2015\n",
      "Fold 5, Epoch 9: Train Loss=1.1667, Train Acc=57.25%, Val Loss=2.7931, Val Acc=30.33%, Grad Norm=4.1765\n",
      "Fold 5, Epoch 10: Train Loss=1.1080, Train Acc=59.50%, Val Loss=2.7760, Val Acc=32.16%, Grad Norm=4.2243\n",
      "Fold 5, Epoch 11: Train Loss=1.0216, Train Acc=62.55%, Val Loss=2.6715, Val Acc=35.00%, Grad Norm=4.1608\n",
      "Fold 5, Epoch 12: Train Loss=0.9908, Train Acc=63.76%, Val Loss=2.8431, Val Acc=34.08%, Grad Norm=4.3339\n",
      "Fold 5, Epoch 13: Train Loss=0.9601, Train Acc=64.80%, Val Loss=2.6400, Val Acc=37.61%, Grad Norm=4.4595\n",
      "Fold 5, Epoch 14: Train Loss=0.9360, Train Acc=66.02%, Val Loss=2.6972, Val Acc=39.21%, Grad Norm=4.5650\n",
      "Fold 5, Epoch 15: Train Loss=0.9072, Train Acc=67.10%, Val Loss=2.6360, Val Acc=37.87%, Grad Norm=4.6297\n",
      "Fold 5, Epoch 16: Train Loss=0.8886, Train Acc=67.57%, Val Loss=2.6958, Val Acc=37.77%, Grad Norm=4.7674\n",
      "Fold 5, Epoch 17: Train Loss=0.8640, Train Acc=68.53%, Val Loss=2.7166, Val Acc=39.52%, Grad Norm=4.8387\n",
      "Fold 5, Epoch 18: Train Loss=0.8418, Train Acc=69.34%, Val Loss=2.6184, Val Acc=39.40%, Grad Norm=4.8889\n",
      "Fold 5, Epoch 19: Train Loss=0.8288, Train Acc=69.82%, Val Loss=2.7674, Val Acc=39.20%, Grad Norm=4.9837\n",
      "Fold 5, Epoch 20: Train Loss=0.8085, Train Acc=70.48%, Val Loss=2.7231, Val Acc=39.91%, Grad Norm=5.0780\n",
      "Fold 5, Epoch 21: Train Loss=0.7570, Train Acc=72.77%, Val Loss=2.7773, Val Acc=39.56%, Grad Norm=5.0584\n",
      "Fold 5, Epoch 22: Train Loss=0.7378, Train Acc=73.54%, Val Loss=2.5330, Val Acc=41.78%, Grad Norm=5.2304\n",
      "Fold 5, Epoch 23: Train Loss=0.7295, Train Acc=73.65%, Val Loss=2.6050, Val Acc=41.50%, Grad Norm=5.4029\n",
      "Fold 5, Epoch 24: Train Loss=0.7152, Train Acc=74.38%, Val Loss=2.5836, Val Acc=43.49%, Grad Norm=5.5034\n",
      "Fold 5, Epoch 25: Train Loss=0.7040, Train Acc=74.68%, Val Loss=2.8347, Val Acc=41.49%, Grad Norm=5.6309\n",
      "Fold 5, Epoch 26: Train Loss=0.6984, Train Acc=74.95%, Val Loss=2.7392, Val Acc=42.12%, Grad Norm=5.7560\n",
      "Fold 5, Epoch 27: Train Loss=0.6857, Train Acc=75.37%, Val Loss=2.6521, Val Acc=42.38%, Grad Norm=5.8697\n",
      "Fold 5, Epoch 28: Train Loss=0.6756, Train Acc=75.75%, Val Loss=2.6889, Val Acc=42.45%, Grad Norm=5.9253\n",
      "Fold 5, Epoch 29: Train Loss=0.6584, Train Acc=76.39%, Val Loss=2.7326, Val Acc=42.61%, Grad Norm=6.0542\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=49.43%\n",
      "\n",
      "SNR  15 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-24_18-08-46_LTE-V_XFR_SingleFileBlock_SNR15dB_fd655_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=10 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "统一目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:09<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 216, 每 block 样本数: 864, 每样本长度: 288\n",
      "[INFO] 总 block 数: 216, 类别数: 9\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "[INFO] 训练 block 数: 162, 测试 block 数: 54\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.1063, Train Acc=19.01%, Val Loss=3.7447, Val Acc=15.40%, Grad Norm=4.9373\n",
      "Fold 1, Epoch 2: Train Loss=1.8811, Train Acc=29.14%, Val Loss=3.9021, Val Acc=20.04%, Grad Norm=5.2582\n",
      "Fold 1, Epoch 3: Train Loss=1.7534, Train Acc=34.58%, Val Loss=2.7511, Val Acc=21.70%, Grad Norm=4.7435\n",
      "Fold 1, Epoch 4: Train Loss=1.6506, Train Acc=38.72%, Val Loss=2.7563, Val Acc=21.74%, Grad Norm=4.4673\n",
      "Fold 1, Epoch 5: Train Loss=1.5697, Train Acc=42.08%, Val Loss=2.4038, Val Acc=30.68%, Grad Norm=4.3612\n",
      "Fold 1, Epoch 6: Train Loss=1.4952, Train Acc=44.88%, Val Loss=2.5977, Val Acc=25.23%, Grad Norm=4.3306\n",
      "Fold 1, Epoch 7: Train Loss=1.4324, Train Acc=47.66%, Val Loss=2.5948, Val Acc=25.45%, Grad Norm=4.2983\n",
      "Fold 1, Epoch 8: Train Loss=1.3678, Train Acc=49.99%, Val Loss=2.5068, Val Acc=25.74%, Grad Norm=4.2797\n",
      "Fold 1, Epoch 9: Train Loss=1.3033, Train Acc=52.58%, Val Loss=2.3005, Val Acc=32.98%, Grad Norm=4.2872\n",
      "Fold 1, Epoch 10: Train Loss=1.2336, Train Acc=54.97%, Val Loss=2.2340, Val Acc=33.64%, Grad Norm=4.3262\n",
      "Fold 1, Epoch 11: Train Loss=1.1370, Train Acc=59.04%, Val Loss=2.1779, Val Acc=35.05%, Grad Norm=4.3315\n",
      "Fold 1, Epoch 12: Train Loss=1.1016, Train Acc=60.40%, Val Loss=2.1803, Val Acc=36.24%, Grad Norm=4.5080\n",
      "Fold 1, Epoch 13: Train Loss=1.0627, Train Acc=61.95%, Val Loss=2.1029, Val Acc=37.77%, Grad Norm=4.6521\n",
      "Fold 1, Epoch 14: Train Loss=1.0347, Train Acc=63.02%, Val Loss=2.1041, Val Acc=37.15%, Grad Norm=4.7173\n",
      "Fold 1, Epoch 15: Train Loss=1.0011, Train Acc=64.41%, Val Loss=2.2396, Val Acc=35.56%, Grad Norm=4.7609\n",
      "Fold 1, Epoch 16: Train Loss=0.9749, Train Acc=65.44%, Val Loss=2.2248, Val Acc=35.11%, Grad Norm=4.8889\n",
      "Fold 1, Epoch 17: Train Loss=0.9483, Train Acc=66.45%, Val Loss=2.2307, Val Acc=35.69%, Grad Norm=4.9583\n",
      "Fold 1, Epoch 18: Train Loss=0.9308, Train Acc=67.26%, Val Loss=2.1183, Val Acc=37.71%, Grad Norm=5.0204\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=47.05%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.0874, Train Acc=19.39%, Val Loss=4.1858, Val Acc=12.95%, Grad Norm=5.0515\n",
      "Fold 2, Epoch 2: Train Loss=1.8668, Train Acc=29.41%, Val Loss=3.8731, Val Acc=15.40%, Grad Norm=5.1494\n",
      "Fold 2, Epoch 3: Train Loss=1.7209, Train Acc=35.91%, Val Loss=2.7654, Val Acc=17.61%, Grad Norm=4.6843\n",
      "Fold 2, Epoch 4: Train Loss=1.6005, Train Acc=40.63%, Val Loss=2.6161, Val Acc=19.63%, Grad Norm=4.4311\n",
      "Fold 2, Epoch 5: Train Loss=1.5210, Train Acc=44.18%, Val Loss=2.5091, Val Acc=21.57%, Grad Norm=4.2794\n",
      "Fold 2, Epoch 6: Train Loss=1.4398, Train Acc=47.07%, Val Loss=2.6695, Val Acc=22.26%, Grad Norm=4.2197\n",
      "Fold 2, Epoch 7: Train Loss=1.3685, Train Acc=49.89%, Val Loss=2.6007, Val Acc=25.99%, Grad Norm=4.2480\n",
      "Fold 2, Epoch 8: Train Loss=1.2964, Train Acc=52.70%, Val Loss=2.5960, Val Acc=23.92%, Grad Norm=4.2216\n",
      "Fold 2, Epoch 9: Train Loss=1.2222, Train Acc=55.10%, Val Loss=2.3925, Val Acc=30.74%, Grad Norm=4.2328\n",
      "Fold 2, Epoch 10: Train Loss=1.1593, Train Acc=57.76%, Val Loss=2.4252, Val Acc=29.34%, Grad Norm=4.2608\n",
      "Fold 2, Epoch 11: Train Loss=1.0725, Train Acc=61.10%, Val Loss=2.5890, Val Acc=28.81%, Grad Norm=4.2271\n",
      "Fold 2, Epoch 12: Train Loss=1.0377, Train Acc=62.30%, Val Loss=2.4577, Val Acc=32.19%, Grad Norm=4.4203\n",
      "Fold 2, Epoch 13: Train Loss=1.0142, Train Acc=63.28%, Val Loss=2.3750, Val Acc=33.81%, Grad Norm=4.5707\n",
      "Fold 2, Epoch 14: Train Loss=0.9778, Train Acc=64.60%, Val Loss=2.4895, Val Acc=34.48%, Grad Norm=4.5909\n",
      "Fold 2, Epoch 15: Train Loss=0.9555, Train Acc=65.59%, Val Loss=2.6037, Val Acc=31.63%, Grad Norm=4.6872\n",
      "Fold 2, Epoch 16: Train Loss=0.9287, Train Acc=66.52%, Val Loss=2.3978, Val Acc=34.62%, Grad Norm=4.7943\n",
      "Fold 2, Epoch 17: Train Loss=0.9097, Train Acc=67.20%, Val Loss=2.5289, Val Acc=32.81%, Grad Norm=4.8860\n",
      "Fold 2, Epoch 18: Train Loss=0.8867, Train Acc=68.04%, Val Loss=2.3109, Val Acc=35.24%, Grad Norm=4.9348\n",
      "Fold 2, Epoch 19: Train Loss=0.8618, Train Acc=69.01%, Val Loss=2.5016, Val Acc=34.88%, Grad Norm=5.0416\n",
      "Fold 2, Epoch 20: Train Loss=0.8414, Train Acc=69.93%, Val Loss=2.4540, Val Acc=35.01%, Grad Norm=5.0872\n",
      "Fold 2, Epoch 21: Train Loss=0.7937, Train Acc=71.62%, Val Loss=2.3519, Val Acc=36.63%, Grad Norm=5.0754\n",
      "Fold 2, Epoch 22: Train Loss=0.7776, Train Acc=72.03%, Val Loss=2.4374, Val Acc=37.11%, Grad Norm=5.2705\n",
      "Fold 2, Epoch 23: Train Loss=0.7702, Train Acc=72.49%, Val Loss=2.3807, Val Acc=36.20%, Grad Norm=5.4365\n",
      "Fold 2, Epoch 24: Train Loss=0.7620, Train Acc=72.65%, Val Loss=2.4410, Val Acc=36.27%, Grad Norm=5.5157\n",
      "Fold 2, Epoch 25: Train Loss=0.7393, Train Acc=73.66%, Val Loss=2.5230, Val Acc=35.35%, Grad Norm=5.6169\n",
      "Fold 2, Epoch 26: Train Loss=0.7345, Train Acc=73.78%, Val Loss=2.4990, Val Acc=35.92%, Grad Norm=5.7387\n",
      "Fold 2, Epoch 27: Train Loss=0.7210, Train Acc=74.26%, Val Loss=2.5023, Val Acc=34.76%, Grad Norm=5.7809\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=49.34%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.1089, Train Acc=18.67%, Val Loss=4.5364, Val Acc=15.56%, Grad Norm=4.7047\n",
      "Fold 3, Epoch 2: Train Loss=1.8974, Train Acc=28.65%, Val Loss=3.7734, Val Acc=17.35%, Grad Norm=5.0989\n",
      "Fold 3, Epoch 3: Train Loss=1.7341, Train Acc=35.26%, Val Loss=3.5310, Val Acc=18.49%, Grad Norm=4.6805\n",
      "Fold 3, Epoch 4: Train Loss=1.6196, Train Acc=39.84%, Val Loss=3.4464, Val Acc=24.23%, Grad Norm=4.3765\n",
      "Fold 3, Epoch 5: Train Loss=1.5455, Train Acc=42.47%, Val Loss=3.1276, Val Acc=23.12%, Grad Norm=4.2609\n",
      "Fold 3, Epoch 6: Train Loss=1.4657, Train Acc=45.56%, Val Loss=2.9725, Val Acc=27.64%, Grad Norm=4.2015\n",
      "Fold 3, Epoch 7: Train Loss=1.3933, Train Acc=48.54%, Val Loss=3.1557, Val Acc=26.74%, Grad Norm=4.1815\n",
      "Fold 3, Epoch 8: Train Loss=1.3226, Train Acc=51.06%, Val Loss=2.9818, Val Acc=31.11%, Grad Norm=4.1940\n",
      "Fold 3, Epoch 9: Train Loss=1.2576, Train Acc=53.83%, Val Loss=2.6976, Val Acc=33.22%, Grad Norm=4.1927\n",
      "Fold 3, Epoch 10: Train Loss=1.1989, Train Acc=56.01%, Val Loss=2.6203, Val Acc=36.09%, Grad Norm=4.1600\n",
      "Fold 3, Epoch 11: Train Loss=1.1269, Train Acc=59.04%, Val Loss=2.6967, Val Acc=34.48%, Grad Norm=4.1575\n",
      "Fold 3, Epoch 12: Train Loss=1.0929, Train Acc=60.05%, Val Loss=2.5793, Val Acc=36.94%, Grad Norm=4.3154\n",
      "Fold 3, Epoch 13: Train Loss=1.0586, Train Acc=61.55%, Val Loss=2.6138, Val Acc=36.97%, Grad Norm=4.4201\n",
      "Fold 3, Epoch 14: Train Loss=1.0394, Train Acc=62.16%, Val Loss=2.5404, Val Acc=39.80%, Grad Norm=4.5244\n",
      "Fold 3, Epoch 15: Train Loss=1.0059, Train Acc=63.64%, Val Loss=2.6096, Val Acc=39.05%, Grad Norm=4.5741\n",
      "Fold 3, Epoch 16: Train Loss=0.9907, Train Acc=64.25%, Val Loss=2.7567, Val Acc=36.32%, Grad Norm=4.7157\n",
      "Fold 3, Epoch 17: Train Loss=0.9707, Train Acc=64.99%, Val Loss=2.7208, Val Acc=34.67%, Grad Norm=4.7868\n",
      "Fold 3, Epoch 18: Train Loss=0.9418, Train Acc=65.92%, Val Loss=2.6385, Val Acc=39.71%, Grad Norm=4.8103\n",
      "Fold 3, Epoch 19: Train Loss=0.9278, Train Acc=66.50%, Val Loss=2.6068, Val Acc=37.53%, Grad Norm=4.9044\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=48.05%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1144, Train Acc=19.14%, Val Loss=4.0206, Val Acc=11.39%, Grad Norm=4.8169\n",
      "Fold 4, Epoch 2: Train Loss=1.9343, Train Acc=27.51%, Val Loss=4.4386, Val Acc=16.05%, Grad Norm=4.9310\n",
      "Fold 4, Epoch 3: Train Loss=1.7989, Train Acc=32.91%, Val Loss=3.7496, Val Acc=20.66%, Grad Norm=4.6006\n",
      "Fold 4, Epoch 4: Train Loss=1.6692, Train Acc=38.39%, Val Loss=3.8334, Val Acc=21.50%, Grad Norm=4.4090\n",
      "Fold 4, Epoch 5: Train Loss=1.5673, Train Acc=41.56%, Val Loss=3.7080, Val Acc=24.58%, Grad Norm=4.2948\n",
      "Fold 4, Epoch 6: Train Loss=1.4789, Train Acc=44.93%, Val Loss=3.4434, Val Acc=28.43%, Grad Norm=4.2287\n",
      "Fold 4, Epoch 7: Train Loss=1.3973, Train Acc=47.94%, Val Loss=3.6511, Val Acc=30.25%, Grad Norm=4.2041\n",
      "Fold 4, Epoch 8: Train Loss=1.3323, Train Acc=50.79%, Val Loss=3.8988, Val Acc=26.88%, Grad Norm=4.1610\n",
      "Fold 4, Epoch 9: Train Loss=1.2678, Train Acc=53.39%, Val Loss=3.4732, Val Acc=33.17%, Grad Norm=4.1625\n",
      "Fold 4, Epoch 10: Train Loss=1.2209, Train Acc=54.95%, Val Loss=3.5930, Val Acc=36.55%, Grad Norm=4.1942\n",
      "Fold 4, Epoch 11: Train Loss=1.1403, Train Acc=58.06%, Val Loss=3.6414, Val Acc=34.96%, Grad Norm=4.1380\n",
      "Fold 4, Epoch 12: Train Loss=1.1085, Train Acc=59.47%, Val Loss=3.6176, Val Acc=35.46%, Grad Norm=4.3140\n",
      "Fold 4, Epoch 13: Train Loss=1.0799, Train Acc=60.72%, Val Loss=3.5351, Val Acc=37.97%, Grad Norm=4.4525\n",
      "Fold 4, Epoch 14: Train Loss=1.0615, Train Acc=61.06%, Val Loss=3.7059, Val Acc=37.06%, Grad Norm=4.5345\n",
      "Fold 4, Epoch 15: Train Loss=1.0330, Train Acc=62.22%, Val Loss=3.7834, Val Acc=37.12%, Grad Norm=4.6056\n",
      "Fold 4, Epoch 16: Train Loss=1.0064, Train Acc=63.26%, Val Loss=3.5748, Val Acc=35.93%, Grad Norm=4.6864\n",
      "Fold 4, Epoch 17: Train Loss=0.9898, Train Acc=63.89%, Val Loss=3.5141, Val Acc=37.84%, Grad Norm=4.7872\n",
      "Fold 4, Epoch 18: Train Loss=0.9671, Train Acc=64.85%, Val Loss=3.4499, Val Acc=40.65%, Grad Norm=4.8410\n",
      "Fold 4, Epoch 19: Train Loss=0.9532, Train Acc=65.34%, Val Loss=3.4971, Val Acc=38.10%, Grad Norm=4.9952\n",
      "Fold 4, Epoch 20: Train Loss=0.9329, Train Acc=66.24%, Val Loss=3.5876, Val Acc=40.43%, Grad Norm=5.0348\n",
      "Fold 4, Epoch 21: Train Loss=0.8895, Train Acc=67.92%, Val Loss=3.5060, Val Acc=41.18%, Grad Norm=5.0765\n",
      "Fold 4, Epoch 22: Train Loss=0.8695, Train Acc=68.67%, Val Loss=3.4536, Val Acc=40.69%, Grad Norm=5.1931\n",
      "Fold 4, Epoch 23: Train Loss=0.8623, Train Acc=69.01%, Val Loss=3.4526, Val Acc=41.11%, Grad Norm=5.3943\n",
      "Fold 4, Epoch 24: Train Loss=0.8510, Train Acc=69.30%, Val Loss=3.4526, Val Acc=40.79%, Grad Norm=5.4859\n",
      "Fold 4, Epoch 25: Train Loss=0.8326, Train Acc=70.22%, Val Loss=3.5420, Val Acc=42.12%, Grad Norm=5.5503\n",
      "Fold 4, Epoch 26: Train Loss=0.8252, Train Acc=69.95%, Val Loss=3.4152, Val Acc=42.68%, Grad Norm=5.7070\n",
      "Fold 4, Epoch 27: Train Loss=0.8127, Train Acc=70.64%, Val Loss=3.3737, Val Acc=42.81%, Grad Norm=5.7885\n",
      "Fold 4, Epoch 28: Train Loss=0.8041, Train Acc=71.22%, Val Loss=3.3991, Val Acc=42.36%, Grad Norm=5.8877\n",
      "Fold 4, Epoch 29: Train Loss=0.7940, Train Acc=71.34%, Val Loss=3.5578, Val Acc=42.70%, Grad Norm=6.0000\n",
      "Fold 4, Epoch 30: Train Loss=0.7860, Train Acc=71.84%, Val Loss=3.4758, Val Acc=43.78%, Grad Norm=6.0938\n",
      "Fold 4, Epoch 31: Train Loss=0.7594, Train Acc=72.83%, Val Loss=3.4045, Val Acc=42.12%, Grad Norm=6.0970\n",
      "Fold 4, Epoch 32: Train Loss=0.7475, Train Acc=73.30%, Val Loss=3.4470, Val Acc=42.84%, Grad Norm=6.1836\n",
      "Fold 4, Epoch 33: Train Loss=0.7450, Train Acc=73.53%, Val Loss=3.3583, Val Acc=44.54%, Grad Norm=6.3498\n",
      "Fold 4, Epoch 34: Train Loss=0.7373, Train Acc=73.53%, Val Loss=3.3847, Val Acc=44.30%, Grad Norm=6.4496\n",
      "Fold 4, Epoch 35: Train Loss=0.7299, Train Acc=73.99%, Val Loss=3.4578, Val Acc=42.39%, Grad Norm=6.5064\n",
      "Fold 4, Epoch 36: Train Loss=0.7242, Train Acc=74.22%, Val Loss=3.3961, Val Acc=41.96%, Grad Norm=6.6674\n",
      "Fold 4, Epoch 37: Train Loss=0.7165, Train Acc=74.66%, Val Loss=3.4487, Val Acc=44.27%, Grad Norm=6.7338\n",
      "Fold 4, Epoch 38: Train Loss=0.7150, Train Acc=74.52%, Val Loss=3.4284, Val Acc=43.88%, Grad Norm=6.8194\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=48.15%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.1167, Train Acc=18.84%, Val Loss=4.2172, Val Acc=13.41%, Grad Norm=5.0135\n",
      "Fold 5, Epoch 2: Train Loss=1.9069, Train Acc=27.99%, Val Loss=4.9339, Val Acc=18.70%, Grad Norm=5.3390\n",
      "Fold 5, Epoch 3: Train Loss=1.7434, Train Acc=34.77%, Val Loss=4.0042, Val Acc=20.44%, Grad Norm=4.8261\n",
      "Fold 5, Epoch 4: Train Loss=1.6193, Train Acc=39.78%, Val Loss=3.2837, Val Acc=23.16%, Grad Norm=4.5414\n",
      "Fold 5, Epoch 5: Train Loss=1.5190, Train Acc=43.46%, Val Loss=3.2608, Val Acc=23.24%, Grad Norm=4.3698\n",
      "Fold 5, Epoch 6: Train Loss=1.4249, Train Acc=47.30%, Val Loss=3.6073, Val Acc=22.48%, Grad Norm=4.3066\n",
      "Fold 5, Epoch 7: Train Loss=1.3447, Train Acc=50.15%, Val Loss=3.0414, Val Acc=26.43%, Grad Norm=4.2450\n",
      "Fold 5, Epoch 8: Train Loss=1.2809, Train Acc=52.95%, Val Loss=2.9105, Val Acc=28.47%, Grad Norm=4.2031\n",
      "Fold 5, Epoch 9: Train Loss=1.2207, Train Acc=54.90%, Val Loss=3.0385, Val Acc=28.32%, Grad Norm=4.1599\n",
      "Fold 5, Epoch 10: Train Loss=1.1728, Train Acc=56.69%, Val Loss=2.8282, Val Acc=31.86%, Grad Norm=4.1568\n",
      "Fold 5, Epoch 11: Train Loss=1.0916, Train Acc=60.02%, Val Loss=2.7612, Val Acc=30.77%, Grad Norm=4.1140\n",
      "Fold 5, Epoch 12: Train Loss=1.0641, Train Acc=61.26%, Val Loss=2.7514, Val Acc=32.77%, Grad Norm=4.2548\n",
      "Fold 5, Epoch 13: Train Loss=1.0316, Train Acc=62.42%, Val Loss=2.8263, Val Acc=33.52%, Grad Norm=4.3727\n",
      "Fold 5, Epoch 14: Train Loss=1.0105, Train Acc=63.15%, Val Loss=2.6687, Val Acc=33.45%, Grad Norm=4.4765\n",
      "Fold 5, Epoch 15: Train Loss=0.9863, Train Acc=63.99%, Val Loss=2.7578, Val Acc=34.00%, Grad Norm=4.5745\n",
      "Fold 5, Epoch 16: Train Loss=0.9646, Train Acc=64.98%, Val Loss=2.8361, Val Acc=34.29%, Grad Norm=4.6380\n",
      "Fold 5, Epoch 17: Train Loss=0.9464, Train Acc=65.75%, Val Loss=2.8354, Val Acc=33.46%, Grad Norm=4.7054\n",
      "Fold 5, Epoch 18: Train Loss=0.9233, Train Acc=66.48%, Val Loss=2.8707, Val Acc=36.15%, Grad Norm=4.7758\n",
      "Fold 5, Epoch 19: Train Loss=0.9039, Train Acc=67.12%, Val Loss=2.6344, Val Acc=35.81%, Grad Norm=4.8794\n",
      "Fold 5, Epoch 20: Train Loss=0.8895, Train Acc=67.49%, Val Loss=2.7229, Val Acc=35.33%, Grad Norm=4.9834\n",
      "Fold 5, Epoch 21: Train Loss=0.8425, Train Acc=69.70%, Val Loss=2.6775, Val Acc=37.93%, Grad Norm=4.9691\n",
      "Fold 5, Epoch 22: Train Loss=0.8207, Train Acc=70.26%, Val Loss=2.7837, Val Acc=36.69%, Grad Norm=5.1172\n",
      "Fold 5, Epoch 23: Train Loss=0.8141, Train Acc=70.65%, Val Loss=2.7337, Val Acc=38.00%, Grad Norm=5.2661\n",
      "Fold 5, Epoch 24: Train Loss=0.8034, Train Acc=71.26%, Val Loss=2.7072, Val Acc=38.27%, Grad Norm=5.3949\n",
      "Fold 5, Epoch 25: Train Loss=0.7883, Train Acc=71.56%, Val Loss=2.7942, Val Acc=37.68%, Grad Norm=5.5249\n",
      "Fold 5, Epoch 26: Train Loss=0.7830, Train Acc=71.87%, Val Loss=2.7294, Val Acc=37.04%, Grad Norm=5.6090\n",
      "Fold 5, Epoch 27: Train Loss=0.7690, Train Acc=72.48%, Val Loss=2.7771, Val Acc=37.32%, Grad Norm=5.6975\n",
      "Fold 5, Epoch 28: Train Loss=0.7630, Train Acc=72.68%, Val Loss=2.7608, Val Acc=38.15%, Grad Norm=5.8318\n",
      "Fold 5, Epoch 29: Train Loss=0.7538, Train Acc=73.06%, Val Loss=2.8085, Val Acc=38.53%, Grad Norm=5.9084\n",
      "Fold 5, Epoch 30: Train Loss=0.7400, Train Acc=73.51%, Val Loss=2.8314, Val Acc=37.89%, Grad Norm=5.9827\n",
      "Fold 5, Epoch 31: Train Loss=0.7140, Train Acc=74.60%, Val Loss=2.7543, Val Acc=40.54%, Grad Norm=5.9940\n",
      "Fold 5, Epoch 32: Train Loss=0.7020, Train Acc=74.98%, Val Loss=2.7040, Val Acc=39.43%, Grad Norm=6.0679\n",
      "Fold 5, Epoch 33: Train Loss=0.6981, Train Acc=75.01%, Val Loss=2.7805, Val Acc=38.80%, Grad Norm=6.2486\n",
      "Fold 5, Epoch 34: Train Loss=0.6998, Train Acc=75.00%, Val Loss=2.7458, Val Acc=38.56%, Grad Norm=6.3616\n",
      "Fold 5, Epoch 35: Train Loss=0.6902, Train Acc=75.57%, Val Loss=2.8121, Val Acc=37.88%, Grad Norm=6.4616\n",
      "Fold 5, Epoch 36: Train Loss=0.6824, Train Acc=75.83%, Val Loss=2.6181, Val Acc=41.18%, Grad Norm=6.5489\n",
      "Fold 5, Epoch 37: Train Loss=0.6809, Train Acc=75.73%, Val Loss=2.7823, Val Acc=39.13%, Grad Norm=6.6294\n",
      "Fold 5, Epoch 38: Train Loss=0.6732, Train Acc=75.92%, Val Loss=2.7129, Val Acc=39.88%, Grad Norm=6.7075\n",
      "Fold 5, Epoch 39: Train Loss=0.6672, Train Acc=76.20%, Val Loss=2.6943, Val Acc=39.77%, Grad Norm=6.7350\n",
      "Fold 5, Epoch 40: Train Loss=0.6600, Train Acc=76.50%, Val Loss=2.7702, Val Acc=39.30%, Grad Norm=6.8217\n",
      "Fold 5, Epoch 41: Train Loss=0.6468, Train Acc=77.39%, Val Loss=2.7693, Val Acc=39.91%, Grad Norm=6.8259\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=48.59%\n",
      "\n",
      "SNR  10 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-24_18-23-10_LTE-V_XFR_SingleFileBlock_SNR10dB_fd655_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=5 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "统一目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 216, 每 block 样本数: 864, 每样本长度: 288\n",
      "[INFO] 总 block 数: 216, 类别数: 9\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "[INFO] 训练 block 数: 162, 测试 block 数: 54\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.1350, Train Acc=17.49%, Val Loss=3.3514, Val Acc=6.68%, Grad Norm=4.8919\n",
      "Fold 1, Epoch 2: Train Loss=1.9677, Train Acc=25.45%, Val Loss=3.3025, Val Acc=14.06%, Grad Norm=5.0827\n",
      "Fold 1, Epoch 3: Train Loss=1.8557, Train Acc=29.78%, Val Loss=2.8243, Val Acc=13.43%, Grad Norm=4.5256\n",
      "Fold 1, Epoch 4: Train Loss=1.7784, Train Acc=33.36%, Val Loss=2.6296, Val Acc=18.73%, Grad Norm=4.2399\n",
      "Fold 1, Epoch 5: Train Loss=1.7205, Train Acc=35.98%, Val Loss=2.5366, Val Acc=21.43%, Grad Norm=4.0620\n",
      "Fold 1, Epoch 6: Train Loss=1.6583, Train Acc=38.82%, Val Loss=2.3167, Val Acc=25.52%, Grad Norm=4.0355\n",
      "Fold 1, Epoch 7: Train Loss=1.5944, Train Acc=41.36%, Val Loss=2.2647, Val Acc=27.54%, Grad Norm=3.9918\n",
      "Fold 1, Epoch 8: Train Loss=1.5504, Train Acc=43.13%, Val Loss=2.2396, Val Acc=26.17%, Grad Norm=4.0088\n",
      "Fold 1, Epoch 9: Train Loss=1.5063, Train Acc=44.81%, Val Loss=2.2577, Val Acc=29.83%, Grad Norm=4.0024\n",
      "Fold 1, Epoch 10: Train Loss=1.4637, Train Acc=46.70%, Val Loss=2.4515, Val Acc=21.95%, Grad Norm=3.9888\n",
      "Fold 1, Epoch 11: Train Loss=1.3919, Train Acc=49.63%, Val Loss=2.1038, Val Acc=31.30%, Grad Norm=4.0012\n",
      "Fold 1, Epoch 12: Train Loss=1.3596, Train Acc=50.67%, Val Loss=2.0183, Val Acc=33.12%, Grad Norm=4.1862\n",
      "Fold 1, Epoch 13: Train Loss=1.3262, Train Acc=52.05%, Val Loss=2.0682, Val Acc=31.80%, Grad Norm=4.2885\n",
      "Fold 1, Epoch 14: Train Loss=1.2975, Train Acc=53.28%, Val Loss=2.1080, Val Acc=34.73%, Grad Norm=4.3722\n",
      "Fold 1, Epoch 15: Train Loss=1.2691, Train Acc=54.45%, Val Loss=2.0916, Val Acc=32.33%, Grad Norm=4.4431\n",
      "Fold 1, Epoch 16: Train Loss=1.2430, Train Acc=55.59%, Val Loss=2.1195, Val Acc=31.19%, Grad Norm=4.5776\n",
      "Fold 1, Epoch 17: Train Loss=1.2207, Train Acc=56.20%, Val Loss=2.0942, Val Acc=33.35%, Grad Norm=4.6579\n",
      "Fold 1, Epoch 18: Train Loss=1.1841, Train Acc=57.61%, Val Loss=2.0729, Val Acc=35.12%, Grad Norm=4.7515\n",
      "Fold 1, Epoch 19: Train Loss=1.1679, Train Acc=57.85%, Val Loss=2.1009, Val Acc=34.97%, Grad Norm=4.8764\n",
      "Fold 1, Epoch 20: Train Loss=1.1443, Train Acc=58.86%, Val Loss=2.0469, Val Acc=36.43%, Grad Norm=4.9416\n",
      "Fold 1, Epoch 21: Train Loss=1.0954, Train Acc=61.02%, Val Loss=1.9876, Val Acc=38.07%, Grad Norm=4.9578\n",
      "Fold 1, Epoch 22: Train Loss=1.0826, Train Acc=61.15%, Val Loss=2.0762, Val Acc=34.56%, Grad Norm=5.1625\n",
      "Fold 1, Epoch 23: Train Loss=1.0662, Train Acc=62.26%, Val Loss=2.1523, Val Acc=35.15%, Grad Norm=5.2861\n",
      "Fold 1, Epoch 24: Train Loss=1.0455, Train Acc=62.92%, Val Loss=2.0467, Val Acc=36.36%, Grad Norm=5.4315\n",
      "Fold 1, Epoch 25: Train Loss=1.0387, Train Acc=63.26%, Val Loss=2.0670, Val Acc=35.93%, Grad Norm=5.5402\n",
      "Fold 1, Epoch 26: Train Loss=1.0234, Train Acc=63.63%, Val Loss=2.1153, Val Acc=34.49%, Grad Norm=5.6363\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=47.78%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.1428, Train Acc=16.71%, Val Loss=3.1862, Val Acc=11.13%, Grad Norm=4.8867\n",
      "Fold 2, Epoch 2: Train Loss=1.9651, Train Acc=25.83%, Val Loss=2.7490, Val Acc=18.50%, Grad Norm=5.1308\n",
      "Fold 2, Epoch 3: Train Loss=1.8267, Train Acc=32.21%, Val Loss=3.2079, Val Acc=13.64%, Grad Norm=4.7868\n",
      "Fold 2, Epoch 4: Train Loss=1.7245, Train Acc=36.56%, Val Loss=2.7073, Val Acc=16.08%, Grad Norm=4.3059\n",
      "Fold 2, Epoch 5: Train Loss=1.6515, Train Acc=39.18%, Val Loss=2.5973, Val Acc=17.60%, Grad Norm=4.1081\n",
      "Fold 2, Epoch 6: Train Loss=1.6017, Train Acc=41.20%, Val Loss=2.8657, Val Acc=15.55%, Grad Norm=4.0352\n",
      "Fold 2, Epoch 7: Train Loss=1.5573, Train Acc=43.37%, Val Loss=2.5236, Val Acc=19.21%, Grad Norm=3.9674\n",
      "Fold 2, Epoch 8: Train Loss=1.5139, Train Acc=44.74%, Val Loss=2.8439, Val Acc=16.49%, Grad Norm=3.9496\n",
      "Fold 2, Epoch 9: Train Loss=1.4609, Train Acc=46.64%, Val Loss=2.4233, Val Acc=22.42%, Grad Norm=4.0000\n",
      "Fold 2, Epoch 10: Train Loss=1.4015, Train Acc=48.93%, Val Loss=2.4695, Val Acc=21.86%, Grad Norm=4.0456\n",
      "Fold 2, Epoch 11: Train Loss=1.3104, Train Acc=52.57%, Val Loss=2.3851, Val Acc=26.60%, Grad Norm=4.0430\n",
      "Fold 2, Epoch 12: Train Loss=1.2843, Train Acc=53.54%, Val Loss=2.3878, Val Acc=27.08%, Grad Norm=4.2423\n",
      "Fold 2, Epoch 13: Train Loss=1.2467, Train Acc=54.80%, Val Loss=2.4346, Val Acc=26.56%, Grad Norm=4.3747\n",
      "Fold 2, Epoch 14: Train Loss=1.2111, Train Acc=56.36%, Val Loss=2.3616, Val Acc=27.68%, Grad Norm=4.4627\n",
      "Fold 2, Epoch 15: Train Loss=1.1887, Train Acc=56.77%, Val Loss=2.4847, Val Acc=25.40%, Grad Norm=4.5454\n",
      "Fold 2, Epoch 16: Train Loss=1.1576, Train Acc=58.05%, Val Loss=2.4784, Val Acc=27.60%, Grad Norm=4.6473\n",
      "Fold 2, Epoch 17: Train Loss=1.1325, Train Acc=59.19%, Val Loss=2.4553, Val Acc=30.24%, Grad Norm=4.7005\n",
      "Fold 2, Epoch 18: Train Loss=1.1175, Train Acc=59.61%, Val Loss=2.3657, Val Acc=30.98%, Grad Norm=4.8031\n",
      "Fold 2, Epoch 19: Train Loss=1.0816, Train Acc=60.98%, Val Loss=2.4667, Val Acc=30.60%, Grad Norm=4.8636\n",
      "Fold 2, Epoch 20: Train Loss=1.0660, Train Acc=61.78%, Val Loss=2.3743, Val Acc=31.05%, Grad Norm=4.9807\n",
      "Fold 2, Epoch 21: Train Loss=1.0222, Train Acc=63.82%, Val Loss=2.4574, Val Acc=28.63%, Grad Norm=5.0042\n",
      "Fold 2, Epoch 22: Train Loss=1.0064, Train Acc=64.01%, Val Loss=2.4003, Val Acc=31.82%, Grad Norm=5.1572\n",
      "Fold 2, Epoch 23: Train Loss=0.9938, Train Acc=64.45%, Val Loss=2.3972, Val Acc=30.70%, Grad Norm=5.2948\n",
      "Fold 2, Epoch 24: Train Loss=0.9826, Train Acc=64.94%, Val Loss=2.4313, Val Acc=32.03%, Grad Norm=5.4187\n",
      "Fold 2, Epoch 25: Train Loss=0.9738, Train Acc=64.98%, Val Loss=2.4461, Val Acc=30.47%, Grad Norm=5.4997\n",
      "Fold 2, Epoch 26: Train Loss=0.9535, Train Acc=65.96%, Val Loss=2.3887, Val Acc=31.83%, Grad Norm=5.5937\n",
      "Fold 2, Epoch 27: Train Loss=0.9476, Train Acc=66.21%, Val Loss=2.4304, Val Acc=32.12%, Grad Norm=5.6929\n",
      "Fold 2, Epoch 28: Train Loss=0.9377, Train Acc=66.48%, Val Loss=2.4351, Val Acc=31.05%, Grad Norm=5.8339\n",
      "Fold 2, Epoch 29: Train Loss=0.9274, Train Acc=66.83%, Val Loss=2.3975, Val Acc=31.10%, Grad Norm=5.9330\n",
      "Fold 2, Epoch 30: Train Loss=0.9132, Train Acc=67.75%, Val Loss=2.3844, Val Acc=32.61%, Grad Norm=5.9414\n",
      "Fold 2, Epoch 31: Train Loss=0.8929, Train Acc=68.05%, Val Loss=2.4092, Val Acc=32.86%, Grad Norm=5.9922\n",
      "Fold 2, Epoch 32: Train Loss=0.8804, Train Acc=68.72%, Val Loss=2.4622, Val Acc=31.04%, Grad Norm=6.0893\n",
      "Fold 2, Epoch 33: Train Loss=0.8769, Train Acc=68.72%, Val Loss=2.3874, Val Acc=33.25%, Grad Norm=6.2136\n",
      "Fold 2, Epoch 34: Train Loss=0.8706, Train Acc=69.03%, Val Loss=2.4101, Val Acc=32.08%, Grad Norm=6.2949\n",
      "Fold 2, Epoch 35: Train Loss=0.8660, Train Acc=69.30%, Val Loss=2.3520, Val Acc=34.30%, Grad Norm=6.4054\n",
      "Fold 2, Epoch 36: Train Loss=0.8635, Train Acc=69.16%, Val Loss=2.4067, Val Acc=33.91%, Grad Norm=6.4957\n",
      "Fold 2, Epoch 37: Train Loss=0.8555, Train Acc=69.50%, Val Loss=2.3395, Val Acc=34.11%, Grad Norm=6.5797\n",
      "Fold 2, Epoch 38: Train Loss=0.8529, Train Acc=69.50%, Val Loss=2.3962, Val Acc=33.90%, Grad Norm=6.6937\n",
      "Fold 2, Epoch 39: Train Loss=0.8435, Train Acc=70.24%, Val Loss=2.3437, Val Acc=34.53%, Grad Norm=6.7198\n",
      "Fold 2, Epoch 40: Train Loss=0.8318, Train Acc=70.45%, Val Loss=2.3918, Val Acc=34.22%, Grad Norm=6.7794\n",
      "Fold 2, Epoch 41: Train Loss=0.8178, Train Acc=70.94%, Val Loss=2.3984, Val Acc=33.47%, Grad Norm=6.7973\n",
      "Fold 2, Epoch 42: Train Loss=0.8205, Train Acc=70.81%, Val Loss=2.4090, Val Acc=33.19%, Grad Norm=6.9253\n",
      "Fold 2, Epoch 43: Train Loss=0.8141, Train Acc=71.16%, Val Loss=2.3947, Val Acc=33.63%, Grad Norm=6.9441\n",
      "Fold 2, Epoch 44: Train Loss=0.8069, Train Acc=71.41%, Val Loss=2.4449, Val Acc=32.88%, Grad Norm=6.9921\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=47.42%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.1281, Train Acc=17.44%, Val Loss=4.9048, Val Acc=14.91%, Grad Norm=4.5446\n",
      "Fold 3, Epoch 2: Train Loss=1.9930, Train Acc=24.18%, Val Loss=4.2274, Val Acc=15.49%, Grad Norm=4.7457\n",
      "Fold 3, Epoch 3: Train Loss=1.8927, Train Acc=28.39%, Val Loss=3.7594, Val Acc=18.12%, Grad Norm=4.4040\n",
      "Fold 3, Epoch 4: Train Loss=1.7868, Train Acc=33.95%, Val Loss=3.6047, Val Acc=18.63%, Grad Norm=4.3437\n",
      "Fold 3, Epoch 5: Train Loss=1.6902, Train Acc=37.49%, Val Loss=3.0828, Val Acc=24.75%, Grad Norm=4.1825\n",
      "Fold 3, Epoch 6: Train Loss=1.6287, Train Acc=39.50%, Val Loss=3.2364, Val Acc=25.54%, Grad Norm=4.0726\n",
      "Fold 3, Epoch 7: Train Loss=1.5670, Train Acc=42.22%, Val Loss=2.9256, Val Acc=26.01%, Grad Norm=4.0534\n",
      "Fold 3, Epoch 8: Train Loss=1.5091, Train Acc=44.17%, Val Loss=2.7590, Val Acc=27.44%, Grad Norm=4.0017\n",
      "Fold 3, Epoch 9: Train Loss=1.4614, Train Acc=45.95%, Val Loss=3.0392, Val Acc=25.39%, Grad Norm=4.0034\n",
      "Fold 3, Epoch 10: Train Loss=1.4094, Train Acc=48.04%, Val Loss=2.6601, Val Acc=33.36%, Grad Norm=4.0473\n",
      "Fold 3, Epoch 11: Train Loss=1.3297, Train Acc=51.33%, Val Loss=2.9239, Val Acc=27.52%, Grad Norm=4.0647\n",
      "Fold 3, Epoch 12: Train Loss=1.2960, Train Acc=52.69%, Val Loss=2.7739, Val Acc=30.01%, Grad Norm=4.2171\n",
      "Fold 3, Epoch 13: Train Loss=1.2734, Train Acc=53.43%, Val Loss=2.8807, Val Acc=32.53%, Grad Norm=4.3527\n",
      "Fold 3, Epoch 14: Train Loss=1.2442, Train Acc=54.72%, Val Loss=2.8494, Val Acc=31.48%, Grad Norm=4.4416\n",
      "Fold 3, Epoch 15: Train Loss=1.2162, Train Acc=55.79%, Val Loss=2.8402, Val Acc=30.44%, Grad Norm=4.5065\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=39.47%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1340, Train Acc=18.19%, Val Loss=4.3965, Val Acc=14.70%, Grad Norm=4.7852\n",
      "Fold 4, Epoch 2: Train Loss=1.9847, Train Acc=25.58%, Val Loss=4.5220, Val Acc=10.58%, Grad Norm=4.9971\n",
      "Fold 4, Epoch 3: Train Loss=1.8779, Train Acc=30.13%, Val Loss=4.1056, Val Acc=11.26%, Grad Norm=4.4053\n",
      "Fold 4, Epoch 4: Train Loss=1.8011, Train Acc=33.04%, Val Loss=4.0094, Val Acc=17.78%, Grad Norm=4.1505\n",
      "Fold 4, Epoch 5: Train Loss=1.7350, Train Acc=35.50%, Val Loss=3.8156, Val Acc=19.65%, Grad Norm=4.0245\n",
      "Fold 4, Epoch 6: Train Loss=1.6641, Train Acc=38.43%, Val Loss=4.1595, Val Acc=19.44%, Grad Norm=4.0136\n",
      "Fold 4, Epoch 7: Train Loss=1.5999, Train Acc=40.93%, Val Loss=3.7099, Val Acc=22.96%, Grad Norm=3.9983\n",
      "Fold 4, Epoch 8: Train Loss=1.5405, Train Acc=43.27%, Val Loss=3.5325, Val Acc=22.61%, Grad Norm=3.9772\n",
      "Fold 4, Epoch 9: Train Loss=1.4839, Train Acc=44.93%, Val Loss=3.9839, Val Acc=21.45%, Grad Norm=3.9600\n",
      "Fold 4, Epoch 10: Train Loss=1.4375, Train Acc=47.11%, Val Loss=3.5702, Val Acc=26.38%, Grad Norm=3.9330\n",
      "Fold 4, Epoch 11: Train Loss=1.3628, Train Acc=49.92%, Val Loss=3.4828, Val Acc=27.53%, Grad Norm=3.9632\n",
      "Fold 4, Epoch 12: Train Loss=1.3293, Train Acc=51.30%, Val Loss=3.4882, Val Acc=28.99%, Grad Norm=4.1278\n",
      "Fold 4, Epoch 13: Train Loss=1.3021, Train Acc=52.39%, Val Loss=3.6464, Val Acc=28.78%, Grad Norm=4.2181\n",
      "Fold 4, Epoch 14: Train Loss=1.2798, Train Acc=53.07%, Val Loss=3.6751, Val Acc=29.33%, Grad Norm=4.3228\n",
      "Fold 4, Epoch 15: Train Loss=1.2547, Train Acc=54.41%, Val Loss=3.5063, Val Acc=33.67%, Grad Norm=4.4449\n",
      "Fold 4, Epoch 16: Train Loss=1.2297, Train Acc=55.20%, Val Loss=3.6172, Val Acc=29.83%, Grad Norm=4.5089\n",
      "Fold 4, Epoch 17: Train Loss=1.2053, Train Acc=56.13%, Val Loss=3.6885, Val Acc=31.07%, Grad Norm=4.5805\n",
      "Fold 4, Epoch 18: Train Loss=1.1889, Train Acc=56.58%, Val Loss=3.4813, Val Acc=32.64%, Grad Norm=4.7071\n",
      "Fold 4, Epoch 19: Train Loss=1.1730, Train Acc=57.25%, Val Loss=3.4565, Val Acc=32.87%, Grad Norm=4.7656\n",
      "Fold 4, Epoch 20: Train Loss=1.1534, Train Acc=58.04%, Val Loss=3.5841, Val Acc=31.75%, Grad Norm=4.8100\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=40.42%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.1492, Train Acc=17.22%, Val Loss=3.4430, Val Acc=5.98%, Grad Norm=4.7237\n",
      "Fold 5, Epoch 2: Train Loss=1.9770, Train Acc=24.56%, Val Loss=3.8286, Val Acc=8.64%, Grad Norm=5.0134\n",
      "Fold 5, Epoch 3: Train Loss=1.8510, Train Acc=30.33%, Val Loss=3.2407, Val Acc=15.52%, Grad Norm=4.5643\n",
      "Fold 5, Epoch 4: Train Loss=1.7510, Train Acc=34.59%, Val Loss=3.3035, Val Acc=14.39%, Grad Norm=4.2330\n",
      "Fold 5, Epoch 5: Train Loss=1.6868, Train Acc=37.16%, Val Loss=3.0394, Val Acc=17.12%, Grad Norm=4.0767\n",
      "Fold 5, Epoch 6: Train Loss=1.6201, Train Acc=39.54%, Val Loss=2.7034, Val Acc=23.63%, Grad Norm=3.9960\n",
      "Fold 5, Epoch 7: Train Loss=1.5602, Train Acc=42.15%, Val Loss=2.8768, Val Acc=22.54%, Grad Norm=4.0161\n",
      "Fold 5, Epoch 8: Train Loss=1.5006, Train Acc=44.69%, Val Loss=2.7849, Val Acc=22.55%, Grad Norm=4.0074\n",
      "Fold 5, Epoch 9: Train Loss=1.4451, Train Acc=46.49%, Val Loss=2.7084, Val Acc=24.96%, Grad Norm=4.0075\n",
      "Fold 5, Epoch 10: Train Loss=1.3950, Train Acc=48.59%, Val Loss=2.5708, Val Acc=27.41%, Grad Norm=4.0458\n",
      "Fold 5, Epoch 11: Train Loss=1.3090, Train Acc=52.00%, Val Loss=2.6109, Val Acc=27.31%, Grad Norm=4.0075\n",
      "Fold 5, Epoch 12: Train Loss=1.2726, Train Acc=53.37%, Val Loss=2.6741, Val Acc=28.60%, Grad Norm=4.2140\n",
      "Fold 5, Epoch 13: Train Loss=1.2468, Train Acc=54.29%, Val Loss=2.7863, Val Acc=28.95%, Grad Norm=4.2931\n",
      "Fold 5, Epoch 14: Train Loss=1.2200, Train Acc=55.30%, Val Loss=2.6396, Val Acc=30.35%, Grad Norm=4.3695\n",
      "Fold 5, Epoch 15: Train Loss=1.2001, Train Acc=56.01%, Val Loss=2.7610, Val Acc=29.94%, Grad Norm=4.4877\n",
      "Fold 5, Epoch 16: Train Loss=1.1756, Train Acc=57.16%, Val Loss=2.6127, Val Acc=30.78%, Grad Norm=4.5539\n",
      "Fold 5, Epoch 17: Train Loss=1.1553, Train Acc=57.78%, Val Loss=2.5567, Val Acc=32.98%, Grad Norm=4.6574\n",
      "Fold 5, Epoch 18: Train Loss=1.1305, Train Acc=58.20%, Val Loss=2.7379, Val Acc=31.37%, Grad Norm=4.7008\n",
      "Fold 5, Epoch 19: Train Loss=1.1094, Train Acc=59.68%, Val Loss=2.6866, Val Acc=32.80%, Grad Norm=4.8153\n",
      "Fold 5, Epoch 20: Train Loss=1.0951, Train Acc=59.99%, Val Loss=2.6879, Val Acc=30.98%, Grad Norm=4.8747\n",
      "Fold 5, Epoch 21: Train Loss=1.0558, Train Acc=61.57%, Val Loss=2.7265, Val Acc=31.29%, Grad Norm=4.8952\n",
      "Fold 5, Epoch 22: Train Loss=1.0326, Train Acc=62.39%, Val Loss=2.6246, Val Acc=32.18%, Grad Norm=5.0573\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=42.61%\n",
      "\n",
      "SNR   5 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-24_18-38-06_LTE-V_XFR_SingleFileBlock_SNR5dB_fd655_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=0 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "统一目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 216, 每 block 样本数: 864, 每样本长度: 288\n",
      "[INFO] 总 block 数: 216, 类别数: 9\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "[INFO] 训练 block 数: 162, 测试 block 数: 54\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.1794, Train Acc=14.71%, Val Loss=3.4540, Val Acc=6.83%, Grad Norm=4.6873\n",
      "Fold 1, Epoch 2: Train Loss=2.0796, Train Acc=20.46%, Val Loss=5.1630, Val Acc=6.29%, Grad Norm=5.1689\n",
      "Fold 1, Epoch 3: Train Loss=2.0048, Train Acc=23.75%, Val Loss=3.2286, Val Acc=9.50%, Grad Norm=4.3941\n",
      "Fold 1, Epoch 4: Train Loss=1.9588, Train Acc=25.59%, Val Loss=2.9386, Val Acc=15.24%, Grad Norm=3.8501\n",
      "Fold 1, Epoch 5: Train Loss=1.9308, Train Acc=26.65%, Val Loss=2.4786, Val Acc=18.30%, Grad Norm=3.5380\n",
      "Fold 1, Epoch 6: Train Loss=1.9037, Train Acc=27.88%, Val Loss=2.6003, Val Acc=9.82%, Grad Norm=3.4023\n",
      "Fold 1, Epoch 7: Train Loss=1.8812, Train Acc=28.89%, Val Loss=2.4152, Val Acc=12.37%, Grad Norm=3.2968\n",
      "Fold 1, Epoch 8: Train Loss=1.8527, Train Acc=30.54%, Val Loss=2.4508, Val Acc=15.71%, Grad Norm=3.2593\n",
      "Fold 1, Epoch 9: Train Loss=1.8284, Train Acc=31.34%, Val Loss=2.2432, Val Acc=21.88%, Grad Norm=3.3051\n",
      "Fold 1, Epoch 10: Train Loss=1.8030, Train Acc=32.58%, Val Loss=2.4165, Val Acc=18.34%, Grad Norm=3.3223\n",
      "Fold 1, Epoch 11: Train Loss=1.7622, Train Acc=34.74%, Val Loss=2.3320, Val Acc=21.70%, Grad Norm=3.3530\n",
      "Fold 1, Epoch 12: Train Loss=1.7418, Train Acc=35.60%, Val Loss=2.2683, Val Acc=22.71%, Grad Norm=3.4635\n",
      "Fold 1, Epoch 13: Train Loss=1.7248, Train Acc=36.52%, Val Loss=2.2543, Val Acc=22.39%, Grad Norm=3.5493\n",
      "Fold 1, Epoch 14: Train Loss=1.7047, Train Acc=37.48%, Val Loss=2.2486, Val Acc=21.48%, Grad Norm=3.6507\n",
      "Fold 1, Epoch 15: Train Loss=1.6910, Train Acc=37.92%, Val Loss=2.2745, Val Acc=23.75%, Grad Norm=3.7245\n",
      "Fold 1, Epoch 16: Train Loss=1.6741, Train Acc=38.42%, Val Loss=2.2550, Val Acc=23.54%, Grad Norm=3.7916\n",
      "Fold 1, Epoch 17: Train Loss=1.6572, Train Acc=39.46%, Val Loss=2.2077, Val Acc=24.72%, Grad Norm=3.8372\n",
      "Fold 1, Epoch 18: Train Loss=1.6436, Train Acc=39.53%, Val Loss=2.3054, Val Acc=23.95%, Grad Norm=3.9574\n",
      "Fold 1, Epoch 19: Train Loss=1.6237, Train Acc=40.55%, Val Loss=2.2149, Val Acc=25.70%, Grad Norm=4.0202\n",
      "Fold 1, Epoch 20: Train Loss=1.6089, Train Acc=41.17%, Val Loss=2.1827, Val Acc=24.53%, Grad Norm=4.1648\n",
      "Fold 1, Epoch 21: Train Loss=1.5789, Train Acc=42.89%, Val Loss=2.1766, Val Acc=26.72%, Grad Norm=4.2310\n",
      "Fold 1, Epoch 22: Train Loss=1.5630, Train Acc=43.17%, Val Loss=2.1847, Val Acc=26.73%, Grad Norm=4.4024\n",
      "Fold 1, Epoch 23: Train Loss=1.5528, Train Acc=43.82%, Val Loss=2.1539, Val Acc=27.26%, Grad Norm=4.5354\n",
      "Fold 1, Epoch 24: Train Loss=1.5439, Train Acc=44.22%, Val Loss=2.1995, Val Acc=25.15%, Grad Norm=4.6594\n",
      "Fold 1, Epoch 25: Train Loss=1.5323, Train Acc=44.47%, Val Loss=2.1602, Val Acc=27.14%, Grad Norm=4.7644\n",
      "Fold 1, Epoch 26: Train Loss=1.5246, Train Acc=44.99%, Val Loss=2.2153, Val Acc=25.69%, Grad Norm=4.9198\n",
      "Fold 1, Epoch 27: Train Loss=1.5116, Train Acc=45.19%, Val Loss=2.1741, Val Acc=27.45%, Grad Norm=4.9920\n",
      "Fold 1, Epoch 28: Train Loss=1.5045, Train Acc=45.94%, Val Loss=2.1986, Val Acc=28.03%, Grad Norm=5.1345\n",
      "Fold 1, Epoch 29: Train Loss=1.4942, Train Acc=46.07%, Val Loss=2.2199, Val Acc=27.63%, Grad Norm=5.2386\n",
      "Fold 1, Epoch 30: Train Loss=1.4872, Train Acc=46.65%, Val Loss=2.0822, Val Acc=29.38%, Grad Norm=5.3265\n",
      "Fold 1, Epoch 31: Train Loss=1.4661, Train Acc=47.27%, Val Loss=2.1576, Val Acc=28.72%, Grad Norm=5.3839\n",
      "Fold 1, Epoch 32: Train Loss=1.4572, Train Acc=47.67%, Val Loss=2.1851, Val Acc=26.91%, Grad Norm=5.5151\n",
      "Fold 1, Epoch 33: Train Loss=1.4504, Train Acc=48.26%, Val Loss=2.1871, Val Acc=26.73%, Grad Norm=5.6164\n",
      "Fold 1, Epoch 34: Train Loss=1.4456, Train Acc=48.22%, Val Loss=2.2131, Val Acc=26.39%, Grad Norm=5.7356\n",
      "Fold 1, Epoch 35: Train Loss=1.4442, Train Acc=48.45%, Val Loss=2.1789, Val Acc=28.21%, Grad Norm=5.8564\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=31.80%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.1843, Train Acc=14.94%, Val Loss=6.5561, Val Acc=11.87%, Grad Norm=4.5685\n",
      "Fold 2, Epoch 2: Train Loss=2.0937, Train Acc=19.80%, Val Loss=3.7893, Val Acc=16.17%, Grad Norm=4.9290\n",
      "Fold 2, Epoch 3: Train Loss=2.0174, Train Acc=23.27%, Val Loss=3.4238, Val Acc=17.75%, Grad Norm=4.2961\n",
      "Fold 2, Epoch 4: Train Loss=1.9585, Train Acc=25.43%, Val Loss=3.4841, Val Acc=14.49%, Grad Norm=3.8872\n",
      "Fold 2, Epoch 5: Train Loss=1.9215, Train Acc=27.40%, Val Loss=3.0245, Val Acc=14.81%, Grad Norm=3.6420\n",
      "Fold 2, Epoch 6: Train Loss=1.8791, Train Acc=29.92%, Val Loss=2.7107, Val Acc=14.29%, Grad Norm=3.6234\n",
      "Fold 2, Epoch 7: Train Loss=1.8335, Train Acc=32.00%, Val Loss=2.7765, Val Acc=12.13%, Grad Norm=3.5722\n",
      "Fold 2, Epoch 8: Train Loss=1.7971, Train Acc=33.58%, Val Loss=2.7053, Val Acc=12.55%, Grad Norm=3.4937\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=18.13%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.1771, Train Acc=14.54%, Val Loss=6.1668, Val Acc=3.73%, Grad Norm=4.4213\n",
      "Fold 3, Epoch 2: Train Loss=2.1143, Train Acc=18.35%, Val Loss=5.6887, Val Acc=15.90%, Grad Norm=4.7375\n",
      "Fold 3, Epoch 3: Train Loss=2.0538, Train Acc=21.50%, Val Loss=4.8451, Val Acc=9.03%, Grad Norm=4.3308\n",
      "Fold 3, Epoch 4: Train Loss=2.0012, Train Acc=24.05%, Val Loss=3.6085, Val Acc=8.62%, Grad Norm=3.8446\n",
      "Fold 3, Epoch 5: Train Loss=1.9586, Train Acc=25.86%, Val Loss=3.3298, Val Acc=12.89%, Grad Norm=3.6191\n",
      "Fold 3, Epoch 6: Train Loss=1.9269, Train Acc=27.43%, Val Loss=3.6929, Val Acc=8.20%, Grad Norm=3.5209\n",
      "Fold 3, Epoch 7: Train Loss=1.8990, Train Acc=28.54%, Val Loss=3.1275, Val Acc=13.92%, Grad Norm=3.4491\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=12.89%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1807, Train Acc=15.19%, Val Loss=6.5204, Val Acc=0.03%, Grad Norm=4.5362\n",
      "Fold 4, Epoch 2: Train Loss=2.1171, Train Acc=18.94%, Val Loss=4.7823, Val Acc=7.32%, Grad Norm=5.1550\n",
      "Fold 4, Epoch 3: Train Loss=2.0562, Train Acc=22.26%, Val Loss=4.7955, Val Acc=8.85%, Grad Norm=4.4334\n",
      "Fold 4, Epoch 4: Train Loss=2.0150, Train Acc=24.14%, Val Loss=4.0309, Val Acc=10.66%, Grad Norm=3.9074\n",
      "Fold 4, Epoch 5: Train Loss=1.9796, Train Acc=25.45%, Val Loss=4.3271, Val Acc=14.53%, Grad Norm=3.5670\n",
      "Fold 4, Epoch 6: Train Loss=1.9582, Train Acc=26.43%, Val Loss=3.6853, Val Acc=14.53%, Grad Norm=3.3940\n",
      "Fold 4, Epoch 7: Train Loss=1.9230, Train Acc=28.39%, Val Loss=3.8872, Val Acc=13.60%, Grad Norm=3.3592\n",
      "Fold 4, Epoch 8: Train Loss=1.8900, Train Acc=29.68%, Val Loss=3.3910, Val Acc=14.39%, Grad Norm=3.2960\n",
      "Fold 4, Epoch 9: Train Loss=1.8555, Train Acc=31.31%, Val Loss=3.3658, Val Acc=16.79%, Grad Norm=3.2990\n",
      "Fold 4, Epoch 10: Train Loss=1.8330, Train Acc=32.24%, Val Loss=3.2777, Val Acc=15.68%, Grad Norm=3.2650\n",
      "Fold 4, Epoch 11: Train Loss=1.7917, Train Acc=33.95%, Val Loss=3.4522, Val Acc=13.09%, Grad Norm=3.3022\n",
      "Fold 4, Epoch 12: Train Loss=1.7730, Train Acc=34.89%, Val Loss=3.3412, Val Acc=15.55%, Grad Norm=3.4497\n",
      "Fold 4, Epoch 13: Train Loss=1.7577, Train Acc=35.66%, Val Loss=3.2040, Val Acc=16.87%, Grad Norm=3.4978\n",
      "Fold 4, Epoch 14: Train Loss=1.7419, Train Acc=36.08%, Val Loss=3.3504, Val Acc=16.08%, Grad Norm=3.5922\n",
      "Fold 4, Epoch 15: Train Loss=1.7263, Train Acc=36.91%, Val Loss=3.2532, Val Acc=18.02%, Grad Norm=3.6619\n",
      "Fold 4, Epoch 16: Train Loss=1.7089, Train Acc=37.46%, Val Loss=3.2372, Val Acc=17.69%, Grad Norm=3.7222\n",
      "Fold 4, Epoch 17: Train Loss=1.6909, Train Acc=38.25%, Val Loss=3.1632, Val Acc=16.87%, Grad Norm=3.8280\n",
      "Fold 4, Epoch 18: Train Loss=1.6777, Train Acc=38.57%, Val Loss=3.1839, Val Acc=17.66%, Grad Norm=3.8974\n",
      "Fold 4, Epoch 19: Train Loss=1.6583, Train Acc=39.47%, Val Loss=3.2135, Val Acc=17.75%, Grad Norm=3.9796\n",
      "Fold 4, Epoch 20: Train Loss=1.6458, Train Acc=39.75%, Val Loss=3.0186, Val Acc=20.58%, Grad Norm=4.1017\n",
      "Fold 4, Epoch 21: Train Loss=1.6144, Train Acc=41.35%, Val Loss=3.1637, Val Acc=19.35%, Grad Norm=4.1362\n",
      "Fold 4, Epoch 22: Train Loss=1.6061, Train Acc=41.52%, Val Loss=3.3049, Val Acc=18.51%, Grad Norm=4.3337\n",
      "Fold 4, Epoch 23: Train Loss=1.5962, Train Acc=41.91%, Val Loss=3.0962, Val Acc=20.93%, Grad Norm=4.3936\n",
      "Fold 4, Epoch 24: Train Loss=1.5855, Train Acc=42.48%, Val Loss=3.1586, Val Acc=19.88%, Grad Norm=4.5150\n",
      "Fold 4, Epoch 25: Train Loss=1.5784, Train Acc=42.86%, Val Loss=3.1564, Val Acc=21.18%, Grad Norm=4.6514\n",
      "Fold 4, Epoch 26: Train Loss=1.5647, Train Acc=43.49%, Val Loss=3.1527, Val Acc=19.74%, Grad Norm=4.7754\n",
      "Fold 4, Epoch 27: Train Loss=1.5582, Train Acc=43.54%, Val Loss=3.2051, Val Acc=21.39%, Grad Norm=4.8848\n",
      "Fold 4, Epoch 28: Train Loss=1.5477, Train Acc=44.04%, Val Loss=3.0614, Val Acc=21.84%, Grad Norm=4.9852\n",
      "Fold 4, Epoch 29: Train Loss=1.5341, Train Acc=44.37%, Val Loss=3.1451, Val Acc=20.76%, Grad Norm=5.0565\n",
      "Fold 4, Epoch 30: Train Loss=1.5295, Train Acc=44.70%, Val Loss=3.1244, Val Acc=22.40%, Grad Norm=5.1803\n",
      "Fold 4, Epoch 31: Train Loss=1.5043, Train Acc=45.65%, Val Loss=3.1527, Val Acc=21.03%, Grad Norm=5.2909\n",
      "Fold 4, Epoch 32: Train Loss=1.5015, Train Acc=45.92%, Val Loss=3.0140, Val Acc=23.46%, Grad Norm=5.4008\n",
      "Fold 4, Epoch 33: Train Loss=1.4937, Train Acc=46.19%, Val Loss=3.0596, Val Acc=21.98%, Grad Norm=5.5046\n",
      "Fold 4, Epoch 34: Train Loss=1.4920, Train Acc=46.12%, Val Loss=3.0540, Val Acc=22.53%, Grad Norm=5.6120\n",
      "Fold 4, Epoch 35: Train Loss=1.4820, Train Acc=46.56%, Val Loss=3.1190, Val Acc=23.23%, Grad Norm=5.7029\n",
      "Fold 4, Epoch 36: Train Loss=1.4797, Train Acc=46.76%, Val Loss=3.1113, Val Acc=21.83%, Grad Norm=5.8069\n",
      "Fold 4, Epoch 37: Train Loss=1.4691, Train Acc=47.26%, Val Loss=3.1316, Val Acc=22.71%, Grad Norm=5.9219\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=30.36%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.1790, Train Acc=15.56%, Val Loss=5.1612, Val Acc=15.32%, Grad Norm=4.5300\n",
      "Fold 5, Epoch 2: Train Loss=2.1020, Train Acc=19.79%, Val Loss=4.2261, Val Acc=7.58%, Grad Norm=4.9836\n",
      "Fold 5, Epoch 3: Train Loss=2.0267, Train Acc=22.68%, Val Loss=3.7345, Val Acc=12.17%, Grad Norm=4.3548\n",
      "Fold 5, Epoch 4: Train Loss=1.9763, Train Acc=24.71%, Val Loss=3.1391, Val Acc=11.51%, Grad Norm=3.8343\n",
      "Fold 5, Epoch 5: Train Loss=1.9373, Train Acc=26.64%, Val Loss=3.6718, Val Acc=6.53%, Grad Norm=3.5983\n",
      "Fold 5, Epoch 6: Train Loss=1.9059, Train Acc=27.96%, Val Loss=2.9243, Val Acc=11.53%, Grad Norm=3.4607\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=12.08%\n",
      "\n",
      "SNR   0 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-24_18-51-25_LTE-V_XFR_SingleFileBlock_SNR0dB_fd655_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-5 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "统一目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 216, 每 block 样本数: 864, 每样本长度: 288\n",
      "[INFO] 总 block 数: 216, 类别数: 9\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "[INFO] 训练 block 数: 162, 测试 block 数: 54\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.1996, Train Acc=12.74%, Val Loss=2.9372, Val Acc=16.67%, Grad Norm=4.4524\n",
      "Fold 1, Epoch 2: Train Loss=2.1880, Train Acc=13.58%, Val Loss=4.1100, Val Acc=17.91%, Grad Norm=4.8341\n",
      "Fold 1, Epoch 3: Train Loss=2.1619, Train Acc=15.95%, Val Loss=4.7605, Val Acc=6.16%, Grad Norm=4.6384\n",
      "Fold 1, Epoch 4: Train Loss=2.1392, Train Acc=17.40%, Val Loss=3.3189, Val Acc=6.76%, Grad Norm=4.1296\n",
      "Fold 1, Epoch 5: Train Loss=2.1139, Train Acc=18.54%, Val Loss=3.3298, Val Acc=6.49%, Grad Norm=3.5707\n",
      "Fold 1, Epoch 6: Train Loss=2.0912, Train Acc=19.67%, Val Loss=2.5360, Val Acc=6.90%, Grad Norm=3.1585\n",
      "Fold 1, Epoch 7: Train Loss=2.0788, Train Acc=20.50%, Val Loss=2.4852, Val Acc=7.23%, Grad Norm=2.8896\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.01%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2027, Train Acc=12.95%, Val Loss=5.2570, Val Acc=11.97%, Grad Norm=4.5121\n",
      "Fold 2, Epoch 2: Train Loss=2.1927, Train Acc=13.64%, Val Loss=4.9272, Val Acc=12.02%, Grad Norm=4.8787\n",
      "Fold 2, Epoch 3: Train Loss=2.1774, Train Acc=14.95%, Val Loss=6.3265, Val Acc=11.29%, Grad Norm=4.5791\n",
      "Fold 2, Epoch 4: Train Loss=2.1509, Train Acc=16.89%, Val Loss=3.8007, Val Acc=11.88%, Grad Norm=4.2401\n",
      "Fold 2, Epoch 5: Train Loss=2.1289, Train Acc=18.20%, Val Loss=3.2223, Val Acc=5.33%, Grad Norm=3.6842\n",
      "Fold 2, Epoch 6: Train Loss=2.1068, Train Acc=19.18%, Val Loss=3.1648, Val Acc=12.71%, Grad Norm=3.2178\n",
      "Fold 2, Epoch 7: Train Loss=2.0898, Train Acc=20.05%, Val Loss=3.0215, Val Acc=5.43%, Grad Norm=3.0114\n",
      "Fold 2, Epoch 8: Train Loss=2.0745, Train Acc=21.00%, Val Loss=2.5992, Val Acc=10.87%, Grad Norm=2.8268\n",
      "Fold 2, Epoch 9: Train Loss=2.0630, Train Acc=21.32%, Val Loss=2.7121, Val Acc=6.91%, Grad Norm=2.6921\n",
      "Fold 2, Epoch 10: Train Loss=2.0495, Train Acc=22.12%, Val Loss=2.6291, Val Acc=8.88%, Grad Norm=2.6524\n",
      "Fold 2, Epoch 11: Train Loss=2.0320, Train Acc=22.95%, Val Loss=2.5551, Val Acc=6.81%, Grad Norm=2.5991\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=13.48%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.1968, Train Acc=13.32%, Val Loss=6.5521, Val Acc=18.66%, Grad Norm=4.4867\n",
      "Fold 3, Epoch 2: Train Loss=2.1815, Train Acc=14.65%, Val Loss=6.5496, Val Acc=18.52%, Grad Norm=4.7040\n",
      "Fold 3, Epoch 3: Train Loss=2.1674, Train Acc=15.66%, Val Loss=5.2129, Val Acc=3.71%, Grad Norm=4.3851\n",
      "Fold 3, Epoch 4: Train Loss=2.1511, Train Acc=16.61%, Val Loss=3.8019, Val Acc=8.21%, Grad Norm=3.7703\n",
      "Fold 3, Epoch 5: Train Loss=2.1337, Train Acc=17.76%, Val Loss=3.9512, Val Acc=14.95%, Grad Norm=3.3160\n",
      "Fold 3, Epoch 6: Train Loss=2.1178, Train Acc=18.77%, Val Loss=3.0464, Val Acc=4.43%, Grad Norm=2.9908\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.09%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1928, Train Acc=13.41%, Val Loss=4.9648, Val Acc=2.92%, Grad Norm=4.5810\n",
      "Fold 4, Epoch 2: Train Loss=2.1825, Train Acc=14.72%, Val Loss=6.1146, Val Acc=0.20%, Grad Norm=4.7921\n",
      "Fold 4, Epoch 3: Train Loss=2.1665, Train Acc=16.10%, Val Loss=5.3769, Val Acc=16.04%, Grad Norm=4.5378\n",
      "Fold 4, Epoch 4: Train Loss=2.1475, Train Acc=17.32%, Val Loss=4.2690, Val Acc=1.22%, Grad Norm=4.0737\n",
      "Fold 4, Epoch 5: Train Loss=2.1271, Train Acc=18.91%, Val Loss=3.7153, Val Acc=1.73%, Grad Norm=3.4613\n",
      "Fold 4, Epoch 6: Train Loss=2.1091, Train Acc=19.59%, Val Loss=4.2368, Val Acc=1.56%, Grad Norm=3.0391\n",
      "Fold 4, Epoch 7: Train Loss=2.0980, Train Acc=20.33%, Val Loss=3.1959, Val Acc=2.00%, Grad Norm=2.7521\n",
      "Fold 4, Epoch 8: Train Loss=2.0842, Train Acc=20.76%, Val Loss=3.0253, Val Acc=6.45%, Grad Norm=2.6357\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.65%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.1979, Train Acc=13.50%, Val Loss=6.6003, Val Acc=3.09%, Grad Norm=4.5014\n",
      "Fold 5, Epoch 2: Train Loss=2.1897, Train Acc=14.34%, Val Loss=6.3676, Val Acc=3.30%, Grad Norm=4.6585\n",
      "Fold 5, Epoch 3: Train Loss=2.1707, Train Acc=15.48%, Val Loss=6.0113, Val Acc=3.59%, Grad Norm=4.5421\n",
      "Fold 5, Epoch 4: Train Loss=2.1452, Train Acc=17.61%, Val Loss=4.1794, Val Acc=15.41%, Grad Norm=4.0375\n",
      "Fold 5, Epoch 5: Train Loss=2.1179, Train Acc=18.58%, Val Loss=3.0209, Val Acc=4.18%, Grad Norm=3.3801\n",
      "Fold 5, Epoch 6: Train Loss=2.0988, Train Acc=19.23%, Val Loss=3.1047, Val Acc=5.00%, Grad Norm=3.0479\n",
      "Fold 5, Epoch 7: Train Loss=2.0846, Train Acc=20.34%, Val Loss=2.9131, Val Acc=4.08%, Grad Norm=2.7908\n",
      "Fold 5, Epoch 8: Train Loss=2.0678, Train Acc=21.16%, Val Loss=2.6927, Val Acc=8.19%, Grad Norm=2.6774\n",
      "Fold 5, Epoch 9: Train Loss=2.0612, Train Acc=21.41%, Val Loss=2.6622, Val Acc=13.81%, Grad Norm=2.5353\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.42%\n",
      "\n",
      "SNR  -5 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-24_19-01-15_LTE-V_XFR_SingleFileBlock_SNR-5dB_fd655_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-10 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "统一目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 216, 每 block 样本数: 864, 每样本长度: 288\n",
      "[INFO] 总 block 数: 216, 类别数: 9\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "[INFO] 训练 block 数: 162, 测试 block 数: 54\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2007, Train Acc=12.17%, Val Loss=2.2598, Val Acc=19.93%, Grad Norm=4.5263\n",
      "Fold 1, Epoch 2: Train Loss=2.2015, Train Acc=12.14%, Val Loss=2.4311, Val Acc=6.09%, Grad Norm=4.6905\n",
      "Fold 1, Epoch 3: Train Loss=2.2008, Train Acc=12.54%, Val Loss=2.3254, Val Acc=18.20%, Grad Norm=4.6181\n",
      "Fold 1, Epoch 4: Train Loss=2.1987, Train Acc=12.59%, Val Loss=2.4282, Val Acc=17.72%, Grad Norm=4.4775\n",
      "Fold 1, Epoch 5: Train Loss=2.1970, Train Acc=12.56%, Val Loss=3.1926, Val Acc=6.06%, Grad Norm=4.2772\n",
      "Fold 1, Epoch 6: Train Loss=2.1952, Train Acc=12.72%, Val Loss=2.8272, Val Acc=6.06%, Grad Norm=4.0178\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.05%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2055, Train Acc=12.45%, Val Loss=2.4751, Val Acc=3.96%, Grad Norm=4.5288\n",
      "Fold 2, Epoch 2: Train Loss=2.2044, Train Acc=12.20%, Val Loss=2.5420, Val Acc=12.12%, Grad Norm=4.7059\n",
      "Fold 2, Epoch 3: Train Loss=2.2041, Train Acc=12.34%, Val Loss=3.6246, Val Acc=3.03%, Grad Norm=4.7104\n",
      "Fold 2, Epoch 4: Train Loss=2.2037, Train Acc=12.30%, Val Loss=3.4452, Val Acc=3.40%, Grad Norm=4.4760\n",
      "Fold 2, Epoch 5: Train Loss=2.2016, Train Acc=12.73%, Val Loss=3.5619, Val Acc=3.03%, Grad Norm=4.2595\n",
      "Fold 2, Epoch 6: Train Loss=2.1994, Train Acc=12.87%, Val Loss=3.3809, Val Acc=7.97%, Grad Norm=4.1092\n",
      "Fold 2, Epoch 7: Train Loss=2.1965, Train Acc=13.07%, Val Loss=3.6334, Val Acc=3.01%, Grad Norm=3.6892\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2039, Train Acc=12.15%, Val Loss=3.2085, Val Acc=3.12%, Grad Norm=4.4472\n",
      "Fold 3, Epoch 2: Train Loss=2.2025, Train Acc=12.61%, Val Loss=3.2604, Val Acc=3.16%, Grad Norm=4.5837\n",
      "Fold 3, Epoch 3: Train Loss=2.2013, Train Acc=12.61%, Val Loss=4.5644, Val Acc=17.22%, Grad Norm=4.4154\n",
      "Fold 3, Epoch 4: Train Loss=2.1982, Train Acc=12.89%, Val Loss=3.4398, Val Acc=18.71%, Grad Norm=4.1684\n",
      "Fold 3, Epoch 5: Train Loss=2.1955, Train Acc=13.30%, Val Loss=3.3601, Val Acc=3.12%, Grad Norm=3.7090\n",
      "Fold 3, Epoch 6: Train Loss=2.1912, Train Acc=13.51%, Val Loss=2.8366, Val Acc=15.60%, Grad Norm=2.8452\n",
      "Fold 3, Epoch 7: Train Loss=2.1867, Train Acc=14.01%, Val Loss=3.2490, Val Acc=3.15%, Grad Norm=2.4801\n",
      "Fold 3, Epoch 8: Train Loss=2.1814, Train Acc=14.36%, Val Loss=2.9435, Val Acc=17.70%, Grad Norm=2.3226\n",
      "Fold 3, Epoch 9: Train Loss=2.1792, Train Acc=14.24%, Val Loss=2.6814, Val Acc=3.37%, Grad Norm=2.0963\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.01%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1991, Train Acc=12.60%, Val Loss=2.6555, Val Acc=15.62%, Grad Norm=4.5311\n",
      "Fold 4, Epoch 2: Train Loss=2.1986, Train Acc=12.67%, Val Loss=2.9224, Val Acc=14.72%, Grad Norm=4.6836\n",
      "Fold 4, Epoch 3: Train Loss=2.1960, Train Acc=12.89%, Val Loss=3.9960, Val Acc=0.00%, Grad Norm=4.6231\n",
      "Fold 4, Epoch 4: Train Loss=2.1949, Train Acc=13.25%, Val Loss=3.6700, Val Acc=15.61%, Grad Norm=4.3609\n",
      "Fold 4, Epoch 5: Train Loss=2.1935, Train Acc=13.30%, Val Loss=2.6787, Val Acc=15.60%, Grad Norm=4.1560\n",
      "Fold 4, Epoch 6: Train Loss=2.1893, Train Acc=13.38%, Val Loss=2.4846, Val Acc=18.68%, Grad Norm=3.6574\n",
      "Fold 4, Epoch 7: Train Loss=2.1841, Train Acc=13.79%, Val Loss=2.7134, Val Acc=15.59%, Grad Norm=2.8297\n",
      "Fold 4, Epoch 8: Train Loss=2.1830, Train Acc=14.04%, Val Loss=2.5410, Val Acc=17.02%, Grad Norm=2.3708\n",
      "Fold 4, Epoch 9: Train Loss=2.1817, Train Acc=14.21%, Val Loss=2.5924, Val Acc=0.16%, Grad Norm=2.1410\n",
      "Fold 4, Epoch 10: Train Loss=2.1778, Train Acc=14.92%, Val Loss=2.6076, Val Acc=0.12%, Grad Norm=2.0603\n",
      "Fold 4, Epoch 11: Train Loss=2.1731, Train Acc=15.39%, Val Loss=2.5463, Val Acc=0.92%, Grad Norm=1.9878\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.19%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2031, Train Acc=12.52%, Val Loss=2.5954, Val Acc=3.45%, Grad Norm=4.5246\n",
      "Fold 5, Epoch 2: Train Loss=2.2039, Train Acc=12.38%, Val Loss=2.9433, Val Acc=3.12%, Grad Norm=4.7367\n",
      "Fold 5, Epoch 3: Train Loss=2.2026, Train Acc=12.65%, Val Loss=3.4683, Val Acc=3.48%, Grad Norm=4.5920\n",
      "Fold 5, Epoch 4: Train Loss=2.1999, Train Acc=13.13%, Val Loss=4.3331, Val Acc=3.14%, Grad Norm=4.3749\n",
      "Fold 5, Epoch 5: Train Loss=2.1986, Train Acc=13.27%, Val Loss=4.1121, Val Acc=3.20%, Grad Norm=4.1921\n",
      "Fold 5, Epoch 6: Train Loss=2.1970, Train Acc=13.19%, Val Loss=3.4836, Val Acc=15.62%, Grad Norm=3.7529\n",
      "Fold 5, Epoch 7: Train Loss=2.1921, Train Acc=13.91%, Val Loss=2.9059, Val Acc=3.11%, Grad Norm=3.0462\n",
      "Fold 5, Epoch 8: Train Loss=2.1879, Train Acc=14.13%, Val Loss=2.4784, Val Acc=3.22%, Grad Norm=2.5174\n",
      "Fold 5, Epoch 9: Train Loss=2.1839, Train Acc=14.70%, Val Loss=2.8729, Val Acc=3.23%, Grad Norm=2.3237\n",
      "Fold 5, Epoch 10: Train Loss=2.1777, Train Acc=15.22%, Val Loss=2.5641, Val Acc=3.19%, Grad Norm=2.2450\n",
      "Fold 5, Epoch 11: Train Loss=2.1678, Train Acc=15.75%, Val Loss=2.6834, Val Acc=3.31%, Grad Norm=2.1511\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.12%\n",
      "\n",
      "SNR -10 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-24_19-05-44_LTE-V_XFR_SingleFileBlock_SNR-10dB_fd655_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-15 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "统一目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:09<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 216, 每 block 样本数: 864, 每样本长度: 288\n",
      "[INFO] 总 block 数: 216, 类别数: 9\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "[INFO] 训练 block 数: 162, 测试 block 数: 54\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2004, Train Acc=12.08%, Val Loss=2.3224, Val Acc=9.06%, Grad Norm=4.5776\n",
      "Fold 1, Epoch 2: Train Loss=2.2032, Train Acc=11.84%, Val Loss=2.3150, Val Acc=6.04%, Grad Norm=4.7060\n",
      "Fold 1, Epoch 3: Train Loss=2.2010, Train Acc=12.36%, Val Loss=2.4154, Val Acc=6.06%, Grad Norm=4.5944\n",
      "Fold 1, Epoch 4: Train Loss=2.2015, Train Acc=12.13%, Val Loss=2.2591, Val Acc=6.13%, Grad Norm=4.5008\n",
      "Fold 1, Epoch 5: Train Loss=2.2008, Train Acc=11.97%, Val Loss=2.2435, Val Acc=17.88%, Grad Norm=4.3102\n",
      "Fold 1, Epoch 6: Train Loss=2.1995, Train Acc=12.08%, Val Loss=2.2605, Val Acc=6.06%, Grad Norm=4.1085\n",
      "Fold 1, Epoch 7: Train Loss=2.1998, Train Acc=12.23%, Val Loss=2.3419, Val Acc=6.11%, Grad Norm=3.8799\n",
      "Fold 1, Epoch 8: Train Loss=2.1969, Train Acc=12.52%, Val Loss=2.3096, Val Acc=11.13%, Grad Norm=3.7246\n",
      "Fold 1, Epoch 9: Train Loss=2.1963, Train Acc=12.30%, Val Loss=2.3017, Val Acc=11.08%, Grad Norm=3.4032\n",
      "Fold 1, Epoch 10: Train Loss=2.1941, Train Acc=12.49%, Val Loss=2.3020, Val Acc=6.50%, Grad Norm=2.8103\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.01%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2069, Train Acc=12.63%, Val Loss=2.3440, Val Acc=2.99%, Grad Norm=4.5314\n",
      "Fold 2, Epoch 2: Train Loss=2.2063, Train Acc=12.41%, Val Loss=2.4888, Val Acc=3.07%, Grad Norm=4.7073\n",
      "Fold 2, Epoch 3: Train Loss=2.2055, Train Acc=12.35%, Val Loss=2.4723, Val Acc=3.03%, Grad Norm=4.5492\n",
      "Fold 2, Epoch 4: Train Loss=2.2057, Train Acc=12.12%, Val Loss=2.3791, Val Acc=3.05%, Grad Norm=4.4353\n",
      "Fold 2, Epoch 5: Train Loss=2.2046, Train Acc=12.38%, Val Loss=2.6217, Val Acc=10.16%, Grad Norm=4.2776\n",
      "Fold 2, Epoch 6: Train Loss=2.2044, Train Acc=12.33%, Val Loss=2.6504, Val Acc=3.03%, Grad Norm=4.0260\n",
      "Fold 2, Epoch 7: Train Loss=2.2022, Train Acc=12.17%, Val Loss=2.4647, Val Acc=3.03%, Grad Norm=3.7654\n",
      "Fold 2, Epoch 8: Train Loss=2.2005, Train Acc=12.82%, Val Loss=2.3959, Val Acc=3.03%, Grad Norm=3.3396\n",
      "Fold 2, Epoch 9: Train Loss=2.1980, Train Acc=12.79%, Val Loss=2.2859, Val Acc=3.14%, Grad Norm=2.4376\n",
      "Fold 2, Epoch 10: Train Loss=2.1949, Train Acc=12.91%, Val Loss=2.4715, Val Acc=3.03%, Grad Norm=2.0094\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.23%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2030, Train Acc=12.08%, Val Loss=2.4787, Val Acc=3.12%, Grad Norm=4.5361\n",
      "Fold 3, Epoch 2: Train Loss=2.2032, Train Acc=12.39%, Val Loss=2.4423, Val Acc=6.26%, Grad Norm=4.7642\n",
      "Fold 3, Epoch 3: Train Loss=2.2049, Train Acc=12.12%, Val Loss=2.1790, Val Acc=13.53%, Grad Norm=4.5531\n",
      "Fold 3, Epoch 4: Train Loss=2.2041, Train Acc=11.96%, Val Loss=2.4545, Val Acc=18.75%, Grad Norm=4.4041\n",
      "Fold 3, Epoch 5: Train Loss=2.2012, Train Acc=12.47%, Val Loss=2.4150, Val Acc=13.31%, Grad Norm=4.3398\n",
      "Fold 3, Epoch 6: Train Loss=2.2011, Train Acc=12.50%, Val Loss=2.3094, Val Acc=3.31%, Grad Norm=4.1350\n",
      "Fold 3, Epoch 7: Train Loss=2.2004, Train Acc=12.36%, Val Loss=2.6511, Val Acc=3.14%, Grad Norm=3.8303\n",
      "Fold 3, Epoch 8: Train Loss=2.1989, Train Acc=12.58%, Val Loss=2.2832, Val Acc=17.47%, Grad Norm=3.3178\n",
      "Fold 3, Epoch 9: Train Loss=2.1946, Train Acc=12.65%, Val Loss=2.2949, Val Acc=18.74%, Grad Norm=2.4983\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.12%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1978, Train Acc=12.76%, Val Loss=2.2749, Val Acc=2.34%, Grad Norm=4.5044\n",
      "Fold 4, Epoch 2: Train Loss=2.1989, Train Acc=12.77%, Val Loss=2.5147, Val Acc=6.25%, Grad Norm=4.6603\n",
      "Fold 4, Epoch 3: Train Loss=2.1983, Train Acc=12.40%, Val Loss=2.2660, Val Acc=6.25%, Grad Norm=4.5648\n",
      "Fold 4, Epoch 4: Train Loss=2.1969, Train Acc=12.58%, Val Loss=2.3475, Val Acc=9.38%, Grad Norm=4.4099\n",
      "Fold 4, Epoch 5: Train Loss=2.1980, Train Acc=12.67%, Val Loss=2.5417, Val Acc=0.20%, Grad Norm=4.2309\n",
      "Fold 4, Epoch 6: Train Loss=2.1961, Train Acc=12.64%, Val Loss=2.3957, Val Acc=15.64%, Grad Norm=3.9898\n",
      "Fold 4, Epoch 7: Train Loss=2.1955, Train Acc=12.81%, Val Loss=2.4344, Val Acc=0.00%, Grad Norm=3.7319\n",
      "Fold 4, Epoch 8: Train Loss=2.1918, Train Acc=12.99%, Val Loss=2.3686, Val Acc=0.04%, Grad Norm=3.0034\n",
      "Fold 4, Epoch 9: Train Loss=2.1889, Train Acc=13.04%, Val Loss=2.2501, Val Acc=15.61%, Grad Norm=2.2135\n",
      "Fold 4, Epoch 10: Train Loss=2.1876, Train Acc=13.13%, Val Loss=2.2799, Val Acc=0.00%, Grad Norm=1.9303\n",
      "Fold 4, Epoch 11: Train Loss=2.1846, Train Acc=13.47%, Val Loss=2.2180, Val Acc=15.62%, Grad Norm=1.8385\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.13%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2032, Train Acc=12.47%, Val Loss=2.3279, Val Acc=4.62%, Grad Norm=4.4992\n",
      "Fold 5, Epoch 2: Train Loss=2.2036, Train Acc=12.40%, Val Loss=2.5521, Val Acc=3.12%, Grad Norm=4.6828\n",
      "Fold 5, Epoch 3: Train Loss=2.2030, Train Acc=12.76%, Val Loss=2.4338, Val Acc=15.62%, Grad Norm=4.5784\n",
      "Fold 5, Epoch 4: Train Loss=2.2039, Train Acc=12.43%, Val Loss=2.5433, Val Acc=3.16%, Grad Norm=4.3704\n",
      "Fold 5, Epoch 5: Train Loss=2.2013, Train Acc=12.68%, Val Loss=2.4554, Val Acc=3.12%, Grad Norm=4.2254\n",
      "Fold 5, Epoch 6: Train Loss=2.2002, Train Acc=12.94%, Val Loss=2.4201, Val Acc=15.62%, Grad Norm=4.0280\n",
      "Fold 5, Epoch 7: Train Loss=2.2002, Train Acc=12.47%, Val Loss=2.3990, Val Acc=3.61%, Grad Norm=3.4001\n",
      "Fold 5, Epoch 8: Train Loss=2.1953, Train Acc=12.84%, Val Loss=2.3446, Val Acc=15.54%, Grad Norm=2.4684\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.11%\n",
      "\n",
      "SNR -15 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-24_19-10-38_LTE-V_XFR_SingleFileBlock_SNR-15dB_fd655_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-20 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "统一目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:09<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 216, 每 block 样本数: 864, 每样本长度: 288\n",
      "[INFO] 总 block 数: 216, 类别数: 9\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "[INFO] 训练 block 数: 162, 测试 block 数: 54\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2052, Train Acc=12.12%, Val Loss=2.3989, Val Acc=6.06%, Grad Norm=4.5092\n",
      "Fold 1, Epoch 2: Train Loss=2.2019, Train Acc=12.15%, Val Loss=2.4092, Val Acc=5.96%, Grad Norm=4.6584\n",
      "Fold 1, Epoch 3: Train Loss=2.2023, Train Acc=12.33%, Val Loss=2.3719, Val Acc=6.06%, Grad Norm=4.5191\n",
      "Fold 1, Epoch 4: Train Loss=2.2010, Train Acc=11.95%, Val Loss=2.3887, Val Acc=6.05%, Grad Norm=4.3525\n",
      "Fold 1, Epoch 5: Train Loss=2.1992, Train Acc=12.24%, Val Loss=2.4152, Val Acc=8.95%, Grad Norm=4.1780\n",
      "Fold 1, Epoch 6: Train Loss=2.1997, Train Acc=12.20%, Val Loss=2.3854, Val Acc=9.09%, Grad Norm=3.9847\n",
      "Fold 1, Epoch 7: Train Loss=2.1980, Train Acc=12.26%, Val Loss=2.4455, Val Acc=6.06%, Grad Norm=3.5307\n",
      "Fold 1, Epoch 8: Train Loss=2.1934, Train Acc=12.74%, Val Loss=2.3383, Val Acc=6.06%, Grad Norm=2.5670\n",
      "Fold 1, Epoch 9: Train Loss=2.1920, Train Acc=12.62%, Val Loss=2.2827, Val Acc=5.92%, Grad Norm=2.0467\n",
      "Fold 1, Epoch 10: Train Loss=2.1906, Train Acc=12.63%, Val Loss=2.2263, Val Acc=6.06%, Grad Norm=1.8266\n",
      "Fold 1, Epoch 11: Train Loss=2.1885, Train Acc=12.77%, Val Loss=2.3291, Val Acc=8.26%, Grad Norm=1.7165\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2046, Train Acc=12.47%, Val Loss=2.4311, Val Acc=3.03%, Grad Norm=4.4910\n",
      "Fold 2, Epoch 2: Train Loss=2.2077, Train Acc=12.43%, Val Loss=2.2934, Val Acc=15.12%, Grad Norm=4.6438\n",
      "Fold 2, Epoch 3: Train Loss=2.2056, Train Acc=12.16%, Val Loss=2.3594, Val Acc=12.29%, Grad Norm=4.5116\n",
      "Fold 2, Epoch 4: Train Loss=2.2053, Train Acc=12.25%, Val Loss=2.3390, Val Acc=12.12%, Grad Norm=4.4088\n",
      "Fold 2, Epoch 5: Train Loss=2.2044, Train Acc=12.13%, Val Loss=2.3907, Val Acc=4.03%, Grad Norm=4.1287\n",
      "Fold 2, Epoch 6: Train Loss=2.2040, Train Acc=12.54%, Val Loss=2.4103, Val Acc=3.03%, Grad Norm=3.8595\n",
      "Fold 2, Epoch 7: Train Loss=2.2008, Train Acc=12.65%, Val Loss=2.2347, Val Acc=11.16%, Grad Norm=3.1276\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.10%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2060, Train Acc=12.08%, Val Loss=2.3663, Val Acc=3.14%, Grad Norm=4.5394\n",
      "Fold 3, Epoch 2: Train Loss=2.2038, Train Acc=12.23%, Val Loss=2.5828, Val Acc=3.12%, Grad Norm=4.6884\n",
      "Fold 3, Epoch 3: Train Loss=2.2032, Train Acc=12.61%, Val Loss=2.2606, Val Acc=12.97%, Grad Norm=4.6554\n",
      "Fold 3, Epoch 4: Train Loss=2.2030, Train Acc=12.28%, Val Loss=2.4500, Val Acc=9.81%, Grad Norm=4.4652\n",
      "Fold 3, Epoch 5: Train Loss=2.2034, Train Acc=12.28%, Val Loss=2.4968, Val Acc=15.61%, Grad Norm=4.2662\n",
      "Fold 3, Epoch 6: Train Loss=2.2023, Train Acc=12.17%, Val Loss=2.4542, Val Acc=6.39%, Grad Norm=4.0562\n",
      "Fold 3, Epoch 7: Train Loss=2.2010, Train Acc=12.20%, Val Loss=2.4375, Val Acc=8.36%, Grad Norm=3.8018\n",
      "Fold 3, Epoch 8: Train Loss=2.1992, Train Acc=12.59%, Val Loss=2.3463, Val Acc=9.39%, Grad Norm=3.4819\n",
      "Fold 3, Epoch 9: Train Loss=2.1972, Train Acc=12.77%, Val Loss=2.2748, Val Acc=15.40%, Grad Norm=2.9750\n",
      "Fold 3, Epoch 10: Train Loss=2.1947, Train Acc=12.76%, Val Loss=2.3393, Val Acc=3.15%, Grad Norm=2.1625\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1974, Train Acc=12.40%, Val Loss=2.5145, Val Acc=6.25%, Grad Norm=4.5082\n",
      "Fold 4, Epoch 2: Train Loss=2.1982, Train Acc=12.65%, Val Loss=2.3468, Val Acc=6.30%, Grad Norm=4.6520\n",
      "Fold 4, Epoch 3: Train Loss=2.1983, Train Acc=12.61%, Val Loss=2.4379, Val Acc=8.54%, Grad Norm=4.5855\n",
      "Fold 4, Epoch 4: Train Loss=2.1968, Train Acc=12.63%, Val Loss=2.6304, Val Acc=9.33%, Grad Norm=4.4142\n",
      "Fold 4, Epoch 5: Train Loss=2.1966, Train Acc=12.76%, Val Loss=2.5497, Val Acc=9.38%, Grad Norm=4.2531\n",
      "Fold 4, Epoch 6: Train Loss=2.1965, Train Acc=12.73%, Val Loss=2.3410, Val Acc=15.35%, Grad Norm=4.0434\n",
      "Fold 4, Epoch 7: Train Loss=2.1942, Train Acc=12.67%, Val Loss=2.4677, Val Acc=6.25%, Grad Norm=3.7660\n",
      "Fold 4, Epoch 8: Train Loss=2.1938, Train Acc=12.58%, Val Loss=2.3344, Val Acc=9.36%, Grad Norm=3.3338\n",
      "Fold 4, Epoch 9: Train Loss=2.1904, Train Acc=13.06%, Val Loss=2.3922, Val Acc=0.21%, Grad Norm=2.5123\n",
      "Fold 4, Epoch 10: Train Loss=2.1882, Train Acc=13.38%, Val Loss=2.3885, Val Acc=6.83%, Grad Norm=2.0503\n",
      "Fold 4, Epoch 11: Train Loss=2.1862, Train Acc=13.27%, Val Loss=2.4083, Val Acc=6.25%, Grad Norm=1.8817\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.17%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2037, Train Acc=12.45%, Val Loss=2.3420, Val Acc=3.12%, Grad Norm=4.4670\n",
      "Fold 5, Epoch 2: Train Loss=2.2027, Train Acc=12.44%, Val Loss=2.3163, Val Acc=12.50%, Grad Norm=4.6555\n",
      "Fold 5, Epoch 3: Train Loss=2.2051, Train Acc=12.19%, Val Loss=2.4209, Val Acc=15.62%, Grad Norm=4.5253\n",
      "Fold 5, Epoch 4: Train Loss=2.2040, Train Acc=12.20%, Val Loss=2.3423, Val Acc=8.67%, Grad Norm=4.2954\n",
      "Fold 5, Epoch 5: Train Loss=2.2015, Train Acc=12.16%, Val Loss=2.5280, Val Acc=5.60%, Grad Norm=4.1042\n",
      "Fold 5, Epoch 6: Train Loss=2.2015, Train Acc=12.62%, Val Loss=2.3232, Val Acc=3.18%, Grad Norm=3.8808\n",
      "Fold 5, Epoch 7: Train Loss=2.1988, Train Acc=12.75%, Val Loss=2.4049, Val Acc=7.73%, Grad Norm=3.4670\n",
      "Fold 5, Epoch 8: Train Loss=2.1964, Train Acc=12.80%, Val Loss=2.3437, Val Acc=3.11%, Grad Norm=2.5731\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.11%\n",
      "\n",
      "SNR -20 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-24_19-15-51_LTE-V_XFR_SingleFileBlock_SNR-20dB_fd655_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-25 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "统一目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:09<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 216, 每 block 样本数: 864, 每样本长度: 288\n",
      "[INFO] 总 block 数: 216, 类别数: 9\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "[INFO] 训练 block 数: 162, 测试 block 数: 54\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2012, Train Acc=11.94%, Val Loss=2.4194, Val Acc=6.04%, Grad Norm=4.4856\n",
      "Fold 1, Epoch 2: Train Loss=2.2021, Train Acc=12.25%, Val Loss=2.3996, Val Acc=6.43%, Grad Norm=4.6277\n",
      "Fold 1, Epoch 3: Train Loss=2.2008, Train Acc=12.13%, Val Loss=2.3495, Val Acc=18.17%, Grad Norm=4.5390\n",
      "Fold 1, Epoch 4: Train Loss=2.2010, Train Acc=12.27%, Val Loss=2.4947, Val Acc=21.21%, Grad Norm=4.2925\n",
      "Fold 1, Epoch 5: Train Loss=2.2004, Train Acc=12.07%, Val Loss=2.1999, Val Acc=21.26%, Grad Norm=4.1409\n",
      "Fold 1, Epoch 6: Train Loss=2.1985, Train Acc=12.23%, Val Loss=2.3482, Val Acc=6.16%, Grad Norm=3.8888\n",
      "Fold 1, Epoch 7: Train Loss=2.1973, Train Acc=12.09%, Val Loss=2.3749, Val Acc=6.06%, Grad Norm=3.3672\n",
      "Fold 1, Epoch 8: Train Loss=2.1929, Train Acc=12.33%, Val Loss=2.2648, Val Acc=17.67%, Grad Norm=2.5280\n",
      "Fold 1, Epoch 9: Train Loss=2.1920, Train Acc=12.36%, Val Loss=2.2620, Val Acc=11.34%, Grad Norm=2.0885\n",
      "Fold 1, Epoch 10: Train Loss=2.1910, Train Acc=12.50%, Val Loss=2.2959, Val Acc=9.01%, Grad Norm=1.8905\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.09%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2053, Train Acc=12.31%, Val Loss=2.1622, Val Acc=12.12%, Grad Norm=4.5431\n",
      "Fold 2, Epoch 2: Train Loss=2.2065, Train Acc=12.18%, Val Loss=2.3796, Val Acc=15.15%, Grad Norm=4.7081\n",
      "Fold 2, Epoch 3: Train Loss=2.2064, Train Acc=12.04%, Val Loss=2.3684, Val Acc=3.20%, Grad Norm=4.6400\n",
      "Fold 2, Epoch 4: Train Loss=2.2064, Train Acc=12.19%, Val Loss=2.2252, Val Acc=12.21%, Grad Norm=4.4242\n",
      "Fold 2, Epoch 5: Train Loss=2.2052, Train Acc=12.31%, Val Loss=2.4353, Val Acc=10.55%, Grad Norm=4.2606\n",
      "Fold 2, Epoch 6: Train Loss=2.2030, Train Acc=12.57%, Val Loss=2.2213, Val Acc=14.78%, Grad Norm=4.1393\n",
      "Fold 2, Epoch 7: Train Loss=2.2026, Train Acc=12.41%, Val Loss=2.2997, Val Acc=12.09%, Grad Norm=3.7379\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2037, Train Acc=12.14%, Val Loss=2.4672, Val Acc=3.16%, Grad Norm=4.5176\n",
      "Fold 3, Epoch 2: Train Loss=2.2044, Train Acc=12.15%, Val Loss=2.3909, Val Acc=3.93%, Grad Norm=4.7021\n",
      "Fold 3, Epoch 3: Train Loss=2.2051, Train Acc=12.32%, Val Loss=2.3751, Val Acc=3.12%, Grad Norm=4.5810\n",
      "Fold 3, Epoch 4: Train Loss=2.2036, Train Acc=12.10%, Val Loss=2.4695, Val Acc=3.20%, Grad Norm=4.4035\n",
      "Fold 3, Epoch 5: Train Loss=2.2027, Train Acc=12.16%, Val Loss=2.4319, Val Acc=9.38%, Grad Norm=4.2760\n",
      "Fold 3, Epoch 6: Train Loss=2.2015, Train Acc=12.34%, Val Loss=2.1969, Val Acc=18.75%, Grad Norm=4.0238\n",
      "Fold 3, Epoch 7: Train Loss=2.2016, Train Acc=12.64%, Val Loss=2.2875, Val Acc=7.87%, Grad Norm=3.7228\n",
      "Fold 3, Epoch 8: Train Loss=2.1982, Train Acc=12.46%, Val Loss=2.2166, Val Acc=8.58%, Grad Norm=3.0325\n",
      "Fold 3, Epoch 9: Train Loss=2.1955, Train Acc=12.28%, Val Loss=2.2690, Val Acc=3.75%, Grad Norm=2.2992\n",
      "Fold 3, Epoch 10: Train Loss=2.1937, Train Acc=12.58%, Val Loss=2.2929, Val Acc=3.88%, Grad Norm=1.9876\n",
      "Fold 3, Epoch 11: Train Loss=2.1918, Train Acc=12.78%, Val Loss=2.2861, Val Acc=3.12%, Grad Norm=1.8676\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1972, Train Acc=12.91%, Val Loss=2.4027, Val Acc=6.18%, Grad Norm=4.5551\n",
      "Fold 4, Epoch 2: Train Loss=2.1978, Train Acc=13.17%, Val Loss=2.3255, Val Acc=6.25%, Grad Norm=4.7365\n",
      "Fold 4, Epoch 3: Train Loss=2.1987, Train Acc=12.66%, Val Loss=2.4187, Val Acc=0.02%, Grad Norm=4.6288\n",
      "Fold 4, Epoch 4: Train Loss=2.1982, Train Acc=12.42%, Val Loss=2.6270, Val Acc=0.00%, Grad Norm=4.4057\n",
      "Fold 4, Epoch 5: Train Loss=2.1975, Train Acc=12.44%, Val Loss=2.2466, Val Acc=9.07%, Grad Norm=4.3000\n",
      "Fold 4, Epoch 6: Train Loss=2.1962, Train Acc=12.71%, Val Loss=2.3890, Val Acc=7.00%, Grad Norm=4.0980\n",
      "Fold 4, Epoch 7: Train Loss=2.1950, Train Acc=12.69%, Val Loss=2.2761, Val Acc=9.38%, Grad Norm=3.8701\n",
      "Fold 4, Epoch 8: Train Loss=2.1944, Train Acc=12.92%, Val Loss=2.4479, Val Acc=0.00%, Grad Norm=3.5690\n",
      "Fold 4, Epoch 9: Train Loss=2.1922, Train Acc=13.03%, Val Loss=2.4856, Val Acc=14.67%, Grad Norm=3.0078\n",
      "Fold 4, Epoch 10: Train Loss=2.1891, Train Acc=13.05%, Val Loss=2.3455, Val Acc=15.62%, Grad Norm=2.3791\n",
      "Fold 4, Epoch 11: Train Loss=2.1864, Train Acc=13.40%, Val Loss=2.3034, Val Acc=15.56%, Grad Norm=2.0558\n",
      "Fold 4, Epoch 12: Train Loss=2.1862, Train Acc=13.43%, Val Loss=2.3468, Val Acc=6.17%, Grad Norm=1.9438\n",
      "Fold 4, Epoch 13: Train Loss=2.1855, Train Acc=13.80%, Val Loss=2.3294, Val Acc=6.25%, Grad Norm=1.8957\n",
      "Fold 4, Epoch 14: Train Loss=2.1851, Train Acc=13.62%, Val Loss=2.4933, Val Acc=0.00%, Grad Norm=1.8192\n",
      "Fold 4, Epoch 15: Train Loss=2.1848, Train Acc=13.87%, Val Loss=2.3701, Val Acc=15.56%, Grad Norm=1.7787\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2039, Train Acc=12.81%, Val Loss=2.2460, Val Acc=6.90%, Grad Norm=4.5412\n",
      "Fold 5, Epoch 2: Train Loss=2.2050, Train Acc=12.56%, Val Loss=2.4862, Val Acc=3.20%, Grad Norm=4.7117\n",
      "Fold 5, Epoch 3: Train Loss=2.2035, Train Acc=12.39%, Val Loss=2.2919, Val Acc=3.14%, Grad Norm=4.6634\n",
      "Fold 5, Epoch 4: Train Loss=2.2029, Train Acc=12.48%, Val Loss=2.5086, Val Acc=4.04%, Grad Norm=4.4476\n",
      "Fold 5, Epoch 5: Train Loss=2.2024, Train Acc=12.57%, Val Loss=2.6029, Val Acc=3.17%, Grad Norm=4.2806\n",
      "Fold 5, Epoch 6: Train Loss=2.2022, Train Acc=12.52%, Val Loss=2.2690, Val Acc=12.50%, Grad Norm=4.0808\n",
      "Fold 5, Epoch 7: Train Loss=2.2010, Train Acc=12.64%, Val Loss=2.4291, Val Acc=9.38%, Grad Norm=3.8777\n",
      "Fold 5, Epoch 8: Train Loss=2.1991, Train Acc=12.72%, Val Loss=2.2865, Val Acc=5.51%, Grad Norm=3.5827\n",
      "Fold 5, Epoch 9: Train Loss=2.1981, Train Acc=12.55%, Val Loss=2.4251, Val Acc=5.94%, Grad Norm=3.2370\n",
      "Fold 5, Epoch 10: Train Loss=2.1952, Train Acc=12.87%, Val Loss=2.3410, Val Acc=15.62%, Grad Norm=2.5730\n",
      "Fold 5, Epoch 11: Train Loss=2.1919, Train Acc=13.08%, Val Loss=2.2684, Val Acc=3.10%, Grad Norm=2.2171\n",
      "Fold 5, Epoch 12: Train Loss=2.1914, Train Acc=12.96%, Val Loss=2.2451, Val Acc=15.97%, Grad Norm=2.0730\n",
      "Fold 5, Epoch 13: Train Loss=2.1907, Train Acc=13.23%, Val Loss=2.2206, Val Acc=12.65%, Grad Norm=1.9496\n",
      "Fold 5, Epoch 14: Train Loss=2.1907, Train Acc=13.27%, Val Loss=2.3030, Val Acc=15.62%, Grad Norm=1.8966\n",
      "Fold 5, Epoch 15: Train Loss=2.1904, Train Acc=13.27%, Val Loss=2.2579, Val Acc=10.72%, Grad Norm=1.8150\n",
      "Fold 5, Epoch 16: Train Loss=2.1902, Train Acc=13.51%, Val Loss=2.2243, Val Acc=3.16%, Grad Norm=1.7601\n",
      "Fold 5, Epoch 17: Train Loss=2.1893, Train Acc=13.25%, Val Loss=2.3029, Val Acc=12.50%, Grad Norm=1.7310\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=10.82%\n",
      "\n",
      "SNR -25 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-24_19-21-01_LTE-V_XFR_SingleFileBlock_SNR-25dB_fd655_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-30 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "统一目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:09<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 216, 每 block 样本数: 864, 每样本长度: 288\n",
      "[INFO] 总 block 数: 216, 类别数: 9\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "[INFO] 训练 block 数: 162, 测试 block 数: 54\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2034, Train Acc=12.06%, Val Loss=2.2548, Val Acc=6.06%, Grad Norm=4.5484\n",
      "Fold 1, Epoch 2: Train Loss=2.2019, Train Acc=11.93%, Val Loss=2.2035, Val Acc=6.20%, Grad Norm=4.6852\n",
      "Fold 1, Epoch 3: Train Loss=2.2004, Train Acc=12.39%, Val Loss=2.4411, Val Acc=6.07%, Grad Norm=4.6290\n",
      "Fold 1, Epoch 4: Train Loss=2.2012, Train Acc=12.23%, Val Loss=2.4673, Val Acc=6.05%, Grad Norm=4.4485\n",
      "Fold 1, Epoch 5: Train Loss=2.2003, Train Acc=12.17%, Val Loss=2.5887, Val Acc=6.06%, Grad Norm=4.3473\n",
      "Fold 1, Epoch 6: Train Loss=2.1991, Train Acc=12.33%, Val Loss=2.3019, Val Acc=6.06%, Grad Norm=4.1214\n",
      "Fold 1, Epoch 7: Train Loss=2.1973, Train Acc=12.65%, Val Loss=2.6493, Val Acc=6.06%, Grad Norm=3.9183\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.12%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2053, Train Acc=12.46%, Val Loss=2.2440, Val Acc=14.03%, Grad Norm=4.5622\n",
      "Fold 2, Epoch 2: Train Loss=2.2065, Train Acc=12.38%, Val Loss=2.2085, Val Acc=13.55%, Grad Norm=4.7130\n",
      "Fold 2, Epoch 3: Train Loss=2.2052, Train Acc=12.41%, Val Loss=2.2705, Val Acc=12.12%, Grad Norm=4.6110\n",
      "Fold 2, Epoch 4: Train Loss=2.2055, Train Acc=12.28%, Val Loss=2.2506, Val Acc=15.15%, Grad Norm=4.4027\n",
      "Fold 2, Epoch 5: Train Loss=2.2057, Train Acc=12.36%, Val Loss=2.2558, Val Acc=15.15%, Grad Norm=4.1793\n",
      "Fold 2, Epoch 6: Train Loss=2.2035, Train Acc=12.55%, Val Loss=2.2657, Val Acc=15.14%, Grad Norm=3.8974\n",
      "Fold 2, Epoch 7: Train Loss=2.2011, Train Acc=12.37%, Val Loss=2.2226, Val Acc=10.83%, Grad Norm=3.0804\n",
      "Fold 2, Epoch 8: Train Loss=2.1982, Train Acc=12.91%, Val Loss=2.2851, Val Acc=3.03%, Grad Norm=2.3480\n",
      "Fold 2, Epoch 9: Train Loss=2.1954, Train Acc=12.89%, Val Loss=2.1892, Val Acc=12.12%, Grad Norm=1.9998\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2031, Train Acc=12.25%, Val Loss=2.5844, Val Acc=3.12%, Grad Norm=4.5171\n",
      "Fold 3, Epoch 2: Train Loss=2.2037, Train Acc=12.27%, Val Loss=2.2922, Val Acc=6.34%, Grad Norm=4.7539\n",
      "Fold 3, Epoch 3: Train Loss=2.2036, Train Acc=12.10%, Val Loss=2.3819, Val Acc=14.79%, Grad Norm=4.6640\n",
      "Fold 3, Epoch 4: Train Loss=2.2036, Train Acc=12.33%, Val Loss=2.2189, Val Acc=18.75%, Grad Norm=4.4532\n",
      "Fold 3, Epoch 5: Train Loss=2.2029, Train Acc=12.21%, Val Loss=2.3422, Val Acc=4.85%, Grad Norm=4.2838\n",
      "Fold 3, Epoch 6: Train Loss=2.2028, Train Acc=12.17%, Val Loss=2.4296, Val Acc=6.24%, Grad Norm=3.9932\n",
      "Fold 3, Epoch 7: Train Loss=2.2003, Train Acc=12.32%, Val Loss=2.4617, Val Acc=6.25%, Grad Norm=3.6208\n",
      "Fold 3, Epoch 8: Train Loss=2.1975, Train Acc=12.47%, Val Loss=2.2678, Val Acc=6.25%, Grad Norm=2.7941\n",
      "Fold 3, Epoch 9: Train Loss=2.1949, Train Acc=12.56%, Val Loss=2.4023, Val Acc=6.25%, Grad Norm=2.2360\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.2009, Train Acc=12.51%, Val Loss=2.2288, Val Acc=9.38%, Grad Norm=4.5625\n",
      "Fold 4, Epoch 2: Train Loss=2.1983, Train Acc=12.76%, Val Loss=2.4699, Val Acc=9.38%, Grad Norm=4.6594\n",
      "Fold 4, Epoch 3: Train Loss=2.1977, Train Acc=12.83%, Val Loss=2.2824, Val Acc=9.38%, Grad Norm=4.6009\n",
      "Fold 4, Epoch 4: Train Loss=2.1988, Train Acc=12.74%, Val Loss=2.4776, Val Acc=9.38%, Grad Norm=4.4230\n",
      "Fold 4, Epoch 5: Train Loss=2.1962, Train Acc=12.81%, Val Loss=2.5013, Val Acc=9.03%, Grad Norm=4.2854\n",
      "Fold 4, Epoch 6: Train Loss=2.1976, Train Acc=12.50%, Val Loss=2.4637, Val Acc=6.24%, Grad Norm=4.0559\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2041, Train Acc=12.38%, Val Loss=2.2923, Val Acc=14.54%, Grad Norm=4.5354\n",
      "Fold 5, Epoch 2: Train Loss=2.2036, Train Acc=12.65%, Val Loss=2.3823, Val Acc=9.66%, Grad Norm=4.7375\n",
      "Fold 5, Epoch 3: Train Loss=2.2050, Train Acc=12.52%, Val Loss=2.2843, Val Acc=15.62%, Grad Norm=4.6443\n",
      "Fold 5, Epoch 4: Train Loss=2.2028, Train Acc=12.61%, Val Loss=2.2886, Val Acc=18.75%, Grad Norm=4.4114\n",
      "Fold 5, Epoch 5: Train Loss=2.2019, Train Acc=12.66%, Val Loss=2.3207, Val Acc=9.38%, Grad Norm=4.2502\n",
      "Fold 5, Epoch 6: Train Loss=2.2024, Train Acc=12.69%, Val Loss=2.5987, Val Acc=3.12%, Grad Norm=4.0723\n",
      "Fold 5, Epoch 7: Train Loss=2.2000, Train Acc=12.55%, Val Loss=2.6380, Val Acc=3.12%, Grad Norm=3.8907\n",
      "Fold 5, Epoch 8: Train Loss=2.2002, Train Acc=12.47%, Val Loss=2.3962, Val Acc=3.12%, Grad Norm=3.5198\n",
      "Fold 5, Epoch 9: Train Loss=2.1962, Train Acc=13.16%, Val Loss=2.3439, Val Acc=12.46%, Grad Norm=2.9426\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.12%\n",
      "\n",
      "SNR -30 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-24_19-27-33_LTE-V_XFR_SingleFileBlock_SNR-30dB_fd655_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-35 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "统一目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:09<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 216, 每 block 样本数: 864, 每样本长度: 288\n",
      "[INFO] 总 block 数: 216, 类别数: 9\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "[INFO] 训练 block 数: 162, 测试 block 数: 54\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2027, Train Acc=11.82%, Val Loss=2.3418, Val Acc=6.06%, Grad Norm=4.4820\n",
      "Fold 1, Epoch 2: Train Loss=2.2019, Train Acc=11.95%, Val Loss=2.6085, Val Acc=6.06%, Grad Norm=4.6389\n",
      "Fold 1, Epoch 3: Train Loss=2.2015, Train Acc=12.38%, Val Loss=2.4665, Val Acc=6.06%, Grad Norm=4.5500\n",
      "Fold 1, Epoch 4: Train Loss=2.2005, Train Acc=12.42%, Val Loss=2.3271, Val Acc=9.09%, Grad Norm=4.4145\n",
      "Fold 1, Epoch 5: Train Loss=2.2001, Train Acc=12.35%, Val Loss=2.3028, Val Acc=14.93%, Grad Norm=4.2258\n",
      "Fold 1, Epoch 6: Train Loss=2.1995, Train Acc=12.42%, Val Loss=2.3547, Val Acc=10.87%, Grad Norm=4.0395\n",
      "Fold 1, Epoch 7: Train Loss=2.1985, Train Acc=12.28%, Val Loss=2.4427, Val Acc=6.08%, Grad Norm=3.6749\n",
      "Fold 1, Epoch 8: Train Loss=2.1950, Train Acc=12.42%, Val Loss=2.3704, Val Acc=6.06%, Grad Norm=2.9738\n",
      "Fold 1, Epoch 9: Train Loss=2.1926, Train Acc=12.53%, Val Loss=2.3219, Val Acc=9.02%, Grad Norm=2.2927\n",
      "Fold 1, Epoch 10: Train Loss=2.1913, Train Acc=12.44%, Val Loss=2.4158, Val Acc=6.06%, Grad Norm=1.9700\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=10.91%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2061, Train Acc=12.56%, Val Loss=2.2184, Val Acc=12.83%, Grad Norm=4.5491\n",
      "Fold 2, Epoch 2: Train Loss=2.2058, Train Acc=12.34%, Val Loss=2.3315, Val Acc=15.15%, Grad Norm=4.7491\n",
      "Fold 2, Epoch 3: Train Loss=2.2057, Train Acc=12.20%, Val Loss=2.2446, Val Acc=12.23%, Grad Norm=4.6723\n",
      "Fold 2, Epoch 4: Train Loss=2.2061, Train Acc=12.26%, Val Loss=2.2804, Val Acc=12.04%, Grad Norm=4.5083\n",
      "Fold 2, Epoch 5: Train Loss=2.2043, Train Acc=12.52%, Val Loss=2.3774, Val Acc=5.23%, Grad Norm=4.2766\n",
      "Fold 2, Epoch 6: Train Loss=2.2034, Train Acc=12.58%, Val Loss=2.3458, Val Acc=14.34%, Grad Norm=4.1362\n",
      "Fold 2, Epoch 7: Train Loss=2.2030, Train Acc=12.63%, Val Loss=2.3025, Val Acc=5.61%, Grad Norm=3.8613\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2035, Train Acc=12.10%, Val Loss=2.3024, Val Acc=6.25%, Grad Norm=4.5734\n",
      "Fold 3, Epoch 2: Train Loss=2.2035, Train Acc=12.65%, Val Loss=2.3208, Val Acc=6.25%, Grad Norm=4.7536\n",
      "Fold 3, Epoch 3: Train Loss=2.2050, Train Acc=12.23%, Val Loss=2.3009, Val Acc=6.25%, Grad Norm=4.6223\n",
      "Fold 3, Epoch 4: Train Loss=2.2037, Train Acc=12.09%, Val Loss=2.2934, Val Acc=6.25%, Grad Norm=4.4309\n",
      "Fold 3, Epoch 5: Train Loss=2.2029, Train Acc=12.18%, Val Loss=2.5559, Val Acc=9.40%, Grad Norm=4.2452\n",
      "Fold 3, Epoch 6: Train Loss=2.2022, Train Acc=12.39%, Val Loss=2.2169, Val Acc=15.62%, Grad Norm=4.0145\n",
      "Fold 3, Epoch 7: Train Loss=2.2010, Train Acc=12.25%, Val Loss=2.3814, Val Acc=6.27%, Grad Norm=3.6261\n",
      "Fold 3, Epoch 8: Train Loss=2.1974, Train Acc=12.48%, Val Loss=2.3582, Val Acc=9.38%, Grad Norm=2.8567\n",
      "Fold 3, Epoch 9: Train Loss=2.1942, Train Acc=12.71%, Val Loss=2.2540, Val Acc=9.38%, Grad Norm=2.2826\n",
      "Fold 3, Epoch 10: Train Loss=2.1938, Train Acc=12.82%, Val Loss=2.2544, Val Acc=13.95%, Grad Norm=2.0367\n",
      "Fold 3, Epoch 11: Train Loss=2.1918, Train Acc=12.88%, Val Loss=2.2256, Val Acc=9.38%, Grad Norm=1.8308\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1986, Train Acc=12.84%, Val Loss=2.7669, Val Acc=6.25%, Grad Norm=4.5567\n",
      "Fold 4, Epoch 2: Train Loss=2.1980, Train Acc=12.63%, Val Loss=2.3605, Val Acc=9.43%, Grad Norm=4.7558\n",
      "Fold 4, Epoch 3: Train Loss=2.1976, Train Acc=12.85%, Val Loss=2.5072, Val Acc=6.25%, Grad Norm=4.7410\n",
      "Fold 4, Epoch 4: Train Loss=2.1963, Train Acc=13.08%, Val Loss=2.5611, Val Acc=9.38%, Grad Norm=4.5400\n",
      "Fold 4, Epoch 5: Train Loss=2.1972, Train Acc=12.94%, Val Loss=2.4642, Val Acc=9.38%, Grad Norm=4.3456\n",
      "Fold 4, Epoch 6: Train Loss=2.1954, Train Acc=12.87%, Val Loss=2.4654, Val Acc=15.43%, Grad Norm=4.1344\n",
      "Fold 4, Epoch 7: Train Loss=2.1946, Train Acc=12.90%, Val Loss=2.6335, Val Acc=6.25%, Grad Norm=3.8553\n",
      "Fold 4, Epoch 8: Train Loss=2.1940, Train Acc=12.99%, Val Loss=2.6146, Val Acc=9.13%, Grad Norm=3.3820\n",
      "Fold 4, Epoch 9: Train Loss=2.1907, Train Acc=12.92%, Val Loss=2.4892, Val Acc=9.10%, Grad Norm=2.6068\n",
      "Fold 4, Epoch 10: Train Loss=2.1886, Train Acc=13.18%, Val Loss=2.3954, Val Acc=9.38%, Grad Norm=2.1008\n",
      "Fold 4, Epoch 11: Train Loss=2.1854, Train Acc=13.77%, Val Loss=2.4303, Val Acc=9.38%, Grad Norm=1.9387\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.18%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2020, Train Acc=12.60%, Val Loss=2.4667, Val Acc=3.12%, Grad Norm=4.4988\n",
      "Fold 5, Epoch 2: Train Loss=2.2052, Train Acc=12.34%, Val Loss=2.4011, Val Acc=9.38%, Grad Norm=4.7469\n",
      "Fold 5, Epoch 3: Train Loss=2.2030, Train Acc=12.44%, Val Loss=2.3932, Val Acc=13.66%, Grad Norm=4.6051\n",
      "Fold 5, Epoch 4: Train Loss=2.2026, Train Acc=12.45%, Val Loss=2.2429, Val Acc=15.64%, Grad Norm=4.4644\n",
      "Fold 5, Epoch 5: Train Loss=2.2004, Train Acc=12.70%, Val Loss=2.4083, Val Acc=3.12%, Grad Norm=4.2688\n",
      "Fold 5, Epoch 6: Train Loss=2.2024, Train Acc=12.38%, Val Loss=2.1934, Val Acc=14.45%, Grad Norm=4.0729\n",
      "Fold 5, Epoch 7: Train Loss=2.2009, Train Acc=12.74%, Val Loss=2.3003, Val Acc=9.38%, Grad Norm=3.9295\n",
      "Fold 5, Epoch 8: Train Loss=2.1998, Train Acc=12.78%, Val Loss=2.2862, Val Acc=9.38%, Grad Norm=3.4770\n",
      "Fold 5, Epoch 9: Train Loss=2.1961, Train Acc=12.94%, Val Loss=2.3088, Val Acc=4.92%, Grad Norm=2.7654\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.13%\n",
      "\n",
      "SNR -35 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-24_19-31-59_LTE-V_XFR_SingleFileBlock_SNR-35dB_fd655_ResNet\n",
      "\n",
      "\n",
      "================== 当前实验 SNR=-40 dB ==================\n",
      "\n",
      "[INFO] 使用设备: cuda\n",
      "共找到 72 个 .mat 文件\n",
      "统一目标速度 120 km/h，多普勒频移 655.56 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 100%|██████████| 72/72 [00:10<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 生成 block 数: 216, 每 block 样本数: 864, 每样本长度: 288\n",
      "[INFO] 总 block 数: 216, 类别数: 9\n",
      "[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\n",
      "[INFO] 训练 block 数: 162, 测试 block 数: 54\n",
      "\n",
      "====== Fold 1/5 ======\n",
      "Fold 1, Epoch 1: Train Loss=2.2009, Train Acc=12.29%, Val Loss=2.3528, Val Acc=9.07%, Grad Norm=4.4669\n",
      "Fold 1, Epoch 2: Train Loss=2.2028, Train Acc=12.25%, Val Loss=2.5362, Val Acc=7.06%, Grad Norm=4.6711\n",
      "Fold 1, Epoch 3: Train Loss=2.2016, Train Acc=12.31%, Val Loss=2.5084, Val Acc=6.10%, Grad Norm=4.5459\n",
      "Fold 1, Epoch 4: Train Loss=2.2000, Train Acc=12.58%, Val Loss=2.6266, Val Acc=6.03%, Grad Norm=4.4969\n",
      "Fold 1, Epoch 5: Train Loss=2.2007, Train Acc=12.16%, Val Loss=2.3662, Val Acc=6.09%, Grad Norm=4.1751\n",
      "Fold 1, Epoch 6: Train Loss=2.1999, Train Acc=12.41%, Val Loss=2.5808, Val Acc=5.97%, Grad Norm=3.8580\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 1 Test Acc=11.09%\n",
      "\n",
      "\n",
      "====== Fold 2/5 ======\n",
      "Fold 2, Epoch 1: Train Loss=2.2059, Train Acc=12.50%, Val Loss=2.4034, Val Acc=3.03%, Grad Norm=4.5692\n",
      "Fold 2, Epoch 2: Train Loss=2.2061, Train Acc=12.27%, Val Loss=2.2780, Val Acc=15.15%, Grad Norm=4.7148\n",
      "Fold 2, Epoch 3: Train Loss=2.2062, Train Acc=12.63%, Val Loss=2.3234, Val Acc=7.47%, Grad Norm=4.6014\n",
      "Fold 2, Epoch 4: Train Loss=2.2056, Train Acc=12.61%, Val Loss=2.3072, Val Acc=4.62%, Grad Norm=4.3938\n",
      "Fold 2, Epoch 5: Train Loss=2.2039, Train Acc=12.46%, Val Loss=2.3031, Val Acc=8.43%, Grad Norm=4.2526\n",
      "Fold 2, Epoch 6: Train Loss=2.2037, Train Acc=12.63%, Val Loss=2.2179, Val Acc=14.77%, Grad Norm=4.0638\n",
      "Fold 2, Epoch 7: Train Loss=2.2027, Train Acc=12.63%, Val Loss=2.2600, Val Acc=11.93%, Grad Norm=3.8135\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 2 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 3/5 ======\n",
      "Fold 3, Epoch 1: Train Loss=2.2033, Train Acc=12.32%, Val Loss=2.3415, Val Acc=6.25%, Grad Norm=4.5322\n",
      "Fold 3, Epoch 2: Train Loss=2.2041, Train Acc=12.32%, Val Loss=2.4048, Val Acc=3.12%, Grad Norm=4.7309\n",
      "Fold 3, Epoch 3: Train Loss=2.2043, Train Acc=12.10%, Val Loss=2.4109, Val Acc=9.36%, Grad Norm=4.5844\n",
      "Fold 3, Epoch 4: Train Loss=2.2038, Train Acc=12.14%, Val Loss=2.4740, Val Acc=6.25%, Grad Norm=4.3848\n",
      "Fold 3, Epoch 5: Train Loss=2.2027, Train Acc=12.26%, Val Loss=2.3402, Val Acc=12.50%, Grad Norm=4.2356\n",
      "Fold 3, Epoch 6: Train Loss=2.2010, Train Acc=12.53%, Val Loss=2.5866, Val Acc=3.12%, Grad Norm=4.0146\n",
      "Fold 3, Epoch 7: Train Loss=2.2019, Train Acc=12.39%, Val Loss=2.4430, Val Acc=6.25%, Grad Norm=3.7231\n",
      "Fold 3, Epoch 8: Train Loss=2.1976, Train Acc=12.28%, Val Loss=2.3234, Val Acc=3.85%, Grad Norm=2.7955\n",
      "Fold 3, Epoch 9: Train Loss=2.1945, Train Acc=12.53%, Val Loss=2.3029, Val Acc=9.38%, Grad Norm=2.1123\n",
      "Fold 3, Epoch 10: Train Loss=2.1936, Train Acc=12.90%, Val Loss=2.2914, Val Acc=9.38%, Grad Norm=1.8679\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 3 Test Acc=11.11%\n",
      "\n",
      "\n",
      "====== Fold 4/5 ======\n",
      "Fold 4, Epoch 1: Train Loss=2.1972, Train Acc=12.82%, Val Loss=2.5964, Val Acc=7.56%, Grad Norm=4.5442\n",
      "Fold 4, Epoch 2: Train Loss=2.1982, Train Acc=12.91%, Val Loss=2.4300, Val Acc=0.01%, Grad Norm=4.7141\n",
      "Fold 4, Epoch 3: Train Loss=2.1975, Train Acc=12.70%, Val Loss=2.5146, Val Acc=6.25%, Grad Norm=4.6505\n",
      "Fold 4, Epoch 4: Train Loss=2.1970, Train Acc=12.72%, Val Loss=2.4149, Val Acc=8.16%, Grad Norm=4.3945\n",
      "Fold 4, Epoch 5: Train Loss=2.1962, Train Acc=13.01%, Val Loss=2.2828, Val Acc=9.38%, Grad Norm=4.2477\n",
      "Fold 4, Epoch 6: Train Loss=2.1958, Train Acc=12.95%, Val Loss=2.3874, Val Acc=9.35%, Grad Norm=4.0270\n",
      "Fold 4, Epoch 7: Train Loss=2.1953, Train Acc=12.61%, Val Loss=2.4483, Val Acc=9.38%, Grad Norm=3.6891\n",
      "Fold 4, Epoch 8: Train Loss=2.1931, Train Acc=13.14%, Val Loss=2.2902, Val Acc=9.40%, Grad Norm=3.0256\n",
      "Fold 4, Epoch 9: Train Loss=2.1894, Train Acc=13.18%, Val Loss=2.2552, Val Acc=9.38%, Grad Norm=2.2147\n",
      "Fold 4, Epoch 10: Train Loss=2.1872, Train Acc=13.39%, Val Loss=2.4107, Val Acc=0.00%, Grad Norm=1.9078\n",
      "Fold 4, Epoch 11: Train Loss=2.1856, Train Acc=13.70%, Val Loss=2.3347, Val Acc=0.00%, Grad Norm=1.7659\n",
      "Fold 4, Epoch 12: Train Loss=2.1852, Train Acc=13.90%, Val Loss=2.3004, Val Acc=0.01%, Grad Norm=1.7059\n",
      "Fold 4, Epoch 13: Train Loss=2.1848, Train Acc=13.66%, Val Loss=2.3357, Val Acc=5.40%, Grad Norm=1.6637\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 4 Test Acc=11.07%\n",
      "\n",
      "\n",
      "====== Fold 5/5 ======\n",
      "Fold 5, Epoch 1: Train Loss=2.2033, Train Acc=12.37%, Val Loss=2.3985, Val Acc=5.59%, Grad Norm=4.4617\n",
      "Fold 5, Epoch 2: Train Loss=2.2028, Train Acc=12.43%, Val Loss=2.4005, Val Acc=14.14%, Grad Norm=4.6142\n",
      "Fold 5, Epoch 3: Train Loss=2.2042, Train Acc=12.37%, Val Loss=2.5117, Val Acc=5.63%, Grad Norm=4.5180\n",
      "Fold 5, Epoch 4: Train Loss=2.2032, Train Acc=12.16%, Val Loss=2.4868, Val Acc=9.16%, Grad Norm=4.3565\n",
      "Fold 5, Epoch 5: Train Loss=2.2026, Train Acc=12.57%, Val Loss=2.4515, Val Acc=15.62%, Grad Norm=4.1844\n",
      "Fold 5, Epoch 6: Train Loss=2.2012, Train Acc=12.41%, Val Loss=2.6176, Val Acc=9.38%, Grad Norm=3.8165\n",
      "Fold 5, Epoch 7: Train Loss=2.1979, Train Acc=12.53%, Val Loss=2.4853, Val Acc=3.12%, Grad Norm=2.9103\n",
      "Fold 5, Epoch 8: Train Loss=2.1951, Train Acc=12.77%, Val Loss=2.3094, Val Acc=9.38%, Grad Norm=2.2595\n",
      "Fold 5, Epoch 9: Train Loss=2.1937, Train Acc=12.92%, Val Loss=2.3431, Val Acc=3.21%, Grad Norm=1.9675\n",
      "Fold 5, Epoch 10: Train Loss=2.1929, Train Acc=13.15%, Val Loss=2.3443, Val Acc=3.12%, Grad Norm=1.7972\n",
      "早停，连续 5 个 epoch 验证集未提升\n",
      "Fold 5 Test Acc=11.11%\n",
      "\n",
      "SNR -40 dB → results in: d:\\Program\\MW-RFF\\MW-RFF\\training_results\\2026-01-24_19-37-09_LTE-V_XFR_SingleFileBlock_SNR-40dB_fd655_ResNet\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SNR vs 测试准确率曲线已保存到 d:\\Program\\MW-RFF\\MW-RFF\\training_results\\SNR_vs_accuracy_2026-01-24_19-37-09.png\n"
     ]
    }
   ],
   "source": [
    "# ResNet 1D 自动 SNR 循环训练脚本（按 block 整体划分训练/测试）\n",
    "# 修改要点：\n",
    "# - block 生成方式改为“单个文件内顺序抽取补满 block”，不进行跨文件抽取/拼接\n",
    "# - 每个文件按 group_size 顺序切分为多个 block；不足一个 block 的剩余帧直接舍弃\n",
    "# - 其余训练/测试划分与 KFold 逻辑保持一致（按 block 划分）\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "from data_utilities import *\n",
    "\n",
    "# ================= 参数设置 =================\n",
    "data_path = \"E:/rf_datasets/\"  # 数据文件夹\n",
    "fs = 5e6\n",
    "fc = 5.9e9\n",
    "v = 120                 # Doppler 统一按这个速度计算（km/h）\n",
    "apply_doppler = True\n",
    "apply_awgn = True\n",
    "\n",
    "# 模型超参数\n",
    "batch_size = 64\n",
    "num_epochs = 300\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-3\n",
    "in_planes = 64\n",
    "dropout = 0.5\n",
    "patience = 5\n",
    "n_splits = 5\n",
    "\n",
    "# ================= 多普勒和AWGN处理函数 =================\n",
    "def compute_doppler_shift(v, fc):\n",
    "    c = 3e8\n",
    "    v = v / 3.6\n",
    "    return (v / c) * fc\n",
    "\n",
    "def apply_doppler_shift(signal, fd, fs):\n",
    "    t = np.arange(signal.shape[-1]) / fs\n",
    "    doppler_phase = np.exp(1j * 2 * np.pi * fd * t)\n",
    "    return signal * doppler_phase\n",
    "\n",
    "def add_awgn(signal, snr_db):\n",
    "    signal_power = np.mean(np.abs(signal) ** 2)\n",
    "    noise_power = signal_power / (10 ** (snr_db / 10))\n",
    "    noise_real = np.random.randn(*signal.shape)\n",
    "    noise_imag = np.random.randn(*signal.shape)\n",
    "    noise = np.sqrt(noise_power / 2) * (noise_real + 1j * noise_imag)\n",
    "    return signal + noise\n",
    "\n",
    "# ================= 数据加载（单文件内切 block，并翻转 block） =================\n",
    "def load_and_preprocess_with_grouping(mat_folder, group_size=288, apply_doppler=False,\n",
    "                                      target_velocity=120, apply_awgn=False, snr_db=20,\n",
    "                                      fs=5e6, fc=5.9e9):\n",
    "    \"\"\"\n",
    "    单文件成块版本：\n",
    "      - 每个文件独立处理：processed_signals shape (num_frames, sample_len, 2)\n",
    "      - 在该文件内顺序切分 block：每个 block = group_size 帧\n",
    "      - 不足一个 block 的尾部直接丢弃\n",
    "      - 每个 block 做翻转： (group_size, sample_len, 2) -> (sample_len, group_size, 2)\n",
    "      - 返回：\n",
    "          X_blocks: (num_blocks, sample_len, group_size, 2)\n",
    "          y_blocks: (num_blocks,)  每个 block 的 tx label\n",
    "          label_to_idx: dict\n",
    "    \"\"\"\n",
    "    mat_files = glob.glob(os.path.join(mat_folder, \"*.mat\"))\n",
    "    print(f\"共找到 {len(mat_files)} 个 .mat 文件\")\n",
    "\n",
    "    fd = compute_doppler_shift(target_velocity, fc)\n",
    "    print(f\"统一目标速度 {target_velocity} km/h，多普勒频移 {fd:.2f} Hz\")\n",
    "\n",
    "    X_blocks_list = []         # 每个元素: (sample_len, group_size, 2)\n",
    "    y_blocks_str_list = []     # 每个元素: tx_id (string)\n",
    "    label_set = set()\n",
    "\n",
    "    for file in tqdm(mat_files, desc=\"读取数据\"):\n",
    "        with h5py.File(file, \"r\") as f:\n",
    "            if \"rfDataset\" not in f:\n",
    "                continue\n",
    "            rfDataset = f[\"rfDataset\"]\n",
    "\n",
    "            if \"dmrs\" not in rfDataset or \"txID\" not in rfDataset:\n",
    "                continue\n",
    "\n",
    "            dmrs_struct = rfDataset[\"dmrs\"][:]\n",
    "            try:\n",
    "                dmrs_complex = dmrs_struct[\"real\"] + 1j * dmrs_struct[\"imag\"]\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            txID_uint16 = rfDataset[\"txID\"][:].flatten()\n",
    "            tx_id = \"\".join(chr(int(c)) for c in txID_uint16 if int(c) != 0)\n",
    "            if tx_id == \"\":\n",
    "                continue\n",
    "\n",
    "            # ===== 逐帧预处理（归一化 -> Doppler -> AWGN）=====\n",
    "            processed_signals = []\n",
    "            for i in range(dmrs_complex.shape[0]):\n",
    "                sig = dmrs_complex[i, :]\n",
    "\n",
    "                # step1: 功率归一化\n",
    "                sig = sig / (np.sqrt(np.mean(np.abs(sig) ** 2)) + 1e-12)\n",
    "\n",
    "                # step2: Doppler（统一使用 target_velocity 对应的 fd）\n",
    "                if apply_doppler:\n",
    "                    sig = apply_doppler_shift(sig, fd, fs)\n",
    "\n",
    "                # step3: AWGN\n",
    "                if apply_awgn:\n",
    "                    sig = add_awgn(sig, snr_db)\n",
    "\n",
    "                iq = np.stack((sig.real, sig.imag), axis=-1)  # (sample_len, 2)\n",
    "                processed_signals.append(iq)\n",
    "\n",
    "            processed_signals = np.asarray(processed_signals)  # (num_frames, sample_len, 2)\n",
    "            num_frames = processed_signals.shape[0]\n",
    "\n",
    "            # ===== 单文件内顺序切 block，不跨文件 =====\n",
    "            num_full_blocks = num_frames // group_size\n",
    "            if num_full_blocks <= 0:\n",
    "                continue\n",
    "\n",
    "            for b in range(num_full_blocks):\n",
    "                start = b * group_size\n",
    "                end = start + group_size\n",
    "                big_block = processed_signals[start:end]  # (group_size, sample_len, 2)\n",
    "\n",
    "                # 翻转 block： (group_size, sample_len, 2) -> (sample_len, group_size, 2)\n",
    "                big_block = np.transpose(big_block, (1, 0, 2))\n",
    "\n",
    "                X_blocks_list.append(big_block)\n",
    "                y_blocks_str_list.append(tx_id)\n",
    "                label_set.add(tx_id)\n",
    "\n",
    "    if len(X_blocks_list) == 0:\n",
    "        raise RuntimeError(\"没有生成任何 block，请检查数据路径或 group_size 设置\")\n",
    "\n",
    "    label_list = sorted(list(label_set))\n",
    "    label_to_idx = {label: i for i, label in enumerate(label_list)}\n",
    "    y_blocks = np.asarray([label_to_idx[s] for s in y_blocks_str_list], dtype=np.int64)\n",
    "    X_blocks = np.stack(X_blocks_list, axis=0)  # (num_blocks, sample_len, group_size, 2)\n",
    "\n",
    "    print(f\"[INFO] 生成 block 数: {X_blocks.shape[0]}, 每 block 样本数: {X_blocks.shape[2]}, 每样本长度: {X_blocks.shape[1]}\")\n",
    "    return X_blocks, y_blocks, label_to_idx\n",
    "\n",
    "# ================= 1D ResNet18（增加 dropout 和 in_planes） =================\n",
    "class BasicBlock1D(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "class ResNet18_1D(nn.Module):\n",
    "    def __init__(self, num_classes=10, in_planes=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.conv1 = nn.Conv1d(2, in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1, dropout=dropout)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2, dropout=dropout)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2, dropout=dropout)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2, dropout=dropout)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride, dropout):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes)\n",
    "            )\n",
    "        layers = [BasicBlock1D(self.in_planes, planes, stride, downsample, dropout)]\n",
    "        self.in_planes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock1D(self.in_planes, planes, dropout=dropout))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (B, sample_len, 2) -> (B, 2, sample_len)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ================= 辅助函数 =================\n",
    "def compute_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm += (p.grad.data.norm(2).item()) ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def moving_average(x, w=5):\n",
    "    x = np.array(x)\n",
    "    if len(x) == 0:\n",
    "        return np.array([])\n",
    "    if w <= 0:\n",
    "        w = 1\n",
    "    if len(x) < w:\n",
    "        w = len(x)\n",
    "    return np.convolve(x, np.ones(w), \"valid\") / w\n",
    "\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    acc = 100 * correct / total if total > 0 else 0.0\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=range(num_classes))\n",
    "    return acc, cm\n",
    "\n",
    "def plot_training_curves(fold_results, save_folder):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i, res in enumerate(fold_results):\n",
    "        plt.plot(moving_average(res[\"train_loss\"]), label=f\"Fold{i+1} Train Loss\")\n",
    "        plt.plot(moving_average(res[\"val_loss\"]), label=f\"Fold{i+1} Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"训练和验证Loss曲线\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(save_folder, \"loss_curves.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_grad_norms(avg_grad_norms, save_folder):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(range(1, len(avg_grad_norms) + 1), avg_grad_norms)\n",
    "    plt.xlabel(\"Fold\")\n",
    "    plt.ylabel(\"平均梯度范数\")\n",
    "    plt.title(\"各Fold平均梯度范数\")\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(save_folder, \"avg_grad_norms.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(cm, save_path=None):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "    plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel(\"Reference\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def check_block_overlap(train_blocks_idx, val_blocks_idx, test_blocks_idx):\n",
    "    train_set = set(train_blocks_idx)\n",
    "    val_set = set(val_blocks_idx)\n",
    "    test_set = set(test_blocks_idx)\n",
    "\n",
    "    overlap_train_val = train_set & val_set\n",
    "    overlap_train_test = train_set & test_set\n",
    "    overlap_val_test = val_set & test_set\n",
    "\n",
    "    if overlap_train_val or overlap_train_test or overlap_val_test:\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] Block 重叠检测失败！\"\n",
    "            f\"\\nTrain-Val overlap: {overlap_train_val}\"\n",
    "            f\"\\nTrain-Test overlap: {overlap_train_test}\"\n",
    "            f\"\\nVal-Test overlap: {overlap_val_test}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"[INFO] Block 重叠检查通过，训练/验证/测试 block 互斥。\")\n",
    "\n",
    "# ================= 主训练函数（按 block 划分） =================\n",
    "def train_for_snr(SNR_dB, save_folder, results_file, group_size=288):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] 使用设备: {device}\")\n",
    "\n",
    "    # 1) 加载 blocks（单文件成块，不跨文件）\n",
    "    X_blocks, y_blocks, label_to_idx = load_and_preprocess_with_grouping(\n",
    "        data_path,\n",
    "        group_size=group_size,\n",
    "        apply_doppler=apply_doppler,\n",
    "        target_velocity=v,\n",
    "        apply_awgn=apply_awgn,\n",
    "        snr_db=SNR_dB,\n",
    "        fs=fs,\n",
    "        fc=fc,\n",
    "    )\n",
    "    num_blocks = X_blocks.shape[0]\n",
    "    num_classes = len(label_to_idx)\n",
    "    print(f\"[INFO] 总 block 数: {num_blocks}, 类别数: {num_classes}\")\n",
    "\n",
    "    # 2) 按 block 做 train/test 划分（保证同一 block 不会被拆分）\n",
    "    block_idx = np.arange(num_blocks)\n",
    "    train_block_idx, test_block_idx = train_test_split(\n",
    "        block_idx, test_size=0.25, stratify=y_blocks, random_state=42\n",
    "    )\n",
    "    check_block_overlap(train_block_idx, [], test_block_idx)\n",
    "\n",
    "    X_train_blocks = X_blocks[train_block_idx]\n",
    "    y_train_blocks = y_blocks[train_block_idx]\n",
    "    X_test_blocks = X_blocks[test_block_idx]\n",
    "    y_test_blocks = y_blocks[test_block_idx]\n",
    "\n",
    "    # 展开测试 block 用于最终评估\n",
    "    X_test = X_test_blocks.reshape(-1, X_test_blocks.shape[2], X_test_blocks.shape[3])\n",
    "    y_test = np.repeat(y_test_blocks, X_test_blocks.shape[1])\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                 torch.tensor(y_test, dtype=torch.long))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"[INFO] 训练 block 数: {len(train_block_idx)}, 测试 block 数: {len(test_block_idx)}\")\n",
    "\n",
    "    # 3) KFold 按 block 做 fold（在训练块内划分 train/val）\n",
    "    effective_splits = min(n_splits, len(X_train_blocks))\n",
    "    if effective_splits < 2:\n",
    "        raise RuntimeError(f\"[ERROR] 训练 blocks 太少：{len(X_train_blocks)}，无法做 KFold\")\n",
    "    kfold = KFold(n_splits=effective_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_results = []\n",
    "    fold_test_accs = []\n",
    "\n",
    "    for fold, (train_idx_fold, val_idx_fold) in enumerate(kfold.split(X_train_blocks)):\n",
    "        print(f\"\\n====== Fold {fold+1}/{effective_splits} ======\")\n",
    "\n",
    "        # 展开训练 block\n",
    "        X_train = X_train_blocks[train_idx_fold].reshape(-1, X_train_blocks.shape[2], X_train_blocks.shape[3])\n",
    "        y_train = np.repeat(y_train_blocks[train_idx_fold], X_train_blocks.shape[1])\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                      torch.tensor(y_train, dtype=torch.long))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # 展开验证 block\n",
    "        X_val = X_train_blocks[val_idx_fold].reshape(-1, X_train_blocks.shape[2], X_train_blocks.shape[3])\n",
    "        y_val = np.repeat(y_train_blocks[val_idx_fold], X_train_blocks.shape[1])\n",
    "        val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                                    torch.tensor(y_val, dtype=torch.long))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # 模型、损失函数、优化器\n",
    "        model = ResNet18_1D(num_classes=num_classes, in_planes=in_planes, dropout=dropout).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        best_model_wts = None\n",
    "        train_losses, val_losses, grad_norms = [], [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "            batch_grad_norms = []\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                batch_grad_norms.append(compute_grad_norm(model))\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            train_loss = running_loss / max(1, len(train_loader))\n",
    "            train_acc = 100 * correct_train / max(1, total_train)\n",
    "            avg_grad_norm = float(np.mean(batch_grad_norms)) if len(batch_grad_norms) else 0.0\n",
    "            train_losses.append(train_loss)\n",
    "            grad_norms.append(avg_grad_norm)\n",
    "\n",
    "            # 验证\n",
    "            model.eval()\n",
    "            running_val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "            all_val_labels, all_val_preds = [], []\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    loss_val = criterion(val_outputs, val_labels)\n",
    "                    running_val_loss += loss_val.item()\n",
    "                    _, val_predicted = torch.max(val_outputs, 1)\n",
    "                    total_val += val_labels.size(0)\n",
    "                    correct_val += (val_predicted == val_labels).sum().item()\n",
    "                    all_val_labels.extend(val_labels.cpu().numpy())\n",
    "                    all_val_preds.extend(val_predicted.cpu().numpy())\n",
    "\n",
    "            val_loss = running_val_loss / max(1, len(val_loader))\n",
    "            val_acc = 100 * correct_val / max(1, total_val)\n",
    "            val_losses.append(val_loss)\n",
    "            val_cm = confusion_matrix(all_val_labels, all_val_preds, labels=range(num_classes))\n",
    "\n",
    "            log_msg = (f\"Fold {fold+1}, Epoch {epoch+1}: \"\n",
    "                       f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, \"\n",
    "                       f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%, Grad Norm={avg_grad_norm:.4f}\")\n",
    "            print(log_msg)\n",
    "            with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(log_msg + \"\\n\")\n",
    "\n",
    "            # 早停\n",
    "            if val_acc > best_val_acc + 0.01:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_model_wts = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    msg = f\"早停，连续 {patience} 个 epoch 验证集未提升\"\n",
    "                    print(msg)\n",
    "                    with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                        f.write(msg + \"\\n\")\n",
    "                    break\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        if best_model_wts is None:\n",
    "            best_model_wts = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        # 测试\n",
    "        model.load_state_dict(best_model_wts, strict=True)\n",
    "        model.to(device)\n",
    "        test_acc, test_cm = evaluate_model(model, test_loader, device, num_classes)\n",
    "        fold_test_accs.append(test_acc)\n",
    "\n",
    "        fold_results.append({\n",
    "            \"train_loss\": train_losses,\n",
    "            \"val_loss\": val_losses,\n",
    "            \"grad_norms\": grad_norms,\n",
    "            \"val_cm\": val_cm,\n",
    "            \"test_cm\": test_cm,\n",
    "        })\n",
    "\n",
    "        plot_confusion_matrix(val_cm, save_path=os.path.join(save_folder, f\"confusion_matrix_val_fold{fold+1}.png\"))\n",
    "        plot_confusion_matrix(test_cm, save_path=os.path.join(save_folder, f\"confusion_matrix_test_fold{fold+1}.png\"))\n",
    "        torch.save(best_model_wts, os.path.join(save_folder, f\"best_model_fold{fold+1}.pth\"))\n",
    "\n",
    "        print(f\"Fold {fold+1} Test Acc={test_acc:.2f}%\\n\")\n",
    "        with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Fold {fold+1} Test Acc={test_acc:.2f}%\\n\")\n",
    "\n",
    "    plot_training_curves(fold_results, save_folder)\n",
    "    plot_grad_norms([float(np.mean(f[\"grad_norms\"])) for f in fold_results], save_folder)\n",
    "\n",
    "    return float(np.mean(fold_test_accs)) if len(fold_test_accs) else 0.0\n",
    "\n",
    "# ================= SNR 循环训练 + 绘制 SNR 曲线 =================\n",
    "if __name__ == \"__main__\":\n",
    "    snr_list = list(range(20, -45, -5))\n",
    "    snr_accs = []\n",
    "\n",
    "    for snr_db in snr_list:\n",
    "        print(f\"\\n\\n================== 当前实验 SNR={snr_db} dB ==================\\n\")\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        script_name = \"LTE-V_XFR_SingleFileBlock\"\n",
    "        folder_name = (\n",
    "            f\"{timestamp}_{script_name}_SNR{snr_db}dB_\"\n",
    "            f\"fd{int(compute_doppler_shift(v, fc))}_\"\n",
    "            f\"ResNet\"\n",
    "        )\n",
    "        save_folder = os.path.join(os.getcwd(), \"training_results\", folder_name)\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "        results_file = os.path.join(save_folder, \"results.txt\")\n",
    "        with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\n================ SNR={snr_db} dB =================\\n\")\n",
    "            f.write(f\"apply_doppler={apply_doppler}, apply_awgn={apply_awgn}, doppler_v={v}km/h\\n\")\n",
    "\n",
    "        test_acc = train_for_snr(snr_db, save_folder, results_file, group_size=864)\n",
    "        snr_accs.append(test_acc)\n",
    "        print(f\"SNR {snr_db:>3} dB → results in: {save_folder}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(snr_list, snr_accs, marker=\"o\", linestyle=\"-\")\n",
    "    plt.xlabel(\"SNR (dB)\")\n",
    "    plt.ylabel(\"测试集准确率 (%)\")\n",
    "    plt.title(\"SNR vs 测试集准确率\")\n",
    "    plt.grid(True)\n",
    "    snr_curve_path = os.path.join(os.getcwd(), \"training_results\", f\"SNR_vs_accuracy_{timestamp}.png\")\n",
    "    plt.savefig(snr_curve_path)\n",
    "    plt.show()\n",
    "    print(f\"[INFO] SNR vs 测试准确率曲线已保存到 {snr_curve_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
