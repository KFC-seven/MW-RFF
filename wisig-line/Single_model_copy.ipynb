{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Randomly selected 150 classes from 150 total classes\n",
      "\n",
      "Loaded 150 classes from trajectory_pos\n",
      "Class 0 (16-1): 3825 samples\n",
      "Class 1 (20-16): 3570 samples\n",
      "Class 2 (8-8): 3570 samples\n",
      "Class 3 (5-5): 3825 samples\n",
      "Class 4 (18-4): 3570 samples\n",
      "Class 5 (1-12): 3570 samples\n",
      "Class 6 (11-1): 3825 samples\n",
      "Class 7 (11-4): 3825 samples\n",
      "Class 8 (14-12): 3825 samples\n",
      "Class 9 (10-11): 3825 samples\n",
      "Class 10 (13-18): 1785 samples\n",
      "Class 11 (9-14): 2805 samples\n",
      "Class 12 (1-2): 3060 samples\n",
      "Class 13 (19-3): 3570 samples\n",
      "Class 14 (8-3): 3825 samples\n",
      "Class 15 (11-7): 3825 samples\n",
      "Class 16 (14-11): 3570 samples\n",
      "Class 17 (11-20): 3570 samples\n",
      "Class 18 (20-1): 3315 samples\n",
      "Class 19 (14-14): 3825 samples\n",
      "Class 20 (19-2): 2805 samples\n",
      "Class 21 (6-1): 3825 samples\n",
      "Class 22 (2-15): 3825 samples\n",
      "Class 23 (18-11): 3570 samples\n",
      "Class 24 (2-12): 3825 samples\n",
      "Class 25 (12-1): 1785 samples\n",
      "Class 26 (7-11): 3825 samples\n",
      "Class 27 (1-19): 3570 samples\n",
      "Class 28 (19-13): 3825 samples\n",
      "Class 29 (16-5): 3060 samples\n",
      "Class 30 (8-13): 3825 samples\n",
      "Class 31 (2-4): 3825 samples\n",
      "Class 32 (14-9): 3570 samples\n",
      "Class 33 (18-10): 3825 samples\n",
      "Class 34 (19-11): 3825 samples\n",
      "Class 35 (19-7): 3825 samples\n",
      "Class 36 (20-18): 3315 samples\n",
      "Class 37 (10-17): 3570 samples\n",
      "Class 38 (18-20): 3570 samples\n",
      "Class 39 (1-16): 3570 samples\n",
      "Class 40 (3-18): 3570 samples\n",
      "Class 41 (19-9): 3825 samples\n",
      "Class 42 (15-19): 3570 samples\n",
      "Class 43 (15-1): 3825 samples\n",
      "Class 44 (18-14): 3315 samples\n",
      "Class 45 (16-20): 3570 samples\n",
      "Class 46 (5-1): 3570 samples\n",
      "Class 47 (2-6): 3570 samples\n",
      "Class 48 (2-19): 3570 samples\n",
      "Class 49 (8-1): 3825 samples\n",
      "Class 50 (8-20): 3825 samples\n",
      "Class 51 (20-19): 3825 samples\n",
      "Class 52 (18-15): 3570 samples\n",
      "Class 53 (3-13): 3825 samples\n",
      "Class 54 (8-14): 3315 samples\n",
      "Class 55 (11-17): 3060 samples\n",
      "Class 56 (19-8): 3825 samples\n",
      "Class 57 (7-7): 3825 samples\n",
      "Class 58 (20-20): 3570 samples\n",
      "Class 59 (19-12): 3570 samples\n",
      "Class 60 (18-17): 3570 samples\n",
      "Class 61 (18-7): 3570 samples\n",
      "Class 62 (12-19): 3570 samples\n",
      "Class 63 (2-7): 3825 samples\n",
      "Class 64 (9-1): 3825 samples\n",
      "Class 65 (3-20): 3315 samples\n",
      "Class 66 (3-8): 3825 samples\n",
      "Class 67 (7-14): 3570 samples\n",
      "Class 68 (3-2): 3570 samples\n",
      "Class 69 (10-1): 2040 samples\n",
      "Class 70 (7-13): 3315 samples\n",
      "Class 71 (18-9): 3825 samples\n",
      "Class 72 (13-14): 3825 samples\n",
      "Class 73 (2-14): 3570 samples\n",
      "Class 74 (1-1): 3060 samples\n",
      "Class 75 (19-10): 3570 samples\n",
      "Class 76 (20-7): 3825 samples\n",
      "Class 77 (20-8): 3825 samples\n",
      "Class 78 (11-10): 3570 samples\n",
      "Class 79 (17-11): 3825 samples\n",
      "Class 80 (18-5): 3570 samples\n",
      "Class 81 (1-8): 3825 samples\n",
      "Class 82 (2-8): 3825 samples\n",
      "Class 83 (2-20): 2550 samples\n",
      "Class 84 (19-1): 3825 samples\n",
      "Class 85 (20-5): 3825 samples\n",
      "Class 86 (13-7): 3315 samples\n",
      "Class 87 (7-12): 3060 samples\n",
      "Class 88 (18-16): 3570 samples\n",
      "Class 89 (18-8): 3825 samples\n",
      "Class 90 (10-10): 3060 samples\n",
      "Class 91 (19-19): 3570 samples\n",
      "Class 92 (19-6): 3825 samples\n",
      "Class 93 (1-11): 3060 samples\n",
      "Class 94 (2-13): 3825 samples\n",
      "Class 95 (2-1): 2040 samples\n",
      "Class 96 (1-14): 3825 samples\n",
      "Class 97 (4-10): 3570 samples\n",
      "Class 98 (12-7): 3570 samples\n",
      "Class 99 (11-19): 3825 samples\n",
      "Class 100 (8-18): 3825 samples\n",
      "Class 101 (3-1): 3570 samples\n",
      "Class 102 (7-8): 3060 samples\n",
      "Class 103 (13-3): 3570 samples\n",
      "Class 104 (1-18): 3825 samples\n",
      "Class 105 (4-11): 3570 samples\n",
      "Class 106 (9-7): 3315 samples\n",
      "Class 107 (19-14): 3825 samples\n",
      "Class 108 (5-20): 3570 samples\n",
      "Class 109 (16-16): 3825 samples\n",
      "Class 110 (20-4): 3825 samples\n",
      "Class 111 (14-20): 3570 samples\n",
      "Class 112 (20-3): 3315 samples\n",
      "Class 113 (13-20): 3315 samples\n",
      "Class 114 (17-10): 3825 samples\n",
      "Class 115 (3-19): 3060 samples\n",
      "Class 116 (19-20): 3825 samples\n",
      "Class 117 (6-6): 3570 samples\n",
      "Class 118 (7-10): 3825 samples\n",
      "Class 119 (10-7): 3825 samples\n",
      "Class 120 (14-13): 3825 samples\n",
      "Class 121 (19-4): 3570 samples\n",
      "Class 122 (4-1): 3570 samples\n",
      "Class 123 (6-15): 3825 samples\n",
      "Class 124 (7-9): 3315 samples\n",
      "Class 125 (18-13): 3570 samples\n",
      "Class 126 (10-4): 3570 samples\n",
      "Class 127 (14-10): 3825 samples\n",
      "Class 128 (1-10): 3825 samples\n",
      "Class 129 (7-20): 3825 samples\n",
      "Class 130 (2-16): 2805 samples\n",
      "Class 131 (18-2): 3060 samples\n",
      "Class 132 (16-19): 3570 samples\n",
      "Class 133 (14-7): 3570 samples\n",
      "Class 134 (18-12): 3570 samples\n",
      "Class 135 (20-12): 3825 samples\n",
      "Class 136 (20-15): 3825 samples\n",
      "Class 137 (14-8): 3570 samples\n",
      "Class 138 (15-6): 3315 samples\n",
      "Class 139 (18-1): 3060 samples\n",
      "Class 140 (2-5): 3825 samples\n",
      "Class 141 (5-16): 3570 samples\n",
      "Class 142 (9-20): 3570 samples\n",
      "Class 143 (2-17): 3315 samples\n",
      "Class 144 (13-19): 3825 samples\n",
      "Class 145 (8-7): 3060 samples\n",
      "Class 146 (1-15): 3060 samples\n",
      "Class 147 (12-20): 3570 samples\n",
      "Class 148 (2-3): 3825 samples\n",
      "Class 149 (20-14): 3825 samples\n",
      "Randomly selected 150 classes from 150 total classes\n",
      "\n",
      "Loaded 150 classes from trajectory_pos\n",
      "Class 0 (11-7): 3825 samples\n",
      "Class 1 (11-4): 3825 samples\n",
      "Class 2 (20-1): 3315 samples\n",
      "Class 3 (20-12): 3825 samples\n",
      "Class 4 (7-7): 3825 samples\n",
      "Class 5 (18-10): 3825 samples\n",
      "Class 6 (19-9): 3825 samples\n",
      "Class 7 (7-8): 3060 samples\n",
      "Class 8 (2-6): 3570 samples\n",
      "Class 9 (13-7): 3315 samples\n",
      "Class 10 (1-12): 3570 samples\n",
      "Class 11 (20-16): 3570 samples\n",
      "Class 12 (13-18): 1785 samples\n",
      "Class 13 (4-11): 3570 samples\n",
      "Class 14 (8-13): 3825 samples\n",
      "Class 15 (19-12): 3570 samples\n",
      "Class 16 (1-16): 3570 samples\n",
      "Class 17 (11-19): 3825 samples\n",
      "Class 18 (2-5): 3825 samples\n",
      "Class 19 (18-12): 3570 samples\n",
      "Class 20 (17-10): 3825 samples\n",
      "Class 21 (10-7): 3825 samples\n",
      "Class 22 (3-1): 3570 samples\n",
      "Class 23 (20-14): 3825 samples\n",
      "Class 24 (2-19): 3570 samples\n",
      "Class 25 (2-20): 2550 samples\n",
      "Class 26 (20-20): 3570 samples\n",
      "Class 27 (18-14): 3315 samples\n",
      "Class 28 (2-15): 3825 samples\n",
      "Class 29 (18-9): 3825 samples\n",
      "Class 30 (10-17): 3570 samples\n",
      "Class 31 (15-19): 3570 samples\n",
      "Class 32 (1-8): 3825 samples\n",
      "Class 33 (3-19): 3060 samples\n",
      "Class 34 (1-11): 3060 samples\n",
      "Class 35 (14-7): 3570 samples\n",
      "Class 36 (12-19): 3570 samples\n",
      "Class 37 (1-1): 3060 samples\n",
      "Class 38 (8-20): 3825 samples\n",
      "Class 39 (14-8): 3570 samples\n",
      "Class 40 (9-20): 3570 samples\n",
      "Class 41 (8-3): 3825 samples\n",
      "Class 42 (12-1): 1785 samples\n",
      "Class 43 (5-1): 3570 samples\n",
      "Class 44 (14-11): 3570 samples\n",
      "Class 45 (8-14): 3315 samples\n",
      "Class 46 (19-2): 2805 samples\n",
      "Class 47 (9-1): 3825 samples\n",
      "Class 48 (1-15): 3060 samples\n",
      "Class 49 (2-16): 2805 samples\n",
      "Class 50 (2-12): 3825 samples\n",
      "Class 51 (19-4): 3570 samples\n",
      "Class 52 (10-11): 3825 samples\n",
      "Class 53 (2-1): 2040 samples\n",
      "Class 54 (8-7): 3060 samples\n",
      "Class 55 (12-7): 3570 samples\n",
      "Class 56 (3-20): 3315 samples\n",
      "Class 57 (19-20): 3825 samples\n",
      "Class 58 (6-6): 3570 samples\n",
      "Class 59 (19-11): 3825 samples\n",
      "Class 60 (16-19): 3570 samples\n",
      "Class 61 (2-13): 3825 samples\n",
      "Class 62 (19-8): 3825 samples\n",
      "Class 63 (16-20): 3570 samples\n",
      "Class 64 (18-8): 3825 samples\n",
      "Class 65 (7-13): 3315 samples\n",
      "Class 66 (6-1): 3825 samples\n",
      "Class 67 (13-14): 3825 samples\n",
      "Class 68 (8-18): 3825 samples\n",
      "Class 69 (14-9): 3570 samples\n",
      "Class 70 (19-19): 3570 samples\n",
      "Class 71 (1-10): 3825 samples\n",
      "Class 72 (18-15): 3570 samples\n",
      "Class 73 (17-11): 3825 samples\n",
      "Class 74 (11-20): 3570 samples\n",
      "Class 75 (2-17): 3315 samples\n",
      "Class 76 (19-13): 3825 samples\n",
      "Class 77 (18-2): 3060 samples\n",
      "Class 78 (19-14): 3825 samples\n",
      "Class 79 (20-19): 3825 samples\n",
      "Class 80 (2-14): 3570 samples\n",
      "Class 81 (5-16): 3570 samples\n",
      "Class 82 (3-2): 3570 samples\n",
      "Class 83 (16-1): 3825 samples\n",
      "Class 84 (11-1): 3825 samples\n",
      "Class 85 (18-16): 3570 samples\n",
      "Class 86 (20-5): 3825 samples\n",
      "Class 87 (10-10): 3060 samples\n",
      "Class 88 (14-14): 3825 samples\n",
      "Class 89 (18-20): 3570 samples\n",
      "Class 90 (19-3): 3570 samples\n",
      "Class 91 (18-7): 3570 samples\n",
      "Class 92 (14-10): 3825 samples\n",
      "Class 93 (7-10): 3825 samples\n",
      "Class 94 (20-7): 3825 samples\n",
      "Class 95 (18-17): 3570 samples\n",
      "Class 96 (20-18): 3315 samples\n",
      "Class 97 (20-4): 3825 samples\n",
      "Class 98 (16-5): 3060 samples\n",
      "Class 99 (13-19): 3825 samples\n",
      "Class 100 (20-3): 3315 samples\n",
      "Class 101 (16-16): 3825 samples\n",
      "Class 102 (3-13): 3825 samples\n",
      "Class 103 (3-18): 3570 samples\n",
      "Class 104 (12-20): 3570 samples\n",
      "Class 105 (14-13): 3825 samples\n",
      "Class 106 (18-1): 3060 samples\n",
      "Class 107 (3-8): 3825 samples\n",
      "Class 108 (13-20): 3315 samples\n",
      "Class 109 (13-3): 3570 samples\n",
      "Class 110 (2-8): 3825 samples\n",
      "Class 111 (18-11): 3570 samples\n",
      "Class 112 (19-10): 3570 samples\n",
      "Class 113 (4-10): 3570 samples\n",
      "Class 114 (15-6): 3315 samples\n",
      "Class 115 (18-4): 3570 samples\n",
      "Class 116 (5-20): 3570 samples\n",
      "Class 117 (6-15): 3825 samples\n",
      "Class 118 (8-8): 3570 samples\n",
      "Class 119 (14-12): 3825 samples\n",
      "Class 120 (2-3): 3825 samples\n",
      "Class 121 (1-18): 3825 samples\n",
      "Class 122 (8-1): 3825 samples\n",
      "Class 123 (2-7): 3825 samples\n",
      "Class 124 (11-17): 3060 samples\n",
      "Class 125 (19-6): 3825 samples\n",
      "Class 126 (9-7): 3315 samples\n",
      "Class 127 (19-1): 3825 samples\n",
      "Class 128 (7-20): 3825 samples\n",
      "Class 129 (19-7): 3825 samples\n",
      "Class 130 (10-4): 3570 samples\n",
      "Class 131 (18-5): 3570 samples\n",
      "Class 132 (14-20): 3570 samples\n",
      "Class 133 (7-14): 3570 samples\n",
      "Class 134 (2-4): 3825 samples\n",
      "Class 135 (7-9): 3315 samples\n",
      "Class 136 (10-1): 2040 samples\n",
      "Class 137 (20-8): 3825 samples\n",
      "Class 138 (20-15): 3825 samples\n",
      "Class 139 (7-12): 3060 samples\n",
      "Class 140 (9-14): 2805 samples\n",
      "Class 141 (18-13): 3570 samples\n",
      "Class 142 (1-19): 3570 samples\n",
      "Class 143 (5-5): 3825 samples\n",
      "Class 144 (1-14): 3825 samples\n",
      "Class 145 (7-11): 3825 samples\n",
      "Class 146 (4-1): 3570 samples\n",
      "Class 147 (1-2): 3060 samples\n",
      "Class 148 (11-10): 3570 samples\n",
      "Class 149 (15-1): 3825 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:  12%|█▏        | 151/1247 [14:42<1:46:46,  5.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 472\u001b[0m\n\u001b[0;32m    455\u001b[0m classifier \u001b[38;5;241m=\u001b[39m ImageClassifier()\n\u001b[0;32m    457\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroot_dir\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../IQ_signal_plots\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_folder\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrajectory_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 可替换为其他特征文件夹\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m    470\u001b[0m }\n\u001b[1;32m--> 472\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 186\u001b[0m, in \u001b[0;36mImageClassifier.train\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    183\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# 训练循环\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 传递保存路径\u001b[39;49;00m\n\u001b[0;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# 最终评估\u001b[39;00m\n\u001b[0;32m    194\u001b[0m test_acc, cm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(model, test_loader)\n",
      "Cell \u001b[1;32mIn[1], line 251\u001b[0m, in \u001b[0;36mImageClassifier._train_loop\u001b[1;34m(self, model, optimizer, criterion, scheduler, scaler, train_loader, val_loader, epochs, patience, save_path)\u001b[0m\n\u001b[0;32m    248\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    249\u001b[0m train_loss, correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    252\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    254\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\MW-RFF\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\MW-RFF\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\MW-RFF\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\MW-RFF\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\MW-RFF\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m, in \u001b[0;36mImageClassifier.ImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 42\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     44\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\MW-RFF\\lib\\site-packages\\PIL\\Image.py:3465\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3462\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3465\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3466\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "class ImageClassifier:\n",
    "    def __init__(self, device='cuda:0'):\n",
    "        self.device = torch.device(device)\n",
    "        self._setup_device()\n",
    "    \n",
    "    def _setup_device(self):\n",
    "        \"\"\"初始化设备配置\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"CUDA is not available. Please enable a GPU.\")\n",
    "        torch.cuda.set_device(self.device)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "    # 将 ImageDataset 定义在类作用域内\n",
    "    class ImageDataset(Dataset):\n",
    "        def __init__(self, file_paths, labels, transform=None):\n",
    "            self.file_paths = file_paths\n",
    "            self.labels = labels\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.file_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image = Image.open(self.file_paths[idx]).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, self.labels[idx]\n",
    "        \n",
    "    def load_data(self, root_dir, target_folder, num_classes_to_select=None, \n",
    "             limit_per_class=None, use_all_images=False):\n",
    "        \"\"\"\n",
    "        修正后的参数逻辑:\n",
    "        :param num_classes_to_select: 选择训练的类别数量\n",
    "        :param limit_per_class: 每个类别的最大图片数\n",
    "        \"\"\"\n",
    "        # 获取所有有效类别\n",
    "        all_classes = [d.name for d in os.scandir(root_dir) if d.is_dir()]\n",
    "        if not all_classes:\n",
    "            raise ValueError(f\"No valid classes found in {root_dir}\")\n",
    "\n",
    "        # 类别选择验证逻辑\n",
    "        if num_classes_to_select is not None:\n",
    "            if not isinstance(num_classes_to_select, int) or num_classes_to_select <= 0:\n",
    "                raise ValueError(\"num_classes_to_select must be a positive integer\")\n",
    "            \n",
    "            # 当请求类别数超过实际数量时自动修正\n",
    "            if num_classes_to_select > len(all_classes):\n",
    "                print(f\"Warning: Requested {num_classes_to_select} classes but only {len(all_classes)} available. Using all classes.\")\n",
    "                num_classes_to_select = len(all_classes)\n",
    "            \n",
    "            selected_classes = random.sample(all_classes, num_classes_to_select)\n",
    "            print(f\"Randomly selected {num_classes_to_select} classes from {len(all_classes)} total classes\")\n",
    "        else:\n",
    "            selected_classes = all_classes\n",
    "            num_classes_to_select = len(all_classes)  # 保持参数记录准确\n",
    "\n",
    "        # 处理每个类别的图片数量\n",
    "        class_mapping = {cls: idx for idx, cls in enumerate(selected_classes)}\n",
    "        file_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        for cls_name in selected_classes:\n",
    "            target_path = os.path.join(root_dir, cls_name, target_folder)\n",
    "            if not os.path.exists(target_path):\n",
    "                print(f\"Warning: Missing {target_folder} in {cls_name}\")\n",
    "                continue\n",
    "                \n",
    "            # 获取所有图片文件\n",
    "            images = [\n",
    "                os.path.join(target_path, f) \n",
    "                for f in os.listdir(target_path) \n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "            ]\n",
    "            \n",
    "            # 图片数量控制逻辑\n",
    "            if limit_per_class and not use_all_images:\n",
    "                if len(images) < limit_per_class:\n",
    "                    print(f\"Warning: Class {cls_name} only has {len(images)} images (requested {limit_per_class})\")\n",
    "                images = images[:limit_per_class]  # 安全截断\n",
    "                \n",
    "            file_paths.extend(images)\n",
    "            labels.extend([class_mapping[cls_name]] * len(images))\n",
    "\n",
    "        print(f\"\\nLoaded {len(class_mapping)} classes from {target_folder}\")\n",
    "        self._print_class_stats(labels, class_mapping)\n",
    "        return file_paths, labels, class_mapping\n",
    "\n",
    "    def _print_class_stats(self, labels, class_mapping):\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        for cls_idx, count in zip(unique, counts):\n",
    "            cls_name = list(class_mapping.keys())[cls_idx]\n",
    "            print(f\"Class {cls_idx} ({cls_name}): {count} samples\")\n",
    "\n",
    "    def create_model(self, num_classes, model_name='resnet18'):\n",
    "        \"\"\"创建可配置的模型\"\"\"\n",
    "        model_map = {\n",
    "            'resnet18': models.resnet18,\n",
    "            'resnet50': models.resnet50,\n",
    "            'efficientnet_b0': models.efficientnet_b0\n",
    "        }\n",
    "        \n",
    "        model = model_map[model_name](weights='DEFAULT')\n",
    "        if 'resnet' in model_name:\n",
    "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        elif 'efficientnet' in model_name:\n",
    "            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "            \n",
    "        return model.to(self.device)\n",
    "\n",
    "    def train(self, config):\n",
    "        \"\"\"完整的训练流程\"\"\"\n",
    "        # 初始化配置\n",
    "        config.setdefault('save_dir', 'model_results')\n",
    "        config.setdefault('model_name', 'resnet18')\n",
    "        config.setdefault('input_size', 224)\n",
    "        config.setdefault('patience', 5)\n",
    "\n",
    "        # 必须先加载数据以获取 class_mapping\n",
    "        file_paths, labels, class_mapping = self.load_data(\n",
    "            root_dir=config['root_dir'],\n",
    "            target_folder=config['target_folder'],\n",
    "            num_classes_to_select=config.get('num_classes_to_select'),\n",
    "            limit_per_class=config.get('limit_per_class'),\n",
    "            use_all_images=config.get('use_all_images', False)\n",
    "        )\n",
    "\n",
    "        # 创建唯一保存目录\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        folder_name = (\n",
    "            f\"{config['target_folder']}_\"\n",
    "            f\"{timestamp}-\"\n",
    "            f\"{len(class_mapping)}_classes\"\n",
    "        )\n",
    "        if config.get('num_classes_to_select'):\n",
    "            folder_name += f\"-selected_{config['num_classes_to_select']}\"\n",
    "        if config.get('limit_per_class'):\n",
    "            folder_name += f\"-limit_{config['limit_per_class']}\"\n",
    "        save_path = os.path.join(config['save_dir'], folder_name)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # 数据加载\n",
    "        file_paths, labels, class_mapping = self.load_data(\n",
    "            root_dir=config['root_dir'],\n",
    "            target_folder=config['target_folder'],\n",
    "            num_classes_to_select=config.get('num_classes_to_select'),\n",
    "            limit_per_class=config.get('limit_per_class'),\n",
    "            use_all_images=config.get('use_all_images', False)\n",
    "        )\n",
    "\n",
    "        # 数据预处理\n",
    "        train_transform, test_transform = self._get_transforms(config['input_size'])\n",
    "        \n",
    "        # 数据集划分\n",
    "        train_loader, val_loader, test_loader = self._create_data_loaders(\n",
    "            file_paths, labels, \n",
    "            train_transform, test_transform,\n",
    "            config['batch_size']\n",
    "        )\n",
    "        \n",
    "        # 模型初始化\n",
    "        model = self.create_model(len(class_mapping), config['model_name'])\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scaler = torch.amp.GradScaler()\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n",
    "        \n",
    "        # 训练循环\n",
    "        history = self._train_loop(\n",
    "            model, optimizer, criterion, scheduler, scaler,\n",
    "            train_loader, val_loader,\n",
    "            config['epochs'], config['patience'],\n",
    "            save_path  # 传递保存路径\n",
    "        )\n",
    "        \n",
    "        # 最终评估\n",
    "        test_acc, cm = self.evaluate(model, test_loader)\n",
    "        \n",
    "        # 保存结果\n",
    "        self._save_results(\n",
    "            model, history, cm, class_mapping,\n",
    "            config, test_acc, save_path, labels\n",
    "        )\n",
    "\n",
    "    def _get_transforms(self, input_size):\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return train_transform, test_transform\n",
    "\n",
    "    def _create_data_loaders(self, file_paths, labels, train_trans, test_trans, batch_size):\n",
    "        # 数据集划分（60-20-20）\n",
    "        train_p, test_p, train_l, test_l = train_test_split(\n",
    "            file_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "        )\n",
    "        train_p, val_p, train_l, val_l = train_test_split(\n",
    "            train_p, train_l, test_size=0.25, stratify=train_l, random_state=42\n",
    "        )\n",
    "        \n",
    "        return (\n",
    "            DataLoader(self.ImageDataset(train_p, train_l, train_trans), \n",
    "                      batch_size, shuffle=True, pin_memory=True),\n",
    "            DataLoader(self.ImageDataset(val_p, val_l, test_trans), \n",
    "                      batch_size, shuffle=False, pin_memory=True),\n",
    "            DataLoader(self.ImageDataset(test_p, test_l, test_trans), \n",
    "                      batch_size, shuffle=False, pin_memory=True)\n",
    "        )\n",
    "\n",
    "    def _train_loop(self, model, optimizer, criterion, scheduler, scaler, \n",
    "                   train_loader, val_loader, epochs, patience, save_path):\n",
    "        history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_acc': [], 'val_acc': [],\n",
    "            'lr': []\n",
    "        }\n",
    "        best_acc = 0.0\n",
    "        early_stop_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 训练阶段\n",
    "            model.train()\n",
    "            train_loss, correct, total = 0.0, 0, 0\n",
    "            \n",
    "            for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # 验证阶段\n",
    "            val_acc, val_loss = self._validate(model, criterion, val_loader)\n",
    "            \n",
    "            # 记录指标\n",
    "            train_acc = correct / total\n",
    "            history['train_loss'].append(train_loss/len(train_loader))\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # 学习率调整\n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "            # 早停机制和模型保存\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                early_stop_counter = 0\n",
    "                # 保存到指定路径\n",
    "                model_save_path = os.path.join(save_path, \"best_model.pth\")\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: \"\n",
    "                  f\"Train Loss: {history['train_loss'][-1]:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Acc: {val_acc:.4f} | \"\n",
    "                  f\"LR: {history['lr'][-1]:.2e}\")\n",
    "        \n",
    "         # 加载指定路径的最佳模型\n",
    "        model.load_state_dict(torch.load(os.path.join(save_path, \"best_model.pth\")))\n",
    "        return history\n",
    "\n",
    "    def _validate(self, model, criterion, val_loader):\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "        return correct / total, val_loss / len(val_loader)\n",
    "\n",
    "    def evaluate(self, model, test_loader):\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        test_acc = correct / total\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        return test_acc, cm\n",
    "\n",
    "    def _save_results(self, model, history, cm, class_mapping, config, test_acc, save_path, labels):\n",
    "        # 生成带特征文件夹信息的目录名\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        folder_name = (\n",
    "            f\"{config['target_folder']}_\"\n",
    "            f\"{timestamp}-\"\n",
    "            f\"{len(class_mapping)}_classes\"\n",
    "        )\n",
    "        \n",
    "        # 添加参数标记\n",
    "        if config.get('num_classes_to_select'):\n",
    "            folder_name += f\"-selected_{config['num_classes_to_select']}\"\n",
    "        if config.get('limit_per_class'):\n",
    "            folder_name += f\"-limit_{config['limit_per_class']}\"\n",
    "        elif not config.get('use_all_images'):\n",
    "            folder_name += \"-default_limit\"\n",
    "        \n",
    "        save_path = os.path.join(config['save_dir'], folder_name)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # 保存最佳模型到指定路径\n",
    "        model_save_path = os.path.join(save_path, \"best_model.pth\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "        # 保存训练曲线\n",
    "        self._plot_training_curves(history, save_path)\n",
    "        \n",
    "        # 保存混淆矩阵\n",
    "        self._plot_confusion_matrix(cm, class_mapping, save_path)\n",
    "        \n",
    "        # 保存报告\n",
    "        self._save_report(config, history, test_acc, class_mapping, save_path, labels)\n",
    "\n",
    "    def _plot_training_curves(self, history, save_path):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['train_loss'], label='Train')\n",
    "        plt.plot(history['val_loss'], label='Validation')\n",
    "        plt.title('Loss Curves')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['train_acc'], label='Train')\n",
    "        plt.plot(history['val_acc'], label='Validation')\n",
    "        plt.title('Accuracy Curves')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "         # 保存并关闭\n",
    "        plt.savefig(os.path.join(save_path, \"training_curves.png\"), bbox_inches='tight')\n",
    "        plt.close()  # 关键：防止内存泄漏\n",
    "\n",
    "    def _plot_confusion_matrix(self, cm, class_mapping, save_path):\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=class_mapping.keys(),\n",
    "                    yticklabels=class_mapping.keys())\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "\n",
    "        # 保存并关闭\n",
    "        plt.savefig(os.path.join(save_path, \"confusion_matrix.png\"), bbox_inches='tight')\n",
    "        plt.close()  # 关键：确保保存完成\n",
    "\n",
    "    def _save_report(self, config, history, test_acc, class_mapping, save_path, labels):\n",
    "        with open(os.path.join(save_path, \"results.txt\"), \"w\") as f:\n",
    "            f.write(\"=== Experiment Summary ===\\n\")\n",
    "            f.write(f\"Feature Folder: {config['target_folder']}\\n\")  # 新增特征目录信息\n",
    "            f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Total Classes: {len(class_mapping)}\\n\")\n",
    "            f.write(f\"Best Train Accuracy: {max(history['train_acc']):.4f}\\n\")\n",
    "            f.write(f\"Best Val Accuracy: {max(history['val_acc']):.4f}\\n\")\n",
    "            f.write(f\"Final Test Accuracy: {test_acc:.4f}\\n\\n\")\n",
    "            \n",
    "            f.write(\"=== Training Parameters ===\\n\")\n",
    "            f.write(f\"num_classes_to_select: {config.get('num_classes_to_select', 'All')}\\n\")\n",
    "            f.write(f\"limit_per_class: {config.get('limit_per_class', 'No limit')}\\n\")\n",
    "            f.write(f\"use_all_images: {config.get('use_all_images', False)}\\n\")\n",
    "            f.write(f\"batch_size: {config['batch_size']}\\n\")\n",
    "            f.write(f\"epochs: {config['epochs']}\\n\")\n",
    "            f.write(f\"learning_rate: {config['lr']}\\n\")\n",
    "            f.write(f\"input_size: {config['input_size']}\\n\")\n",
    "            f.write(f\"model_name: {config['model_name']}\\n\\n\")\n",
    "            \n",
    "            # 按epoch记录详细数据\n",
    "            f.write(\"=== Epoch-wise Results ===\\n\")\n",
    "            f.write(\"Epoch | Train Acc | Val Acc | Learning Rate\\n\")\n",
    "            f.write(\"--------------------------------------------\\n\")\n",
    "            for epoch, (train_acc, val_acc, lr) in enumerate(zip(\n",
    "                history['train_acc'], \n",
    "                history['val_acc'],\n",
    "                history['lr']\n",
    "            )):\n",
    "                f.write(f\"{epoch+1:5d} | {train_acc:.4f}   | {val_acc:.4f}  | {lr:.2e}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"=== Class Distribution ===\\n\")\n",
    "            # 使用传入的labels直接统计\n",
    "            unique, counts = np.unique(labels, return_counts=True)\n",
    "            for cls_idx, count in zip(unique, counts):\n",
    "                cls_name = [k for k, v in class_mapping.items() if v == cls_idx][0]\n",
    "                f.write(f\"Class {cls_idx} ({cls_name}): {count} samples\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = ImageClassifier()\n",
    "    \n",
    "    config = {\n",
    "        'root_dir': \"../../IQ_signal_plots\",\n",
    "        'target_folder': \"trajectory_pos\",  # 可替换为其他特征文件夹\n",
    "        'save_dir': \"training_results\",\n",
    "        'batch_size': 256,\n",
    "        'epochs': 30,\n",
    "        'lr': 0.001,\n",
    "        'num_classes_to_select': 30,      # 选择训练多少个类\n",
    "        'limit_per_class': 1000,           # 每类最大样本数\n",
    "        'use_all_images': True,          # 是否忽略limit_per_class\n",
    "        'input_size': 224,\n",
    "        'model_name': \"resnet18\",\n",
    "        'patience': 5\n",
    "    }\n",
    "    \n",
    "    classifier.train(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MW-RFF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
