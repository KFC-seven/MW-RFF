{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Randomly selected 5 classes from 150 total classes\n",
      "\n",
      "Loaded 5 classes from scatter_plots\n",
      "Class 0 (9-1): 100 samples\n",
      "Class 1 (19-8): 100 samples\n",
      "Class 2 (18-9): 100 samples\n",
      "Class 3 (13-14): 100 samples\n",
      "Class 4 (3-18): 100 samples\n",
      "Randomly selected 5 classes from 150 total classes\n",
      "\n",
      "Loaded 5 classes from scatter_plots\n",
      "Class 0 (8-3): 100 samples\n",
      "Class 1 (3-8): 100 samples\n",
      "Class 2 (11-1): 100 samples\n",
      "Class 3 (10-4): 100 samples\n",
      "Class 4 (3-2): 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.7015 | Val Loss: 1.8099 | Train Acc: 0.2033 | Val Acc: 0.2000 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 2/2 [00:02<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 1.5453 | Val Loss: 2.4775 | Train Acc: 0.4267 | Val Acc: 0.2000 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 1.1399 | Val Loss: 5.3327 | Train Acc: 0.5300 | Val Acc: 0.2900 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.9302 | Val Loss: 2.7098 | Train Acc: 0.6100 | Val Acc: 0.3600 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.7460 | Val Loss: 2.2383 | Train Acc: 0.6867 | Val Acc: 0.4100 | LR: 1.00e-03\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "class ImageClassifier:\n",
    "    def __init__(self, device='cuda:0'):\n",
    "        self.device = torch.device(device)\n",
    "        self._setup_device()\n",
    "    \n",
    "    def _setup_device(self):\n",
    "        \"\"\"初始化设备配置\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"CUDA is not available. Please enable a GPU.\")\n",
    "        torch.cuda.set_device(self.device)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "    # 将 ImageDataset 定义在类作用域内\n",
    "    class ImageDataset(Dataset):\n",
    "        def __init__(self, file_paths, labels, transform=None):\n",
    "            self.file_paths = file_paths\n",
    "            self.labels = labels\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.file_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image = Image.open(self.file_paths[idx]).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, self.labels[idx]\n",
    "        \n",
    "    def load_data(self, root_dir, target_folder, num_classes_to_select=None, \n",
    "             limit_per_class=None, use_all_images=False):\n",
    "        \"\"\"\n",
    "        修正后的参数逻辑:\n",
    "        :param num_classes_to_select: 选择训练的类别数量\n",
    "        :param limit_per_class: 每个类别的最大图片数\n",
    "        \"\"\"\n",
    "        # 获取所有有效类别\n",
    "        all_classes = [d.name for d in os.scandir(root_dir) if d.is_dir()]\n",
    "        if not all_classes:\n",
    "            raise ValueError(f\"No valid classes found in {root_dir}\")\n",
    "\n",
    "        # 类别选择验证逻辑\n",
    "        if num_classes_to_select is not None:\n",
    "            if not isinstance(num_classes_to_select, int) or num_classes_to_select <= 0:\n",
    "                raise ValueError(\"num_classes_to_select must be a positive integer\")\n",
    "            \n",
    "            # 当请求类别数超过实际数量时自动修正\n",
    "            if num_classes_to_select > len(all_classes):\n",
    "                print(f\"Warning: Requested {num_classes_to_select} classes but only {len(all_classes)} available. Using all classes.\")\n",
    "                num_classes_to_select = len(all_classes)\n",
    "            \n",
    "            selected_classes = random.sample(all_classes, num_classes_to_select)\n",
    "            print(f\"Randomly selected {num_classes_to_select} classes from {len(all_classes)} total classes\")\n",
    "        else:\n",
    "            selected_classes = all_classes\n",
    "            num_classes_to_select = len(all_classes)  # 保持参数记录准确\n",
    "\n",
    "        # 处理每个类别的图片数量\n",
    "        class_mapping = {cls: idx for idx, cls in enumerate(selected_classes)}\n",
    "        file_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        for cls_name in selected_classes:\n",
    "            target_path = os.path.join(root_dir, cls_name, target_folder)\n",
    "            if not os.path.exists(target_path):\n",
    "                print(f\"Warning: Missing {target_folder} in {cls_name}\")\n",
    "                continue\n",
    "                \n",
    "            # 获取所有图片文件\n",
    "            images = [\n",
    "                os.path.join(target_path, f) \n",
    "                for f in os.listdir(target_path) \n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "            ]\n",
    "            \n",
    "            # 图片数量控制逻辑\n",
    "            if limit_per_class and not use_all_images:\n",
    "                if len(images) < limit_per_class:\n",
    "                    print(f\"Warning: Class {cls_name} only has {len(images)} images (requested {limit_per_class})\")\n",
    "                images = images[:limit_per_class]  # 安全截断\n",
    "                \n",
    "            file_paths.extend(images)\n",
    "            labels.extend([class_mapping[cls_name]] * len(images))\n",
    "\n",
    "        print(f\"\\nLoaded {len(class_mapping)} classes from {target_folder}\")\n",
    "        self._print_class_stats(labels, class_mapping)\n",
    "        return file_paths, labels, class_mapping\n",
    "\n",
    "    def _print_class_stats(self, labels, class_mapping):\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        for cls_idx, count in zip(unique, counts):\n",
    "            cls_name = list(class_mapping.keys())[cls_idx]\n",
    "            print(f\"Class {cls_idx} ({cls_name}): {count} samples\")\n",
    "\n",
    "    def create_model(self, num_classes, model_name='resnet18'):\n",
    "        \"\"\"创建可配置的模型\"\"\"\n",
    "        model_map = {\n",
    "            'resnet18': models.resnet18,\n",
    "            'resnet50': models.resnet50,\n",
    "            'efficientnet_b0': models.efficientnet_b0\n",
    "        }\n",
    "        \n",
    "        model = model_map[model_name](weights='DEFAULT')\n",
    "        if 'resnet' in model_name:\n",
    "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        elif 'efficientnet' in model_name:\n",
    "            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "            \n",
    "        return model.to(self.device)\n",
    "\n",
    "    def train(self, config):\n",
    "        \"\"\"完整的训练流程\"\"\"\n",
    "        # 初始化配置\n",
    "        config.setdefault('save_dir', 'model_results')\n",
    "        config.setdefault('model_name', 'resnet18')\n",
    "        config.setdefault('input_size', 224)\n",
    "        config.setdefault('patience', 5)\n",
    "\n",
    "        # 必须先加载数据以获取 class_mapping\n",
    "        file_paths, labels, class_mapping = self.load_data(\n",
    "            root_dir=config['root_dir'],\n",
    "            target_folder=config['target_folder'],\n",
    "            num_classes_to_select=config.get('num_classes_to_select'),\n",
    "            limit_per_class=config.get('limit_per_class'),\n",
    "            use_all_images=config.get('use_all_images', False)\n",
    "        )\n",
    "\n",
    "        # 创建唯一保存目录\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        folder_name = (\n",
    "            f\"{config['target_folder']}_\"\n",
    "            f\"{timestamp}-\"\n",
    "            f\"{len(class_mapping)}_classes\"\n",
    "        )\n",
    "        if config.get('num_classes_to_select'):\n",
    "            folder_name += f\"-selected_{config['num_classes_to_select']}\"\n",
    "        if config.get('limit_per_class'):\n",
    "            folder_name += f\"-limit_{config['limit_per_class']}\"\n",
    "        save_path = os.path.join(config['save_dir'], folder_name)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # 数据加载\n",
    "        file_paths, labels, class_mapping = self.load_data(\n",
    "            root_dir=config['root_dir'],\n",
    "            target_folder=config['target_folder'],\n",
    "            num_classes_to_select=config.get('num_classes_to_select'),\n",
    "            limit_per_class=config.get('limit_per_class'),\n",
    "            use_all_images=config.get('use_all_images', False)\n",
    "        )\n",
    "\n",
    "        # 数据预处理\n",
    "        train_transform, test_transform = self._get_transforms(config['input_size'])\n",
    "        \n",
    "        # 数据集划分\n",
    "        train_loader, val_loader, test_loader = self._create_data_loaders(\n",
    "            file_paths, labels, \n",
    "            train_transform, test_transform,\n",
    "            config['batch_size']\n",
    "        )\n",
    "        \n",
    "        # 模型初始化\n",
    "        model = self.create_model(len(class_mapping), config['model_name'])\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scaler = torch.amp.GradScaler()\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n",
    "        \n",
    "        # 训练循环\n",
    "        history = self._train_loop(\n",
    "            model, optimizer, criterion, scheduler, scaler,\n",
    "            train_loader, val_loader,\n",
    "            config['epochs'], config['patience'],\n",
    "            save_path  # 传递保存路径\n",
    "        )\n",
    "        \n",
    "        # 最终评估\n",
    "        test_acc, cm = self.evaluate(model, test_loader)\n",
    "        \n",
    "        # 保存结果\n",
    "        self._save_results(\n",
    "            model, history, cm, class_mapping,\n",
    "            config, test_acc, save_path, labels\n",
    "        )\n",
    "\n",
    "    def _get_transforms(self, input_size):\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return train_transform, test_transform\n",
    "\n",
    "    def _create_data_loaders(self, file_paths, labels, train_trans, test_trans, batch_size):\n",
    "        # 数据集划分（60-20-20）\n",
    "        train_p, test_p, train_l, test_l = train_test_split(\n",
    "            file_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "        )\n",
    "        train_p, val_p, train_l, val_l = train_test_split(\n",
    "            train_p, train_l, test_size=0.25, stratify=train_l, random_state=42\n",
    "        )\n",
    "        \n",
    "        return (\n",
    "            DataLoader(self.ImageDataset(train_p, train_l, train_trans), \n",
    "                      batch_size, shuffle=True, pin_memory=True),\n",
    "            DataLoader(self.ImageDataset(val_p, val_l, test_trans), \n",
    "                      batch_size, shuffle=False, pin_memory=True),\n",
    "            DataLoader(self.ImageDataset(test_p, test_l, test_trans), \n",
    "                      batch_size, shuffle=False, pin_memory=True)\n",
    "        )\n",
    "\n",
    "    def _train_loop(self, model, optimizer, criterion, scheduler, scaler, \n",
    "                   train_loader, val_loader, epochs, patience, save_path):\n",
    "        history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_acc': [], 'val_acc': [],\n",
    "            'lr': []\n",
    "        }\n",
    "        best_acc = 0.0\n",
    "        early_stop_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 训练阶段\n",
    "            model.train()\n",
    "            train_loss, correct, total = 0.0, 0, 0\n",
    "            \n",
    "            for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # 验证阶段\n",
    "            val_acc, val_loss = self._validate(model, criterion, val_loader)\n",
    "            \n",
    "            # 记录指标\n",
    "            train_acc = correct / total\n",
    "            history['train_loss'].append(train_loss/len(train_loader))\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # 学习率调整\n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "            # 早停机制和模型保存\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                early_stop_counter = 0\n",
    "                # 保存到指定路径\n",
    "                model_save_path = os.path.join(save_path, \"best_model.pth\")\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: \"\n",
    "                  f\"Train Loss: {history['train_loss'][-1]:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Acc: {val_acc:.4f} | \"\n",
    "                  f\"LR: {history['lr'][-1]:.2e}\")\n",
    "        \n",
    "         # 加载指定路径的最佳模型\n",
    "        model.load_state_dict(torch.load(os.path.join(save_path, \"best_model.pth\")))\n",
    "        return history\n",
    "\n",
    "    def _validate(self, model, criterion, val_loader):\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "        return correct / total, val_loss / len(val_loader)\n",
    "\n",
    "    def evaluate(self, model, test_loader):\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        test_acc = correct / total\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        return test_acc, cm\n",
    "\n",
    "    def _save_results(self, model, history, cm, class_mapping, config, test_acc, save_path, labels):\n",
    "        # 生成带特征文件夹信息的目录名\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        folder_name = (\n",
    "            f\"{config['target_folder']}_\"\n",
    "            f\"{timestamp}-\"\n",
    "            f\"{len(class_mapping)}_classes\"\n",
    "        )\n",
    "        \n",
    "        # 添加参数标记\n",
    "        if config.get('num_classes_to_select'):\n",
    "            folder_name += f\"-selected_{config['num_classes_to_select']}\"\n",
    "        if config.get('limit_per_class'):\n",
    "            folder_name += f\"-limit_{config['limit_per_class']}\"\n",
    "        elif not config.get('use_all_images'):\n",
    "            folder_name += \"-default_limit\"\n",
    "        \n",
    "        save_path = os.path.join(config['save_dir'], folder_name)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # 保存最佳模型到指定路径\n",
    "        model_save_path = os.path.join(save_path, \"best_model.pth\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "        # 保存训练曲线\n",
    "        self._plot_training_curves(history, save_path)\n",
    "        \n",
    "        # 保存混淆矩阵\n",
    "        self._plot_confusion_matrix(cm, class_mapping, save_path)\n",
    "        \n",
    "        # 保存报告\n",
    "        self._save_report(config, history, test_acc, class_mapping, save_path, labels)\n",
    "\n",
    "    def _plot_training_curves(self, history, save_path):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['train_loss'], label='Train')\n",
    "        plt.plot(history['val_loss'], label='Validation')\n",
    "        plt.title('Loss Curves')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['train_acc'], label='Train')\n",
    "        plt.plot(history['val_acc'], label='Validation')\n",
    "        plt.title('Accuracy Curves')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "         # 保存并关闭\n",
    "        plt.savefig(os.path.join(save_path, \"training_curves.png\"), bbox_inches='tight')\n",
    "        plt.close()  # 关键：防止内存泄漏\n",
    "\n",
    "    def _plot_confusion_matrix(self, cm, class_mapping, save_path):\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=class_mapping.keys(),\n",
    "                    yticklabels=class_mapping.keys())\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "\n",
    "        # 保存并关闭\n",
    "        plt.savefig(os.path.join(save_path, \"confusion_matrix.png\"), bbox_inches='tight')\n",
    "        plt.close()  # 关键：确保保存完成\n",
    "\n",
    "    def _save_report(self, config, history, test_acc, class_mapping, save_path, labels):\n",
    "        with open(os.path.join(save_path, \"results.txt\"), \"w\") as f:\n",
    "            f.write(\"=== Experiment Summary ===\\n\")\n",
    "            f.write(f\"Feature Folder: {config['target_folder']}\\n\")  # 新增特征目录信息\n",
    "            f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Total Classes: {len(class_mapping)}\\n\")\n",
    "            f.write(f\"Best Train Accuracy: {max(history['train_acc']):.4f}\\n\")\n",
    "            f.write(f\"Best Val Accuracy: {max(history['val_acc']):.4f}\\n\")\n",
    "            f.write(f\"Final Test Accuracy: {test_acc:.4f}\\n\\n\")\n",
    "            \n",
    "            f.write(\"=== Training Parameters ===\\n\")\n",
    "            f.write(f\"num_classes_to_select: {config.get('num_classes_to_select', 'All')}\\n\")\n",
    "            f.write(f\"limit_per_class: {config.get('limit_per_class', 'No limit')}\\n\")\n",
    "            f.write(f\"use_all_images: {config.get('use_all_images', False)}\\n\")\n",
    "            f.write(f\"batch_size: {config['batch_size']}\\n\")\n",
    "            f.write(f\"epochs: {config['epochs']}\\n\")\n",
    "            f.write(f\"learning_rate: {config['lr']}\\n\")\n",
    "            f.write(f\"input_size: {config['input_size']}\\n\")\n",
    "            f.write(f\"model_name: {config['model_name']}\\n\\n\")\n",
    "            \n",
    "            # 按epoch记录详细数据\n",
    "            f.write(\"=== Epoch-wise Results ===\\n\")\n",
    "            f.write(\"Epoch | Train Acc | Val Acc | Learning Rate\\n\")\n",
    "            f.write(\"--------------------------------------------\\n\")\n",
    "            for epoch, (train_acc, val_acc, lr) in enumerate(zip(\n",
    "                history['train_acc'], \n",
    "                history['val_acc'],\n",
    "                history['lr']\n",
    "            )):\n",
    "                f.write(f\"{epoch+1:5d} | {train_acc:.4f}   | {val_acc:.4f}  | {lr:.2e}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"=== Class Distribution ===\\n\")\n",
    "            # 使用传入的labels直接统计\n",
    "            unique, counts = np.unique(labels, return_counts=True)\n",
    "            for cls_idx, count in zip(unique, counts):\n",
    "                cls_name = [k for k, v in class_mapping.items() if v == cls_idx][0]\n",
    "                f.write(f\"Class {cls_idx} ({cls_name}): {count} samples\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = ImageClassifier()\n",
    "    \n",
    "    config = {\n",
    "        'root_dir': \"../../IQ_signal_plots\",\n",
    "        'target_folder': \"scatter_plots\",  # 可替换为其他特征文件夹\n",
    "        'save_dir': \"training_results\",\n",
    "        'batch_size': 256,\n",
    "        'epochs': 5,\n",
    "        'lr': 0.001,\n",
    "        'num_classes_to_select': 5,      # 选择训练多少个类\n",
    "        'limit_per_class': 100,           # 每类最大样本数\n",
    "        'use_all_images': False,          # 是否忽略limit_per_class\n",
    "        'input_size': 224,\n",
    "        'model_name': \"resnet18\",\n",
    "        'patience': 5\n",
    "    }\n",
    "    \n",
    "    classifier.train(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MW-RFF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
