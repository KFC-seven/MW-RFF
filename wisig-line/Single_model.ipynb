{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Randomly selected 10 classes from 150 total classes\n",
      "\n",
      "Loaded 10 classes from trajectory_pos\n",
      "Class 0 (6-1): 100 samples\n",
      "Class 1 (2-12): 100 samples\n",
      "Class 2 (5-16): 100 samples\n",
      "Class 3 (20-7): 100 samples\n",
      "Class 4 (3-13): 100 samples\n",
      "Class 5 (2-20): 100 samples\n",
      "Class 6 (7-14): 100 samples\n",
      "Class 7 (13-3): 100 samples\n",
      "Class 8 (8-3): 100 samples\n",
      "Class 9 (19-6): 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 3/3 [00:08<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.4078 | Val Loss: 2.4562 | Train Acc: 0.1350 | Val Acc: 0.1150 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 3/3 [00:07<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 2.1021 | Val Loss: 4.9783 | Train Acc: 0.2283 | Val Acc: 0.1300 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 3/3 [00:08<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 1.8167 | Val Loss: 4.9747 | Train Acc: 0.3233 | Val Acc: 0.1250 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 3/3 [00:08<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 1.5729 | Val Loss: 3.2965 | Train Acc: 0.4417 | Val Acc: 0.1550 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 3/3 [00:07<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 1.3991 | Val Loss: 3.4089 | Train Acc: 0.5133 | Val Acc: 0.1700 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23268\\1382172929.py:289: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "class ImageClassifier:\n",
    "    def __init__(self, device='cuda:0'):\n",
    "        self.device = torch.device(device)\n",
    "        self._setup_device()\n",
    "    \n",
    "    def _setup_device(self):\n",
    "        \"\"\"初始化设备配置\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"CUDA is not available. Please enable a GPU.\")\n",
    "        torch.cuda.set_device(self.device)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "    # 将 ImageDataset 定义在类作用域内\n",
    "    class ImageDataset(Dataset):\n",
    "        def __init__(self, file_paths, labels, transform=None):\n",
    "            self.file_paths = file_paths\n",
    "            self.labels = labels\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.file_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image = Image.open(self.file_paths[idx]).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, self.labels[idx]\n",
    "        \n",
    "    def load_data(self, root_dir, target_folder, num_classes_to_select=None, \n",
    "             limit_per_class=None, use_all_images=False):\n",
    "        \"\"\"\n",
    "        修正后的参数逻辑:\n",
    "        :param num_classes_to_select: 选择训练的类别数量\n",
    "        :param limit_per_class: 每个类别的最大图片数\n",
    "        \"\"\"\n",
    "        # 获取所有有效类别\n",
    "        all_classes = [d.name for d in os.scandir(root_dir) if d.is_dir()]\n",
    "        if not all_classes:\n",
    "            raise ValueError(f\"No valid classes found in {root_dir}\")\n",
    "\n",
    "        # 类别选择验证逻辑\n",
    "        if num_classes_to_select is not None:\n",
    "            if not isinstance(num_classes_to_select, int) or num_classes_to_select <= 0:\n",
    "                raise ValueError(\"num_classes_to_select must be a positive integer\")\n",
    "            \n",
    "            # 当请求类别数超过实际数量时自动修正\n",
    "            if num_classes_to_select > len(all_classes):\n",
    "                print(f\"Warning: Requested {num_classes_to_select} classes but only {len(all_classes)} available. Using all classes.\")\n",
    "                num_classes_to_select = len(all_classes)\n",
    "            \n",
    "            selected_classes = random.sample(all_classes, num_classes_to_select)\n",
    "            print(f\"Randomly selected {num_classes_to_select} classes from {len(all_classes)} total classes\")\n",
    "        else:\n",
    "            selected_classes = all_classes\n",
    "            num_classes_to_select = len(all_classes)  # 保持参数记录准确\n",
    "\n",
    "        # 处理每个类别的图片数量\n",
    "        class_mapping = {cls: idx for idx, cls in enumerate(selected_classes)}\n",
    "        file_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        for cls_name in selected_classes:\n",
    "            target_path = os.path.join(root_dir, cls_name, target_folder)\n",
    "            if not os.path.exists(target_path):\n",
    "                print(f\"Warning: Missing {target_folder} in {cls_name}\")\n",
    "                continue\n",
    "                \n",
    "            # 获取所有图片文件\n",
    "            images = [\n",
    "                os.path.join(target_path, f) \n",
    "                for f in os.listdir(target_path) \n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "            ]\n",
    "            \n",
    "            # 图片数量控制逻辑\n",
    "            if limit_per_class and not use_all_images:\n",
    "                if len(images) < limit_per_class:\n",
    "                    print(f\"Warning: Class {cls_name} only has {len(images)} images (requested {limit_per_class})\")\n",
    "                images = images[:limit_per_class]  # 安全截断\n",
    "                \n",
    "            file_paths.extend(images)\n",
    "            labels.extend([class_mapping[cls_name]] * len(images))\n",
    "\n",
    "        print(f\"\\nLoaded {len(class_mapping)} classes from {target_folder}\")\n",
    "        self._print_class_stats(labels, class_mapping)\n",
    "        return file_paths, labels, class_mapping\n",
    "\n",
    "    def _print_class_stats(self, labels, class_mapping):\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        for cls_idx, count in zip(unique, counts):\n",
    "            cls_name = list(class_mapping.keys())[cls_idx]\n",
    "            print(f\"Class {cls_idx} ({cls_name}): {count} samples\")\n",
    "\n",
    "    def create_model(self, num_classes, model_name='resnet18'):\n",
    "        \"\"\"创建可配置的模型\"\"\"\n",
    "        model_map = {\n",
    "            'resnet18': models.resnet18,\n",
    "            'resnet50': models.resnet50,\n",
    "            'efficientnet_b0': models.efficientnet_b0\n",
    "        }\n",
    "        \n",
    "        model = model_map[model_name](weights='DEFAULT')\n",
    "        if 'resnet' in model_name:\n",
    "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        elif 'efficientnet' in model_name:\n",
    "            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "            \n",
    "        return model.to(self.device)\n",
    "\n",
    "    def train(self, config):\n",
    "        \"\"\"\n",
    "        完整的训练流程\n",
    "        :param config: 包含以下键的配置字典:\n",
    "            - root_dir: 数据根目录\n",
    "            - target_folder: 目标特征文件夹名称\n",
    "            - save_dir: 结果保存目录\n",
    "            - batch_size: 批大小\n",
    "            - epochs: 训练轮数\n",
    "            - lr: 学习率\n",
    "            - input_size: 输入尺寸\n",
    "            - limit_per_class: 每类最大样本数\n",
    "            - model_name: 模型名称\n",
    "            - patience: 早停耐心值\n",
    "        \"\"\"\n",
    "        # 初始化配置\n",
    "        config.setdefault('save_dir', 'model_results')\n",
    "        config.setdefault('model_name', 'resnet18')\n",
    "        config.setdefault('input_size', 224)\n",
    "        config.setdefault('patience', 5)\n",
    "        \n",
    "        # 数据加载\n",
    "        file_paths, labels, class_mapping = self.load_data(\n",
    "        root_dir=config['root_dir'],\n",
    "        target_folder=config['target_folder'],\n",
    "        num_classes_to_select=config.get('num_classes_to_select'),  \n",
    "        limit_per_class=config.get('limit_per_class'),  \n",
    "        use_all_images=config.get('use_all_images', False) \n",
    "    )\n",
    "        \n",
    "        # 数据预处理\n",
    "        train_transform, test_transform = self._get_transforms(config['input_size'])\n",
    "        \n",
    "        # 数据集划分\n",
    "        train_loader, val_loader, test_loader = self._create_data_loaders(\n",
    "            file_paths, labels, \n",
    "            train_transform, test_transform,\n",
    "            config['batch_size']\n",
    "        )\n",
    "        \n",
    "        # 模型初始化\n",
    "        model = self.create_model(len(class_mapping), config['model_name'])\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scaler = torch.amp.GradScaler()\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n",
    "        \n",
    "        # 训练循环\n",
    "        history = self._train_loop(\n",
    "            model, optimizer, criterion, scheduler, scaler,\n",
    "            train_loader, val_loader,\n",
    "            config['epochs'], config['patience']\n",
    "        )\n",
    "        \n",
    "        # 最终评估\n",
    "        test_acc, cm = self.evaluate(model, test_loader)\n",
    "        \n",
    "        # 保存结果\n",
    "        self._save_results(\n",
    "            model, history, cm, class_mapping,\n",
    "            config, test_acc\n",
    "        )\n",
    "\n",
    "    def _get_transforms(self, input_size):\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return train_transform, test_transform\n",
    "\n",
    "    def _create_data_loaders(self, file_paths, labels, train_trans, test_trans, batch_size):\n",
    "        # 数据集划分（60-20-20）\n",
    "        train_p, test_p, train_l, test_l = train_test_split(\n",
    "            file_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "        )\n",
    "        train_p, val_p, train_l, val_l = train_test_split(\n",
    "            train_p, train_l, test_size=0.25, stratify=train_l, random_state=42\n",
    "        )\n",
    "        \n",
    "        return (\n",
    "            DataLoader(self.ImageDataset(train_p, train_l, train_trans), \n",
    "                      batch_size, shuffle=True, pin_memory=True),\n",
    "            DataLoader(self.ImageDataset(val_p, val_l, test_trans), \n",
    "                      batch_size, shuffle=False, pin_memory=True),\n",
    "            DataLoader(self.ImageDataset(test_p, test_l, test_trans), \n",
    "                      batch_size, shuffle=False, pin_memory=True)\n",
    "        )\n",
    "\n",
    "    def _train_loop(self, model, optimizer, criterion, scheduler, scaler, \n",
    "                   train_loader, val_loader, epochs, patience):\n",
    "        history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_acc': [], 'val_acc': [],\n",
    "            'lr': []\n",
    "        }\n",
    "        best_acc = 0.0\n",
    "        early_stop_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 训练阶段\n",
    "            model.train()\n",
    "            train_loss, correct, total = 0.0, 0, 0\n",
    "            \n",
    "            for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # 验证阶段\n",
    "            val_acc, val_loss = self._validate(model, criterion, val_loader)\n",
    "            \n",
    "            # 记录指标\n",
    "            train_acc = correct / total\n",
    "            history['train_loss'].append(train_loss/len(train_loader))\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # 学习率调整\n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "            # 早停机制\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                early_stop_counter = 0\n",
    "                torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: \"\n",
    "                  f\"Train Loss: {history['train_loss'][-1]:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Acc: {val_acc:.4f} | \"\n",
    "                  f\"LR: {history['lr'][-1]:.2e}\")\n",
    "        \n",
    "        model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "        return history\n",
    "\n",
    "    def _validate(self, model, criterion, val_loader):\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "        return correct / total, val_loss / len(val_loader)\n",
    "\n",
    "    def evaluate(self, model, test_loader):\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        test_acc = correct / total\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        return test_acc, cm\n",
    "\n",
    "    def _save_results(self, model, history, cm, class_mapping, config, test_acc):\n",
    "        # 生成带特征文件夹信息的目录名\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        folder_name = (\n",
    "            f\"{config['target_folder']}_\"  # 新增特征文件夹信息\n",
    "            f\"{timestamp}-\"\n",
    "            f\"{len(class_mapping)}_classes\"\n",
    "        )\n",
    "        \n",
    "        # 添加参数标记\n",
    "        if config.get('num_classes_to_select'):\n",
    "            folder_name += f\"-selected_{config['num_classes_to_select']}\"\n",
    "        if config.get('limit_per_class'):\n",
    "            folder_name += f\"-limit_{config['limit_per_class']}\"\n",
    "        elif not config.get('use_all_images'):\n",
    "            folder_name += \"-default_limit\"  # 明确无限制的情况\n",
    "        \n",
    "        save_path = os.path.join(config['save_dir'], folder_name)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    def _plot_training_curves(self, history, save_path):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['train_loss'], label='Train')\n",
    "        plt.plot(history['val_loss'], label='Validation')\n",
    "        plt.title('Loss Curves')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['train_acc'], label='Train')\n",
    "        plt.plot(history['val_acc'], label='Validation')\n",
    "        plt.title('Accuracy Curves')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_path, \"training_curves.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    def _plot_confusion_matrix(self, cm, class_mapping, save_path):\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=class_mapping.keys(),\n",
    "                    yticklabels=class_mapping.keys())\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_path, \"confusion_matrix.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    def _save_report(self, config, history, test_acc, class_mapping, save_path):\n",
    "        with open(os.path.join(save_path, \"results.txt\"), \"w\") as f:\n",
    "            f.write(\"=== Experiment Summary ===\\n\")\n",
    "            f.write(f\"Feature Folder: {config['target_folder']}\\n\")  # 新增特征目录信息\n",
    "            f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Total Classes: {len(class_mapping)}\\n\")\n",
    "            f.write(f\"Test Accuracy: {test_acc:.4f}\\n\\n\")\n",
    "            \n",
    "            f.write(\"=== Training Parameters ===\\n\")\n",
    "            f.write(f\"num_classes_to_select: {config.get('num_classes_to_select', 'All')}\\n\")\n",
    "            f.write(f\"limit_per_class: {config.get('limit_per_class', 'No limit')}\\n\")\n",
    "            f.write(f\"use_all_images: {config.get('use_all_images', False)}\\n\")\n",
    "            f.write(f\"batch_size: {config['batch_size']}\\n\")\n",
    "            f.write(f\"epochs: {config['epochs']}\\n\")\n",
    "            f.write(f\"learning_rate: {config['lr']}\\n\")\n",
    "            f.write(f\"input_size: {config['input_size']}\\n\")\n",
    "            f.write(f\"model_name: {config['model_name']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"=== Class Distribution ===\\n\")\n",
    "            for cls_name, idx in class_mapping.items():\n",
    "                count = sum(1 for lbl in history['all_labels'] if lbl == idx)\n",
    "                f.write(f\"Class {idx} ({cls_name}): {count} samples\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = ImageClassifier()\n",
    "    \n",
    "    config = {\n",
    "        'root_dir': \"../../IQ_signal_plots\",\n",
    "        'target_folder': \"trajectory_pos\",  # 可替换为其他特征文件夹\n",
    "        'save_dir': \"training_results\",\n",
    "        'batch_size': 256,\n",
    "        'epochs': 5,\n",
    "        'lr': 0.001,\n",
    "        'num_classes_to_select': 10,      # 选择训练多少个类\n",
    "        'limit_per_class': 100,           # 每类最大样本数\n",
    "        'use_all_images': False,          # 是否忽略limit_per_class\n",
    "        'input_size': 224,\n",
    "        'model_name': \"resnet18\",\n",
    "        'patience': 5\n",
    "    }\n",
    "    \n",
    "    classifier.train(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MW-RFF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
