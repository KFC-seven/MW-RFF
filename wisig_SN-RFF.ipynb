{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# WiSig (ManySig.pkl) - Spec + Siamese (FAST GPU PIPELINE + LOGGING)\n",
    "#\n",
    "# 保存内容：\n",
    "#  - config.txt\n",
    "#  - results.txt\n",
    "#  - fold{K}_trainlog.csv              (每 epoch 训练日志)\n",
    "#  - fold{K}_test_snr.csv              (每 fold 的 SNR sweep 结果)\n",
    "#  - test_snr_sweep.csv                (跨 folds 汇总 mean/std + fold1..foldN)\n",
    "#  - model_fold{K}.pth                 (最后/early-stop 时模型)\n",
    "#  - best_model_fold{K}.pth            (val_loss 最佳)\n",
    "#\n",
    "# 训练增强（每样本随机）：\n",
    "#  - Multipath: 指数 PDP TDL, RMS DS ~ U[5,300] ns\n",
    "#  - Doppler: v ~ U[0,120] km/h\n",
    "#  - AWGN: SNR ~ U[-40,20] dB\n",
    "#\n",
    "# 测试：\n",
    "#  - Doppler 固定 120 km/h\n",
    "#  - AWGN SNR sweep: 20,15,...,-40 dB\n",
    "#  - 默认不额外加 multipath（TEST_USE_MULTIPATH 控制）\n",
    "#\n",
    "# 加速点：\n",
    "#  - Dataset 只输出 IQ（不在 __getitem__ 做 STFT）\n",
    "#  - 增强 + STFT 全部 GPU batch\n",
    "#  - AMP 混合精度（NT-Xent 强制 fp32 防溢出）\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from data_utilities import load_compact_pkl_dataset\n",
    "\n",
    "# ----------------------------\n",
    "# 0) 全局配置\n",
    "# ----------------------------\n",
    "SEED = 42\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "USE_AMP = (DEVICE.type == \"cuda\")\n",
    "\n",
    "dataset_name = \"ManySig\"\n",
    "dataset_path = \"../ManySig.pkl/\"  # 按你工程路径\n",
    "equalized = 0\n",
    "max_sig = None  # None 全部；或限制每个 (tx,rx,date) 使用前 max_sig 条\n",
    "\n",
    "train_dates = [\"2021_03_15\"]\n",
    "test_dates  = [\"2021_03_01\"]\n",
    "\n",
    "# 训练超参\n",
    "BATCH_SIZE = 256           # 过大可能 OOM（NT-Xent 的 sim 矩阵随 batch^2）\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 0.0\n",
    "MAX_EPOCHS = 200\n",
    "N_SPLITS = 5\n",
    "LR_PATIENCE = 10\n",
    "LR_FACTOR = 0.5\n",
    "ES_PATIENCE = 30\n",
    "\n",
    "TAU = 0.05\n",
    "LAMBDA_CL = 1.0\n",
    "LAMBDA_CE = 1.0\n",
    "\n",
    "# RF 参数（与你 WiSig XFR 一致）\n",
    "FS = 20e6\n",
    "FC = 2.4e9\n",
    "\n",
    "# 训练增强\n",
    "AUG_USE_MULTIPATH = True\n",
    "AUG_USE_DOPPLER   = True\n",
    "AUG_USE_AWGN      = True\n",
    "RMS_DS_NS_RANGE = (5.0, 300.0)\n",
    "TRAIN_V_KMH_RANGE = (0.0, 120.0)\n",
    "TRAIN_SNR_DB_RANGE = (-40.0, 20.0)\n",
    "\n",
    "# 测试增强\n",
    "TEST_V_KMH_FIXED = 120.0\n",
    "TEST_USE_MULTIPATH = False\n",
    "TEST_RMS_DS_NS_RANGE = RMS_DS_NS_RANGE\n",
    "TEST_SNR_LIST = list(range(20, -45, -5))\n",
    "\n",
    "# spectrogram\n",
    "SPEC_NFFT = 128\n",
    "SPEC_WIN  = 128\n",
    "SPEC_HOP  = 16\n",
    "SPEC_SIZE = 64\n",
    "\n",
    "# multipath taps\n",
    "MAX_TAPS = 16\n",
    "\n",
    "# DataLoader workers（Windows 建议先用 0）\n",
    "NUM_WORKERS_TRAIN = 0\n",
    "NUM_WORKERS_EVAL  = 0\n",
    "\n",
    "SAVE_ROOT = \"./training_results\"\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "SCRIPT_NAME = \"WiSig_SpecSiamese_FASTGPU_SNRsweep_Doppler120\"\n",
    "\n",
    "RETURN_CM = False  # 如需 confusion matrix 可打开（更慢）\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) 工具：日志写入\n",
    "# ----------------------------\n",
    "class CSVLogger:\n",
    "    def __init__(self, path, header):\n",
    "        self.path = path\n",
    "        self.header = header\n",
    "        self._init_file()\n",
    "\n",
    "    def _init_file(self):\n",
    "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
    "        with open(self.path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.header)\n",
    "\n",
    "    def log_row(self, row):\n",
    "        with open(self.path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def write_line(path, line):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line.rstrip() + \"\\n\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) WiSig index 构建\n",
    "# ----------------------------\n",
    "def build_index_list(compact_dataset, tx_names, dates, equalized=0, max_sig=None):\n",
    "    eq_i = compact_dataset[\"equalized_list\"].index(equalized)\n",
    "\n",
    "    tx_i_list = []\n",
    "    for name in tx_names:\n",
    "        if name in compact_dataset[\"tx_list\"]:\n",
    "            tx_i_list.append(compact_dataset[\"tx_list\"].index(name))\n",
    "    tx_i_to_label = {tx_i: j for j, tx_i in enumerate(tx_i_list)}\n",
    "\n",
    "    index_list = []\n",
    "    for tx_i in tx_i_list:\n",
    "        for date in dates:\n",
    "            if date not in compact_dataset[\"capture_date_list\"]:\n",
    "                continue\n",
    "            date_i = compact_dataset[\"capture_date_list\"].index(date)\n",
    "            for rx_i in range(len(compact_dataset[\"rx_list\"])):\n",
    "                seq = compact_dataset[\"data\"][tx_i][rx_i][date_i][eq_i]\n",
    "                n = len(seq) if max_sig is None else min(len(seq), max_sig)\n",
    "                for k in range(n):\n",
    "                    index_list.append((tx_i, rx_i, date_i, k))\n",
    "    return index_list, tx_i_to_label\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Dataset：只返回 IQ（Siamese/Single）\n",
    "# ----------------------------\n",
    "class WiSigSiameseIQDataset(Dataset):\n",
    "    def __init__(self, compact_dataset, index_list, tx_i_to_label, equalized=0, max_sig=None):\n",
    "        self.ds = compact_dataset\n",
    "        self.index_list = index_list\n",
    "        self.tx_i_to_label = tx_i_to_label\n",
    "        self.eq_i = compact_dataset[\"equalized_list\"].index(equalized)\n",
    "        self.max_sig = max_sig\n",
    "\n",
    "        # (tx_i,date_i) -> rx_i -> n\n",
    "        self.len_map = defaultdict(dict)\n",
    "        for (tx_i, rx_i, date_i, _) in index_list:\n",
    "            if rx_i in self.len_map[(tx_i, date_i)]:\n",
    "                continue\n",
    "            seq = self.ds[\"data\"][tx_i][rx_i][date_i][self.eq_i]\n",
    "            n = len(seq) if max_sig is None else min(len(seq), max_sig)\n",
    "            if n > 0:\n",
    "                self.len_map[(tx_i, date_i)][rx_i] = n\n",
    "        self.rx_choices = {k: list(v.keys()) for k, v in self.len_map.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_list)\n",
    "\n",
    "    def _get_iq(self, tx_i, rx_i, date_i, k):\n",
    "        sig = self.ds[\"data\"][tx_i][rx_i][date_i][self.eq_i][k]\n",
    "        return np.asarray(sig, dtype=np.float32)  # (L,2)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tx_i, rx1_i, date_i, k1 = self.index_list[idx]\n",
    "        label = self.tx_i_to_label[tx_i]\n",
    "\n",
    "        rx_list = self.rx_choices[(tx_i, date_i)]\n",
    "        if len(rx_list) < 2:\n",
    "            rx2_i = rx1_i\n",
    "        else:\n",
    "            rx2_i = rx1_i\n",
    "            while rx2_i == rx1_i:\n",
    "                rx2_i = random.choice(rx_list)\n",
    "\n",
    "        n2 = self.len_map[(tx_i, date_i)][rx2_i]\n",
    "        k2 = k1 if k1 < n2 else random.randrange(n2)\n",
    "\n",
    "        iq1 = self._get_iq(tx_i, rx1_i, date_i, k1)\n",
    "        iq2 = self._get_iq(tx_i, rx2_i, date_i, k2)\n",
    "\n",
    "        return iq1, iq2, np.int64(label)\n",
    "\n",
    "class WiSigSingleIQDataset(Dataset):\n",
    "    def __init__(self, compact_dataset, index_list, tx_i_to_label, equalized=0):\n",
    "        self.ds = compact_dataset\n",
    "        self.index_list = index_list\n",
    "        self.tx_i_to_label = tx_i_to_label\n",
    "        self.eq_i = compact_dataset[\"equalized_list\"].index(equalized)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tx_i, rx_i, date_i, k = self.index_list[idx]\n",
    "        sig = self.ds[\"data\"][tx_i][rx_i][date_i][self.eq_i][k]\n",
    "        iq = np.asarray(sig, dtype=np.float32)\n",
    "        y = np.int64(self.tx_i_to_label[tx_i])\n",
    "        return iq, y\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) GPU batch 增强：Multipath / Doppler / AWGN\n",
    "# ----------------------------\n",
    "def _to_complex(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    return iq_b[..., 0].to(torch.float32) + 1j * iq_b[..., 1].to(torch.float32)\n",
    "\n",
    "def _from_complex(sig: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.stack([sig.real, sig.imag], dim=-1)\n",
    "\n",
    "def batch_normalize_power(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    # power: (B,1) -> scale: (B,1,1)\n",
    "    power = (iq_b[..., 0] ** 2 + iq_b[..., 1] ** 2).mean(dim=1, keepdim=True) + 1e-12\n",
    "    scale = torch.rsqrt(power).unsqueeze(-1)\n",
    "    return iq_b * scale\n",
    "\n",
    "def batch_apply_doppler(iq_b: torch.Tensor, v_kmh: torch.Tensor) -> torch.Tensor:\n",
    "    B, L, _ = iq_b.shape\n",
    "    sig = _to_complex(iq_b)\n",
    "\n",
    "    c = 3e8\n",
    "    v = v_kmh / 3.6\n",
    "    fd = (v / c) * FC  # (B,)\n",
    "    n = torch.arange(L, device=iq_b.device, dtype=torch.float32).unsqueeze(0)  # (1,L)\n",
    "    phase = torch.exp(1j * 2.0 * np.pi * fd.unsqueeze(1).to(torch.float32) * n / FS)  # (B,L)\n",
    "    sig = sig * phase\n",
    "    return _from_complex(sig)\n",
    "\n",
    "def _grouped_conv1d_real(x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    # x: (B,1,L), w: (B,1,K)\n",
    "    B, _, L = x.shape\n",
    "    _, _, K = w.shape\n",
    "    x2 = x.permute(1, 0, 2).contiguous()      # (1,B,L)\n",
    "    y2 = F.conv1d(x2, w, padding=K - 1, groups=B)  # (1,B,L+K-1)\n",
    "    return y2.squeeze(0)  # (B, L+K-1)\n",
    "\n",
    "def batch_apply_multipath(iq_b: torch.Tensor, rms_ns: torch.Tensor, max_taps: int = MAX_TAPS) -> torch.Tensor:\n",
    "    B, L, _ = iq_b.shape\n",
    "    device = iq_b.device\n",
    "\n",
    "    rms_s = rms_ns * 1e-9\n",
    "    rms_samples = (rms_s * FS).clamp(min=1e-3)  # (B,)\n",
    "\n",
    "    k = torch.arange(max_taps, device=device, dtype=torch.float32).unsqueeze(0)  # (1,K)\n",
    "    p = torch.exp(-k / rms_samples.unsqueeze(1))  # (B,K)\n",
    "    p = p / (p.sum(dim=1, keepdim=True) + 1e-12)\n",
    "\n",
    "    hr = torch.randn(B, max_taps, device=device) * torch.sqrt(p / 2.0)\n",
    "    hi = torch.randn(B, max_taps, device=device) * torch.sqrt(p / 2.0)\n",
    "\n",
    "    hpow = (hr**2 + hi**2).sum(dim=1, keepdim=True) + 1e-12\n",
    "    norm = torch.rsqrt(hpow)\n",
    "    hr = hr * norm\n",
    "    hi = hi * norm\n",
    "\n",
    "    xr = iq_b[..., 0]\n",
    "    xi = iq_b[..., 1]\n",
    "\n",
    "    xr_ = xr.unsqueeze(1)\n",
    "    xi_ = xi.unsqueeze(1)\n",
    "    hr_ = hr.unsqueeze(1)\n",
    "    hi_ = hi.unsqueeze(1)\n",
    "\n",
    "    xr_hr = _grouped_conv1d_real(xr_, hr_)\n",
    "    xi_hi = _grouped_conv1d_real(xi_, hi_)\n",
    "    xr_hi = _grouped_conv1d_real(xr_, hi_)\n",
    "    xi_hr = _grouped_conv1d_real(xi_, hr_)\n",
    "\n",
    "    yr = xr_hr - xi_hi\n",
    "    yi = xr_hi + xi_hr\n",
    "\n",
    "    yr = yr[:, :L]\n",
    "    yi = yi[:, :L]\n",
    "    return torch.stack([yr, yi], dim=-1)\n",
    "\n",
    "def batch_add_awgn(iq_b: torch.Tensor, snr_db: torch.Tensor) -> torch.Tensor:\n",
    "    B, L, _ = iq_b.shape\n",
    "    sig = _to_complex(iq_b)\n",
    "    p = (sig.real**2 + sig.imag**2).mean(dim=1) + 1e-12\n",
    "    npow = p / (10.0 ** (snr_db / 10.0))\n",
    "    std = torch.sqrt(npow / 2.0).unsqueeze(1)\n",
    "    noise = std * (torch.randn(B, L, device=iq_b.device) + 1j * torch.randn(B, L, device=iq_b.device))\n",
    "    sig = sig + noise\n",
    "    return _from_complex(sig)\n",
    "\n",
    "def augment_train_batch(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    iq_b = batch_normalize_power(iq_b)\n",
    "    B = iq_b.shape[0]\n",
    "\n",
    "    if AUG_USE_MULTIPATH:\n",
    "        rms = (RMS_DS_NS_RANGE[1] - RMS_DS_NS_RANGE[0]) * torch.rand(B, device=iq_b.device) + RMS_DS_NS_RANGE[0]\n",
    "        iq_b = batch_apply_multipath(iq_b, rms, max_taps=MAX_TAPS)\n",
    "\n",
    "    if AUG_USE_DOPPLER:\n",
    "        v = (TRAIN_V_KMH_RANGE[1] - TRAIN_V_KMH_RANGE[0]) * torch.rand(B, device=iq_b.device) + TRAIN_V_KMH_RANGE[0]\n",
    "        iq_b = batch_apply_doppler(iq_b, v)\n",
    "\n",
    "    if AUG_USE_AWGN:\n",
    "        snr = (TRAIN_SNR_DB_RANGE[1] - TRAIN_SNR_DB_RANGE[0]) * torch.rand(B, device=iq_b.device) + TRAIN_SNR_DB_RANGE[0]\n",
    "        iq_b = batch_add_awgn(iq_b, snr)\n",
    "\n",
    "    return iq_b\n",
    "\n",
    "def augment_test_batch(iq_b: torch.Tensor, snr_db: float) -> torch.Tensor:\n",
    "    iq_b = batch_normalize_power(iq_b)\n",
    "    B = iq_b.shape[0]\n",
    "\n",
    "    if TEST_USE_MULTIPATH:\n",
    "        rms = (TEST_RMS_DS_NS_RANGE[1] - TEST_RMS_DS_NS_RANGE[0]) * torch.rand(B, device=iq_b.device) + TEST_RMS_DS_NS_RANGE[0]\n",
    "        iq_b = batch_apply_multipath(iq_b, rms, max_taps=MAX_TAPS)\n",
    "\n",
    "    v = torch.full((B,), float(TEST_V_KMH_FIXED), device=iq_b.device)\n",
    "    iq_b = batch_apply_doppler(iq_b, v)\n",
    "\n",
    "    snr = torch.full((B,), float(snr_db), device=iq_b.device)\n",
    "    iq_b = batch_add_awgn(iq_b, snr)\n",
    "    return iq_b\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5) GPU batch STFT -> logmag -> resize\n",
    "# ----------------------------\n",
    "_WINDOW_CACHE = {}\n",
    "def get_hann_window(device: torch.device):\n",
    "    key = (device.type, device.index, SPEC_WIN)\n",
    "    if key not in _WINDOW_CACHE:\n",
    "        _WINDOW_CACHE[key] = torch.hann_window(SPEC_WIN, periodic=True, device=device)\n",
    "    return _WINDOW_CACHE[key]\n",
    "\n",
    "def iq_to_logspec_batch(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    sig = _to_complex(iq_b).to(torch.complex64)  # (B,L)\n",
    "    win = get_hann_window(iq_b.device)\n",
    "    S = torch.stft(\n",
    "        sig,\n",
    "        n_fft=SPEC_NFFT,\n",
    "        hop_length=SPEC_HOP,\n",
    "        win_length=SPEC_WIN,\n",
    "        window=win,\n",
    "        center=True,\n",
    "        return_complex=True\n",
    "    )  # (B,F,T)\n",
    "\n",
    "    mag = torch.abs(S) + 1e-12\n",
    "    logmag = torch.log(mag)\n",
    "\n",
    "    mu = logmag.mean(dim=(1,2), keepdim=True)\n",
    "    sd = logmag.std(dim=(1,2), keepdim=True) + 1e-6\n",
    "    logmag = (logmag - mu) / sd\n",
    "    try:\n",
    "        logmag = torch.nan_to_num(logmag, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    except Exception:\n",
    "        logmag[~torch.isfinite(logmag)] = 0.0\n",
    "\n",
    "    x = logmag.unsqueeze(1)  # (B,1,F,T)\n",
    "    x = F.interpolate(x, size=(SPEC_SIZE, SPEC_SIZE), mode=\"bilinear\", align_corners=False)\n",
    "    return x\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 6) 模型\n",
    "# ----------------------------\n",
    "class BasicBlock2D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.down = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.down is not None:\n",
    "            identity = self.down(identity)\n",
    "        return self.relu(out + identity)\n",
    "\n",
    "class SpecFeatureNet(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.b1 = BasicBlock2D(32, 32, stride=1)\n",
    "        self.b2 = BasicBlock2D(32, 32, stride=1)\n",
    "        self.b3 = BasicBlock2D(32, 64, stride=1)\n",
    "        self.b4 = BasicBlock2D(64, 64, stride=1)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(64, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)   # z\n",
    "        self.cls = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.b1(x); x = self.b2(x); x = self.b3(x); x = self.b4(x)\n",
    "        x = self.gap(x).squeeze(-1).squeeze(-1)  # (B,64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        z = self.fc2(x)\n",
    "        logits = self.cls(z)\n",
    "        return z, logits\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        z1, p1 = self.forward_once(x1)\n",
    "        if x2 is None:\n",
    "            return z1, p1\n",
    "        z2, p2 = self.forward_once(x2)\n",
    "        return z1, p1, z2, p2\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 7) 损失与评估（NT-Xent 强制 fp32）\n",
    "# ----------------------------\n",
    "def nt_xent_loss(z1: torch.Tensor, z2: torch.Tensor, tau: float = TAU) -> torch.Tensor:\n",
    "    with torch.cuda.amp.autocast(enabled=False):\n",
    "        z1 = z1.float()\n",
    "        z2 = z2.float()\n",
    "\n",
    "        N = z1.size(0)\n",
    "        z = torch.cat([z1, z2], dim=0)\n",
    "        z = F.normalize(z, dim=1)\n",
    "\n",
    "        sim = (z @ z.T) / float(tau)\n",
    "        sim.fill_diagonal_(torch.finfo(sim.dtype).min)\n",
    "\n",
    "        pos = torch.arange(2 * N, device=z.device)\n",
    "        pos = (pos + N) % (2 * N)\n",
    "\n",
    "        log_prob = sim - torch.logsumexp(sim, dim=1, keepdim=True)\n",
    "        loss = -log_prob[torch.arange(2 * N, device=z.device), pos]\n",
    "        return loss.mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_single_iq(model: SpecFeatureNet, loader: DataLoader, num_classes: int, mode: str, snr_db: float = None):\n",
    "    model.eval()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    loss_sum, nb = 0.0, 0\n",
    "    all_y, all_p = [], []\n",
    "\n",
    "    for iq, y in loader:\n",
    "        iq = iq.to(DEVICE, non_blocking=True)\n",
    "        y  = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        if mode == \"test\":\n",
    "            iq = augment_test_batch(iq, snr_db=float(snr_db))\n",
    "\n",
    "        spec = iq_to_logspec_batch(iq)\n",
    "        _, logits = model(spec, None)\n",
    "        loss = ce(logits, y)\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        nb += 1\n",
    "\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "        if RETURN_CM:\n",
    "            all_y.append(y.detach().cpu().numpy())\n",
    "            all_p.append(pred.detach().cpu().numpy())\n",
    "\n",
    "    acc = 100.0 * correct / max(total, 1)\n",
    "    cm = None\n",
    "    if RETURN_CM and total > 0:\n",
    "        all_y = np.concatenate(all_y) if all_y else np.array([])\n",
    "        all_p = np.concatenate(all_p) if all_p else np.array([])\n",
    "        cm = confusion_matrix(all_y, all_p, labels=list(range(num_classes)))\n",
    "    return (loss_sum / max(nb, 1)), acc, cm\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 8) KFold 训练 + 测试 SNR sweep（带日志保存）\n",
    "# ----------------------------\n",
    "def train_kfold_wisig_fast_with_logging(compact_dataset, tx_names):\n",
    "    train_index, tx_i_to_label = build_index_list(compact_dataset, tx_names, train_dates, equalized, max_sig)\n",
    "    test_index,  tx_i_to_label_test = build_index_list(compact_dataset, tx_names, test_dates,  equalized, max_sig)\n",
    "    if tx_i_to_label != tx_i_to_label_test:\n",
    "        raise RuntimeError(\"Train/Test TX label mapping mismatch.\")\n",
    "\n",
    "    num_classes = len(tx_i_to_label)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    save_dir = f\"{timestamp}_{SCRIPT_NAME}\"\n",
    "    save_folder = os.path.join(SAVE_ROOT, save_dir)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    results_txt = os.path.join(save_folder, \"results.txt\")\n",
    "\n",
    "    # config\n",
    "    fd_test = (TEST_V_KMH_FIXED / 3.6) / 3e8 * FC\n",
    "    with open(os.path.join(save_folder, \"config.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"DEVICE={DEVICE}\\nAMP={USE_AMP}\\n\")\n",
    "        f.write(f\"dataset_path={dataset_path}\\ndataset_name={dataset_name}\\n\")\n",
    "        f.write(f\"tx_names={tx_names}\\n\")\n",
    "        f.write(f\"train_dates={train_dates}\\n\")\n",
    "        f.write(f\"test_dates={test_dates}\\n\")\n",
    "        f.write(f\"equalized={equalized}, max_sig={max_sig}\\n\")\n",
    "        f.write(f\"num_classes={num_classes}\\n\")\n",
    "        f.write(f\"BATCH_SIZE={BATCH_SIZE}, LR={LR}, WEIGHT_DECAY={WEIGHT_DECAY}\\n\")\n",
    "        f.write(f\"MAX_EPOCHS={MAX_EPOCHS}, N_SPLITS={N_SPLITS}\\n\")\n",
    "        f.write(f\"LR_PATIENCE={LR_PATIENCE}, LR_FACTOR={LR_FACTOR}, ES_PATIENCE={ES_PATIENCE}\\n\")\n",
    "        f.write(f\"TAU={TAU}, LAMBDA_CL={LAMBDA_CL}, LAMBDA_CE={LAMBDA_CE}\\n\")\n",
    "        f.write(f\"FS={FS}, FC={FC}\\n\")\n",
    "        f.write(f\"AUG_USE_MULTIPATH={AUG_USE_MULTIPATH}, RMS_DS_NS_RANGE={RMS_DS_NS_RANGE}, MAX_TAPS={MAX_TAPS}\\n\")\n",
    "        f.write(f\"AUG_USE_DOPPLER={AUG_USE_DOPPLER}, TRAIN_V_KMH_RANGE={TRAIN_V_KMH_RANGE}\\n\")\n",
    "        f.write(f\"AUG_USE_AWGN={AUG_USE_AWGN}, TRAIN_SNR_DB_RANGE={TRAIN_SNR_DB_RANGE}\\n\")\n",
    "        f.write(f\"TEST_V_KMH_FIXED={TEST_V_KMH_FIXED}, fd_test={fd_test}\\n\")\n",
    "        f.write(f\"TEST_USE_MULTIPATH={TEST_USE_MULTIPATH}, TEST_SNR_LIST={TEST_SNR_LIST}\\n\")\n",
    "        f.write(f\"SPEC_NFFT={SPEC_NFFT}, SPEC_WIN={SPEC_WIN}, SPEC_HOP={SPEC_HOP}, SPEC_SIZE={SPEC_SIZE}\\n\")\n",
    "        f.write(f\"workers(train/eval)={NUM_WORKERS_TRAIN}/{NUM_WORKERS_EVAL}\\n\")\n",
    "\n",
    "    print(f\"[INFO] Classes={num_classes} | TrainSamples={len(train_index)} | TestSamples={len(test_index)}\")\n",
    "    print(f\"[INFO] SaveFolder: {save_folder}\")\n",
    "\n",
    "    write_line(results_txt, f\"Classes={num_classes}, TrainSamples={len(train_index)}, TestSamples={len(test_index)}\")\n",
    "    write_line(results_txt, f\"TrainDates={train_dates}, TestDates={test_dates}\")\n",
    "    write_line(results_txt, f\"Train SNR~U{TRAIN_SNR_DB_RANGE}, v~U{TRAIN_V_KMH_RANGE}, multipath={AUG_USE_MULTIPATH}\")\n",
    "    write_line(results_txt, f\"Test v={TEST_V_KMH_FIXED} (fd={fd_test:.2f}Hz), SNR sweep={TEST_SNR_LIST}, TEST_USE_MULTIPATH={TEST_USE_MULTIPATH}\")\n",
    "    write_line(results_txt, f\"DEVICE={DEVICE}, AMP={USE_AMP}\")\n",
    "    write_line(results_txt, \"-\"*80)\n",
    "\n",
    "    # 复用同一个 test loader（不同 snr 在 eval 内做增强）\n",
    "    test_ds = WiSigSingleIQDataset(compact_dataset, test_index, tx_i_to_label, equalized=equalized)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS_EVAL, pin_memory=True\n",
    "    )\n",
    "\n",
    "    snr_to_accs = {snr: [] for snr in TEST_SNR_LIST}\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "    indices = np.arange(len(train_index))\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(indices), 1):\n",
    "        print(f\"\\n========== Fold {fold}/{N_SPLITS} ==========\")\n",
    "        write_line(results_txt, f\"\\n========== Fold {fold}/{N_SPLITS} ==========\")\n",
    "\n",
    "        tr_list = [train_index[i] for i in tr_idx]\n",
    "        va_list = [train_index[i] for i in va_idx]\n",
    "\n",
    "        tr_ds = WiSigSiameseIQDataset(compact_dataset, tr_list, tx_i_to_label, equalized=equalized, max_sig=max_sig)\n",
    "        va_ds = WiSigSingleIQDataset(compact_dataset, va_list, tx_i_to_label, equalized=equalized)\n",
    "\n",
    "        tr_loader = DataLoader(\n",
    "            tr_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True,\n",
    "            num_workers=NUM_WORKERS_TRAIN, pin_memory=True\n",
    "        )\n",
    "        va_loader = DataLoader(\n",
    "            va_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "            num_workers=NUM_WORKERS_EVAL, pin_memory=True\n",
    "        )\n",
    "\n",
    "        model = SpecFeatureNet(num_classes=num_classes).to(DEVICE)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            opt, mode=\"min\", factor=LR_FACTOR, patience=LR_PATIENCE\n",
    "        )\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_state = None\n",
    "        es_count = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        # fold train log\n",
    "        fold_log_path = os.path.join(save_folder, f\"fold{fold}_trainlog.csv\")\n",
    "        fold_logger = CSVLogger(\n",
    "            fold_log_path,\n",
    "            header=[\"epoch\", \"lr\", \"train_loss\", \"val_loss\", \"val_acc\", \"best_val_loss\", \"es_count\", \"epoch_time_sec\"]\n",
    "        )\n",
    "\n",
    "        for epoch in range(1, MAX_EPOCHS + 1):\n",
    "            t0 = time.time()\n",
    "            model.train()\n",
    "            loss_sum, nb = 0.0, 0\n",
    "\n",
    "            for iq1, iq2, y in tr_loader:\n",
    "                iq1 = iq1.to(DEVICE, non_blocking=True)\n",
    "                iq2 = iq2.to(DEVICE, non_blocking=True)\n",
    "                y   = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "                iq_cat = torch.cat([iq1, iq2], dim=0)    # (2B,L,2)\n",
    "                iq_cat = augment_train_batch(iq_cat)     # (2B,L,2)\n",
    "                spec_cat = iq_to_logspec_batch(iq_cat)   # (2B,1,S,S)\n",
    "                spec1, spec2 = spec_cat.chunk(2, dim=0)\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "                    z1, p1, z2, p2 = model(spec1, spec2)\n",
    "                    loss_cl = nt_xent_loss(z1, z2, tau=TAU)     # fp32 safe\n",
    "                    loss_ce = 0.5 * (ce(p1, y) + ce(p2, y))\n",
    "                    loss = LAMBDA_CL * loss_cl + LAMBDA_CE * loss_ce\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "\n",
    "                loss_sum += float(loss.item())\n",
    "                nb += 1\n",
    "\n",
    "            train_loss = loss_sum / max(nb, 1)\n",
    "            val_loss, val_acc, _ = eval_single_iq(model, va_loader, num_classes=num_classes, mode=\"val\")\n",
    "\n",
    "            prev_lr = opt.param_groups[0][\"lr\"]\n",
    "            scheduler.step(val_loss)\n",
    "            cur_lr = opt.param_groups[0][\"lr\"]\n",
    "            if cur_lr < prev_lr:\n",
    "                msg = f\"[LR DROP] {prev_lr:.2e} -> {cur_lr:.2e} (val_loss={val_loss:.4f})\"\n",
    "                print(msg)\n",
    "                write_line(results_txt, msg)\n",
    "\n",
    "            epoch_time = time.time() - t0\n",
    "            msg = (f\"Epoch {epoch:03d} | LR={cur_lr:.2e} | \"\n",
    "                   f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | ValAcc={val_acc:.2f}% | \"\n",
    "                   f\"BestValLoss={best_val_loss:.4f} | ES={es_count}/{ES_PATIENCE}\")\n",
    "            print(msg)\n",
    "            write_line(results_txt, msg)\n",
    "\n",
    "            fold_logger.log_row([epoch, cur_lr, train_loss, val_loss, val_acc, best_val_loss, es_count, epoch_time])\n",
    "\n",
    "            if val_loss < best_val_loss - 1e-6:\n",
    "                best_val_loss = val_loss\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                best_epoch = epoch\n",
    "                es_count = 0\n",
    "            else:\n",
    "                es_count += 1\n",
    "                if es_count >= ES_PATIENCE:\n",
    "                    msg = \"[INFO] Early stopping triggered.\"\n",
    "                    print(msg)\n",
    "                    write_line(results_txt, msg)\n",
    "                    break\n",
    "\n",
    "        # 保存 best 与 last\n",
    "        torch.save(model.state_dict(), os.path.join(save_folder, f\"model_fold{fold}.pth\"))\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "            torch.save(model.state_dict(), os.path.join(save_folder, f\"best_model_fold{fold}.pth\"))\n",
    "\n",
    "        write_line(results_txt, f\"[FOLD {fold}] BestEpoch={best_epoch}, BestValLoss={best_val_loss:.6f}\")\n",
    "\n",
    "        # fold test sweep（保存 fold{K}_test_snr.csv）\n",
    "        fold_test_csv = os.path.join(save_folder, f\"fold{fold}_test_snr.csv\")\n",
    "        fold_test_logger = CSVLogger(fold_test_csv, header=[\"snr_db\", \"test_loss\", \"test_acc\"])\n",
    "\n",
    "        fold_snr_acc = {}\n",
    "        for snr in TEST_SNR_LIST:\n",
    "            test_loss, test_acc, _ = eval_single_iq(model, test_loader, num_classes=num_classes, mode=\"test\", snr_db=float(snr))\n",
    "            snr_to_accs[snr].append(test_acc)\n",
    "            fold_snr_acc[snr] = test_acc\n",
    "            fold_test_logger.log_row([snr, test_loss, test_acc])\n",
    "\n",
    "        msg = \"[FOLD TEST] \" + \", \".join([f\"{snr}:{fold_snr_acc[snr]:.2f}%\" for snr in TEST_SNR_LIST])\n",
    "        print(msg)\n",
    "        write_line(results_txt, msg)\n",
    "\n",
    "    # 汇总 test sweep\n",
    "    rows = []\n",
    "    for snr in TEST_SNR_LIST:\n",
    "        arr = np.array(snr_to_accs[snr], dtype=np.float64)\n",
    "        mean = float(arr.mean()) if arr.size else 0.0\n",
    "        std  = float(arr.std())  if arr.size else 0.0\n",
    "        rows.append([snr, mean, std] + snr_to_accs[snr])\n",
    "\n",
    "    csv_path = os.path.join(save_folder, \"test_snr_sweep.csv\")\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        header = [\"snr_db\", \"acc_mean\", \"acc_std\"] + [f\"fold{i}\" for i in range(1, N_SPLITS+1)]\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    write_line(results_txt, \"\\n========== Overall Test SNR Sweep (mean±std over folds) ==========\")\n",
    "    for snr in TEST_SNR_LIST:\n",
    "        arr = np.array(snr_to_accs[snr], dtype=np.float64)\n",
    "        mean = float(arr.mean()) if arr.size else 0.0\n",
    "        std  = float(arr.std())  if arr.size else 0.0\n",
    "        write_line(results_txt, f\"SNR {snr:>3} dB | Acc {mean:.2f} ± {std:.2f}\")\n",
    "\n",
    "    print(f\"\\n[INFO] All saved in: {save_folder}\")\n",
    "    print(f\"[INFO] SNR sweep CSV: {csv_path}\")\n",
    "    return save_folder\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 9) main\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    compact_dataset = load_compact_pkl_dataset(dataset_path, dataset_name)\n",
    "\n",
    "    print(\"数据集发射机数量：\", len(compact_dataset[\"tx_list\"]), \"具体为：\", compact_dataset[\"tx_list\"])\n",
    "    print(\"数据集接收机数量：\", len(compact_dataset[\"rx_list\"]), \"具体为：\", compact_dataset[\"rx_list\"])\n",
    "    print(\"数据集采集天数：\", len(compact_dataset[\"capture_date_list\"]), \"具体为：\", compact_dataset[\"capture_date_list\"])\n",
    "\n",
    "    tx_names = compact_dataset[\"tx_list\"]  # 或指定 6TX 子集\n",
    "    train_kfold_wisig_fast_with_logging(compact_dataset, tx_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f4abb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 72 .mat files\n",
      "[INFO] txID classes: 9\n",
      "  001: 8 files\n",
      "  002: 8 files\n",
      "  003: 8 files\n",
      "  004: 8 files\n",
      "  005: 8 files\n",
      "  006: 8 files\n",
      "  007: 8 files\n",
      "  008: 8 files\n",
      "  009: 8 files\n",
      "[INFO] DMRS length stats: min=256, max=256, mode=256\n",
      "[INFO] Split mode: SAMPLE-LEVEL (force)\n",
      "[INFO] Train samples=158950, Test samples=52984\n",
      "[INFO] Train class sample counts: {0: 17698, 1: 17518, 2: 17833, 3: 17671, 4: 17600, 5: 17882, 6: 17465, 7: 17582, 8: 17701}\n",
      "[INFO] Test  class sample counts: {0: 5899, 1: 5839, 2: 5944, 3: 5891, 4: 5867, 5: 5961, 6: 5822, 7: 5860, 8: 5901}\n",
      "[INFO] DEVICE=cuda | AMP=True\n",
      "[INFO] Classes=9, TrainSamples=158950, TestSamples=52984, L=256\n",
      "[INFO] SaveFolder: ./training_results\\2026-01-25_01-46-02_LTEV_SpecSimCLR_FASTGPU_SNRsweep_Doppler120_SampleSplit\n",
      "\n",
      "========== Fold 1/5 ==========\n",
      "[FOLD 1] train_sample_counts: {0: 14158, 1: 14015, 2: 14266, 3: 14137, 4: 14080, 5: 14305, 6: 13972, 7: 14066, 8: 14161}\n",
      "[FOLD 1] val_sample_counts  : {0: 3540, 1: 3503, 2: 3567, 3: 3534, 4: 3520, 5: 3577, 6: 3493, 7: 3516, 8: 3540}\n",
      "Epoch 001 | LR=3.00e-04 | TrainLoss=8.2621 | ValLoss=2.1038 | ValAcc=19.59% | BestValLoss=inf | ES=0/30\n",
      "Epoch 002 | LR=3.00e-04 | TrainLoss=8.1213 | ValLoss=2.0348 | ValAcc=23.10% | BestValLoss=2.1038 | ES=0/30\n",
      "Epoch 003 | LR=3.00e-04 | TrainLoss=8.0121 | ValLoss=2.0047 | ValAcc=24.06% | BestValLoss=2.0348 | ES=0/30\n",
      "Epoch 004 | LR=3.00e-04 | TrainLoss=7.9538 | ValLoss=1.9616 | ValAcc=26.69% | BestValLoss=2.0047 | ES=0/30\n",
      "Epoch 005 | LR=3.00e-04 | TrainLoss=7.9112 | ValLoss=1.9364 | ValAcc=26.47% | BestValLoss=1.9616 | ES=0/30\n",
      "Epoch 006 | LR=3.00e-04 | TrainLoss=7.8678 | ValLoss=1.9358 | ValAcc=27.35% | BestValLoss=1.9364 | ES=0/30\n",
      "Epoch 007 | LR=3.00e-04 | TrainLoss=7.8280 | ValLoss=1.8839 | ValAcc=29.96% | BestValLoss=1.9358 | ES=0/30\n",
      "Epoch 008 | LR=3.00e-04 | TrainLoss=7.8062 | ValLoss=1.9060 | ValAcc=29.79% | BestValLoss=1.8839 | ES=0/30\n",
      "Epoch 009 | LR=3.00e-04 | TrainLoss=7.7730 | ValLoss=1.8548 | ValAcc=31.09% | BestValLoss=1.8839 | ES=1/30\n",
      "Epoch 010 | LR=3.00e-04 | TrainLoss=7.7558 | ValLoss=1.8867 | ValAcc=29.89% | BestValLoss=1.8548 | ES=0/30\n",
      "Epoch 011 | LR=3.00e-04 | TrainLoss=7.7378 | ValLoss=1.8384 | ValAcc=32.31% | BestValLoss=1.8548 | ES=1/30\n",
      "Epoch 012 | LR=3.00e-04 | TrainLoss=7.7110 | ValLoss=1.8769 | ValAcc=31.18% | BestValLoss=1.8384 | ES=0/30\n",
      "Epoch 013 | LR=3.00e-04 | TrainLoss=7.6965 | ValLoss=1.8014 | ValAcc=34.75% | BestValLoss=1.8384 | ES=1/30\n",
      "Epoch 014 | LR=3.00e-04 | TrainLoss=7.6803 | ValLoss=1.8117 | ValAcc=33.91% | BestValLoss=1.8014 | ES=0/30\n",
      "Epoch 015 | LR=3.00e-04 | TrainLoss=7.6614 | ValLoss=1.8038 | ValAcc=34.71% | BestValLoss=1.8014 | ES=1/30\n",
      "Epoch 016 | LR=3.00e-04 | TrainLoss=7.6428 | ValLoss=1.7739 | ValAcc=35.85% | BestValLoss=1.8014 | ES=2/30\n",
      "Epoch 017 | LR=3.00e-04 | TrainLoss=7.6469 | ValLoss=1.7318 | ValAcc=36.69% | BestValLoss=1.7739 | ES=0/30\n",
      "Epoch 018 | LR=3.00e-04 | TrainLoss=7.6245 | ValLoss=1.7390 | ValAcc=36.26% | BestValLoss=1.7318 | ES=0/30\n",
      "Epoch 019 | LR=3.00e-04 | TrainLoss=7.6196 | ValLoss=1.7658 | ValAcc=36.08% | BestValLoss=1.7318 | ES=1/30\n",
      "Epoch 020 | LR=3.00e-04 | TrainLoss=7.5887 | ValLoss=1.7099 | ValAcc=38.37% | BestValLoss=1.7318 | ES=2/30\n",
      "Epoch 021 | LR=3.00e-04 | TrainLoss=7.5918 | ValLoss=1.7348 | ValAcc=36.92% | BestValLoss=1.7099 | ES=0/30\n",
      "Epoch 022 | LR=3.00e-04 | TrainLoss=7.5792 | ValLoss=1.7406 | ValAcc=37.48% | BestValLoss=1.7099 | ES=1/30\n",
      "Epoch 023 | LR=3.00e-04 | TrainLoss=7.5784 | ValLoss=1.6891 | ValAcc=39.00% | BestValLoss=1.7099 | ES=2/30\n",
      "Epoch 024 | LR=3.00e-04 | TrainLoss=7.5736 | ValLoss=1.7296 | ValAcc=37.38% | BestValLoss=1.6891 | ES=0/30\n",
      "Epoch 025 | LR=3.00e-04 | TrainLoss=7.5643 | ValLoss=1.7264 | ValAcc=37.54% | BestValLoss=1.6891 | ES=1/30\n",
      "Epoch 026 | LR=3.00e-04 | TrainLoss=7.5496 | ValLoss=1.7070 | ValAcc=38.88% | BestValLoss=1.6891 | ES=2/30\n",
      "Epoch 027 | LR=3.00e-04 | TrainLoss=7.5403 | ValLoss=1.7132 | ValAcc=38.25% | BestValLoss=1.6891 | ES=3/30\n",
      "Epoch 028 | LR=3.00e-04 | TrainLoss=7.5242 | ValLoss=1.6583 | ValAcc=40.27% | BestValLoss=1.6891 | ES=4/30\n",
      "Epoch 029 | LR=3.00e-04 | TrainLoss=7.5206 | ValLoss=1.6967 | ValAcc=39.04% | BestValLoss=1.6583 | ES=0/30\n",
      "Epoch 030 | LR=3.00e-04 | TrainLoss=7.5229 | ValLoss=1.6708 | ValAcc=40.70% | BestValLoss=1.6583 | ES=1/30\n",
      "Epoch 031 | LR=3.00e-04 | TrainLoss=7.5139 | ValLoss=1.6601 | ValAcc=39.76% | BestValLoss=1.6583 | ES=2/30\n",
      "Epoch 032 | LR=3.00e-04 | TrainLoss=7.5073 | ValLoss=1.7235 | ValAcc=37.83% | BestValLoss=1.6583 | ES=3/30\n",
      "Epoch 033 | LR=3.00e-04 | TrainLoss=7.4997 | ValLoss=1.6652 | ValAcc=39.84% | BestValLoss=1.6583 | ES=4/30\n",
      "Epoch 034 | LR=3.00e-04 | TrainLoss=7.4850 | ValLoss=1.6719 | ValAcc=39.48% | BestValLoss=1.6583 | ES=5/30\n",
      "Epoch 035 | LR=3.00e-04 | TrainLoss=7.4883 | ValLoss=1.6122 | ValAcc=41.93% | BestValLoss=1.6583 | ES=6/30\n",
      "Epoch 036 | LR=3.00e-04 | TrainLoss=7.4947 | ValLoss=1.6939 | ValAcc=39.95% | BestValLoss=1.6122 | ES=0/30\n",
      "Epoch 037 | LR=3.00e-04 | TrainLoss=7.4840 | ValLoss=1.6211 | ValAcc=41.62% | BestValLoss=1.6122 | ES=1/30\n",
      "Epoch 038 | LR=3.00e-04 | TrainLoss=7.4822 | ValLoss=1.7141 | ValAcc=38.47% | BestValLoss=1.6122 | ES=2/30\n",
      "Epoch 039 | LR=3.00e-04 | TrainLoss=7.4658 | ValLoss=1.6313 | ValAcc=41.53% | BestValLoss=1.6122 | ES=3/30\n",
      "Epoch 040 | LR=3.00e-04 | TrainLoss=7.4552 | ValLoss=1.6375 | ValAcc=41.56% | BestValLoss=1.6122 | ES=4/30\n",
      "Epoch 041 | LR=3.00e-04 | TrainLoss=7.4573 | ValLoss=1.6620 | ValAcc=40.14% | BestValLoss=1.6122 | ES=5/30\n",
      "Epoch 042 | LR=3.00e-04 | TrainLoss=7.4491 | ValLoss=1.6452 | ValAcc=40.76% | BestValLoss=1.6122 | ES=6/30\n",
      "Epoch 043 | LR=3.00e-04 | TrainLoss=7.4533 | ValLoss=1.6383 | ValAcc=41.26% | BestValLoss=1.6122 | ES=7/30\n",
      "Epoch 044 | LR=3.00e-04 | TrainLoss=7.4510 | ValLoss=1.6367 | ValAcc=41.24% | BestValLoss=1.6122 | ES=8/30\n",
      "Epoch 045 | LR=3.00e-04 | TrainLoss=7.4433 | ValLoss=1.5897 | ValAcc=43.17% | BestValLoss=1.6122 | ES=9/30\n",
      "Epoch 046 | LR=3.00e-04 | TrainLoss=7.4424 | ValLoss=1.6340 | ValAcc=41.70% | BestValLoss=1.5897 | ES=0/30\n",
      "Epoch 047 | LR=3.00e-04 | TrainLoss=7.4363 | ValLoss=1.5966 | ValAcc=42.62% | BestValLoss=1.5897 | ES=1/30\n",
      "Epoch 048 | LR=3.00e-04 | TrainLoss=7.4293 | ValLoss=1.6639 | ValAcc=40.53% | BestValLoss=1.5897 | ES=2/30\n",
      "Epoch 049 | LR=3.00e-04 | TrainLoss=7.4420 | ValLoss=1.6839 | ValAcc=39.21% | BestValLoss=1.5897 | ES=3/30\n",
      "Epoch 050 | LR=3.00e-04 | TrainLoss=7.4187 | ValLoss=1.6132 | ValAcc=42.31% | BestValLoss=1.5897 | ES=4/30\n",
      "Epoch 051 | LR=3.00e-04 | TrainLoss=7.4181 | ValLoss=1.6330 | ValAcc=41.32% | BestValLoss=1.5897 | ES=5/30\n",
      "Epoch 052 | LR=3.00e-04 | TrainLoss=7.4234 | ValLoss=1.6143 | ValAcc=42.03% | BestValLoss=1.5897 | ES=6/30\n",
      "Epoch 053 | LR=3.00e-04 | TrainLoss=7.4133 | ValLoss=1.6246 | ValAcc=41.26% | BestValLoss=1.5897 | ES=7/30\n",
      "Epoch 054 | LR=3.00e-04 | TrainLoss=7.4208 | ValLoss=1.6173 | ValAcc=42.23% | BestValLoss=1.5897 | ES=8/30\n",
      "Epoch 055 | LR=3.00e-04 | TrainLoss=7.4089 | ValLoss=1.7229 | ValAcc=38.69% | BestValLoss=1.5897 | ES=9/30\n",
      "[LR DROP] 3.00e-04 -> 1.50e-04 (val_loss=1.6427)\n",
      "Epoch 056 | LR=1.50e-04 | TrainLoss=7.4027 | ValLoss=1.6427 | ValAcc=42.07% | BestValLoss=1.5897 | ES=10/30\n",
      "Epoch 057 | LR=1.50e-04 | TrainLoss=7.3792 | ValLoss=1.5827 | ValAcc=43.26% | BestValLoss=1.5897 | ES=11/30\n",
      "Epoch 058 | LR=1.50e-04 | TrainLoss=7.3690 | ValLoss=1.5785 | ValAcc=43.69% | BestValLoss=1.5827 | ES=0/30\n",
      "Epoch 059 | LR=1.50e-04 | TrainLoss=7.3798 | ValLoss=1.5760 | ValAcc=43.80% | BestValLoss=1.5785 | ES=0/30\n",
      "Epoch 060 | LR=1.50e-04 | TrainLoss=7.3833 | ValLoss=1.5705 | ValAcc=44.09% | BestValLoss=1.5760 | ES=0/30\n",
      "Epoch 061 | LR=1.50e-04 | TrainLoss=7.3702 | ValLoss=1.5729 | ValAcc=44.23% | BestValLoss=1.5705 | ES=0/30\n",
      "Epoch 062 | LR=1.50e-04 | TrainLoss=7.3712 | ValLoss=1.5953 | ValAcc=42.92% | BestValLoss=1.5705 | ES=1/30\n",
      "Epoch 063 | LR=1.50e-04 | TrainLoss=7.3533 | ValLoss=1.6057 | ValAcc=42.59% | BestValLoss=1.5705 | ES=2/30\n",
      "Epoch 064 | LR=1.50e-04 | TrainLoss=7.3559 | ValLoss=1.5794 | ValAcc=43.79% | BestValLoss=1.5705 | ES=3/30\n",
      "Epoch 065 | LR=1.50e-04 | TrainLoss=7.3590 | ValLoss=1.6217 | ValAcc=42.48% | BestValLoss=1.5705 | ES=4/30\n",
      "Epoch 066 | LR=1.50e-04 | TrainLoss=7.3512 | ValLoss=1.6068 | ValAcc=42.61% | BestValLoss=1.5705 | ES=5/30\n",
      "Epoch 067 | LR=1.50e-04 | TrainLoss=7.3452 | ValLoss=1.5793 | ValAcc=43.93% | BestValLoss=1.5705 | ES=6/30\n",
      "Epoch 068 | LR=1.50e-04 | TrainLoss=7.3588 | ValLoss=1.5973 | ValAcc=42.67% | BestValLoss=1.5705 | ES=7/30\n",
      "Epoch 069 | LR=1.50e-04 | TrainLoss=7.3426 | ValLoss=1.6288 | ValAcc=42.03% | BestValLoss=1.5705 | ES=8/30\n",
      "Epoch 070 | LR=1.50e-04 | TrainLoss=7.3497 | ValLoss=1.5666 | ValAcc=44.26% | BestValLoss=1.5705 | ES=9/30\n",
      "Epoch 071 | LR=1.50e-04 | TrainLoss=7.3495 | ValLoss=1.5714 | ValAcc=44.20% | BestValLoss=1.5666 | ES=0/30\n",
      "Epoch 072 | LR=1.50e-04 | TrainLoss=7.3497 | ValLoss=1.5657 | ValAcc=44.48% | BestValLoss=1.5666 | ES=1/30\n",
      "Epoch 073 | LR=1.50e-04 | TrainLoss=7.3599 | ValLoss=1.6121 | ValAcc=42.65% | BestValLoss=1.5657 | ES=0/30\n",
      "Epoch 074 | LR=1.50e-04 | TrainLoss=7.3412 | ValLoss=1.5599 | ValAcc=44.23% | BestValLoss=1.5657 | ES=1/30\n",
      "Epoch 075 | LR=1.50e-04 | TrainLoss=7.3463 | ValLoss=1.6235 | ValAcc=41.77% | BestValLoss=1.5599 | ES=0/30\n",
      "Epoch 076 | LR=1.50e-04 | TrainLoss=7.3359 | ValLoss=1.5530 | ValAcc=44.62% | BestValLoss=1.5599 | ES=1/30\n",
      "Epoch 077 | LR=1.50e-04 | TrainLoss=7.3552 | ValLoss=1.5741 | ValAcc=43.61% | BestValLoss=1.5530 | ES=0/30\n",
      "Epoch 078 | LR=1.50e-04 | TrainLoss=7.3323 | ValLoss=1.5688 | ValAcc=44.36% | BestValLoss=1.5530 | ES=1/30\n",
      "Epoch 079 | LR=1.50e-04 | TrainLoss=7.3359 | ValLoss=1.5948 | ValAcc=43.17% | BestValLoss=1.5530 | ES=2/30\n",
      "Epoch 080 | LR=1.50e-04 | TrainLoss=7.3429 | ValLoss=1.5787 | ValAcc=43.72% | BestValLoss=1.5530 | ES=3/30\n",
      "Epoch 081 | LR=1.50e-04 | TrainLoss=7.3366 | ValLoss=1.5614 | ValAcc=44.38% | BestValLoss=1.5530 | ES=4/30\n",
      "Epoch 082 | LR=1.50e-04 | TrainLoss=7.3341 | ValLoss=1.5980 | ValAcc=43.81% | BestValLoss=1.5530 | ES=5/30\n",
      "Epoch 083 | LR=1.50e-04 | TrainLoss=7.3418 | ValLoss=1.5602 | ValAcc=44.19% | BestValLoss=1.5530 | ES=6/30\n",
      "Epoch 084 | LR=1.50e-04 | TrainLoss=7.3287 | ValLoss=1.6168 | ValAcc=42.69% | BestValLoss=1.5530 | ES=7/30\n",
      "Epoch 085 | LR=1.50e-04 | TrainLoss=7.3215 | ValLoss=1.6043 | ValAcc=43.06% | BestValLoss=1.5530 | ES=8/30\n",
      "Epoch 086 | LR=1.50e-04 | TrainLoss=7.3333 | ValLoss=1.5735 | ValAcc=44.07% | BestValLoss=1.5530 | ES=9/30\n",
      "[LR DROP] 1.50e-04 -> 7.50e-05 (val_loss=1.6043)\n",
      "Epoch 087 | LR=7.50e-05 | TrainLoss=7.3296 | ValLoss=1.6043 | ValAcc=43.00% | BestValLoss=1.5530 | ES=10/30\n",
      "Epoch 088 | LR=7.50e-05 | TrainLoss=7.3159 | ValLoss=1.5952 | ValAcc=43.56% | BestValLoss=1.5530 | ES=11/30\n",
      "Epoch 089 | LR=7.50e-05 | TrainLoss=7.3127 | ValLoss=1.5525 | ValAcc=44.67% | BestValLoss=1.5530 | ES=12/30\n",
      "Epoch 090 | LR=7.50e-05 | TrainLoss=7.3014 | ValLoss=1.5830 | ValAcc=43.77% | BestValLoss=1.5525 | ES=0/30\n",
      "Epoch 091 | LR=7.50e-05 | TrainLoss=7.3141 | ValLoss=1.5754 | ValAcc=43.94% | BestValLoss=1.5525 | ES=1/30\n",
      "Epoch 092 | LR=7.50e-05 | TrainLoss=7.3040 | ValLoss=1.5559 | ValAcc=44.79% | BestValLoss=1.5525 | ES=2/30\n",
      "Epoch 093 | LR=7.50e-05 | TrainLoss=7.3029 | ValLoss=1.5765 | ValAcc=43.82% | BestValLoss=1.5525 | ES=3/30\n",
      "Epoch 094 | LR=7.50e-05 | TrainLoss=7.3032 | ValLoss=1.5801 | ValAcc=44.16% | BestValLoss=1.5525 | ES=4/30\n",
      "Epoch 095 | LR=7.50e-05 | TrainLoss=7.3009 | ValLoss=1.5535 | ValAcc=44.60% | BestValLoss=1.5525 | ES=5/30\n",
      "Epoch 096 | LR=7.50e-05 | TrainLoss=7.3093 | ValLoss=1.5340 | ValAcc=45.57% | BestValLoss=1.5525 | ES=6/30\n",
      "Epoch 097 | LR=7.50e-05 | TrainLoss=7.3123 | ValLoss=1.5767 | ValAcc=43.99% | BestValLoss=1.5340 | ES=0/30\n",
      "Epoch 098 | LR=7.50e-05 | TrainLoss=7.3040 | ValLoss=1.5615 | ValAcc=44.69% | BestValLoss=1.5340 | ES=1/30\n",
      "Epoch 099 | LR=7.50e-05 | TrainLoss=7.3062 | ValLoss=1.5411 | ValAcc=44.95% | BestValLoss=1.5340 | ES=2/30\n",
      "Epoch 100 | LR=7.50e-05 | TrainLoss=7.3026 | ValLoss=1.5360 | ValAcc=45.22% | BestValLoss=1.5340 | ES=3/30\n",
      "Epoch 101 | LR=7.50e-05 | TrainLoss=7.3036 | ValLoss=1.5838 | ValAcc=43.43% | BestValLoss=1.5340 | ES=4/30\n",
      "Epoch 102 | LR=7.50e-05 | TrainLoss=7.3041 | ValLoss=1.5593 | ValAcc=44.79% | BestValLoss=1.5340 | ES=5/30\n",
      "Epoch 103 | LR=7.50e-05 | TrainLoss=7.3060 | ValLoss=1.5467 | ValAcc=45.06% | BestValLoss=1.5340 | ES=6/30\n",
      "Epoch 104 | LR=7.50e-05 | TrainLoss=7.2998 | ValLoss=1.5601 | ValAcc=44.50% | BestValLoss=1.5340 | ES=7/30\n",
      "Epoch 105 | LR=7.50e-05 | TrainLoss=7.3110 | ValLoss=1.5773 | ValAcc=43.99% | BestValLoss=1.5340 | ES=8/30\n",
      "Epoch 106 | LR=7.50e-05 | TrainLoss=7.2914 | ValLoss=1.5612 | ValAcc=44.59% | BestValLoss=1.5340 | ES=9/30\n",
      "[LR DROP] 7.50e-05 -> 3.75e-05 (val_loss=1.5660)\n",
      "Epoch 107 | LR=3.75e-05 | TrainLoss=7.3055 | ValLoss=1.5660 | ValAcc=44.33% | BestValLoss=1.5340 | ES=10/30\n",
      "Epoch 108 | LR=3.75e-05 | TrainLoss=7.2870 | ValLoss=1.5525 | ValAcc=44.84% | BestValLoss=1.5340 | ES=11/30\n",
      "Epoch 109 | LR=3.75e-05 | TrainLoss=7.2871 | ValLoss=1.5532 | ValAcc=44.72% | BestValLoss=1.5340 | ES=12/30\n",
      "Epoch 110 | LR=3.75e-05 | TrainLoss=7.2907 | ValLoss=1.5514 | ValAcc=45.01% | BestValLoss=1.5340 | ES=13/30\n",
      "Epoch 111 | LR=3.75e-05 | TrainLoss=7.2984 | ValLoss=1.5581 | ValAcc=44.83% | BestValLoss=1.5340 | ES=14/30\n",
      "Epoch 112 | LR=3.75e-05 | TrainLoss=7.2829 | ValLoss=1.5561 | ValAcc=44.92% | BestValLoss=1.5340 | ES=15/30\n",
      "Epoch 113 | LR=3.75e-05 | TrainLoss=7.2888 | ValLoss=1.5539 | ValAcc=44.94% | BestValLoss=1.5340 | ES=16/30\n",
      "Epoch 114 | LR=3.75e-05 | TrainLoss=7.2823 | ValLoss=1.5535 | ValAcc=44.74% | BestValLoss=1.5340 | ES=17/30\n",
      "Epoch 115 | LR=3.75e-05 | TrainLoss=7.2691 | ValLoss=1.5389 | ValAcc=45.50% | BestValLoss=1.5340 | ES=18/30\n",
      "Epoch 116 | LR=3.75e-05 | TrainLoss=7.2875 | ValLoss=1.5471 | ValAcc=45.06% | BestValLoss=1.5340 | ES=19/30\n",
      "Epoch 117 | LR=3.75e-05 | TrainLoss=7.2807 | ValLoss=1.5495 | ValAcc=45.10% | BestValLoss=1.5340 | ES=20/30\n",
      "[LR DROP] 3.75e-05 -> 1.87e-05 (val_loss=1.5469)\n",
      "Epoch 118 | LR=1.87e-05 | TrainLoss=7.2858 | ValLoss=1.5469 | ValAcc=45.21% | BestValLoss=1.5340 | ES=21/30\n",
      "Epoch 119 | LR=1.87e-05 | TrainLoss=7.2897 | ValLoss=1.5563 | ValAcc=44.84% | BestValLoss=1.5340 | ES=22/30\n",
      "Epoch 120 | LR=1.87e-05 | TrainLoss=7.2724 | ValLoss=1.5384 | ValAcc=45.45% | BestValLoss=1.5340 | ES=23/30\n",
      "Epoch 121 | LR=1.87e-05 | TrainLoss=7.2734 | ValLoss=1.5458 | ValAcc=45.16% | BestValLoss=1.5340 | ES=24/30\n",
      "Epoch 122 | LR=1.87e-05 | TrainLoss=7.2739 | ValLoss=1.5401 | ValAcc=45.33% | BestValLoss=1.5340 | ES=25/30\n",
      "Epoch 123 | LR=1.87e-05 | TrainLoss=7.2665 | ValLoss=1.5439 | ValAcc=45.24% | BestValLoss=1.5340 | ES=26/30\n",
      "Epoch 124 | LR=1.87e-05 | TrainLoss=7.2802 | ValLoss=1.5513 | ValAcc=45.17% | BestValLoss=1.5340 | ES=27/30\n",
      "Epoch 125 | LR=1.87e-05 | TrainLoss=7.2707 | ValLoss=1.5313 | ValAcc=45.59% | BestValLoss=1.5340 | ES=28/30\n",
      "Epoch 126 | LR=1.87e-05 | TrainLoss=7.2798 | ValLoss=1.5474 | ValAcc=45.03% | BestValLoss=1.5313 | ES=0/30\n",
      "Epoch 127 | LR=1.87e-05 | TrainLoss=7.2764 | ValLoss=1.5632 | ValAcc=44.71% | BestValLoss=1.5313 | ES=1/30\n",
      "Epoch 128 | LR=1.87e-05 | TrainLoss=7.2692 | ValLoss=1.5504 | ValAcc=45.01% | BestValLoss=1.5313 | ES=2/30\n",
      "Epoch 129 | LR=1.87e-05 | TrainLoss=7.2788 | ValLoss=1.5484 | ValAcc=44.96% | BestValLoss=1.5313 | ES=3/30\n",
      "Epoch 130 | LR=1.87e-05 | TrainLoss=7.2740 | ValLoss=1.5460 | ValAcc=45.13% | BestValLoss=1.5313 | ES=4/30\n",
      "Epoch 131 | LR=1.87e-05 | TrainLoss=7.2733 | ValLoss=1.5463 | ValAcc=45.22% | BestValLoss=1.5313 | ES=5/30\n",
      "Epoch 132 | LR=1.87e-05 | TrainLoss=7.2797 | ValLoss=1.5439 | ValAcc=45.30% | BestValLoss=1.5313 | ES=6/30\n",
      "Epoch 133 | LR=1.87e-05 | TrainLoss=7.2731 | ValLoss=1.5399 | ValAcc=45.46% | BestValLoss=1.5313 | ES=7/30\n",
      "Epoch 134 | LR=1.87e-05 | TrainLoss=7.2721 | ValLoss=1.5533 | ValAcc=45.11% | BestValLoss=1.5313 | ES=8/30\n",
      "Epoch 135 | LR=1.87e-05 | TrainLoss=7.2818 | ValLoss=1.5419 | ValAcc=45.48% | BestValLoss=1.5313 | ES=9/30\n",
      "[LR DROP] 1.87e-05 -> 9.37e-06 (val_loss=1.5534)\n",
      "Epoch 136 | LR=9.37e-06 | TrainLoss=7.2718 | ValLoss=1.5534 | ValAcc=44.92% | BestValLoss=1.5313 | ES=10/30\n",
      "Epoch 137 | LR=9.37e-06 | TrainLoss=7.2730 | ValLoss=1.5574 | ValAcc=44.93% | BestValLoss=1.5313 | ES=11/30\n",
      "Epoch 138 | LR=9.37e-06 | TrainLoss=7.2738 | ValLoss=1.5489 | ValAcc=45.16% | BestValLoss=1.5313 | ES=12/30\n",
      "Epoch 139 | LR=9.37e-06 | TrainLoss=7.2586 | ValLoss=1.5453 | ValAcc=45.30% | BestValLoss=1.5313 | ES=13/30\n",
      "Epoch 140 | LR=9.37e-06 | TrainLoss=7.2858 | ValLoss=1.5452 | ValAcc=45.29% | BestValLoss=1.5313 | ES=14/30\n",
      "Epoch 141 | LR=9.37e-06 | TrainLoss=7.2721 | ValLoss=1.5468 | ValAcc=45.24% | BestValLoss=1.5313 | ES=15/30\n",
      "Epoch 142 | LR=9.37e-06 | TrainLoss=7.2680 | ValLoss=1.5358 | ValAcc=45.51% | BestValLoss=1.5313 | ES=16/30\n",
      "Epoch 143 | LR=9.37e-06 | TrainLoss=7.2746 | ValLoss=1.5513 | ValAcc=45.11% | BestValLoss=1.5313 | ES=17/30\n",
      "Epoch 144 | LR=9.37e-06 | TrainLoss=7.2716 | ValLoss=1.5464 | ValAcc=45.18% | BestValLoss=1.5313 | ES=18/30\n",
      "Epoch 145 | LR=9.37e-06 | TrainLoss=7.2672 | ValLoss=1.5412 | ValAcc=45.30% | BestValLoss=1.5313 | ES=19/30\n",
      "Epoch 146 | LR=9.37e-06 | TrainLoss=7.2616 | ValLoss=1.5477 | ValAcc=45.23% | BestValLoss=1.5313 | ES=20/30\n",
      "[LR DROP] 9.37e-06 -> 4.69e-06 (val_loss=1.5539)\n",
      "Epoch 147 | LR=4.69e-06 | TrainLoss=7.2779 | ValLoss=1.5539 | ValAcc=44.92% | BestValLoss=1.5313 | ES=21/30\n",
      "Epoch 148 | LR=4.69e-06 | TrainLoss=7.2710 | ValLoss=1.5456 | ValAcc=45.30% | BestValLoss=1.5313 | ES=22/30\n",
      "Epoch 149 | LR=4.69e-06 | TrainLoss=7.2735 | ValLoss=1.5437 | ValAcc=45.40% | BestValLoss=1.5313 | ES=23/30\n",
      "Epoch 150 | LR=4.69e-06 | TrainLoss=7.2616 | ValLoss=1.5458 | ValAcc=45.23% | BestValLoss=1.5313 | ES=24/30\n",
      "Epoch 151 | LR=4.69e-06 | TrainLoss=7.2697 | ValLoss=1.5428 | ValAcc=45.37% | BestValLoss=1.5313 | ES=25/30\n",
      "Epoch 152 | LR=4.69e-06 | TrainLoss=7.2730 | ValLoss=1.5456 | ValAcc=45.25% | BestValLoss=1.5313 | ES=26/30\n",
      "Epoch 153 | LR=4.69e-06 | TrainLoss=7.2620 | ValLoss=1.5504 | ValAcc=45.20% | BestValLoss=1.5313 | ES=27/30\n",
      "Epoch 154 | LR=4.69e-06 | TrainLoss=7.2675 | ValLoss=1.5469 | ValAcc=45.34% | BestValLoss=1.5313 | ES=28/30\n",
      "Epoch 155 | LR=4.69e-06 | TrainLoss=7.2680 | ValLoss=1.5431 | ValAcc=45.39% | BestValLoss=1.5313 | ES=29/30\n",
      "[INFO] Early stopping triggered.\n",
      "[FOLD TEST] 20:46.32%, 15:46.09%, 10:45.46%, 5:43.26%, 0:37.99%, -5:27.92%, -10:17.58%, -15:13.01%, -20:11.74%, -25:11.42%, -30:11.32%, -35:11.07%, -40:11.04%\n",
      "\n",
      "========== Fold 2/5 ==========\n",
      "[FOLD 2] train_sample_counts: {0: 14158, 1: 14015, 2: 14266, 3: 14137, 4: 14080, 5: 14305, 6: 13972, 7: 14066, 8: 14161}\n",
      "[FOLD 2] val_sample_counts  : {0: 3540, 1: 3503, 2: 3567, 3: 3534, 4: 3520, 5: 3577, 6: 3493, 7: 3516, 8: 3540}\n",
      "Epoch 001 | LR=3.00e-04 | TrainLoss=8.2586 | ValLoss=2.1106 | ValAcc=17.96% | BestValLoss=inf | ES=0/30\n",
      "Epoch 002 | LR=3.00e-04 | TrainLoss=8.1071 | ValLoss=2.0949 | ValAcc=19.52% | BestValLoss=2.1106 | ES=0/30\n",
      "Epoch 003 | LR=3.00e-04 | TrainLoss=8.0126 | ValLoss=1.9970 | ValAcc=24.78% | BestValLoss=2.0949 | ES=0/30\n",
      "Epoch 004 | LR=3.00e-04 | TrainLoss=7.9458 | ValLoss=1.9676 | ValAcc=25.62% | BestValLoss=1.9970 | ES=0/30\n",
      "Epoch 005 | LR=3.00e-04 | TrainLoss=7.9038 | ValLoss=1.9733 | ValAcc=25.34% | BestValLoss=1.9676 | ES=0/30\n",
      "Epoch 006 | LR=3.00e-04 | TrainLoss=7.8583 | ValLoss=1.9529 | ValAcc=27.87% | BestValLoss=1.9676 | ES=1/30\n",
      "Epoch 007 | LR=3.00e-04 | TrainLoss=7.8251 | ValLoss=1.9514 | ValAcc=26.74% | BestValLoss=1.9529 | ES=0/30\n",
      "Epoch 008 | LR=3.00e-04 | TrainLoss=7.8006 | ValLoss=1.9093 | ValAcc=28.99% | BestValLoss=1.9514 | ES=0/30\n",
      "Epoch 009 | LR=3.00e-04 | TrainLoss=7.7628 | ValLoss=1.8601 | ValAcc=31.73% | BestValLoss=1.9093 | ES=0/30\n",
      "Epoch 010 | LR=3.00e-04 | TrainLoss=7.7399 | ValLoss=1.8488 | ValAcc=31.56% | BestValLoss=1.8601 | ES=0/30\n",
      "Epoch 011 | LR=3.00e-04 | TrainLoss=7.7228 | ValLoss=1.8507 | ValAcc=31.90% | BestValLoss=1.8488 | ES=0/30\n",
      "Epoch 012 | LR=3.00e-04 | TrainLoss=7.7065 | ValLoss=1.8283 | ValAcc=33.32% | BestValLoss=1.8488 | ES=1/30\n",
      "Epoch 013 | LR=3.00e-04 | TrainLoss=7.6862 | ValLoss=1.8155 | ValAcc=33.62% | BestValLoss=1.8283 | ES=0/30\n",
      "Epoch 014 | LR=3.00e-04 | TrainLoss=7.6684 | ValLoss=1.8167 | ValAcc=33.51% | BestValLoss=1.8155 | ES=0/30\n",
      "Epoch 015 | LR=3.00e-04 | TrainLoss=7.6581 | ValLoss=1.7853 | ValAcc=34.59% | BestValLoss=1.8155 | ES=1/30\n",
      "Epoch 016 | LR=3.00e-04 | TrainLoss=7.6380 | ValLoss=1.7492 | ValAcc=36.79% | BestValLoss=1.7853 | ES=0/30\n",
      "Epoch 017 | LR=3.00e-04 | TrainLoss=7.6281 | ValLoss=1.7544 | ValAcc=36.06% | BestValLoss=1.7492 | ES=0/30\n",
      "Epoch 018 | LR=3.00e-04 | TrainLoss=7.6118 | ValLoss=1.7825 | ValAcc=36.15% | BestValLoss=1.7492 | ES=1/30\n",
      "Epoch 019 | LR=3.00e-04 | TrainLoss=7.6018 | ValLoss=1.7161 | ValAcc=38.63% | BestValLoss=1.7492 | ES=2/30\n",
      "Epoch 020 | LR=3.00e-04 | TrainLoss=7.5951 | ValLoss=1.7309 | ValAcc=38.22% | BestValLoss=1.7161 | ES=0/30\n",
      "Epoch 021 | LR=3.00e-04 | TrainLoss=7.5856 | ValLoss=1.7421 | ValAcc=37.07% | BestValLoss=1.7161 | ES=1/30\n",
      "Epoch 022 | LR=3.00e-04 | TrainLoss=7.5712 | ValLoss=1.7099 | ValAcc=38.22% | BestValLoss=1.7161 | ES=2/30\n",
      "Epoch 023 | LR=3.00e-04 | TrainLoss=7.5636 | ValLoss=1.7080 | ValAcc=39.36% | BestValLoss=1.7099 | ES=0/30\n",
      "Epoch 024 | LR=3.00e-04 | TrainLoss=7.5516 | ValLoss=1.7120 | ValAcc=37.88% | BestValLoss=1.7080 | ES=0/30\n",
      "Epoch 025 | LR=3.00e-04 | TrainLoss=7.5520 | ValLoss=1.7007 | ValAcc=39.07% | BestValLoss=1.7080 | ES=1/30\n",
      "Epoch 026 | LR=3.00e-04 | TrainLoss=7.5430 | ValLoss=1.6961 | ValAcc=39.73% | BestValLoss=1.7007 | ES=0/30\n",
      "Epoch 027 | LR=3.00e-04 | TrainLoss=7.5297 | ValLoss=1.7536 | ValAcc=36.90% | BestValLoss=1.6961 | ES=0/30\n",
      "Epoch 028 | LR=3.00e-04 | TrainLoss=7.5333 | ValLoss=1.6699 | ValAcc=39.39% | BestValLoss=1.6961 | ES=1/30\n",
      "Epoch 029 | LR=3.00e-04 | TrainLoss=7.5278 | ValLoss=1.7139 | ValAcc=38.04% | BestValLoss=1.6699 | ES=0/30\n",
      "Epoch 030 | LR=3.00e-04 | TrainLoss=7.5219 | ValLoss=1.6650 | ValAcc=40.46% | BestValLoss=1.6699 | ES=1/30\n",
      "Epoch 031 | LR=3.00e-04 | TrainLoss=7.5090 | ValLoss=1.7141 | ValAcc=38.69% | BestValLoss=1.6650 | ES=0/30\n",
      "Epoch 032 | LR=3.00e-04 | TrainLoss=7.5106 | ValLoss=1.6827 | ValAcc=39.90% | BestValLoss=1.6650 | ES=1/30\n",
      "Epoch 033 | LR=3.00e-04 | TrainLoss=7.4950 | ValLoss=1.6283 | ValAcc=41.52% | BestValLoss=1.6650 | ES=2/30\n",
      "Epoch 034 | LR=3.00e-04 | TrainLoss=7.4837 | ValLoss=1.6558 | ValAcc=40.80% | BestValLoss=1.6283 | ES=0/30\n",
      "Epoch 035 | LR=3.00e-04 | TrainLoss=7.4964 | ValLoss=1.6562 | ValAcc=40.59% | BestValLoss=1.6283 | ES=1/30\n",
      "Epoch 036 | LR=3.00e-04 | TrainLoss=7.4822 | ValLoss=1.6767 | ValAcc=39.38% | BestValLoss=1.6283 | ES=2/30\n",
      "Epoch 037 | LR=3.00e-04 | TrainLoss=7.4735 | ValLoss=1.6241 | ValAcc=41.73% | BestValLoss=1.6283 | ES=3/30\n",
      "Epoch 038 | LR=3.00e-04 | TrainLoss=7.4739 | ValLoss=1.6239 | ValAcc=41.91% | BestValLoss=1.6241 | ES=0/30\n",
      "Epoch 039 | LR=3.00e-04 | TrainLoss=7.4694 | ValLoss=1.7018 | ValAcc=39.56% | BestValLoss=1.6239 | ES=0/30\n",
      "Epoch 040 | LR=3.00e-04 | TrainLoss=7.4691 | ValLoss=1.6397 | ValAcc=41.00% | BestValLoss=1.6239 | ES=1/30\n",
      "Epoch 041 | LR=3.00e-04 | TrainLoss=7.4608 | ValLoss=1.6629 | ValAcc=40.10% | BestValLoss=1.6239 | ES=2/30\n",
      "Epoch 042 | LR=3.00e-04 | TrainLoss=7.4540 | ValLoss=1.6175 | ValAcc=42.49% | BestValLoss=1.6239 | ES=3/30\n",
      "Epoch 043 | LR=3.00e-04 | TrainLoss=7.4461 | ValLoss=1.6247 | ValAcc=42.06% | BestValLoss=1.6175 | ES=0/30\n",
      "Epoch 044 | LR=3.00e-04 | TrainLoss=7.4620 | ValLoss=1.6596 | ValAcc=40.60% | BestValLoss=1.6175 | ES=1/30\n",
      "Epoch 045 | LR=3.00e-04 | TrainLoss=7.4418 | ValLoss=1.6503 | ValAcc=41.38% | BestValLoss=1.6175 | ES=2/30\n",
      "Epoch 046 | LR=3.00e-04 | TrainLoss=7.4402 | ValLoss=1.6061 | ValAcc=42.85% | BestValLoss=1.6175 | ES=3/30\n",
      "Epoch 047 | LR=3.00e-04 | TrainLoss=7.4363 | ValLoss=1.6452 | ValAcc=40.60% | BestValLoss=1.6061 | ES=0/30\n",
      "Epoch 048 | LR=3.00e-04 | TrainLoss=7.4371 | ValLoss=1.6200 | ValAcc=41.86% | BestValLoss=1.6061 | ES=1/30\n",
      "Epoch 049 | LR=3.00e-04 | TrainLoss=7.4341 | ValLoss=1.6137 | ValAcc=42.01% | BestValLoss=1.6061 | ES=2/30\n",
      "Epoch 050 | LR=3.00e-04 | TrainLoss=7.4368 | ValLoss=1.6256 | ValAcc=41.82% | BestValLoss=1.6061 | ES=3/30\n",
      "Epoch 051 | LR=3.00e-04 | TrainLoss=7.4286 | ValLoss=1.6335 | ValAcc=41.30% | BestValLoss=1.6061 | ES=4/30\n",
      "Epoch 052 | LR=3.00e-04 | TrainLoss=7.4264 | ValLoss=1.6011 | ValAcc=42.14% | BestValLoss=1.6061 | ES=5/30\n",
      "Epoch 053 | LR=3.00e-04 | TrainLoss=7.4186 | ValLoss=1.6057 | ValAcc=42.65% | BestValLoss=1.6011 | ES=0/30\n",
      "Epoch 054 | LR=3.00e-04 | TrainLoss=7.4120 | ValLoss=1.6354 | ValAcc=41.77% | BestValLoss=1.6011 | ES=1/30\n",
      "Epoch 055 | LR=3.00e-04 | TrainLoss=7.4068 | ValLoss=1.6462 | ValAcc=41.21% | BestValLoss=1.6011 | ES=2/30\n",
      "Epoch 056 | LR=3.00e-04 | TrainLoss=7.4039 | ValLoss=1.5688 | ValAcc=44.29% | BestValLoss=1.6011 | ES=3/30\n",
      "Epoch 057 | LR=3.00e-04 | TrainLoss=7.4019 | ValLoss=1.6156 | ValAcc=42.72% | BestValLoss=1.5688 | ES=0/30\n",
      "Epoch 058 | LR=3.00e-04 | TrainLoss=7.4063 | ValLoss=1.5717 | ValAcc=43.88% | BestValLoss=1.5688 | ES=1/30\n",
      "Epoch 059 | LR=3.00e-04 | TrainLoss=7.4019 | ValLoss=1.6169 | ValAcc=41.90% | BestValLoss=1.5688 | ES=2/30\n",
      "Epoch 060 | LR=3.00e-04 | TrainLoss=7.4006 | ValLoss=1.5574 | ValAcc=44.42% | BestValLoss=1.5688 | ES=3/30\n",
      "Epoch 061 | LR=3.00e-04 | TrainLoss=7.3895 | ValLoss=1.6058 | ValAcc=42.28% | BestValLoss=1.5574 | ES=0/30\n",
      "Epoch 062 | LR=3.00e-04 | TrainLoss=7.3848 | ValLoss=1.5987 | ValAcc=42.69% | BestValLoss=1.5574 | ES=1/30\n",
      "Epoch 063 | LR=3.00e-04 | TrainLoss=7.3962 | ValLoss=1.5865 | ValAcc=43.02% | BestValLoss=1.5574 | ES=2/30\n",
      "Epoch 064 | LR=3.00e-04 | TrainLoss=7.3806 | ValLoss=1.6390 | ValAcc=41.19% | BestValLoss=1.5574 | ES=3/30\n",
      "Epoch 065 | LR=3.00e-04 | TrainLoss=7.3864 | ValLoss=1.6017 | ValAcc=42.42% | BestValLoss=1.5574 | ES=4/30\n",
      "Epoch 066 | LR=3.00e-04 | TrainLoss=7.3835 | ValLoss=1.6081 | ValAcc=42.68% | BestValLoss=1.5574 | ES=5/30\n",
      "Epoch 067 | LR=3.00e-04 | TrainLoss=7.3816 | ValLoss=1.5853 | ValAcc=44.29% | BestValLoss=1.5574 | ES=6/30\n",
      "Epoch 068 | LR=3.00e-04 | TrainLoss=7.3704 | ValLoss=1.6318 | ValAcc=41.39% | BestValLoss=1.5574 | ES=7/30\n",
      "Epoch 069 | LR=3.00e-04 | TrainLoss=7.3752 | ValLoss=1.5799 | ValAcc=43.49% | BestValLoss=1.5574 | ES=8/30\n",
      "Epoch 070 | LR=3.00e-04 | TrainLoss=7.3562 | ValLoss=1.5826 | ValAcc=43.83% | BestValLoss=1.5574 | ES=9/30\n",
      "[LR DROP] 3.00e-04 -> 1.50e-04 (val_loss=1.5594)\n",
      "Epoch 071 | LR=1.50e-04 | TrainLoss=7.3792 | ValLoss=1.5594 | ValAcc=44.63% | BestValLoss=1.5574 | ES=10/30\n",
      "Epoch 072 | LR=1.50e-04 | TrainLoss=7.3396 | ValLoss=1.5762 | ValAcc=43.39% | BestValLoss=1.5574 | ES=11/30\n",
      "Epoch 073 | LR=1.50e-04 | TrainLoss=7.3442 | ValLoss=1.5527 | ValAcc=44.55% | BestValLoss=1.5574 | ES=12/30\n",
      "Epoch 074 | LR=1.50e-04 | TrainLoss=7.3451 | ValLoss=1.5839 | ValAcc=43.92% | BestValLoss=1.5527 | ES=0/30\n",
      "Epoch 075 | LR=1.50e-04 | TrainLoss=7.3347 | ValLoss=1.5487 | ValAcc=44.87% | BestValLoss=1.5527 | ES=1/30\n",
      "Epoch 076 | LR=1.50e-04 | TrainLoss=7.3303 | ValLoss=1.5668 | ValAcc=44.06% | BestValLoss=1.5487 | ES=0/30\n",
      "Epoch 077 | LR=1.50e-04 | TrainLoss=7.3253 | ValLoss=1.5526 | ValAcc=44.39% | BestValLoss=1.5487 | ES=1/30\n",
      "Epoch 078 | LR=1.50e-04 | TrainLoss=7.3320 | ValLoss=1.5510 | ValAcc=44.63% | BestValLoss=1.5487 | ES=2/30\n",
      "Epoch 079 | LR=1.50e-04 | TrainLoss=7.3268 | ValLoss=1.5283 | ValAcc=45.51% | BestValLoss=1.5487 | ES=3/30\n",
      "Epoch 080 | LR=1.50e-04 | TrainLoss=7.3224 | ValLoss=1.5495 | ValAcc=45.08% | BestValLoss=1.5283 | ES=0/30\n",
      "Epoch 081 | LR=1.50e-04 | TrainLoss=7.3062 | ValLoss=1.5379 | ValAcc=45.15% | BestValLoss=1.5283 | ES=1/30\n",
      "Epoch 082 | LR=1.50e-04 | TrainLoss=7.3173 | ValLoss=1.5517 | ValAcc=44.95% | BestValLoss=1.5283 | ES=2/30\n",
      "Epoch 083 | LR=1.50e-04 | TrainLoss=7.3189 | ValLoss=1.5664 | ValAcc=44.15% | BestValLoss=1.5283 | ES=3/30\n",
      "Epoch 084 | LR=1.50e-04 | TrainLoss=7.3194 | ValLoss=1.5485 | ValAcc=44.82% | BestValLoss=1.5283 | ES=4/30\n",
      "Epoch 085 | LR=1.50e-04 | TrainLoss=7.3268 | ValLoss=1.5463 | ValAcc=45.34% | BestValLoss=1.5283 | ES=5/30\n",
      "Epoch 086 | LR=1.50e-04 | TrainLoss=7.3141 | ValLoss=1.5411 | ValAcc=45.15% | BestValLoss=1.5283 | ES=6/30\n",
      "Epoch 087 | LR=1.50e-04 | TrainLoss=7.3165 | ValLoss=1.5258 | ValAcc=45.93% | BestValLoss=1.5283 | ES=7/30\n",
      "Epoch 088 | LR=1.50e-04 | TrainLoss=7.3222 | ValLoss=1.5428 | ValAcc=45.34% | BestValLoss=1.5258 | ES=0/30\n",
      "Epoch 089 | LR=1.50e-04 | TrainLoss=7.3200 | ValLoss=1.5645 | ValAcc=44.47% | BestValLoss=1.5258 | ES=1/30\n",
      "Epoch 090 | LR=1.50e-04 | TrainLoss=7.3057 | ValLoss=1.5511 | ValAcc=44.67% | BestValLoss=1.5258 | ES=2/30\n",
      "Epoch 091 | LR=1.50e-04 | TrainLoss=7.3056 | ValLoss=1.5155 | ValAcc=46.14% | BestValLoss=1.5258 | ES=3/30\n",
      "Epoch 092 | LR=1.50e-04 | TrainLoss=7.3083 | ValLoss=1.5518 | ValAcc=44.99% | BestValLoss=1.5155 | ES=0/30\n",
      "Epoch 093 | LR=1.50e-04 | TrainLoss=7.3242 | ValLoss=1.5387 | ValAcc=45.52% | BestValLoss=1.5155 | ES=1/30\n",
      "Epoch 094 | LR=1.50e-04 | TrainLoss=7.3090 | ValLoss=1.5474 | ValAcc=45.12% | BestValLoss=1.5155 | ES=2/30\n",
      "Epoch 095 | LR=1.50e-04 | TrainLoss=7.3085 | ValLoss=1.5592 | ValAcc=44.38% | BestValLoss=1.5155 | ES=3/30\n",
      "Epoch 096 | LR=1.50e-04 | TrainLoss=7.3078 | ValLoss=1.5822 | ValAcc=43.97% | BestValLoss=1.5155 | ES=4/30\n",
      "Epoch 097 | LR=1.50e-04 | TrainLoss=7.3044 | ValLoss=1.5575 | ValAcc=44.42% | BestValLoss=1.5155 | ES=5/30\n",
      "Epoch 098 | LR=1.50e-04 | TrainLoss=7.3009 | ValLoss=1.5506 | ValAcc=45.02% | BestValLoss=1.5155 | ES=6/30\n",
      "Epoch 099 | LR=1.50e-04 | TrainLoss=7.3035 | ValLoss=1.5525 | ValAcc=44.84% | BestValLoss=1.5155 | ES=7/30\n",
      "Epoch 100 | LR=1.50e-04 | TrainLoss=7.3068 | ValLoss=1.5468 | ValAcc=45.17% | BestValLoss=1.5155 | ES=8/30\n",
      "Epoch 101 | LR=1.50e-04 | TrainLoss=7.2994 | ValLoss=1.5456 | ValAcc=44.90% | BestValLoss=1.5155 | ES=9/30\n",
      "[LR DROP] 1.50e-04 -> 7.50e-05 (val_loss=1.5426)\n",
      "Epoch 102 | LR=7.50e-05 | TrainLoss=7.3143 | ValLoss=1.5426 | ValAcc=45.15% | BestValLoss=1.5155 | ES=10/30\n",
      "Epoch 103 | LR=7.50e-05 | TrainLoss=7.2965 | ValLoss=1.5309 | ValAcc=45.67% | BestValLoss=1.5155 | ES=11/30\n",
      "Epoch 104 | LR=7.50e-05 | TrainLoss=7.2797 | ValLoss=1.5283 | ValAcc=45.93% | BestValLoss=1.5155 | ES=12/30\n",
      "Epoch 105 | LR=7.50e-05 | TrainLoss=7.3002 | ValLoss=1.5352 | ValAcc=45.67% | BestValLoss=1.5155 | ES=13/30\n",
      "Epoch 106 | LR=7.50e-05 | TrainLoss=7.2783 | ValLoss=1.5158 | ValAcc=46.01% | BestValLoss=1.5155 | ES=14/30\n",
      "Epoch 107 | LR=7.50e-05 | TrainLoss=7.2718 | ValLoss=1.5261 | ValAcc=45.95% | BestValLoss=1.5155 | ES=15/30\n",
      "Epoch 108 | LR=7.50e-05 | TrainLoss=7.2834 | ValLoss=1.5244 | ValAcc=45.97% | BestValLoss=1.5155 | ES=16/30\n",
      "Epoch 109 | LR=7.50e-05 | TrainLoss=7.2763 | ValLoss=1.5309 | ValAcc=46.14% | BestValLoss=1.5155 | ES=17/30\n",
      "Epoch 110 | LR=7.50e-05 | TrainLoss=7.2672 | ValLoss=1.5177 | ValAcc=46.41% | BestValLoss=1.5155 | ES=18/30\n",
      "Epoch 111 | LR=7.50e-05 | TrainLoss=7.2833 | ValLoss=1.5526 | ValAcc=45.46% | BestValLoss=1.5155 | ES=19/30\n",
      "Epoch 112 | LR=7.50e-05 | TrainLoss=7.2844 | ValLoss=1.5324 | ValAcc=45.54% | BestValLoss=1.5155 | ES=20/30\n",
      "[LR DROP] 7.50e-05 -> 3.75e-05 (val_loss=1.5418)\n",
      "Epoch 113 | LR=3.75e-05 | TrainLoss=7.2779 | ValLoss=1.5418 | ValAcc=45.25% | BestValLoss=1.5155 | ES=21/30\n",
      "Epoch 114 | LR=3.75e-05 | TrainLoss=7.2625 | ValLoss=1.5304 | ValAcc=45.80% | BestValLoss=1.5155 | ES=22/30\n",
      "Epoch 115 | LR=3.75e-05 | TrainLoss=7.2662 | ValLoss=1.5273 | ValAcc=45.89% | BestValLoss=1.5155 | ES=23/30\n",
      "Epoch 116 | LR=3.75e-05 | TrainLoss=7.2551 | ValLoss=1.5176 | ValAcc=46.30% | BestValLoss=1.5155 | ES=24/30\n",
      "Epoch 117 | LR=3.75e-05 | TrainLoss=7.2636 | ValLoss=1.5026 | ValAcc=46.76% | BestValLoss=1.5155 | ES=25/30\n",
      "Epoch 118 | LR=3.75e-05 | TrainLoss=7.2620 | ValLoss=1.5249 | ValAcc=46.14% | BestValLoss=1.5026 | ES=0/30\n",
      "Epoch 119 | LR=3.75e-05 | TrainLoss=7.2777 | ValLoss=1.5322 | ValAcc=45.72% | BestValLoss=1.5026 | ES=1/30\n",
      "Epoch 120 | LR=3.75e-05 | TrainLoss=7.2787 | ValLoss=1.5147 | ValAcc=46.52% | BestValLoss=1.5026 | ES=2/30\n",
      "Epoch 121 | LR=3.75e-05 | TrainLoss=7.2668 | ValLoss=1.5084 | ValAcc=46.66% | BestValLoss=1.5026 | ES=3/30\n",
      "Epoch 122 | LR=3.75e-05 | TrainLoss=7.2439 | ValLoss=1.5155 | ValAcc=46.49% | BestValLoss=1.5026 | ES=4/30\n",
      "Epoch 123 | LR=3.75e-05 | TrainLoss=7.2636 | ValLoss=1.5292 | ValAcc=45.77% | BestValLoss=1.5026 | ES=5/30\n",
      "Epoch 124 | LR=3.75e-05 | TrainLoss=7.2575 | ValLoss=1.5267 | ValAcc=46.01% | BestValLoss=1.5026 | ES=6/30\n",
      "Epoch 125 | LR=3.75e-05 | TrainLoss=7.2658 | ValLoss=1.5229 | ValAcc=46.30% | BestValLoss=1.5026 | ES=7/30\n",
      "Epoch 126 | LR=3.75e-05 | TrainLoss=7.2490 | ValLoss=1.5176 | ValAcc=46.28% | BestValLoss=1.5026 | ES=8/30\n",
      "Epoch 127 | LR=3.75e-05 | TrainLoss=7.2668 | ValLoss=1.5304 | ValAcc=45.97% | BestValLoss=1.5026 | ES=9/30\n",
      "[LR DROP] 3.75e-05 -> 1.87e-05 (val_loss=1.5312)\n",
      "Epoch 128 | LR=1.87e-05 | TrainLoss=7.2661 | ValLoss=1.5312 | ValAcc=46.04% | BestValLoss=1.5026 | ES=10/30\n",
      "Epoch 129 | LR=1.87e-05 | TrainLoss=7.2604 | ValLoss=1.5276 | ValAcc=46.04% | BestValLoss=1.5026 | ES=11/30\n",
      "Epoch 130 | LR=1.87e-05 | TrainLoss=7.2462 | ValLoss=1.5096 | ValAcc=46.64% | BestValLoss=1.5026 | ES=12/30\n",
      "Epoch 131 | LR=1.87e-05 | TrainLoss=7.2494 | ValLoss=1.5082 | ValAcc=46.75% | BestValLoss=1.5026 | ES=13/30\n",
      "Epoch 132 | LR=1.87e-05 | TrainLoss=7.2582 | ValLoss=1.5174 | ValAcc=46.34% | BestValLoss=1.5026 | ES=14/30\n",
      "Epoch 133 | LR=1.87e-05 | TrainLoss=7.2671 | ValLoss=1.5163 | ValAcc=46.36% | BestValLoss=1.5026 | ES=15/30\n",
      "Epoch 134 | LR=1.87e-05 | TrainLoss=7.2486 | ValLoss=1.5164 | ValAcc=46.48% | BestValLoss=1.5026 | ES=16/30\n",
      "Epoch 135 | LR=1.87e-05 | TrainLoss=7.2550 | ValLoss=1.5144 | ValAcc=46.44% | BestValLoss=1.5026 | ES=17/30\n",
      "Epoch 136 | LR=1.87e-05 | TrainLoss=7.2663 | ValLoss=1.5117 | ValAcc=46.63% | BestValLoss=1.5026 | ES=18/30\n",
      "Epoch 137 | LR=1.87e-05 | TrainLoss=7.2622 | ValLoss=1.5101 | ValAcc=46.56% | BestValLoss=1.5026 | ES=19/30\n",
      "Epoch 138 | LR=1.87e-05 | TrainLoss=7.2565 | ValLoss=1.5120 | ValAcc=46.44% | BestValLoss=1.5026 | ES=20/30\n",
      "[LR DROP] 1.87e-05 -> 9.37e-06 (val_loss=1.5133)\n",
      "Epoch 139 | LR=9.37e-06 | TrainLoss=7.2588 | ValLoss=1.5133 | ValAcc=46.47% | BestValLoss=1.5026 | ES=21/30\n",
      "Epoch 140 | LR=9.37e-06 | TrainLoss=7.2509 | ValLoss=1.5190 | ValAcc=46.22% | BestValLoss=1.5026 | ES=22/30\n",
      "Epoch 141 | LR=9.37e-06 | TrainLoss=7.2627 | ValLoss=1.5162 | ValAcc=46.43% | BestValLoss=1.5026 | ES=23/30\n",
      "Epoch 142 | LR=9.37e-06 | TrainLoss=7.2526 | ValLoss=1.5153 | ValAcc=46.48% | BestValLoss=1.5026 | ES=24/30\n",
      "Epoch 143 | LR=9.37e-06 | TrainLoss=7.2537 | ValLoss=1.5186 | ValAcc=46.36% | BestValLoss=1.5026 | ES=25/30\n",
      "Epoch 144 | LR=9.37e-06 | TrainLoss=7.2597 | ValLoss=1.5229 | ValAcc=46.24% | BestValLoss=1.5026 | ES=26/30\n",
      "Epoch 145 | LR=9.37e-06 | TrainLoss=7.2667 | ValLoss=1.5142 | ValAcc=46.51% | BestValLoss=1.5026 | ES=27/30\n",
      "Epoch 146 | LR=9.37e-06 | TrainLoss=7.2566 | ValLoss=1.5218 | ValAcc=46.12% | BestValLoss=1.5026 | ES=28/30\n",
      "Epoch 147 | LR=9.37e-06 | TrainLoss=7.2475 | ValLoss=1.5109 | ValAcc=46.50% | BestValLoss=1.5026 | ES=29/30\n",
      "[INFO] Early stopping triggered.\n",
      "[FOLD TEST] 20:47.78%, 15:47.62%, 10:46.93%, 5:44.07%, 0:38.84%, -5:28.48%, -10:17.78%, -15:12.99%, -20:11.74%, -25:11.08%, -30:11.29%, -35:11.00%, -40:11.03%\n",
      "\n",
      "========== Fold 3/5 ==========\n",
      "[FOLD 3] train_sample_counts: {0: 14158, 1: 14014, 2: 14266, 3: 14137, 4: 14080, 5: 14306, 6: 13972, 7: 14066, 8: 14161}\n",
      "[FOLD 3] val_sample_counts  : {0: 3540, 1: 3504, 2: 3567, 3: 3534, 4: 3520, 5: 3576, 6: 3493, 7: 3516, 8: 3540}\n",
      "Epoch 001 | LR=3.00e-04 | TrainLoss=8.2649 | ValLoss=2.1114 | ValAcc=19.93% | BestValLoss=inf | ES=0/30\n",
      "Epoch 002 | LR=3.00e-04 | TrainLoss=8.1218 | ValLoss=2.0896 | ValAcc=21.07% | BestValLoss=2.1114 | ES=0/30\n",
      "Epoch 003 | LR=3.00e-04 | TrainLoss=8.0182 | ValLoss=1.9945 | ValAcc=24.43% | BestValLoss=2.0896 | ES=0/30\n",
      "Epoch 004 | LR=3.00e-04 | TrainLoss=7.9513 | ValLoss=1.9724 | ValAcc=26.37% | BestValLoss=1.9945 | ES=0/30\n",
      "Epoch 005 | LR=3.00e-04 | TrainLoss=7.9038 | ValLoss=1.9741 | ValAcc=26.16% | BestValLoss=1.9724 | ES=0/30\n",
      "Epoch 006 | LR=3.00e-04 | TrainLoss=7.8650 | ValLoss=1.9466 | ValAcc=27.79% | BestValLoss=1.9724 | ES=1/30\n",
      "Epoch 007 | LR=3.00e-04 | TrainLoss=7.8196 | ValLoss=1.9161 | ValAcc=30.08% | BestValLoss=1.9466 | ES=0/30\n",
      "Epoch 008 | LR=3.00e-04 | TrainLoss=7.7970 | ValLoss=1.8620 | ValAcc=31.91% | BestValLoss=1.9161 | ES=0/30\n",
      "Epoch 009 | LR=3.00e-04 | TrainLoss=7.7665 | ValLoss=1.8547 | ValAcc=31.95% | BestValLoss=1.8620 | ES=0/30\n",
      "Epoch 010 | LR=3.00e-04 | TrainLoss=7.7385 | ValLoss=1.9091 | ValAcc=30.03% | BestValLoss=1.8547 | ES=0/30\n",
      "Epoch 011 | LR=3.00e-04 | TrainLoss=7.7251 | ValLoss=1.8297 | ValAcc=32.87% | BestValLoss=1.8547 | ES=1/30\n",
      "Epoch 012 | LR=3.00e-04 | TrainLoss=7.7064 | ValLoss=1.8068 | ValAcc=34.38% | BestValLoss=1.8297 | ES=0/30\n",
      "Epoch 013 | LR=3.00e-04 | TrainLoss=7.6922 | ValLoss=1.7701 | ValAcc=35.50% | BestValLoss=1.8068 | ES=0/30\n",
      "Epoch 014 | LR=3.00e-04 | TrainLoss=7.6683 | ValLoss=1.8057 | ValAcc=34.31% | BestValLoss=1.7701 | ES=0/30\n",
      "Epoch 015 | LR=3.00e-04 | TrainLoss=7.6566 | ValLoss=1.7790 | ValAcc=35.16% | BestValLoss=1.7701 | ES=1/30\n",
      "Epoch 016 | LR=3.00e-04 | TrainLoss=7.6388 | ValLoss=1.8081 | ValAcc=33.92% | BestValLoss=1.7701 | ES=2/30\n",
      "Epoch 017 | LR=3.00e-04 | TrainLoss=7.6299 | ValLoss=1.7626 | ValAcc=35.68% | BestValLoss=1.7701 | ES=3/30\n",
      "Epoch 018 | LR=3.00e-04 | TrainLoss=7.6228 | ValLoss=1.7434 | ValAcc=36.20% | BestValLoss=1.7626 | ES=0/30\n",
      "Epoch 019 | LR=3.00e-04 | TrainLoss=7.6117 | ValLoss=1.7785 | ValAcc=35.55% | BestValLoss=1.7434 | ES=0/30\n",
      "Epoch 020 | LR=3.00e-04 | TrainLoss=7.5919 | ValLoss=1.7102 | ValAcc=38.30% | BestValLoss=1.7434 | ES=1/30\n",
      "Epoch 021 | LR=3.00e-04 | TrainLoss=7.5940 | ValLoss=1.7426 | ValAcc=36.79% | BestValLoss=1.7102 | ES=0/30\n",
      "Epoch 022 | LR=3.00e-04 | TrainLoss=7.5777 | ValLoss=1.7372 | ValAcc=36.06% | BestValLoss=1.7102 | ES=1/30\n",
      "Epoch 023 | LR=3.00e-04 | TrainLoss=7.5644 | ValLoss=1.7443 | ValAcc=37.01% | BestValLoss=1.7102 | ES=2/30\n",
      "Epoch 024 | LR=3.00e-04 | TrainLoss=7.5695 | ValLoss=1.7001 | ValAcc=38.30% | BestValLoss=1.7102 | ES=3/30\n",
      "Epoch 025 | LR=3.00e-04 | TrainLoss=7.5627 | ValLoss=1.6953 | ValAcc=38.57% | BestValLoss=1.7001 | ES=0/30\n",
      "Epoch 026 | LR=3.00e-04 | TrainLoss=7.5436 | ValLoss=1.6872 | ValAcc=39.26% | BestValLoss=1.6953 | ES=0/30\n",
      "Epoch 027 | LR=3.00e-04 | TrainLoss=7.5541 | ValLoss=1.6884 | ValAcc=38.62% | BestValLoss=1.6872 | ES=0/30\n",
      "Epoch 028 | LR=3.00e-04 | TrainLoss=7.5288 | ValLoss=1.6645 | ValAcc=40.47% | BestValLoss=1.6872 | ES=1/30\n",
      "Epoch 029 | LR=3.00e-04 | TrainLoss=7.5274 | ValLoss=1.6567 | ValAcc=40.32% | BestValLoss=1.6645 | ES=0/30\n",
      "Epoch 030 | LR=3.00e-04 | TrainLoss=7.5225 | ValLoss=1.6545 | ValAcc=40.51% | BestValLoss=1.6567 | ES=0/30\n",
      "Epoch 031 | LR=3.00e-04 | TrainLoss=7.5121 | ValLoss=1.6633 | ValAcc=40.04% | BestValLoss=1.6545 | ES=0/30\n",
      "Epoch 032 | LR=3.00e-04 | TrainLoss=7.5060 | ValLoss=1.6597 | ValAcc=40.03% | BestValLoss=1.6545 | ES=1/30\n",
      "Epoch 033 | LR=3.00e-04 | TrainLoss=7.4915 | ValLoss=1.6404 | ValAcc=40.99% | BestValLoss=1.6545 | ES=2/30\n",
      "Epoch 034 | LR=3.00e-04 | TrainLoss=7.4904 | ValLoss=1.6855 | ValAcc=40.20% | BestValLoss=1.6404 | ES=0/30\n",
      "Epoch 035 | LR=3.00e-04 | TrainLoss=7.4869 | ValLoss=1.6376 | ValAcc=41.40% | BestValLoss=1.6404 | ES=1/30\n",
      "Epoch 036 | LR=3.00e-04 | TrainLoss=7.4915 | ValLoss=1.6599 | ValAcc=39.86% | BestValLoss=1.6376 | ES=0/30\n",
      "Epoch 037 | LR=3.00e-04 | TrainLoss=7.4885 | ValLoss=1.6571 | ValAcc=40.81% | BestValLoss=1.6376 | ES=1/30\n",
      "Epoch 038 | LR=3.00e-04 | TrainLoss=7.4752 | ValLoss=1.6151 | ValAcc=41.53% | BestValLoss=1.6376 | ES=2/30\n",
      "Epoch 039 | LR=3.00e-04 | TrainLoss=7.4663 | ValLoss=1.6401 | ValAcc=40.57% | BestValLoss=1.6151 | ES=0/30\n",
      "Epoch 040 | LR=3.00e-04 | TrainLoss=7.4599 | ValLoss=1.6109 | ValAcc=42.03% | BestValLoss=1.6151 | ES=1/30\n",
      "Epoch 041 | LR=3.00e-04 | TrainLoss=7.4534 | ValLoss=1.6108 | ValAcc=42.05% | BestValLoss=1.6109 | ES=0/30\n",
      "Epoch 042 | LR=3.00e-04 | TrainLoss=7.4581 | ValLoss=1.7426 | ValAcc=38.55% | BestValLoss=1.6108 | ES=0/30\n",
      "Epoch 043 | LR=3.00e-04 | TrainLoss=7.4533 | ValLoss=1.6107 | ValAcc=41.91% | BestValLoss=1.6108 | ES=1/30\n",
      "Epoch 044 | LR=3.00e-04 | TrainLoss=7.4500 | ValLoss=1.6425 | ValAcc=40.92% | BestValLoss=1.6107 | ES=0/30\n",
      "Epoch 045 | LR=3.00e-04 | TrainLoss=7.4411 | ValLoss=1.6521 | ValAcc=41.08% | BestValLoss=1.6107 | ES=1/30\n",
      "Epoch 046 | LR=3.00e-04 | TrainLoss=7.4429 | ValLoss=1.6161 | ValAcc=42.33% | BestValLoss=1.6107 | ES=2/30\n",
      "Epoch 047 | LR=3.00e-04 | TrainLoss=7.4400 | ValLoss=1.6042 | ValAcc=42.42% | BestValLoss=1.6107 | ES=3/30\n",
      "Epoch 048 | LR=3.00e-04 | TrainLoss=7.4359 | ValLoss=1.6589 | ValAcc=40.59% | BestValLoss=1.6042 | ES=0/30\n",
      "Epoch 049 | LR=3.00e-04 | TrainLoss=7.4290 | ValLoss=1.6319 | ValAcc=41.23% | BestValLoss=1.6042 | ES=1/30\n",
      "Epoch 050 | LR=3.00e-04 | TrainLoss=7.4310 | ValLoss=1.6535 | ValAcc=41.14% | BestValLoss=1.6042 | ES=2/30\n",
      "Epoch 051 | LR=3.00e-04 | TrainLoss=7.4234 | ValLoss=1.6132 | ValAcc=41.62% | BestValLoss=1.6042 | ES=3/30\n",
      "Epoch 052 | LR=3.00e-04 | TrainLoss=7.4290 | ValLoss=1.6230 | ValAcc=42.25% | BestValLoss=1.6042 | ES=4/30\n",
      "Epoch 053 | LR=3.00e-04 | TrainLoss=7.4289 | ValLoss=1.6312 | ValAcc=41.87% | BestValLoss=1.6042 | ES=5/30\n",
      "Epoch 054 | LR=3.00e-04 | TrainLoss=7.4283 | ValLoss=1.6557 | ValAcc=40.56% | BestValLoss=1.6042 | ES=6/30\n",
      "Epoch 055 | LR=3.00e-04 | TrainLoss=7.4127 | ValLoss=1.6503 | ValAcc=41.06% | BestValLoss=1.6042 | ES=7/30\n",
      "Epoch 056 | LR=3.00e-04 | TrainLoss=7.4125 | ValLoss=1.6009 | ValAcc=42.98% | BestValLoss=1.6042 | ES=8/30\n",
      "Epoch 057 | LR=3.00e-04 | TrainLoss=7.4119 | ValLoss=1.6147 | ValAcc=42.23% | BestValLoss=1.6009 | ES=0/30\n",
      "Epoch 058 | LR=3.00e-04 | TrainLoss=7.4041 | ValLoss=1.5879 | ValAcc=43.36% | BestValLoss=1.6009 | ES=1/30\n",
      "Epoch 059 | LR=3.00e-04 | TrainLoss=7.4157 | ValLoss=1.6494 | ValAcc=41.32% | BestValLoss=1.5879 | ES=0/30\n",
      "Epoch 060 | LR=3.00e-04 | TrainLoss=7.4063 | ValLoss=1.6285 | ValAcc=41.91% | BestValLoss=1.5879 | ES=1/30\n",
      "Epoch 061 | LR=3.00e-04 | TrainLoss=7.3926 | ValLoss=1.5878 | ValAcc=43.33% | BestValLoss=1.5879 | ES=2/30\n",
      "Epoch 062 | LR=3.00e-04 | TrainLoss=7.4147 | ValLoss=1.5685 | ValAcc=43.66% | BestValLoss=1.5878 | ES=0/30\n",
      "Epoch 063 | LR=3.00e-04 | TrainLoss=7.3952 | ValLoss=1.6237 | ValAcc=42.44% | BestValLoss=1.5685 | ES=0/30\n",
      "Epoch 064 | LR=3.00e-04 | TrainLoss=7.3979 | ValLoss=1.5643 | ValAcc=44.53% | BestValLoss=1.5685 | ES=1/30\n",
      "Epoch 065 | LR=3.00e-04 | TrainLoss=7.3905 | ValLoss=1.6094 | ValAcc=42.61% | BestValLoss=1.5643 | ES=0/30\n",
      "Epoch 066 | LR=3.00e-04 | TrainLoss=7.3945 | ValLoss=1.5820 | ValAcc=43.82% | BestValLoss=1.5643 | ES=1/30\n",
      "Epoch 067 | LR=3.00e-04 | TrainLoss=7.3870 | ValLoss=1.6077 | ValAcc=43.10% | BestValLoss=1.5643 | ES=2/30\n",
      "Epoch 068 | LR=3.00e-04 | TrainLoss=7.3896 | ValLoss=1.5779 | ValAcc=43.90% | BestValLoss=1.5643 | ES=3/30\n",
      "Epoch 069 | LR=3.00e-04 | TrainLoss=7.3832 | ValLoss=1.6454 | ValAcc=42.07% | BestValLoss=1.5643 | ES=4/30\n",
      "Epoch 070 | LR=3.00e-04 | TrainLoss=7.3886 | ValLoss=1.5790 | ValAcc=43.53% | BestValLoss=1.5643 | ES=5/30\n",
      "Epoch 071 | LR=3.00e-04 | TrainLoss=7.3766 | ValLoss=1.5681 | ValAcc=44.16% | BestValLoss=1.5643 | ES=6/30\n",
      "Epoch 072 | LR=3.00e-04 | TrainLoss=7.3683 | ValLoss=1.5840 | ValAcc=43.29% | BestValLoss=1.5643 | ES=7/30\n",
      "Epoch 073 | LR=3.00e-04 | TrainLoss=7.3712 | ValLoss=1.5933 | ValAcc=43.14% | BestValLoss=1.5643 | ES=8/30\n",
      "Epoch 074 | LR=3.00e-04 | TrainLoss=7.3794 | ValLoss=1.5850 | ValAcc=43.81% | BestValLoss=1.5643 | ES=9/30\n",
      "[LR DROP] 3.00e-04 -> 1.50e-04 (val_loss=1.6029)\n",
      "Epoch 075 | LR=1.50e-04 | TrainLoss=7.3581 | ValLoss=1.6029 | ValAcc=43.00% | BestValLoss=1.5643 | ES=10/30\n",
      "Epoch 076 | LR=1.50e-04 | TrainLoss=7.3422 | ValLoss=1.5716 | ValAcc=44.38% | BestValLoss=1.5643 | ES=11/30\n",
      "Epoch 077 | LR=1.50e-04 | TrainLoss=7.3377 | ValLoss=1.5691 | ValAcc=44.40% | BestValLoss=1.5643 | ES=12/30\n",
      "Epoch 078 | LR=1.50e-04 | TrainLoss=7.3497 | ValLoss=1.5480 | ValAcc=44.83% | BestValLoss=1.5643 | ES=13/30\n",
      "Epoch 079 | LR=1.50e-04 | TrainLoss=7.3293 | ValLoss=1.5747 | ValAcc=43.93% | BestValLoss=1.5480 | ES=0/30\n",
      "Epoch 080 | LR=1.50e-04 | TrainLoss=7.3281 | ValLoss=1.5310 | ValAcc=45.50% | BestValLoss=1.5480 | ES=1/30\n",
      "Epoch 081 | LR=1.50e-04 | TrainLoss=7.3341 | ValLoss=1.5777 | ValAcc=44.27% | BestValLoss=1.5310 | ES=0/30\n",
      "Epoch 082 | LR=1.50e-04 | TrainLoss=7.3323 | ValLoss=1.5487 | ValAcc=44.96% | BestValLoss=1.5310 | ES=1/30\n",
      "Epoch 083 | LR=1.50e-04 | TrainLoss=7.3355 | ValLoss=1.5422 | ValAcc=45.43% | BestValLoss=1.5310 | ES=2/30\n",
      "Epoch 084 | LR=1.50e-04 | TrainLoss=7.3117 | ValLoss=1.5549 | ValAcc=44.48% | BestValLoss=1.5310 | ES=3/30\n",
      "Epoch 085 | LR=1.50e-04 | TrainLoss=7.3191 | ValLoss=1.5703 | ValAcc=44.51% | BestValLoss=1.5310 | ES=4/30\n",
      "Epoch 086 | LR=1.50e-04 | TrainLoss=7.3239 | ValLoss=1.5603 | ValAcc=44.73% | BestValLoss=1.5310 | ES=5/30\n",
      "Epoch 087 | LR=1.50e-04 | TrainLoss=7.3107 | ValLoss=1.5428 | ValAcc=45.59% | BestValLoss=1.5310 | ES=6/30\n",
      "Epoch 088 | LR=1.50e-04 | TrainLoss=7.3339 | ValLoss=1.5294 | ValAcc=45.70% | BestValLoss=1.5310 | ES=7/30\n",
      "Epoch 089 | LR=1.50e-04 | TrainLoss=7.3224 | ValLoss=1.5044 | ValAcc=46.53% | BestValLoss=1.5294 | ES=0/30\n",
      "Epoch 090 | LR=1.50e-04 | TrainLoss=7.3224 | ValLoss=1.5550 | ValAcc=44.83% | BestValLoss=1.5044 | ES=0/30\n",
      "Epoch 091 | LR=1.50e-04 | TrainLoss=7.3181 | ValLoss=1.5166 | ValAcc=46.06% | BestValLoss=1.5044 | ES=1/30\n",
      "Epoch 092 | LR=1.50e-04 | TrainLoss=7.3196 | ValLoss=1.5686 | ValAcc=44.67% | BestValLoss=1.5044 | ES=2/30\n",
      "Epoch 093 | LR=1.50e-04 | TrainLoss=7.3150 | ValLoss=1.5553 | ValAcc=45.26% | BestValLoss=1.5044 | ES=3/30\n",
      "Epoch 094 | LR=1.50e-04 | TrainLoss=7.3193 | ValLoss=1.5835 | ValAcc=43.89% | BestValLoss=1.5044 | ES=4/30\n",
      "Epoch 095 | LR=1.50e-04 | TrainLoss=7.3082 | ValLoss=1.5313 | ValAcc=45.84% | BestValLoss=1.5044 | ES=5/30\n",
      "Epoch 096 | LR=1.50e-04 | TrainLoss=7.3054 | ValLoss=1.5527 | ValAcc=44.52% | BestValLoss=1.5044 | ES=6/30\n",
      "Epoch 097 | LR=1.50e-04 | TrainLoss=7.2992 | ValLoss=1.5535 | ValAcc=44.92% | BestValLoss=1.5044 | ES=7/30\n",
      "Epoch 098 | LR=1.50e-04 | TrainLoss=7.3174 | ValLoss=1.5748 | ValAcc=44.32% | BestValLoss=1.5044 | ES=8/30\n",
      "Epoch 099 | LR=1.50e-04 | TrainLoss=7.3105 | ValLoss=1.5289 | ValAcc=45.53% | BestValLoss=1.5044 | ES=9/30\n",
      "[LR DROP] 1.50e-04 -> 7.50e-05 (val_loss=1.5399)\n",
      "Epoch 100 | LR=7.50e-05 | TrainLoss=7.3137 | ValLoss=1.5399 | ValAcc=45.55% | BestValLoss=1.5044 | ES=10/30\n",
      "Epoch 101 | LR=7.50e-05 | TrainLoss=7.2919 | ValLoss=1.5224 | ValAcc=46.40% | BestValLoss=1.5044 | ES=11/30\n",
      "Epoch 102 | LR=7.50e-05 | TrainLoss=7.2927 | ValLoss=1.5210 | ValAcc=46.01% | BestValLoss=1.5044 | ES=12/30\n",
      "Epoch 103 | LR=7.50e-05 | TrainLoss=7.2792 | ValLoss=1.5524 | ValAcc=45.28% | BestValLoss=1.5044 | ES=13/30\n",
      "Epoch 104 | LR=7.50e-05 | TrainLoss=7.2839 | ValLoss=1.5255 | ValAcc=45.84% | BestValLoss=1.5044 | ES=14/30\n",
      "Epoch 105 | LR=7.50e-05 | TrainLoss=7.2937 | ValLoss=1.5448 | ValAcc=45.29% | BestValLoss=1.5044 | ES=15/30\n",
      "Epoch 106 | LR=7.50e-05 | TrainLoss=7.2943 | ValLoss=1.5345 | ValAcc=45.56% | BestValLoss=1.5044 | ES=16/30\n",
      "Epoch 107 | LR=7.50e-05 | TrainLoss=7.3064 | ValLoss=1.5262 | ValAcc=45.74% | BestValLoss=1.5044 | ES=17/30\n",
      "Epoch 108 | LR=7.50e-05 | TrainLoss=7.2881 | ValLoss=1.5088 | ValAcc=46.48% | BestValLoss=1.5044 | ES=18/30\n",
      "Epoch 109 | LR=7.50e-05 | TrainLoss=7.2677 | ValLoss=1.5387 | ValAcc=45.78% | BestValLoss=1.5044 | ES=19/30\n",
      "Epoch 110 | LR=7.50e-05 | TrainLoss=7.2825 | ValLoss=1.5239 | ValAcc=46.22% | BestValLoss=1.5044 | ES=20/30\n",
      "[LR DROP] 7.50e-05 -> 3.75e-05 (val_loss=1.5333)\n",
      "Epoch 111 | LR=3.75e-05 | TrainLoss=7.2825 | ValLoss=1.5333 | ValAcc=45.76% | BestValLoss=1.5044 | ES=21/30\n",
      "Epoch 112 | LR=3.75e-05 | TrainLoss=7.2755 | ValLoss=1.5199 | ValAcc=46.18% | BestValLoss=1.5044 | ES=22/30\n",
      "Epoch 113 | LR=3.75e-05 | TrainLoss=7.2761 | ValLoss=1.5014 | ValAcc=46.93% | BestValLoss=1.5044 | ES=23/30\n",
      "Epoch 114 | LR=3.75e-05 | TrainLoss=7.2738 | ValLoss=1.5184 | ValAcc=46.27% | BestValLoss=1.5014 | ES=0/30\n",
      "Epoch 115 | LR=3.75e-05 | TrainLoss=7.2809 | ValLoss=1.5202 | ValAcc=46.28% | BestValLoss=1.5014 | ES=1/30\n",
      "Epoch 116 | LR=3.75e-05 | TrainLoss=7.2736 | ValLoss=1.5224 | ValAcc=46.19% | BestValLoss=1.5014 | ES=2/30\n",
      "Epoch 117 | LR=3.75e-05 | TrainLoss=7.2734 | ValLoss=1.5229 | ValAcc=46.10% | BestValLoss=1.5014 | ES=3/30\n",
      "Epoch 118 | LR=3.75e-05 | TrainLoss=7.2778 | ValLoss=1.5011 | ValAcc=47.12% | BestValLoss=1.5014 | ES=4/30\n",
      "Epoch 119 | LR=3.75e-05 | TrainLoss=7.2833 | ValLoss=1.5104 | ValAcc=46.55% | BestValLoss=1.5011 | ES=0/30\n",
      "Epoch 120 | LR=3.75e-05 | TrainLoss=7.2659 | ValLoss=1.5131 | ValAcc=46.48% | BestValLoss=1.5011 | ES=1/30\n",
      "Epoch 121 | LR=3.75e-05 | TrainLoss=7.2614 | ValLoss=1.5317 | ValAcc=46.12% | BestValLoss=1.5011 | ES=2/30\n",
      "Epoch 122 | LR=3.75e-05 | TrainLoss=7.2569 | ValLoss=1.5091 | ValAcc=46.57% | BestValLoss=1.5011 | ES=3/30\n",
      "Epoch 123 | LR=3.75e-05 | TrainLoss=7.2643 | ValLoss=1.5213 | ValAcc=46.22% | BestValLoss=1.5011 | ES=4/30\n",
      "Epoch 124 | LR=3.75e-05 | TrainLoss=7.2659 | ValLoss=1.5197 | ValAcc=46.57% | BestValLoss=1.5011 | ES=5/30\n",
      "Epoch 125 | LR=3.75e-05 | TrainLoss=7.2778 | ValLoss=1.5137 | ValAcc=46.79% | BestValLoss=1.5011 | ES=6/30\n",
      "Epoch 126 | LR=3.75e-05 | TrainLoss=7.2721 | ValLoss=1.5078 | ValAcc=46.84% | BestValLoss=1.5011 | ES=7/30\n",
      "Epoch 127 | LR=3.75e-05 | TrainLoss=7.2680 | ValLoss=1.5059 | ValAcc=46.84% | BestValLoss=1.5011 | ES=8/30\n",
      "Epoch 128 | LR=3.75e-05 | TrainLoss=7.2521 | ValLoss=1.5214 | ValAcc=46.38% | BestValLoss=1.5011 | ES=9/30\n",
      "[LR DROP] 3.75e-05 -> 1.87e-05 (val_loss=1.5103)\n",
      "Epoch 129 | LR=1.87e-05 | TrainLoss=7.2698 | ValLoss=1.5103 | ValAcc=46.56% | BestValLoss=1.5011 | ES=10/30\n",
      "Epoch 130 | LR=1.87e-05 | TrainLoss=7.2579 | ValLoss=1.5179 | ValAcc=46.50% | BestValLoss=1.5011 | ES=11/30\n",
      "Epoch 131 | LR=1.87e-05 | TrainLoss=7.2640 | ValLoss=1.5046 | ValAcc=46.74% | BestValLoss=1.5011 | ES=12/30\n",
      "Epoch 132 | LR=1.87e-05 | TrainLoss=7.2730 | ValLoss=1.5254 | ValAcc=46.18% | BestValLoss=1.5011 | ES=13/30\n",
      "Epoch 133 | LR=1.87e-05 | TrainLoss=7.2604 | ValLoss=1.5292 | ValAcc=46.07% | BestValLoss=1.5011 | ES=14/30\n",
      "Epoch 134 | LR=1.87e-05 | TrainLoss=7.2544 | ValLoss=1.5229 | ValAcc=46.26% | BestValLoss=1.5011 | ES=15/30\n",
      "Epoch 135 | LR=1.87e-05 | TrainLoss=7.2650 | ValLoss=1.5088 | ValAcc=46.69% | BestValLoss=1.5011 | ES=16/30\n",
      "Epoch 136 | LR=1.87e-05 | TrainLoss=7.2551 | ValLoss=1.5176 | ValAcc=46.58% | BestValLoss=1.5011 | ES=17/30\n",
      "Epoch 137 | LR=1.87e-05 | TrainLoss=7.2645 | ValLoss=1.5081 | ValAcc=46.73% | BestValLoss=1.5011 | ES=18/30\n",
      "Epoch 138 | LR=1.87e-05 | TrainLoss=7.2590 | ValLoss=1.5169 | ValAcc=46.44% | BestValLoss=1.5011 | ES=19/30\n",
      "Epoch 139 | LR=1.87e-05 | TrainLoss=7.2534 | ValLoss=1.5205 | ValAcc=46.57% | BestValLoss=1.5011 | ES=20/30\n",
      "[LR DROP] 1.87e-05 -> 9.37e-06 (val_loss=1.5159)\n",
      "Epoch 140 | LR=9.37e-06 | TrainLoss=7.2602 | ValLoss=1.5159 | ValAcc=46.46% | BestValLoss=1.5011 | ES=21/30\n",
      "Epoch 141 | LR=9.37e-06 | TrainLoss=7.2720 | ValLoss=1.5059 | ValAcc=46.82% | BestValLoss=1.5011 | ES=22/30\n",
      "Epoch 142 | LR=9.37e-06 | TrainLoss=7.2550 | ValLoss=1.5071 | ValAcc=46.76% | BestValLoss=1.5011 | ES=23/30\n",
      "Epoch 143 | LR=9.37e-06 | TrainLoss=7.2653 | ValLoss=1.5092 | ValAcc=46.85% | BestValLoss=1.5011 | ES=24/30\n",
      "Epoch 144 | LR=9.37e-06 | TrainLoss=7.2639 | ValLoss=1.5128 | ValAcc=46.73% | BestValLoss=1.5011 | ES=25/30\n",
      "Epoch 145 | LR=9.37e-06 | TrainLoss=7.2649 | ValLoss=1.5082 | ValAcc=46.72% | BestValLoss=1.5011 | ES=26/30\n",
      "Epoch 146 | LR=9.37e-06 | TrainLoss=7.2589 | ValLoss=1.5113 | ValAcc=46.76% | BestValLoss=1.5011 | ES=27/30\n",
      "Epoch 147 | LR=9.37e-06 | TrainLoss=7.2699 | ValLoss=1.5098 | ValAcc=46.79% | BestValLoss=1.5011 | ES=28/30\n",
      "Epoch 148 | LR=9.37e-06 | TrainLoss=7.2501 | ValLoss=1.5064 | ValAcc=46.88% | BestValLoss=1.5011 | ES=29/30\n",
      "[INFO] Early stopping triggered.\n",
      "[FOLD TEST] 20:47.82%, 15:47.58%, 10:46.85%, 5:43.99%, 0:38.48%, -5:28.50%, -10:18.08%, -15:13.25%, -20:11.55%, -25:11.38%, -30:11.04%, -35:11.42%, -40:10.95%\n",
      "\n",
      "========== Fold 4/5 ==========\n",
      "[FOLD 4] train_sample_counts: {0: 14159, 1: 14014, 2: 14267, 3: 14137, 4: 14080, 5: 14306, 6: 13972, 7: 14065, 8: 14160}\n",
      "[FOLD 4] val_sample_counts  : {0: 3539, 1: 3504, 2: 3566, 3: 3534, 4: 3520, 5: 3576, 6: 3493, 7: 3517, 8: 3541}\n",
      "Epoch 001 | LR=3.00e-04 | TrainLoss=8.2618 | ValLoss=2.1023 | ValAcc=18.93% | BestValLoss=inf | ES=0/30\n",
      "Epoch 002 | LR=3.00e-04 | TrainLoss=8.1102 | ValLoss=2.0308 | ValAcc=24.05% | BestValLoss=2.1023 | ES=0/30\n",
      "Epoch 003 | LR=3.00e-04 | TrainLoss=8.0140 | ValLoss=1.9831 | ValAcc=25.96% | BestValLoss=2.0308 | ES=0/30\n",
      "Epoch 004 | LR=3.00e-04 | TrainLoss=7.9545 | ValLoss=1.9761 | ValAcc=25.87% | BestValLoss=1.9831 | ES=0/30\n",
      "Epoch 005 | LR=3.00e-04 | TrainLoss=7.9038 | ValLoss=1.9463 | ValAcc=27.32% | BestValLoss=1.9761 | ES=0/30\n",
      "Epoch 006 | LR=3.00e-04 | TrainLoss=7.8582 | ValLoss=1.9186 | ValAcc=28.80% | BestValLoss=1.9463 | ES=0/30\n",
      "Epoch 007 | LR=3.00e-04 | TrainLoss=7.8400 | ValLoss=1.8896 | ValAcc=29.81% | BestValLoss=1.9186 | ES=0/30\n",
      "Epoch 008 | LR=3.00e-04 | TrainLoss=7.7863 | ValLoss=1.8847 | ValAcc=30.56% | BestValLoss=1.8896 | ES=0/30\n",
      "Epoch 009 | LR=3.00e-04 | TrainLoss=7.7587 | ValLoss=1.8489 | ValAcc=31.43% | BestValLoss=1.8847 | ES=0/30\n",
      "Epoch 010 | LR=3.00e-04 | TrainLoss=7.7422 | ValLoss=1.8540 | ValAcc=31.91% | BestValLoss=1.8489 | ES=0/30\n",
      "Epoch 011 | LR=3.00e-04 | TrainLoss=7.7218 | ValLoss=1.8433 | ValAcc=32.36% | BestValLoss=1.8489 | ES=1/30\n",
      "Epoch 012 | LR=3.00e-04 | TrainLoss=7.6993 | ValLoss=1.7886 | ValAcc=34.42% | BestValLoss=1.8433 | ES=0/30\n",
      "Epoch 013 | LR=3.00e-04 | TrainLoss=7.6860 | ValLoss=1.8200 | ValAcc=33.91% | BestValLoss=1.7886 | ES=0/30\n",
      "Epoch 014 | LR=3.00e-04 | TrainLoss=7.6656 | ValLoss=1.7943 | ValAcc=33.83% | BestValLoss=1.7886 | ES=1/30\n",
      "Epoch 015 | LR=3.00e-04 | TrainLoss=7.6608 | ValLoss=1.7615 | ValAcc=35.73% | BestValLoss=1.7886 | ES=2/30\n",
      "Epoch 016 | LR=3.00e-04 | TrainLoss=7.6496 | ValLoss=1.7597 | ValAcc=35.61% | BestValLoss=1.7615 | ES=0/30\n",
      "Epoch 017 | LR=3.00e-04 | TrainLoss=7.6224 | ValLoss=1.7562 | ValAcc=36.29% | BestValLoss=1.7597 | ES=0/30\n",
      "Epoch 018 | LR=3.00e-04 | TrainLoss=7.6116 | ValLoss=1.7421 | ValAcc=37.07% | BestValLoss=1.7562 | ES=0/30\n",
      "Epoch 019 | LR=3.00e-04 | TrainLoss=7.6032 | ValLoss=1.7700 | ValAcc=36.33% | BestValLoss=1.7421 | ES=0/30\n",
      "Epoch 020 | LR=3.00e-04 | TrainLoss=7.6027 | ValLoss=1.7742 | ValAcc=35.34% | BestValLoss=1.7421 | ES=1/30\n",
      "Epoch 021 | LR=3.00e-04 | TrainLoss=7.5899 | ValLoss=1.7249 | ValAcc=37.48% | BestValLoss=1.7421 | ES=2/30\n",
      "Epoch 022 | LR=3.00e-04 | TrainLoss=7.5786 | ValLoss=1.7193 | ValAcc=37.90% | BestValLoss=1.7249 | ES=0/30\n",
      "Epoch 023 | LR=3.00e-04 | TrainLoss=7.5635 | ValLoss=1.7120 | ValAcc=37.61% | BestValLoss=1.7193 | ES=0/30\n",
      "Epoch 024 | LR=3.00e-04 | TrainLoss=7.5547 | ValLoss=1.7400 | ValAcc=37.10% | BestValLoss=1.7120 | ES=0/30\n",
      "Epoch 025 | LR=3.00e-04 | TrainLoss=7.5580 | ValLoss=1.6873 | ValAcc=39.27% | BestValLoss=1.7120 | ES=1/30\n",
      "Epoch 026 | LR=3.00e-04 | TrainLoss=7.5408 | ValLoss=1.6493 | ValAcc=40.02% | BestValLoss=1.6873 | ES=0/30\n",
      "Epoch 027 | LR=3.00e-04 | TrainLoss=7.5414 | ValLoss=1.6927 | ValAcc=39.23% | BestValLoss=1.6493 | ES=0/30\n",
      "Epoch 028 | LR=3.00e-04 | TrainLoss=7.5262 | ValLoss=1.6915 | ValAcc=39.47% | BestValLoss=1.6493 | ES=1/30\n",
      "Epoch 029 | LR=3.00e-04 | TrainLoss=7.5284 | ValLoss=1.6726 | ValAcc=39.14% | BestValLoss=1.6493 | ES=2/30\n",
      "Epoch 030 | LR=3.00e-04 | TrainLoss=7.5218 | ValLoss=1.7072 | ValAcc=38.91% | BestValLoss=1.6493 | ES=3/30\n",
      "Epoch 031 | LR=3.00e-04 | TrainLoss=7.5108 | ValLoss=1.6638 | ValAcc=40.47% | BestValLoss=1.6493 | ES=4/30\n",
      "Epoch 032 | LR=3.00e-04 | TrainLoss=7.5073 | ValLoss=1.6349 | ValAcc=41.55% | BestValLoss=1.6493 | ES=5/30\n",
      "Epoch 033 | LR=3.00e-04 | TrainLoss=7.5011 | ValLoss=1.6569 | ValAcc=40.72% | BestValLoss=1.6349 | ES=0/30\n",
      "Epoch 034 | LR=3.00e-04 | TrainLoss=7.5015 | ValLoss=1.6477 | ValAcc=40.98% | BestValLoss=1.6349 | ES=1/30\n",
      "Epoch 035 | LR=3.00e-04 | TrainLoss=7.4951 | ValLoss=1.6863 | ValAcc=39.41% | BestValLoss=1.6349 | ES=2/30\n",
      "Epoch 036 | LR=3.00e-04 | TrainLoss=7.4850 | ValLoss=1.6963 | ValAcc=39.41% | BestValLoss=1.6349 | ES=3/30\n",
      "Epoch 037 | LR=3.00e-04 | TrainLoss=7.4795 | ValLoss=1.6327 | ValAcc=41.56% | BestValLoss=1.6349 | ES=4/30\n",
      "Epoch 038 | LR=3.00e-04 | TrainLoss=7.4703 | ValLoss=1.6362 | ValAcc=41.27% | BestValLoss=1.6327 | ES=0/30\n",
      "Epoch 039 | LR=3.00e-04 | TrainLoss=7.4612 | ValLoss=1.6419 | ValAcc=41.31% | BestValLoss=1.6327 | ES=1/30\n",
      "Epoch 040 | LR=3.00e-04 | TrainLoss=7.4766 | ValLoss=1.6084 | ValAcc=42.34% | BestValLoss=1.6327 | ES=2/30\n",
      "Epoch 041 | LR=3.00e-04 | TrainLoss=7.4473 | ValLoss=1.6311 | ValAcc=41.53% | BestValLoss=1.6084 | ES=0/30\n",
      "Epoch 042 | LR=3.00e-04 | TrainLoss=7.4543 | ValLoss=1.6790 | ValAcc=40.40% | BestValLoss=1.6084 | ES=1/30\n",
      "Epoch 043 | LR=3.00e-04 | TrainLoss=7.4474 | ValLoss=1.6390 | ValAcc=41.38% | BestValLoss=1.6084 | ES=2/30\n",
      "Epoch 044 | LR=3.00e-04 | TrainLoss=7.4406 | ValLoss=1.6303 | ValAcc=41.63% | BestValLoss=1.6084 | ES=3/30\n",
      "Epoch 045 | LR=3.00e-04 | TrainLoss=7.4490 | ValLoss=1.6032 | ValAcc=42.48% | BestValLoss=1.6084 | ES=4/30\n",
      "Epoch 046 | LR=3.00e-04 | TrainLoss=7.4375 | ValLoss=1.5768 | ValAcc=43.62% | BestValLoss=1.6032 | ES=0/30\n",
      "Epoch 047 | LR=3.00e-04 | TrainLoss=7.4355 | ValLoss=1.5991 | ValAcc=43.11% | BestValLoss=1.5768 | ES=0/30\n",
      "Epoch 048 | LR=3.00e-04 | TrainLoss=7.4328 | ValLoss=1.6668 | ValAcc=41.09% | BestValLoss=1.5768 | ES=1/30\n",
      "Epoch 049 | LR=3.00e-04 | TrainLoss=7.4306 | ValLoss=1.6096 | ValAcc=42.64% | BestValLoss=1.5768 | ES=2/30\n",
      "Epoch 050 | LR=3.00e-04 | TrainLoss=7.4344 | ValLoss=1.6274 | ValAcc=42.29% | BestValLoss=1.5768 | ES=3/30\n",
      "Epoch 051 | LR=3.00e-04 | TrainLoss=7.4268 | ValLoss=1.6239 | ValAcc=42.64% | BestValLoss=1.5768 | ES=4/30\n",
      "Epoch 052 | LR=3.00e-04 | TrainLoss=7.4249 | ValLoss=1.5945 | ValAcc=43.34% | BestValLoss=1.5768 | ES=5/30\n",
      "Epoch 053 | LR=3.00e-04 | TrainLoss=7.4156 | ValLoss=1.5885 | ValAcc=43.85% | BestValLoss=1.5768 | ES=6/30\n",
      "Epoch 054 | LR=3.00e-04 | TrainLoss=7.4197 | ValLoss=1.6184 | ValAcc=42.59% | BestValLoss=1.5768 | ES=7/30\n",
      "Epoch 055 | LR=3.00e-04 | TrainLoss=7.4082 | ValLoss=1.5982 | ValAcc=43.22% | BestValLoss=1.5768 | ES=8/30\n",
      "Epoch 056 | LR=3.00e-04 | TrainLoss=7.4098 | ValLoss=1.5907 | ValAcc=43.57% | BestValLoss=1.5768 | ES=9/30\n",
      "[LR DROP] 3.00e-04 -> 1.50e-04 (val_loss=1.6033)\n",
      "Epoch 057 | LR=1.50e-04 | TrainLoss=7.4072 | ValLoss=1.6033 | ValAcc=43.21% | BestValLoss=1.5768 | ES=10/30\n",
      "Epoch 058 | LR=1.50e-04 | TrainLoss=7.3763 | ValLoss=1.5687 | ValAcc=44.56% | BestValLoss=1.5768 | ES=11/30\n",
      "Epoch 059 | LR=1.50e-04 | TrainLoss=7.3648 | ValLoss=1.5780 | ValAcc=44.35% | BestValLoss=1.5687 | ES=0/30\n",
      "Epoch 060 | LR=1.50e-04 | TrainLoss=7.3794 | ValLoss=1.5619 | ValAcc=44.79% | BestValLoss=1.5687 | ES=1/30\n",
      "Epoch 061 | LR=1.50e-04 | TrainLoss=7.3663 | ValLoss=1.5633 | ValAcc=44.68% | BestValLoss=1.5619 | ES=0/30\n",
      "Epoch 062 | LR=1.50e-04 | TrainLoss=7.3616 | ValLoss=1.5703 | ValAcc=44.49% | BestValLoss=1.5619 | ES=1/30\n",
      "Epoch 063 | LR=1.50e-04 | TrainLoss=7.3568 | ValLoss=1.5527 | ValAcc=45.25% | BestValLoss=1.5619 | ES=2/30\n",
      "Epoch 064 | LR=1.50e-04 | TrainLoss=7.3727 | ValLoss=1.5795 | ValAcc=44.27% | BestValLoss=1.5527 | ES=0/30\n",
      "Epoch 065 | LR=1.50e-04 | TrainLoss=7.3583 | ValLoss=1.5704 | ValAcc=44.26% | BestValLoss=1.5527 | ES=1/30\n",
      "Epoch 066 | LR=1.50e-04 | TrainLoss=7.3578 | ValLoss=1.5670 | ValAcc=44.79% | BestValLoss=1.5527 | ES=2/30\n",
      "Epoch 067 | LR=1.50e-04 | TrainLoss=7.3651 | ValLoss=1.5829 | ValAcc=44.37% | BestValLoss=1.5527 | ES=3/30\n",
      "Epoch 068 | LR=1.50e-04 | TrainLoss=7.3435 | ValLoss=1.5319 | ValAcc=46.12% | BestValLoss=1.5527 | ES=4/30\n",
      "Epoch 069 | LR=1.50e-04 | TrainLoss=7.3562 | ValLoss=1.5370 | ValAcc=45.82% | BestValLoss=1.5319 | ES=0/30\n",
      "Epoch 070 | LR=1.50e-04 | TrainLoss=7.3485 | ValLoss=1.5562 | ValAcc=45.18% | BestValLoss=1.5319 | ES=1/30\n",
      "Epoch 071 | LR=1.50e-04 | TrainLoss=7.3484 | ValLoss=1.5491 | ValAcc=44.91% | BestValLoss=1.5319 | ES=2/30\n",
      "Epoch 072 | LR=1.50e-04 | TrainLoss=7.3378 | ValLoss=1.5350 | ValAcc=45.97% | BestValLoss=1.5319 | ES=3/30\n",
      "Epoch 073 | LR=1.50e-04 | TrainLoss=7.3616 | ValLoss=1.5714 | ValAcc=44.41% | BestValLoss=1.5319 | ES=4/30\n",
      "Epoch 074 | LR=1.50e-04 | TrainLoss=7.3384 | ValLoss=1.5977 | ValAcc=43.75% | BestValLoss=1.5319 | ES=5/30\n",
      "Epoch 075 | LR=1.50e-04 | TrainLoss=7.3313 | ValLoss=1.5771 | ValAcc=44.07% | BestValLoss=1.5319 | ES=6/30\n",
      "Epoch 076 | LR=1.50e-04 | TrainLoss=7.3346 | ValLoss=1.5909 | ValAcc=44.05% | BestValLoss=1.5319 | ES=7/30\n",
      "Epoch 077 | LR=1.50e-04 | TrainLoss=7.3405 | ValLoss=1.5364 | ValAcc=46.08% | BestValLoss=1.5319 | ES=8/30\n",
      "Epoch 078 | LR=1.50e-04 | TrainLoss=7.3331 | ValLoss=1.5669 | ValAcc=44.95% | BestValLoss=1.5319 | ES=9/30\n",
      "Epoch 079 | LR=1.50e-04 | TrainLoss=7.3412 | ValLoss=1.5303 | ValAcc=45.65% | BestValLoss=1.5319 | ES=10/30\n",
      "Epoch 080 | LR=1.50e-04 | TrainLoss=7.3288 | ValLoss=1.5557 | ValAcc=45.11% | BestValLoss=1.5303 | ES=0/30\n",
      "Epoch 081 | LR=1.50e-04 | TrainLoss=7.3265 | ValLoss=1.5470 | ValAcc=45.27% | BestValLoss=1.5303 | ES=1/30\n",
      "Epoch 082 | LR=1.50e-04 | TrainLoss=7.3338 | ValLoss=1.5534 | ValAcc=45.39% | BestValLoss=1.5303 | ES=2/30\n",
      "Epoch 083 | LR=1.50e-04 | TrainLoss=7.3317 | ValLoss=1.5580 | ValAcc=45.22% | BestValLoss=1.5303 | ES=3/30\n",
      "Epoch 084 | LR=1.50e-04 | TrainLoss=7.3315 | ValLoss=1.5407 | ValAcc=45.45% | BestValLoss=1.5303 | ES=4/30\n",
      "Epoch 085 | LR=1.50e-04 | TrainLoss=7.3346 | ValLoss=1.5328 | ValAcc=46.15% | BestValLoss=1.5303 | ES=5/30\n",
      "Epoch 086 | LR=1.50e-04 | TrainLoss=7.3312 | ValLoss=1.5253 | ValAcc=46.03% | BestValLoss=1.5303 | ES=6/30\n",
      "Epoch 087 | LR=1.50e-04 | TrainLoss=7.3299 | ValLoss=1.5524 | ValAcc=45.25% | BestValLoss=1.5253 | ES=0/30\n",
      "Epoch 088 | LR=1.50e-04 | TrainLoss=7.3208 | ValLoss=1.5696 | ValAcc=44.77% | BestValLoss=1.5253 | ES=1/30\n",
      "Epoch 089 | LR=1.50e-04 | TrainLoss=7.3232 | ValLoss=1.5479 | ValAcc=45.41% | BestValLoss=1.5253 | ES=2/30\n",
      "Epoch 090 | LR=1.50e-04 | TrainLoss=7.3216 | ValLoss=1.5673 | ValAcc=44.51% | BestValLoss=1.5253 | ES=3/30\n",
      "Epoch 091 | LR=1.50e-04 | TrainLoss=7.3239 | ValLoss=1.5436 | ValAcc=45.31% | BestValLoss=1.5253 | ES=4/30\n",
      "Epoch 092 | LR=1.50e-04 | TrainLoss=7.3147 | ValLoss=1.5665 | ValAcc=45.02% | BestValLoss=1.5253 | ES=5/30\n",
      "Epoch 093 | LR=1.50e-04 | TrainLoss=7.3168 | ValLoss=1.5586 | ValAcc=44.98% | BestValLoss=1.5253 | ES=6/30\n",
      "Epoch 094 | LR=1.50e-04 | TrainLoss=7.3132 | ValLoss=1.5625 | ValAcc=44.85% | BestValLoss=1.5253 | ES=7/30\n",
      "Epoch 095 | LR=1.50e-04 | TrainLoss=7.3267 | ValLoss=1.5496 | ValAcc=45.59% | BestValLoss=1.5253 | ES=8/30\n",
      "Epoch 096 | LR=1.50e-04 | TrainLoss=7.3165 | ValLoss=1.5446 | ValAcc=45.80% | BestValLoss=1.5253 | ES=9/30\n",
      "[LR DROP] 1.50e-04 -> 7.50e-05 (val_loss=1.5292)\n",
      "Epoch 097 | LR=7.50e-05 | TrainLoss=7.3004 | ValLoss=1.5292 | ValAcc=46.22% | BestValLoss=1.5253 | ES=10/30\n",
      "Epoch 098 | LR=7.50e-05 | TrainLoss=7.2996 | ValLoss=1.5342 | ValAcc=46.02% | BestValLoss=1.5253 | ES=11/30\n",
      "Epoch 099 | LR=7.50e-05 | TrainLoss=7.2999 | ValLoss=1.5368 | ValAcc=45.90% | BestValLoss=1.5253 | ES=12/30\n",
      "Epoch 100 | LR=7.50e-05 | TrainLoss=7.2907 | ValLoss=1.5310 | ValAcc=46.06% | BestValLoss=1.5253 | ES=13/30\n",
      "Epoch 101 | LR=7.50e-05 | TrainLoss=7.2942 | ValLoss=1.5189 | ValAcc=46.58% | BestValLoss=1.5253 | ES=14/30\n",
      "Epoch 102 | LR=7.50e-05 | TrainLoss=7.2918 | ValLoss=1.5155 | ValAcc=46.56% | BestValLoss=1.5189 | ES=0/30\n",
      "Epoch 103 | LR=7.50e-05 | TrainLoss=7.2974 | ValLoss=1.5384 | ValAcc=45.97% | BestValLoss=1.5155 | ES=0/30\n",
      "Epoch 104 | LR=7.50e-05 | TrainLoss=7.3021 | ValLoss=1.5429 | ValAcc=45.64% | BestValLoss=1.5155 | ES=1/30\n",
      "Epoch 105 | LR=7.50e-05 | TrainLoss=7.2885 | ValLoss=1.5264 | ValAcc=46.27% | BestValLoss=1.5155 | ES=2/30\n",
      "Epoch 106 | LR=7.50e-05 | TrainLoss=7.2944 | ValLoss=1.5164 | ValAcc=46.50% | BestValLoss=1.5155 | ES=3/30\n",
      "Epoch 107 | LR=7.50e-05 | TrainLoss=7.2848 | ValLoss=1.5136 | ValAcc=46.76% | BestValLoss=1.5155 | ES=4/30\n",
      "Epoch 108 | LR=7.50e-05 | TrainLoss=7.2973 | ValLoss=1.5210 | ValAcc=46.20% | BestValLoss=1.5136 | ES=0/30\n",
      "Epoch 109 | LR=7.50e-05 | TrainLoss=7.2945 | ValLoss=1.5559 | ValAcc=45.18% | BestValLoss=1.5136 | ES=1/30\n",
      "Epoch 110 | LR=7.50e-05 | TrainLoss=7.2807 | ValLoss=1.5286 | ValAcc=46.00% | BestValLoss=1.5136 | ES=2/30\n",
      "Epoch 111 | LR=7.50e-05 | TrainLoss=7.2920 | ValLoss=1.5586 | ValAcc=44.95% | BestValLoss=1.5136 | ES=3/30\n",
      "Epoch 112 | LR=7.50e-05 | TrainLoss=7.2841 | ValLoss=1.5321 | ValAcc=46.46% | BestValLoss=1.5136 | ES=4/30\n",
      "Epoch 113 | LR=7.50e-05 | TrainLoss=7.2957 | ValLoss=1.5176 | ValAcc=46.09% | BestValLoss=1.5136 | ES=5/30\n",
      "Epoch 114 | LR=7.50e-05 | TrainLoss=7.2966 | ValLoss=1.5390 | ValAcc=45.71% | BestValLoss=1.5136 | ES=6/30\n",
      "Epoch 115 | LR=7.50e-05 | TrainLoss=7.2994 | ValLoss=1.5219 | ValAcc=46.42% | BestValLoss=1.5136 | ES=7/30\n",
      "Epoch 116 | LR=7.50e-05 | TrainLoss=7.2810 | ValLoss=1.5229 | ValAcc=46.23% | BestValLoss=1.5136 | ES=8/30\n",
      "Epoch 117 | LR=7.50e-05 | TrainLoss=7.2930 | ValLoss=1.5292 | ValAcc=46.09% | BestValLoss=1.5136 | ES=9/30\n",
      "[LR DROP] 7.50e-05 -> 3.75e-05 (val_loss=1.5270)\n",
      "Epoch 118 | LR=3.75e-05 | TrainLoss=7.2849 | ValLoss=1.5270 | ValAcc=46.06% | BestValLoss=1.5136 | ES=10/30\n",
      "Epoch 119 | LR=3.75e-05 | TrainLoss=7.2734 | ValLoss=1.5153 | ValAcc=46.68% | BestValLoss=1.5136 | ES=11/30\n",
      "Epoch 120 | LR=3.75e-05 | TrainLoss=7.2638 | ValLoss=1.5141 | ValAcc=46.66% | BestValLoss=1.5136 | ES=12/30\n",
      "Epoch 121 | LR=3.75e-05 | TrainLoss=7.2774 | ValLoss=1.5165 | ValAcc=46.92% | BestValLoss=1.5136 | ES=13/30\n",
      "Epoch 122 | LR=3.75e-05 | TrainLoss=7.2640 | ValLoss=1.5152 | ValAcc=46.58% | BestValLoss=1.5136 | ES=14/30\n",
      "Epoch 123 | LR=3.75e-05 | TrainLoss=7.2722 | ValLoss=1.5147 | ValAcc=46.61% | BestValLoss=1.5136 | ES=15/30\n",
      "Epoch 124 | LR=3.75e-05 | TrainLoss=7.2771 | ValLoss=1.5017 | ValAcc=47.25% | BestValLoss=1.5136 | ES=16/30\n",
      "Epoch 125 | LR=3.75e-05 | TrainLoss=7.2767 | ValLoss=1.5278 | ValAcc=46.08% | BestValLoss=1.5017 | ES=0/30\n",
      "Epoch 126 | LR=3.75e-05 | TrainLoss=7.2747 | ValLoss=1.5292 | ValAcc=46.11% | BestValLoss=1.5017 | ES=1/30\n",
      "Epoch 127 | LR=3.75e-05 | TrainLoss=7.2739 | ValLoss=1.5279 | ValAcc=46.09% | BestValLoss=1.5017 | ES=2/30\n",
      "Epoch 128 | LR=3.75e-05 | TrainLoss=7.2748 | ValLoss=1.5183 | ValAcc=46.43% | BestValLoss=1.5017 | ES=3/30\n",
      "Epoch 129 | LR=3.75e-05 | TrainLoss=7.2739 | ValLoss=1.5210 | ValAcc=46.49% | BestValLoss=1.5017 | ES=4/30\n",
      "Epoch 130 | LR=3.75e-05 | TrainLoss=7.2682 | ValLoss=1.5204 | ValAcc=46.47% | BestValLoss=1.5017 | ES=5/30\n",
      "Epoch 131 | LR=3.75e-05 | TrainLoss=7.2885 | ValLoss=1.5031 | ValAcc=47.16% | BestValLoss=1.5017 | ES=6/30\n",
      "Epoch 132 | LR=3.75e-05 | TrainLoss=7.2633 | ValLoss=1.5070 | ValAcc=46.92% | BestValLoss=1.5017 | ES=7/30\n",
      "Epoch 133 | LR=3.75e-05 | TrainLoss=7.2541 | ValLoss=1.5059 | ValAcc=47.15% | BestValLoss=1.5017 | ES=8/30\n",
      "Epoch 134 | LR=3.75e-05 | TrainLoss=7.2726 | ValLoss=1.5211 | ValAcc=46.81% | BestValLoss=1.5017 | ES=9/30\n",
      "[LR DROP] 3.75e-05 -> 1.87e-05 (val_loss=1.5151)\n",
      "Epoch 135 | LR=1.87e-05 | TrainLoss=7.2799 | ValLoss=1.5151 | ValAcc=46.92% | BestValLoss=1.5017 | ES=10/30\n",
      "Epoch 136 | LR=1.87e-05 | TrainLoss=7.2660 | ValLoss=1.5032 | ValAcc=47.25% | BestValLoss=1.5017 | ES=11/30\n",
      "Epoch 137 | LR=1.87e-05 | TrainLoss=7.2617 | ValLoss=1.5092 | ValAcc=46.95% | BestValLoss=1.5017 | ES=12/30\n",
      "Epoch 138 | LR=1.87e-05 | TrainLoss=7.2711 | ValLoss=1.5130 | ValAcc=46.68% | BestValLoss=1.5017 | ES=13/30\n",
      "Epoch 139 | LR=1.87e-05 | TrainLoss=7.2681 | ValLoss=1.4966 | ValAcc=47.32% | BestValLoss=1.5017 | ES=14/30\n",
      "Epoch 140 | LR=1.87e-05 | TrainLoss=7.2711 | ValLoss=1.5017 | ValAcc=47.13% | BestValLoss=1.4966 | ES=0/30\n",
      "Epoch 141 | LR=1.87e-05 | TrainLoss=7.2649 | ValLoss=1.5120 | ValAcc=46.81% | BestValLoss=1.4966 | ES=1/30\n",
      "Epoch 142 | LR=1.87e-05 | TrainLoss=7.2646 | ValLoss=1.5104 | ValAcc=46.52% | BestValLoss=1.4966 | ES=2/30\n",
      "Epoch 143 | LR=1.87e-05 | TrainLoss=7.2577 | ValLoss=1.5140 | ValAcc=46.72% | BestValLoss=1.4966 | ES=3/30\n",
      "Epoch 144 | LR=1.87e-05 | TrainLoss=7.2677 | ValLoss=1.5169 | ValAcc=46.42% | BestValLoss=1.4966 | ES=4/30\n",
      "Epoch 145 | LR=1.87e-05 | TrainLoss=7.2632 | ValLoss=1.5074 | ValAcc=46.97% | BestValLoss=1.4966 | ES=5/30\n",
      "Epoch 146 | LR=1.87e-05 | TrainLoss=7.2680 | ValLoss=1.5099 | ValAcc=46.88% | BestValLoss=1.4966 | ES=6/30\n",
      "Epoch 147 | LR=1.87e-05 | TrainLoss=7.2634 | ValLoss=1.5097 | ValAcc=46.64% | BestValLoss=1.4966 | ES=7/30\n",
      "Epoch 148 | LR=1.87e-05 | TrainLoss=7.2705 | ValLoss=1.5219 | ValAcc=46.24% | BestValLoss=1.4966 | ES=8/30\n",
      "Epoch 149 | LR=1.87e-05 | TrainLoss=7.2617 | ValLoss=1.5227 | ValAcc=46.37% | BestValLoss=1.4966 | ES=9/30\n",
      "[LR DROP] 1.87e-05 -> 9.37e-06 (val_loss=1.5138)\n",
      "Epoch 150 | LR=9.37e-06 | TrainLoss=7.2642 | ValLoss=1.5138 | ValAcc=46.48% | BestValLoss=1.4966 | ES=10/30\n",
      "Epoch 151 | LR=9.37e-06 | TrainLoss=7.2588 | ValLoss=1.5165 | ValAcc=46.60% | BestValLoss=1.4966 | ES=11/30\n",
      "Epoch 152 | LR=9.37e-06 | TrainLoss=7.2546 | ValLoss=1.5140 | ValAcc=46.63% | BestValLoss=1.4966 | ES=12/30\n",
      "Epoch 153 | LR=9.37e-06 | TrainLoss=7.2632 | ValLoss=1.5164 | ValAcc=46.56% | BestValLoss=1.4966 | ES=13/30\n",
      "Epoch 154 | LR=9.37e-06 | TrainLoss=7.2605 | ValLoss=1.5044 | ValAcc=47.07% | BestValLoss=1.4966 | ES=14/30\n",
      "Epoch 155 | LR=9.37e-06 | TrainLoss=7.2603 | ValLoss=1.5131 | ValAcc=46.80% | BestValLoss=1.4966 | ES=15/30\n",
      "Epoch 156 | LR=9.37e-06 | TrainLoss=7.2560 | ValLoss=1.5142 | ValAcc=46.74% | BestValLoss=1.4966 | ES=16/30\n",
      "Epoch 157 | LR=9.37e-06 | TrainLoss=7.2613 | ValLoss=1.5067 | ValAcc=47.04% | BestValLoss=1.4966 | ES=17/30\n",
      "Epoch 158 | LR=9.37e-06 | TrainLoss=7.2694 | ValLoss=1.5134 | ValAcc=46.75% | BestValLoss=1.4966 | ES=18/30\n",
      "Epoch 159 | LR=9.37e-06 | TrainLoss=7.2486 | ValLoss=1.5132 | ValAcc=46.78% | BestValLoss=1.4966 | ES=19/30\n",
      "Epoch 160 | LR=9.37e-06 | TrainLoss=7.2618 | ValLoss=1.5087 | ValAcc=46.91% | BestValLoss=1.4966 | ES=20/30\n",
      "[LR DROP] 9.37e-06 -> 4.69e-06 (val_loss=1.5152)\n",
      "Epoch 161 | LR=4.69e-06 | TrainLoss=7.2597 | ValLoss=1.5152 | ValAcc=46.67% | BestValLoss=1.4966 | ES=21/30\n",
      "Epoch 162 | LR=4.69e-06 | TrainLoss=7.2620 | ValLoss=1.5054 | ValAcc=47.01% | BestValLoss=1.4966 | ES=22/30\n",
      "Epoch 163 | LR=4.69e-06 | TrainLoss=7.2511 | ValLoss=1.5146 | ValAcc=46.68% | BestValLoss=1.4966 | ES=23/30\n",
      "Epoch 164 | LR=4.69e-06 | TrainLoss=7.2526 | ValLoss=1.5092 | ValAcc=46.82% | BestValLoss=1.4966 | ES=24/30\n",
      "Epoch 165 | LR=4.69e-06 | TrainLoss=7.2640 | ValLoss=1.5102 | ValAcc=46.90% | BestValLoss=1.4966 | ES=25/30\n",
      "Epoch 166 | LR=4.69e-06 | TrainLoss=7.2498 | ValLoss=1.5096 | ValAcc=46.85% | BestValLoss=1.4966 | ES=26/30\n",
      "Epoch 167 | LR=4.69e-06 | TrainLoss=7.2537 | ValLoss=1.5125 | ValAcc=46.81% | BestValLoss=1.4966 | ES=27/30\n",
      "Epoch 168 | LR=4.69e-06 | TrainLoss=7.2558 | ValLoss=1.5084 | ValAcc=46.83% | BestValLoss=1.4966 | ES=28/30\n",
      "Epoch 169 | LR=4.69e-06 | TrainLoss=7.2422 | ValLoss=1.5132 | ValAcc=46.64% | BestValLoss=1.4966 | ES=29/30\n",
      "[INFO] Early stopping triggered.\n",
      "[FOLD TEST] 20:47.96%, 15:47.84%, 10:46.74%, 5:44.22%, 0:38.56%, -5:28.46%, -10:17.64%, -15:12.93%, -20:11.71%, -25:11.35%, -30:11.06%, -35:10.89%, -40:11.09%\n",
      "\n",
      "========== Fold 5/5 ==========\n",
      "[FOLD 5] train_sample_counts: {0: 14159, 1: 14014, 2: 14267, 3: 14136, 4: 14080, 5: 14306, 6: 13972, 7: 14065, 8: 14161}\n",
      "[FOLD 5] val_sample_counts  : {0: 3539, 1: 3504, 2: 3566, 3: 3535, 4: 3520, 5: 3576, 6: 3493, 7: 3517, 8: 3540}\n",
      "Epoch 001 | LR=3.00e-04 | TrainLoss=8.2556 | ValLoss=2.1059 | ValAcc=20.85% | BestValLoss=inf | ES=0/30\n",
      "Epoch 002 | LR=3.00e-04 | TrainLoss=8.1052 | ValLoss=2.0398 | ValAcc=23.70% | BestValLoss=2.1059 | ES=0/30\n",
      "Epoch 003 | LR=3.00e-04 | TrainLoss=8.0176 | ValLoss=2.0106 | ValAcc=24.20% | BestValLoss=2.0398 | ES=0/30\n",
      "Epoch 004 | LR=3.00e-04 | TrainLoss=7.9554 | ValLoss=1.9658 | ValAcc=25.89% | BestValLoss=2.0106 | ES=0/30\n",
      "Epoch 005 | LR=3.00e-04 | TrainLoss=7.9063 | ValLoss=1.9249 | ValAcc=28.72% | BestValLoss=1.9658 | ES=0/30\n",
      "Epoch 006 | LR=3.00e-04 | TrainLoss=7.8601 | ValLoss=1.9180 | ValAcc=28.45% | BestValLoss=1.9249 | ES=0/30\n",
      "Epoch 007 | LR=3.00e-04 | TrainLoss=7.8240 | ValLoss=1.8766 | ValAcc=31.48% | BestValLoss=1.9180 | ES=0/30\n",
      "Epoch 008 | LR=3.00e-04 | TrainLoss=7.8014 | ValLoss=1.9277 | ValAcc=28.52% | BestValLoss=1.8766 | ES=0/30\n",
      "Epoch 009 | LR=3.00e-04 | TrainLoss=7.7695 | ValLoss=1.8963 | ValAcc=30.04% | BestValLoss=1.8766 | ES=1/30\n",
      "Epoch 010 | LR=3.00e-04 | TrainLoss=7.7468 | ValLoss=1.8597 | ValAcc=31.55% | BestValLoss=1.8766 | ES=2/30\n",
      "Epoch 011 | LR=3.00e-04 | TrainLoss=7.7333 | ValLoss=1.8511 | ValAcc=32.09% | BestValLoss=1.8597 | ES=0/30\n",
      "Epoch 012 | LR=3.00e-04 | TrainLoss=7.7078 | ValLoss=1.8378 | ValAcc=32.55% | BestValLoss=1.8511 | ES=0/30\n",
      "Epoch 013 | LR=3.00e-04 | TrainLoss=7.6862 | ValLoss=1.7909 | ValAcc=34.77% | BestValLoss=1.8378 | ES=0/30\n",
      "Epoch 014 | LR=3.00e-04 | TrainLoss=7.6724 | ValLoss=1.8380 | ValAcc=32.34% | BestValLoss=1.7909 | ES=0/30\n",
      "Epoch 015 | LR=3.00e-04 | TrainLoss=7.6672 | ValLoss=1.8264 | ValAcc=33.71% | BestValLoss=1.7909 | ES=1/30\n",
      "Epoch 016 | LR=3.00e-04 | TrainLoss=7.6387 | ValLoss=1.7808 | ValAcc=35.38% | BestValLoss=1.7909 | ES=2/30\n",
      "Epoch 017 | LR=3.00e-04 | TrainLoss=7.6269 | ValLoss=1.7522 | ValAcc=36.53% | BestValLoss=1.7808 | ES=0/30\n",
      "Epoch 018 | LR=3.00e-04 | TrainLoss=7.6169 | ValLoss=1.8000 | ValAcc=34.61% | BestValLoss=1.7522 | ES=0/30\n",
      "Epoch 019 | LR=3.00e-04 | TrainLoss=7.6132 | ValLoss=1.7514 | ValAcc=36.70% | BestValLoss=1.7522 | ES=1/30\n",
      "Epoch 020 | LR=3.00e-04 | TrainLoss=7.5939 | ValLoss=1.6878 | ValAcc=39.10% | BestValLoss=1.7514 | ES=0/30\n",
      "Epoch 021 | LR=3.00e-04 | TrainLoss=7.5869 | ValLoss=1.7650 | ValAcc=36.44% | BestValLoss=1.6878 | ES=0/30\n",
      "Epoch 022 | LR=3.00e-04 | TrainLoss=7.5751 | ValLoss=1.7458 | ValAcc=36.32% | BestValLoss=1.6878 | ES=1/30\n",
      "Epoch 023 | LR=3.00e-04 | TrainLoss=7.5712 | ValLoss=1.7321 | ValAcc=37.77% | BestValLoss=1.6878 | ES=2/30\n",
      "Epoch 024 | LR=3.00e-04 | TrainLoss=7.5645 | ValLoss=1.7380 | ValAcc=37.85% | BestValLoss=1.6878 | ES=3/30\n",
      "Epoch 025 | LR=3.00e-04 | TrainLoss=7.5450 | ValLoss=1.7485 | ValAcc=36.93% | BestValLoss=1.6878 | ES=4/30\n",
      "Epoch 026 | LR=3.00e-04 | TrainLoss=7.5540 | ValLoss=1.6968 | ValAcc=39.04% | BestValLoss=1.6878 | ES=5/30\n",
      "Epoch 027 | LR=3.00e-04 | TrainLoss=7.5328 | ValLoss=1.7321 | ValAcc=37.30% | BestValLoss=1.6878 | ES=6/30\n",
      "Epoch 028 | LR=3.00e-04 | TrainLoss=7.5324 | ValLoss=1.6995 | ValAcc=38.38% | BestValLoss=1.6878 | ES=7/30\n",
      "Epoch 029 | LR=3.00e-04 | TrainLoss=7.5213 | ValLoss=1.6934 | ValAcc=38.64% | BestValLoss=1.6878 | ES=8/30\n",
      "Epoch 030 | LR=3.00e-04 | TrainLoss=7.5194 | ValLoss=1.6802 | ValAcc=39.16% | BestValLoss=1.6878 | ES=9/30\n",
      "Epoch 031 | LR=3.00e-04 | TrainLoss=7.5100 | ValLoss=1.6618 | ValAcc=40.42% | BestValLoss=1.6802 | ES=0/30\n",
      "Epoch 032 | LR=3.00e-04 | TrainLoss=7.4967 | ValLoss=1.7108 | ValAcc=37.57% | BestValLoss=1.6618 | ES=0/30\n",
      "Epoch 033 | LR=3.00e-04 | TrainLoss=7.5007 | ValLoss=1.6831 | ValAcc=40.14% | BestValLoss=1.6618 | ES=1/30\n",
      "Epoch 034 | LR=3.00e-04 | TrainLoss=7.5003 | ValLoss=1.6697 | ValAcc=39.15% | BestValLoss=1.6618 | ES=2/30\n",
      "Epoch 035 | LR=3.00e-04 | TrainLoss=7.4887 | ValLoss=1.6410 | ValAcc=40.77% | BestValLoss=1.6618 | ES=3/30\n",
      "Epoch 036 | LR=3.00e-04 | TrainLoss=7.4791 | ValLoss=1.6522 | ValAcc=40.90% | BestValLoss=1.6410 | ES=0/30\n",
      "Epoch 037 | LR=3.00e-04 | TrainLoss=7.4867 | ValLoss=1.6648 | ValAcc=40.17% | BestValLoss=1.6410 | ES=1/30\n",
      "Epoch 038 | LR=3.00e-04 | TrainLoss=7.4661 | ValLoss=1.6413 | ValAcc=41.43% | BestValLoss=1.6410 | ES=2/30\n",
      "Epoch 039 | LR=3.00e-04 | TrainLoss=7.4665 | ValLoss=1.6626 | ValAcc=40.12% | BestValLoss=1.6410 | ES=3/30\n",
      "Epoch 040 | LR=3.00e-04 | TrainLoss=7.4669 | ValLoss=1.6546 | ValAcc=40.71% | BestValLoss=1.6410 | ES=4/30\n",
      "Epoch 041 | LR=3.00e-04 | TrainLoss=7.4624 | ValLoss=1.6618 | ValAcc=40.07% | BestValLoss=1.6410 | ES=5/30\n",
      "Epoch 042 | LR=3.00e-04 | TrainLoss=7.4567 | ValLoss=1.6494 | ValAcc=40.58% | BestValLoss=1.6410 | ES=6/30\n",
      "Epoch 043 | LR=3.00e-04 | TrainLoss=7.4453 | ValLoss=1.6906 | ValAcc=39.22% | BestValLoss=1.6410 | ES=7/30\n",
      "Epoch 044 | LR=3.00e-04 | TrainLoss=7.4451 | ValLoss=1.6698 | ValAcc=40.61% | BestValLoss=1.6410 | ES=8/30\n",
      "Epoch 045 | LR=3.00e-04 | TrainLoss=7.4470 | ValLoss=1.6696 | ValAcc=41.15% | BestValLoss=1.6410 | ES=9/30\n",
      "[LR DROP] 3.00e-04 -> 1.50e-04 (val_loss=1.6556)\n",
      "Epoch 046 | LR=1.50e-04 | TrainLoss=7.4430 | ValLoss=1.6556 | ValAcc=40.74% | BestValLoss=1.6410 | ES=10/30\n",
      "Epoch 047 | LR=1.50e-04 | TrainLoss=7.4035 | ValLoss=1.6397 | ValAcc=41.42% | BestValLoss=1.6410 | ES=11/30\n",
      "Epoch 048 | LR=1.50e-04 | TrainLoss=7.4187 | ValLoss=1.5966 | ValAcc=43.05% | BestValLoss=1.6397 | ES=0/30\n",
      "Epoch 049 | LR=1.50e-04 | TrainLoss=7.4037 | ValLoss=1.5872 | ValAcc=43.29% | BestValLoss=1.5966 | ES=0/30\n",
      "Epoch 050 | LR=1.50e-04 | TrainLoss=7.3930 | ValLoss=1.5724 | ValAcc=43.97% | BestValLoss=1.5872 | ES=0/30\n",
      "Epoch 051 | LR=1.50e-04 | TrainLoss=7.3915 | ValLoss=1.5956 | ValAcc=43.07% | BestValLoss=1.5724 | ES=0/30\n",
      "Epoch 052 | LR=1.50e-04 | TrainLoss=7.3789 | ValLoss=1.5828 | ValAcc=43.51% | BestValLoss=1.5724 | ES=1/30\n",
      "Epoch 053 | LR=1.50e-04 | TrainLoss=7.3847 | ValLoss=1.5927 | ValAcc=42.95% | BestValLoss=1.5724 | ES=2/30\n",
      "Epoch 054 | LR=1.50e-04 | TrainLoss=7.3878 | ValLoss=1.5737 | ValAcc=43.98% | BestValLoss=1.5724 | ES=3/30\n",
      "Epoch 055 | LR=1.50e-04 | TrainLoss=7.3894 | ValLoss=1.5960 | ValAcc=42.83% | BestValLoss=1.5724 | ES=4/30\n",
      "Epoch 056 | LR=1.50e-04 | TrainLoss=7.3878 | ValLoss=1.5818 | ValAcc=44.10% | BestValLoss=1.5724 | ES=5/30\n",
      "Epoch 057 | LR=1.50e-04 | TrainLoss=7.3804 | ValLoss=1.5867 | ValAcc=43.24% | BestValLoss=1.5724 | ES=6/30\n",
      "Epoch 058 | LR=1.50e-04 | TrainLoss=7.3910 | ValLoss=1.6018 | ValAcc=42.71% | BestValLoss=1.5724 | ES=7/30\n",
      "Epoch 059 | LR=1.50e-04 | TrainLoss=7.3819 | ValLoss=1.5693 | ValAcc=43.84% | BestValLoss=1.5724 | ES=8/30\n",
      "Epoch 060 | LR=1.50e-04 | TrainLoss=7.3797 | ValLoss=1.5815 | ValAcc=43.51% | BestValLoss=1.5693 | ES=0/30\n",
      "Epoch 061 | LR=1.50e-04 | TrainLoss=7.3768 | ValLoss=1.5843 | ValAcc=43.51% | BestValLoss=1.5693 | ES=1/30\n",
      "Epoch 062 | LR=1.50e-04 | TrainLoss=7.3773 | ValLoss=1.5961 | ValAcc=43.46% | BestValLoss=1.5693 | ES=2/30\n",
      "Epoch 063 | LR=1.50e-04 | TrainLoss=7.3673 | ValLoss=1.5865 | ValAcc=43.57% | BestValLoss=1.5693 | ES=3/30\n",
      "Epoch 064 | LR=1.50e-04 | TrainLoss=7.3519 | ValLoss=1.5901 | ValAcc=43.67% | BestValLoss=1.5693 | ES=4/30\n",
      "Epoch 065 | LR=1.50e-04 | TrainLoss=7.3620 | ValLoss=1.5698 | ValAcc=44.02% | BestValLoss=1.5693 | ES=5/30\n",
      "Epoch 066 | LR=1.50e-04 | TrainLoss=7.3696 | ValLoss=1.5682 | ValAcc=44.27% | BestValLoss=1.5693 | ES=6/30\n",
      "Epoch 067 | LR=1.50e-04 | TrainLoss=7.3690 | ValLoss=1.5511 | ValAcc=44.73% | BestValLoss=1.5682 | ES=0/30\n",
      "Epoch 068 | LR=1.50e-04 | TrainLoss=7.3715 | ValLoss=1.5730 | ValAcc=44.04% | BestValLoss=1.5511 | ES=0/30\n",
      "Epoch 069 | LR=1.50e-04 | TrainLoss=7.3602 | ValLoss=1.5835 | ValAcc=43.59% | BestValLoss=1.5511 | ES=1/30\n",
      "Epoch 070 | LR=1.50e-04 | TrainLoss=7.3606 | ValLoss=1.5865 | ValAcc=43.88% | BestValLoss=1.5511 | ES=2/30\n",
      "Epoch 071 | LR=1.50e-04 | TrainLoss=7.3592 | ValLoss=1.5661 | ValAcc=44.25% | BestValLoss=1.5511 | ES=3/30\n",
      "Epoch 072 | LR=1.50e-04 | TrainLoss=7.3565 | ValLoss=1.5888 | ValAcc=43.53% | BestValLoss=1.5511 | ES=4/30\n",
      "Epoch 073 | LR=1.50e-04 | TrainLoss=7.3675 | ValLoss=1.5616 | ValAcc=44.35% | BestValLoss=1.5511 | ES=5/30\n",
      "Epoch 074 | LR=1.50e-04 | TrainLoss=7.3562 | ValLoss=1.5796 | ValAcc=43.47% | BestValLoss=1.5511 | ES=6/30\n",
      "Epoch 075 | LR=1.50e-04 | TrainLoss=7.3498 | ValLoss=1.5421 | ValAcc=45.18% | BestValLoss=1.5511 | ES=7/30\n",
      "Epoch 076 | LR=1.50e-04 | TrainLoss=7.3500 | ValLoss=1.5744 | ValAcc=44.26% | BestValLoss=1.5421 | ES=0/30\n",
      "Epoch 077 | LR=1.50e-04 | TrainLoss=7.3462 | ValLoss=1.5488 | ValAcc=45.11% | BestValLoss=1.5421 | ES=1/30\n",
      "Epoch 078 | LR=1.50e-04 | TrainLoss=7.3478 | ValLoss=1.5878 | ValAcc=43.42% | BestValLoss=1.5421 | ES=2/30\n",
      "Epoch 079 | LR=1.50e-04 | TrainLoss=7.3541 | ValLoss=1.5646 | ValAcc=44.32% | BestValLoss=1.5421 | ES=3/30\n",
      "Epoch 080 | LR=1.50e-04 | TrainLoss=7.3486 | ValLoss=1.5517 | ValAcc=44.63% | BestValLoss=1.5421 | ES=4/30\n",
      "Epoch 081 | LR=1.50e-04 | TrainLoss=7.3488 | ValLoss=1.5664 | ValAcc=44.16% | BestValLoss=1.5421 | ES=5/30\n",
      "Epoch 082 | LR=1.50e-04 | TrainLoss=7.3363 | ValLoss=1.5439 | ValAcc=45.26% | BestValLoss=1.5421 | ES=6/30\n",
      "Epoch 083 | LR=1.50e-04 | TrainLoss=7.3423 | ValLoss=1.5544 | ValAcc=44.93% | BestValLoss=1.5421 | ES=7/30\n",
      "Epoch 084 | LR=1.50e-04 | TrainLoss=7.3380 | ValLoss=1.5674 | ValAcc=44.73% | BestValLoss=1.5421 | ES=8/30\n",
      "Epoch 085 | LR=1.50e-04 | TrainLoss=7.3344 | ValLoss=1.5567 | ValAcc=44.75% | BestValLoss=1.5421 | ES=9/30\n",
      "[LR DROP] 1.50e-04 -> 7.50e-05 (val_loss=1.5605)\n",
      "Epoch 086 | LR=7.50e-05 | TrainLoss=7.3405 | ValLoss=1.5605 | ValAcc=44.57% | BestValLoss=1.5421 | ES=10/30\n",
      "Epoch 087 | LR=7.50e-05 | TrainLoss=7.3195 | ValLoss=1.5375 | ValAcc=45.59% | BestValLoss=1.5421 | ES=11/30\n",
      "Epoch 088 | LR=7.50e-05 | TrainLoss=7.3262 | ValLoss=1.5477 | ValAcc=45.38% | BestValLoss=1.5375 | ES=0/30\n",
      "Epoch 089 | LR=7.50e-05 | TrainLoss=7.3202 | ValLoss=1.5577 | ValAcc=44.89% | BestValLoss=1.5375 | ES=1/30\n",
      "Epoch 090 | LR=7.50e-05 | TrainLoss=7.3118 | ValLoss=1.5602 | ValAcc=44.51% | BestValLoss=1.5375 | ES=2/30\n",
      "Epoch 091 | LR=7.50e-05 | TrainLoss=7.3127 | ValLoss=1.5486 | ValAcc=45.26% | BestValLoss=1.5375 | ES=3/30\n",
      "Epoch 092 | LR=7.50e-05 | TrainLoss=7.3136 | ValLoss=1.5267 | ValAcc=45.88% | BestValLoss=1.5375 | ES=4/30\n",
      "Epoch 093 | LR=7.50e-05 | TrainLoss=7.3218 | ValLoss=1.5475 | ValAcc=45.04% | BestValLoss=1.5267 | ES=0/30\n",
      "Epoch 094 | LR=7.50e-05 | TrainLoss=7.3073 | ValLoss=1.5293 | ValAcc=45.68% | BestValLoss=1.5267 | ES=1/30\n",
      "Epoch 095 | LR=7.50e-05 | TrainLoss=7.3036 | ValLoss=1.5432 | ValAcc=45.22% | BestValLoss=1.5267 | ES=2/30\n",
      "Epoch 096 | LR=7.50e-05 | TrainLoss=7.2993 | ValLoss=1.5465 | ValAcc=45.21% | BestValLoss=1.5267 | ES=3/30\n",
      "Epoch 097 | LR=7.50e-05 | TrainLoss=7.3039 | ValLoss=1.5495 | ValAcc=45.17% | BestValLoss=1.5267 | ES=4/30\n",
      "Epoch 098 | LR=7.50e-05 | TrainLoss=7.3097 | ValLoss=1.5492 | ValAcc=45.19% | BestValLoss=1.5267 | ES=5/30\n",
      "Epoch 099 | LR=7.50e-05 | TrainLoss=7.3146 | ValLoss=1.5349 | ValAcc=45.92% | BestValLoss=1.5267 | ES=6/30\n",
      "Epoch 100 | LR=7.50e-05 | TrainLoss=7.3135 | ValLoss=1.5366 | ValAcc=45.71% | BestValLoss=1.5267 | ES=7/30\n",
      "Epoch 101 | LR=7.50e-05 | TrainLoss=7.3106 | ValLoss=1.5455 | ValAcc=45.31% | BestValLoss=1.5267 | ES=8/30\n",
      "Epoch 102 | LR=7.50e-05 | TrainLoss=7.3043 | ValLoss=1.5393 | ValAcc=45.29% | BestValLoss=1.5267 | ES=9/30\n",
      "[LR DROP] 7.50e-05 -> 3.75e-05 (val_loss=1.5446)\n",
      "Epoch 103 | LR=3.75e-05 | TrainLoss=7.3044 | ValLoss=1.5446 | ValAcc=45.22% | BestValLoss=1.5267 | ES=10/30\n",
      "Epoch 104 | LR=3.75e-05 | TrainLoss=7.2943 | ValLoss=1.5277 | ValAcc=46.15% | BestValLoss=1.5267 | ES=11/30\n",
      "Epoch 105 | LR=3.75e-05 | TrainLoss=7.2945 | ValLoss=1.5452 | ValAcc=45.34% | BestValLoss=1.5267 | ES=12/30\n",
      "Epoch 106 | LR=3.75e-05 | TrainLoss=7.2968 | ValLoss=1.5395 | ValAcc=45.74% | BestValLoss=1.5267 | ES=13/30\n",
      "Epoch 107 | LR=3.75e-05 | TrainLoss=7.2949 | ValLoss=1.5346 | ValAcc=45.89% | BestValLoss=1.5267 | ES=14/30\n",
      "Epoch 108 | LR=3.75e-05 | TrainLoss=7.2921 | ValLoss=1.5242 | ValAcc=46.17% | BestValLoss=1.5267 | ES=15/30\n",
      "Epoch 109 | LR=3.75e-05 | TrainLoss=7.3018 | ValLoss=1.5304 | ValAcc=46.01% | BestValLoss=1.5242 | ES=0/30\n",
      "Epoch 110 | LR=3.75e-05 | TrainLoss=7.2880 | ValLoss=1.5177 | ValAcc=46.36% | BestValLoss=1.5242 | ES=1/30\n",
      "Epoch 111 | LR=3.75e-05 | TrainLoss=7.2834 | ValLoss=1.5407 | ValAcc=45.53% | BestValLoss=1.5177 | ES=0/30\n",
      "Epoch 112 | LR=3.75e-05 | TrainLoss=7.2808 | ValLoss=1.5463 | ValAcc=45.58% | BestValLoss=1.5177 | ES=1/30\n",
      "Epoch 113 | LR=3.75e-05 | TrainLoss=7.2946 | ValLoss=1.5269 | ValAcc=46.08% | BestValLoss=1.5177 | ES=2/30\n",
      "Epoch 114 | LR=3.75e-05 | TrainLoss=7.2923 | ValLoss=1.5483 | ValAcc=45.16% | BestValLoss=1.5177 | ES=3/30\n",
      "Epoch 115 | LR=3.75e-05 | TrainLoss=7.2866 | ValLoss=1.5135 | ValAcc=46.71% | BestValLoss=1.5177 | ES=4/30\n",
      "Epoch 116 | LR=3.75e-05 | TrainLoss=7.2819 | ValLoss=1.5310 | ValAcc=45.82% | BestValLoss=1.5135 | ES=0/30\n",
      "Epoch 117 | LR=3.75e-05 | TrainLoss=7.2888 | ValLoss=1.5249 | ValAcc=46.11% | BestValLoss=1.5135 | ES=1/30\n",
      "Epoch 118 | LR=3.75e-05 | TrainLoss=7.2882 | ValLoss=1.5244 | ValAcc=46.03% | BestValLoss=1.5135 | ES=2/30\n",
      "Epoch 119 | LR=3.75e-05 | TrainLoss=7.3008 | ValLoss=1.5215 | ValAcc=46.26% | BestValLoss=1.5135 | ES=3/30\n",
      "Epoch 120 | LR=3.75e-05 | TrainLoss=7.2896 | ValLoss=1.5349 | ValAcc=45.66% | BestValLoss=1.5135 | ES=4/30\n",
      "Epoch 121 | LR=3.75e-05 | TrainLoss=7.2809 | ValLoss=1.5295 | ValAcc=45.91% | BestValLoss=1.5135 | ES=5/30\n",
      "Epoch 122 | LR=3.75e-05 | TrainLoss=7.2972 | ValLoss=1.5124 | ValAcc=46.75% | BestValLoss=1.5135 | ES=6/30\n",
      "Epoch 123 | LR=3.75e-05 | TrainLoss=7.2802 | ValLoss=1.5404 | ValAcc=45.61% | BestValLoss=1.5124 | ES=0/30\n",
      "Epoch 124 | LR=3.75e-05 | TrainLoss=7.2789 | ValLoss=1.5394 | ValAcc=45.76% | BestValLoss=1.5124 | ES=1/30\n",
      "Epoch 125 | LR=3.75e-05 | TrainLoss=7.2917 | ValLoss=1.5187 | ValAcc=46.51% | BestValLoss=1.5124 | ES=2/30\n",
      "Epoch 126 | LR=3.75e-05 | TrainLoss=7.2814 | ValLoss=1.5325 | ValAcc=45.96% | BestValLoss=1.5124 | ES=3/30\n",
      "Epoch 127 | LR=3.75e-05 | TrainLoss=7.2913 | ValLoss=1.5207 | ValAcc=46.33% | BestValLoss=1.5124 | ES=4/30\n",
      "Epoch 128 | LR=3.75e-05 | TrainLoss=7.2882 | ValLoss=1.5358 | ValAcc=45.91% | BestValLoss=1.5124 | ES=5/30\n",
      "Epoch 129 | LR=3.75e-05 | TrainLoss=7.2759 | ValLoss=1.5435 | ValAcc=45.36% | BestValLoss=1.5124 | ES=6/30\n",
      "Epoch 130 | LR=3.75e-05 | TrainLoss=7.2705 | ValLoss=1.5404 | ValAcc=45.73% | BestValLoss=1.5124 | ES=7/30\n",
      "Epoch 131 | LR=3.75e-05 | TrainLoss=7.2874 | ValLoss=1.5394 | ValAcc=45.74% | BestValLoss=1.5124 | ES=8/30\n",
      "Epoch 132 | LR=3.75e-05 | TrainLoss=7.2815 | ValLoss=1.5401 | ValAcc=45.50% | BestValLoss=1.5124 | ES=9/30\n",
      "[LR DROP] 3.75e-05 -> 1.87e-05 (val_loss=1.5182)\n",
      "Epoch 133 | LR=1.87e-05 | TrainLoss=7.2900 | ValLoss=1.5182 | ValAcc=46.58% | BestValLoss=1.5124 | ES=10/30\n",
      "Epoch 134 | LR=1.87e-05 | TrainLoss=7.2744 | ValLoss=1.5251 | ValAcc=46.09% | BestValLoss=1.5124 | ES=11/30\n",
      "Epoch 135 | LR=1.87e-05 | TrainLoss=7.2786 | ValLoss=1.5324 | ValAcc=45.79% | BestValLoss=1.5124 | ES=12/30\n",
      "Epoch 136 | LR=1.87e-05 | TrainLoss=7.2787 | ValLoss=1.5237 | ValAcc=46.03% | BestValLoss=1.5124 | ES=13/30\n",
      "Epoch 137 | LR=1.87e-05 | TrainLoss=7.2695 | ValLoss=1.5414 | ValAcc=45.52% | BestValLoss=1.5124 | ES=14/30\n",
      "Epoch 138 | LR=1.87e-05 | TrainLoss=7.2764 | ValLoss=1.5261 | ValAcc=46.35% | BestValLoss=1.5124 | ES=15/30\n",
      "Epoch 139 | LR=1.87e-05 | TrainLoss=7.2788 | ValLoss=1.5281 | ValAcc=46.19% | BestValLoss=1.5124 | ES=16/30\n",
      "Epoch 140 | LR=1.87e-05 | TrainLoss=7.2804 | ValLoss=1.5208 | ValAcc=46.30% | BestValLoss=1.5124 | ES=17/30\n",
      "Epoch 141 | LR=1.87e-05 | TrainLoss=7.2844 | ValLoss=1.5216 | ValAcc=46.29% | BestValLoss=1.5124 | ES=18/30\n",
      "Epoch 142 | LR=1.87e-05 | TrainLoss=7.2780 | ValLoss=1.5297 | ValAcc=46.24% | BestValLoss=1.5124 | ES=19/30\n",
      "Epoch 143 | LR=1.87e-05 | TrainLoss=7.2717 | ValLoss=1.5240 | ValAcc=46.31% | BestValLoss=1.5124 | ES=20/30\n",
      "[LR DROP] 1.87e-05 -> 9.37e-06 (val_loss=1.5283)\n",
      "Epoch 144 | LR=9.37e-06 | TrainLoss=7.2800 | ValLoss=1.5283 | ValAcc=46.06% | BestValLoss=1.5124 | ES=21/30\n",
      "Epoch 145 | LR=9.37e-06 | TrainLoss=7.2699 | ValLoss=1.5179 | ValAcc=46.37% | BestValLoss=1.5124 | ES=22/30\n",
      "Epoch 146 | LR=9.37e-06 | TrainLoss=7.2775 | ValLoss=1.5310 | ValAcc=46.00% | BestValLoss=1.5124 | ES=23/30\n",
      "Epoch 147 | LR=9.37e-06 | TrainLoss=7.2818 | ValLoss=1.5168 | ValAcc=46.41% | BestValLoss=1.5124 | ES=24/30\n",
      "Epoch 148 | LR=9.37e-06 | TrainLoss=7.2640 | ValLoss=1.5278 | ValAcc=46.17% | BestValLoss=1.5124 | ES=25/30\n",
      "Epoch 149 | LR=9.37e-06 | TrainLoss=7.2822 | ValLoss=1.5196 | ValAcc=46.34% | BestValLoss=1.5124 | ES=26/30\n",
      "Epoch 150 | LR=9.37e-06 | TrainLoss=7.2712 | ValLoss=1.5218 | ValAcc=46.38% | BestValLoss=1.5124 | ES=27/30\n",
      "Epoch 151 | LR=9.37e-06 | TrainLoss=7.2658 | ValLoss=1.5279 | ValAcc=46.23% | BestValLoss=1.5124 | ES=28/30\n",
      "Epoch 152 | LR=9.37e-06 | TrainLoss=7.2657 | ValLoss=1.5170 | ValAcc=46.50% | BestValLoss=1.5124 | ES=29/30\n",
      "[INFO] Early stopping triggered.\n",
      "[FOLD TEST] 20:47.57%, 15:47.23%, 10:46.54%, 5:43.94%, 0:38.41%, -5:28.25%, -10:17.91%, -15:12.81%, -20:11.55%, -25:11.31%, -30:11.32%, -35:11.24%, -40:11.29%\n",
      "\n",
      "[INFO] All saved in: ./training_results\\2026-01-25_01-46-02_LTEV_SpecSimCLR_FASTGPU_SNRsweep_Doppler120_SampleSplit\n",
      "[INFO] SNR sweep CSV: ./training_results\\2026-01-25_01-46-02_LTEV_SpecSimCLR_FASTGPU_SNRsweep_Doppler120_SampleSplit\\test_snr_sweep.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LTE-V (.mat via HDF5) - Spec + SimCLR-style Siamese (FAST GPU PIPELINE + LOGGING)\n",
    "#\n",
    "# Key changes vs your previous LTEV SpecSiamese:\n",
    "#   1) Split mode: FORCED SAMPLE-LEVEL stratified split over all (fi, si)\n",
    "#   2) CV: StratifiedKFold on train sample indices\n",
    "#   3) Positive pair (SimCLR): two random augmented views of THE SAME sample\n",
    "#      (no cross-file / cross-sample positive pairing)\n",
    "#\n",
    "# Keeps:\n",
    "#   - Contrastive learning (NT-Xent)\n",
    "#   - Classification head CE term (can disable by setting LAMBDA_CE=0.0)\n",
    "#   - GPU batch augmentation + GPU STFT + AMP\n",
    "#   - SNR sweep evaluation with Doppler fixed at TEST_V_KMH_FIXED\n",
    "#\n",
    "# Saved files:\n",
    "#   - config.txt\n",
    "#   - results.txt\n",
    "#   - fold{K}_trainlog.csv\n",
    "#   - fold{K}_test_snr.csv\n",
    "#   - test_snr_sweep.csv\n",
    "#   - model_fold{K}.pth\n",
    "#   - best_model_fold{K}.pth\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Global config\n",
    "# ----------------------------\n",
    "SEED = 42\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "USE_AMP = (DEVICE.type == \"cuda\")\n",
    "\n",
    "DATA_PATH = \"E:/rf_datasets_IQ_raw/\"   # <-- change to your LTE-V folder\n",
    "RECURSIVE_GLOB = True\n",
    "\n",
    "FS = 5e6\n",
    "FC = 5.9e9\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 0.0\n",
    "MAX_EPOCHS = 300\n",
    "\n",
    "N_SPLITS = 5\n",
    "TEST_SIZE = 0.25\n",
    "\n",
    "LR_PATIENCE = 10\n",
    "LR_FACTOR = 0.5\n",
    "ES_PATIENCE = 30\n",
    "\n",
    "# Contrastive loss\n",
    "TAU = 0.05\n",
    "LAMBDA_CL = 1.0\n",
    "\n",
    "# CE head (set LAMBDA_CE=0.0 if you want strictly self-supervised training)\n",
    "LAMBDA_CE = 1.0\n",
    "\n",
    "# Augmentation switches\n",
    "AUG_USE_MULTIPATH = True\n",
    "AUG_USE_DOPPLER   = True\n",
    "AUG_USE_AWGN      = True\n",
    "\n",
    "RMS_DS_NS_RANGE = (5.0, 300.0)\n",
    "\n",
    "# If you want Doppler fixed at 120 for train/val/test, set range to (120,120).\n",
    "TRAIN_V_KMH_RANGE = (120.0, 120.0)\n",
    "TRAIN_SNR_DB_RANGE = (-40.0, 20.0)\n",
    "\n",
    "TEST_V_KMH_FIXED = 120.0\n",
    "TEST_USE_MULTIPATH = False\n",
    "TEST_RMS_DS_NS_RANGE = RMS_DS_NS_RANGE\n",
    "TEST_SNR_LIST = list(range(20, -45, -5))\n",
    "\n",
    "# Spectrogram\n",
    "SPEC_NFFT = 128\n",
    "SPEC_WIN  = 128\n",
    "SPEC_HOP  = 16\n",
    "SPEC_SIZE = 64\n",
    "\n",
    "# Multipath taps\n",
    "MAX_TAPS = 16\n",
    "\n",
    "# Dataset cap (optional)\n",
    "MAX_SAMPLES_PER_FILE = None\n",
    "\n",
    "# Dataloader\n",
    "NUM_WORKERS_TRAIN = 0\n",
    "NUM_WORKERS_EVAL  = 0\n",
    "\n",
    "# Logging / saving\n",
    "SAVE_ROOT = \"./training_results\"\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "SCRIPT_NAME = \"LTEV_SpecSimCLR_FASTGPU_SNRsweep_Doppler120_SampleSplit\"\n",
    "\n",
    "RETURN_CM = False\n",
    "\n",
    "# ----------------------------\n",
    "# AMP compatibility (robust across torch versions)\n",
    "# ----------------------------\n",
    "from contextlib import nullcontext\n",
    "\n",
    "def amp_autocast(enabled: bool = True):\n",
    "    \"\"\"\n",
    "    Robust autocast wrapper:\n",
    "      - torch.amp.autocast may accept (device_type=...) or (device, ...)\n",
    "      - fallback to torch.cuda.amp.autocast on older versions\n",
    "    \"\"\"\n",
    "    if not enabled:\n",
    "        return nullcontext()\n",
    "\n",
    "    # Prefer torch.amp.autocast if available\n",
    "    if hasattr(torch, \"amp\") and hasattr(torch.amp, \"autocast\"):\n",
    "        try:\n",
    "            # Newer signature: autocast(device_type=\"cuda\", ...)\n",
    "            return torch.amp.autocast(device_type=DEVICE.type, enabled=True)\n",
    "        except TypeError:\n",
    "            # Older signature: autocast(\"cuda\", ...)\n",
    "            return torch.amp.autocast(DEVICE.type, enabled=True)\n",
    "\n",
    "    # Fallback\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        return torch.cuda.amp.autocast(enabled=True)\n",
    "\n",
    "    return nullcontext()\n",
    "\n",
    "def make_grad_scaler(enabled: bool = True):\n",
    "    \"\"\"\n",
    "    Robust GradScaler factory:\n",
    "      - torch.amp.GradScaler may accept (device_type=...), or (\"cuda\", ...), or only (enabled=...)\n",
    "      - fallback to torch.cuda.amp.GradScaler\n",
    "    \"\"\"\n",
    "    if (not enabled) or (DEVICE.type != \"cuda\"):\n",
    "        return None\n",
    "\n",
    "    if hasattr(torch, \"amp\") and hasattr(torch.amp, \"GradScaler\"):\n",
    "        # Try multiple ctor signatures\n",
    "        for ctor in (\n",
    "            lambda: torch.amp.GradScaler(device_type=\"cuda\", enabled=True),  # some versions\n",
    "            lambda: torch.amp.GradScaler(\"cuda\", enabled=True),              # recommended warning style\n",
    "            lambda: torch.amp.GradScaler(enabled=True),                      # minimal\n",
    "        ):\n",
    "            try:\n",
    "                return ctor()\n",
    "            except TypeError:\n",
    "                pass\n",
    "\n",
    "    return torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Logging helpers\n",
    "# ----------------------------\n",
    "class CSVLogger:\n",
    "    def __init__(self, path, header):\n",
    "        self.path = path\n",
    "        self.header = header\n",
    "        self._init_file()\n",
    "\n",
    "    def _init_file(self):\n",
    "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
    "        with open(self.path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.writer(f).writerow(self.header)\n",
    "\n",
    "    def log_row(self, row):\n",
    "        with open(self.path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.writer(f).writerow(row)\n",
    "\n",
    "def write_line(path, line):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line.rstrip() + \"\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) LTE-V data reading\n",
    "# ----------------------------\n",
    "def decode_txid(txid_arr: np.ndarray) -> str:\n",
    "    txid_arr = txid_arr.flatten()\n",
    "    chars = []\n",
    "    for c in txid_arr:\n",
    "        if int(c) == 0:\n",
    "            continue\n",
    "        try:\n",
    "            chars.append(chr(int(c)))\n",
    "        except Exception:\n",
    "            pass\n",
    "    s = \"\".join(chars).strip()\n",
    "    return s if s else \"UNKNOWN\"\n",
    "\n",
    "def load_dmrs_complex_from_file(h5f: h5py.File) -> np.ndarray:\n",
    "    rfDataset = h5f[\"rfDataset\"]\n",
    "    dmrs_obj = rfDataset[\"dmrs\"]\n",
    "\n",
    "    if isinstance(dmrs_obj, h5py.Dataset) and dmrs_obj.dtype.fields is not None:\n",
    "        real = dmrs_obj[\"real\"][()]\n",
    "        imag = dmrs_obj[\"imag\"][()]\n",
    "    elif isinstance(dmrs_obj, h5py.Group):\n",
    "        real = dmrs_obj[\"real\"][()]\n",
    "        imag = dmrs_obj[\"imag\"][()]\n",
    "    else:\n",
    "        tmp = dmrs_obj[()]\n",
    "        if hasattr(tmp, \"dtype\") and tmp.dtype.fields is not None:\n",
    "            real = tmp[\"real\"]\n",
    "            imag = tmp[\"imag\"]\n",
    "        else:\n",
    "            raise RuntimeError(\"Cannot parse dmrs (not compound dataset nor group real/imag).\")\n",
    "\n",
    "    dmrs_complex = np.asarray(real + 1j * imag)\n",
    "    if dmrs_complex.ndim != 2:\n",
    "        raise RuntimeError(f\"dmrs_complex dim error: {dmrs_complex.shape}\")\n",
    "\n",
    "    if dmrs_complex.shape[0] <= 2048 and dmrs_complex.shape[1] > dmrs_complex.shape[0]:\n",
    "        dmrs_complex = dmrs_complex.T\n",
    "\n",
    "    return dmrs_complex.astype(np.complex64)\n",
    "\n",
    "def load_ltev_dataset(data_path: str, recursive: bool = True, max_samples_per_file=None):\n",
    "    if recursive:\n",
    "        mat_files = glob.glob(os.path.join(data_path, \"**\", \"*.mat\"), recursive=True)\n",
    "    else:\n",
    "        mat_files = glob.glob(os.path.join(data_path, \"*.mat\"))\n",
    "\n",
    "    if len(mat_files) == 0:\n",
    "        raise RuntimeError(f\"No .mat files found: {data_path}\")\n",
    "\n",
    "    signals_by_file = []\n",
    "    label_str_by_file = []\n",
    "    file_paths = []\n",
    "\n",
    "    print(f\"[INFO] Found {len(mat_files)} .mat files\")\n",
    "    for fp in mat_files:\n",
    "        try:\n",
    "            with h5py.File(fp, \"r\") as f:\n",
    "                rfDataset = f[\"rfDataset\"]\n",
    "                txid_arr = np.asarray(rfDataset[\"txID\"][()])\n",
    "                tx_str = decode_txid(txid_arr)\n",
    "\n",
    "                dmrs_complex = load_dmrs_complex_from_file(f)\n",
    "                if max_samples_per_file is not None:\n",
    "                    dmrs_complex = dmrs_complex[: int(max_samples_per_file)]\n",
    "\n",
    "                if dmrs_complex.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                signals_by_file.append(dmrs_complex)\n",
    "                label_str_by_file.append(tx_str)\n",
    "                file_paths.append(fp)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skip file due to read/parse error: {fp} | {repr(e)}\")\n",
    "\n",
    "    if len(signals_by_file) == 0:\n",
    "        raise RuntimeError(\"All files failed to read or are empty.\")\n",
    "\n",
    "    cnt = Counter(label_str_by_file)\n",
    "    print(\"[INFO] txID classes:\", len(cnt))\n",
    "    for k, v in sorted(cnt.items(), key=lambda x: (-x[1], x[0])):\n",
    "        print(f\"  {k}: {v} files\")\n",
    "\n",
    "    Ls = [arr.shape[1] for arr in signals_by_file]\n",
    "    mode_L = Counter(Ls).most_common(1)[0][0]\n",
    "    print(f\"[INFO] DMRS length stats: min={min(Ls)}, max={max(Ls)}, mode={mode_L}\")\n",
    "\n",
    "    # unify length to mode_L\n",
    "    for i in range(len(signals_by_file)):\n",
    "        x = signals_by_file[i]\n",
    "        if x.shape[1] == mode_L:\n",
    "            continue\n",
    "        if x.shape[1] > mode_L:\n",
    "            signals_by_file[i] = x[:, :mode_L].astype(np.complex64)\n",
    "        else:\n",
    "            pad = mode_L - x.shape[1]\n",
    "            signals_by_file[i] = np.pad(x, ((0, 0), (0, pad)), mode=\"constant\").astype(np.complex64)\n",
    "\n",
    "    return signals_by_file, label_str_by_file, file_paths, mode_L\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Sample-level split + StratifiedKFold\n",
    "# ----------------------------\n",
    "def build_all_sample_list(signals_by_file):\n",
    "    sample_list = []\n",
    "    for fi, arr in enumerate(signals_by_file):\n",
    "        n = arr.shape[0]\n",
    "        sample_list.extend([(fi, si) for si in range(n)])\n",
    "    return sample_list\n",
    "\n",
    "def split_train_test_by_samples(signals_by_file, label_idx_by_file, test_size=TEST_SIZE):\n",
    "    all_samples = build_all_sample_list(signals_by_file)\n",
    "    y_samples = np.array([label_idx_by_file[fi] for (fi, _) in all_samples], dtype=np.int64)\n",
    "\n",
    "    tr_i, te_i = train_test_split(\n",
    "        np.arange(len(all_samples)),\n",
    "        test_size=test_size,\n",
    "        stratify=y_samples,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    train_samples = [all_samples[i] for i in tr_i]\n",
    "    test_samples  = [all_samples[i] for i in te_i]\n",
    "\n",
    "    print(\"[INFO] Split mode: SAMPLE-LEVEL (force)\")\n",
    "    print(f\"[INFO] Train samples={len(train_samples)}, Test samples={len(test_samples)}\")\n",
    "\n",
    "    c_tr = Counter([label_idx_by_file[fi] for (fi, _) in train_samples])\n",
    "    c_te = Counter([label_idx_by_file[fi] for (fi, _) in test_samples])\n",
    "    print(\"[INFO] Train class sample counts:\", dict(sorted(c_tr.items())))\n",
    "    print(\"[INFO] Test  class sample counts:\", dict(sorted(c_te.items())))\n",
    "\n",
    "    return train_samples, test_samples\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Datasets\n",
    "# ----------------------------\n",
    "class LTEVSimCLRIQDataset(Dataset):\n",
    "    \"\"\"\n",
    "    SimCLR positive pair is generated in the training loop:\n",
    "    two random augmented views of the SAME iq.\n",
    "    Dataset returns (iq, label) for optional CE head & evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, signals_by_file, label_idx_by_file, sample_list):\n",
    "        self.signals_by_file = signals_by_file\n",
    "        self.label_idx_by_file = label_idx_by_file\n",
    "        self.sample_list = sample_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fi, si = self.sample_list[idx]\n",
    "        lab = int(self.label_idx_by_file[fi])\n",
    "        sig = self.signals_by_file[fi][si]  # complex (L,)\n",
    "        iq = np.stack([sig.real, sig.imag], axis=-1).astype(np.float32)  # (L,2)\n",
    "        return iq, np.int64(lab)\n",
    "\n",
    "class LTEVSingleIQDataset(Dataset):\n",
    "    def __init__(self, signals_by_file, label_idx_by_file, sample_list):\n",
    "        self.signals_by_file = signals_by_file\n",
    "        self.label_idx_by_file = label_idx_by_file\n",
    "        self.sample_list = sample_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fi, si = self.sample_list[idx]\n",
    "        lab = int(self.label_idx_by_file[fi])\n",
    "        sig = self.signals_by_file[fi][si]\n",
    "        iq = np.stack([sig.real, sig.imag], axis=-1).astype(np.float32)\n",
    "        return iq, np.int64(lab)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) GPU batch augmentation\n",
    "# ----------------------------\n",
    "def _to_complex(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    return iq_b[..., 0].to(torch.float32) + 1j * iq_b[..., 1].to(torch.float32)\n",
    "\n",
    "def _from_complex(sig: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.stack([sig.real, sig.imag], dim=-1)\n",
    "\n",
    "def batch_normalize_power(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    power = (iq_b[..., 0] ** 2 + iq_b[..., 1] ** 2).mean(dim=1, keepdim=True) + 1e-12\n",
    "    scale = torch.rsqrt(power).unsqueeze(-1)\n",
    "    return iq_b * scale\n",
    "\n",
    "def batch_apply_doppler(iq_b: torch.Tensor, v_kmh: torch.Tensor) -> torch.Tensor:\n",
    "    B, L, _ = iq_b.shape\n",
    "    sig = _to_complex(iq_b)\n",
    "\n",
    "    c = 3e8\n",
    "    v = v_kmh / 3.6\n",
    "    fd = (v / c) * FC  # (B,)\n",
    "    n = torch.arange(L, device=iq_b.device, dtype=torch.float32).unsqueeze(0)  # (1,L)\n",
    "    phase = torch.exp(1j * 2.0 * np.pi * fd.unsqueeze(1).to(torch.float32) * n / FS)  # (B,L)\n",
    "    sig = sig * phase\n",
    "    return _from_complex(sig)\n",
    "\n",
    "def _grouped_conv1d_real(x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    # x: (B,1,L), w: (B,1,K)\n",
    "    B, _, L = x.shape\n",
    "    x2 = x.permute(1, 0, 2).contiguous()  # (1,B,L)\n",
    "    y2 = F.conv1d(x2, w, padding=w.shape[-1] - 1, groups=B)  # (1,B,L+K-1)\n",
    "    return y2.squeeze(0)  # (B,L+K-1)\n",
    "\n",
    "def batch_apply_multipath(iq_b: torch.Tensor, rms_ns: torch.Tensor, max_taps: int = MAX_TAPS) -> torch.Tensor:\n",
    "    B, L, _ = iq_b.shape\n",
    "    device = iq_b.device\n",
    "\n",
    "    rms_s = rms_ns * 1e-9\n",
    "    rms_samples = (rms_s * FS).clamp(min=1e-3)\n",
    "\n",
    "    k = torch.arange(max_taps, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "    p = torch.exp(-k / rms_samples.unsqueeze(1))\n",
    "    p = p / (p.sum(dim=1, keepdim=True) + 1e-12)\n",
    "\n",
    "    hr = torch.randn(B, max_taps, device=device) * torch.sqrt(p / 2.0)\n",
    "    hi = torch.randn(B, max_taps, device=device) * torch.sqrt(p / 2.0)\n",
    "\n",
    "    hpow = (hr**2 + hi**2).sum(dim=1, keepdim=True) + 1e-12\n",
    "    norm = torch.rsqrt(hpow)\n",
    "    hr = hr * norm\n",
    "    hi = hi * norm\n",
    "\n",
    "    xr = iq_b[..., 0]\n",
    "    xi = iq_b[..., 1]\n",
    "\n",
    "    xr_ = xr.unsqueeze(1)\n",
    "    xi_ = xi.unsqueeze(1)\n",
    "    hr_ = hr.unsqueeze(1)\n",
    "    hi_ = hi.unsqueeze(1)\n",
    "\n",
    "    xr_hr = _grouped_conv1d_real(xr_, hr_)\n",
    "    xi_hi = _grouped_conv1d_real(xi_, hi_)\n",
    "    xr_hi = _grouped_conv1d_real(xr_, hi_)\n",
    "    xi_hr = _grouped_conv1d_real(xi_, hr_)\n",
    "\n",
    "    yr = xr_hr - xi_hi\n",
    "    yi = xr_hi + xi_hr\n",
    "\n",
    "    yr = yr[:, :L]\n",
    "    yi = yi[:, :L]\n",
    "    return torch.stack([yr, yi], dim=-1)\n",
    "\n",
    "def batch_add_awgn(iq_b: torch.Tensor, snr_db: torch.Tensor) -> torch.Tensor:\n",
    "    B, L, _ = iq_b.shape\n",
    "    sig = _to_complex(iq_b)\n",
    "    p = (sig.real**2 + sig.imag**2).mean(dim=1) + 1e-12\n",
    "    npow = p / (10.0 ** (snr_db / 10.0))\n",
    "    std = torch.sqrt(npow / 2.0).unsqueeze(1)\n",
    "    noise = std * (torch.randn(B, L, device=iq_b.device) + 1j * torch.randn(B, L, device=iq_b.device))\n",
    "    sig = sig + noise\n",
    "    return _from_complex(sig)\n",
    "\n",
    "def augment_train_batch(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    iq_b = batch_normalize_power(iq_b)\n",
    "    B = iq_b.shape[0]\n",
    "\n",
    "    if AUG_USE_MULTIPATH:\n",
    "        rms = (RMS_DS_NS_RANGE[1] - RMS_DS_NS_RANGE[0]) * torch.rand(B, device=iq_b.device) + RMS_DS_NS_RANGE[0]\n",
    "        iq_b = batch_apply_multipath(iq_b, rms, max_taps=MAX_TAPS)\n",
    "\n",
    "    if AUG_USE_DOPPLER:\n",
    "        vmin, vmax = TRAIN_V_KMH_RANGE\n",
    "        if abs(vmax - vmin) < 1e-12:\n",
    "            v = torch.full((B,), float(vmin), device=iq_b.device)\n",
    "        else:\n",
    "            v = (vmax - vmin) * torch.rand(B, device=iq_b.device) + vmin\n",
    "        iq_b = batch_apply_doppler(iq_b, v)\n",
    "\n",
    "    if AUG_USE_AWGN:\n",
    "        smin, smax = TRAIN_SNR_DB_RANGE\n",
    "        snr = (smax - smin) * torch.rand(B, device=iq_b.device) + smin\n",
    "        iq_b = batch_add_awgn(iq_b, snr)\n",
    "\n",
    "    return iq_b\n",
    "\n",
    "def augment_test_batch(iq_b: torch.Tensor, snr_db: float) -> torch.Tensor:\n",
    "    iq_b = batch_normalize_power(iq_b)\n",
    "    B = iq_b.shape[0]\n",
    "\n",
    "    if TEST_USE_MULTIPATH:\n",
    "        rms = (TEST_RMS_DS_NS_RANGE[1] - TEST_RMS_DS_NS_RANGE[0]) * torch.rand(B, device=iq_b.device) + TEST_RMS_DS_NS_RANGE[0]\n",
    "        iq_b = batch_apply_multipath(iq_b, rms, max_taps=MAX_TAPS)\n",
    "\n",
    "    v = torch.full((B,), float(TEST_V_KMH_FIXED), device=iq_b.device)\n",
    "    iq_b = batch_apply_doppler(iq_b, v)\n",
    "\n",
    "    snr = torch.full((B,), float(snr_db), device=iq_b.device)\n",
    "    iq_b = batch_add_awgn(iq_b, snr)\n",
    "    return iq_b\n",
    "\n",
    "# ----------------------------\n",
    "# 6) GPU batch STFT\n",
    "# ----------------------------\n",
    "_WINDOW_CACHE = {}\n",
    "def get_hann_window(device: torch.device):\n",
    "    key = (device.type, device.index, SPEC_WIN)\n",
    "    if key not in _WINDOW_CACHE:\n",
    "        _WINDOW_CACHE[key] = torch.hann_window(SPEC_WIN, periodic=True, device=device)\n",
    "    return _WINDOW_CACHE[key]\n",
    "\n",
    "def iq_to_logspec_batch(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    sig = _to_complex(iq_b).to(torch.complex64)  # (B,L)\n",
    "    win = get_hann_window(iq_b.device)\n",
    "    S = torch.stft(\n",
    "        sig,\n",
    "        n_fft=SPEC_NFFT,\n",
    "        hop_length=SPEC_HOP,\n",
    "        win_length=SPEC_WIN,\n",
    "        window=win,\n",
    "        center=True,\n",
    "        return_complex=True\n",
    "    )  # (B,F,T)\n",
    "\n",
    "    mag = torch.abs(S) + 1e-12\n",
    "    logmag = torch.log(mag)\n",
    "\n",
    "    # per-sample z-score\n",
    "    mu = logmag.mean(dim=(1, 2), keepdim=True)\n",
    "    sd = logmag.std(dim=(1, 2), keepdim=True) + 1e-6\n",
    "    logmag = (logmag - mu) / sd\n",
    "    try:\n",
    "        logmag = torch.nan_to_num(logmag, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    except Exception:\n",
    "        logmag[~torch.isfinite(logmag)] = 0.0\n",
    "\n",
    "    x = logmag.unsqueeze(1)  # (B,1,F,T)\n",
    "    x = F.interpolate(x, size=(SPEC_SIZE, SPEC_SIZE), mode=\"bilinear\", align_corners=False)\n",
    "    return x\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Model\n",
    "# ----------------------------\n",
    "class BasicBlock2D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.down = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.down is not None:\n",
    "            identity = self.down(identity)\n",
    "        return self.relu(out + identity)\n",
    "\n",
    "class SpecFeatureNet(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.b1 = BasicBlock2D(32, 32, stride=1)\n",
    "        self.b2 = BasicBlock2D(32, 32, stride=1)\n",
    "        self.b3 = BasicBlock2D(32, 64, stride=1)\n",
    "        self.b4 = BasicBlock2D(64, 64, stride=1)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(64, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)      # embedding z\n",
    "        self.cls = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.b1(x); x = self.b2(x); x = self.b3(x); x = self.b4(x)\n",
    "        x = self.gap(x).squeeze(-1).squeeze(-1)  # (B,64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        z = self.fc2(x)\n",
    "        logits = self.cls(z)\n",
    "        return z, logits\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        z1, p1 = self.forward_once(x1)\n",
    "        if x2 is None:\n",
    "            return z1, p1\n",
    "        z2, p2 = self.forward_once(x2)\n",
    "        return z1, p1, z2, p2\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Loss + Eval\n",
    "# ----------------------------\n",
    "def nt_xent_loss(z1: torch.Tensor, z2: torch.Tensor, tau: float = TAU) -> torch.Tensor:\n",
    "    # force fp32 for numerical safety\n",
    "    with amp_autocast(enabled=False):\n",
    "        z1 = z1.float()\n",
    "        z2 = z2.float()\n",
    "\n",
    "        N = z1.size(0)\n",
    "        z = torch.cat([z1, z2], dim=0)\n",
    "        z = F.normalize(z, dim=1)\n",
    "\n",
    "        sim = (z @ z.T) / float(tau)\n",
    "        sim.fill_diagonal_(torch.finfo(sim.dtype).min)\n",
    "\n",
    "        pos = torch.arange(2 * N, device=z.device)\n",
    "        pos = (pos + N) % (2 * N)\n",
    "\n",
    "        log_prob = sim - torch.logsumexp(sim, dim=1, keepdim=True)\n",
    "        loss = -log_prob[torch.arange(2 * N, device=z.device), pos]\n",
    "        return loss.mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_single_iq(model: SpecFeatureNet, loader: DataLoader, num_classes: int, mode: str, snr_db: float = None):\n",
    "    model.eval()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    loss_sum, nb = 0.0, 0\n",
    "    all_y, all_p = [], []\n",
    "\n",
    "    for iq, y in loader:\n",
    "        iq = iq.to(DEVICE, non_blocking=True)\n",
    "        y  = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        if mode == \"test\":\n",
    "            iq = augment_test_batch(iq, snr_db=float(snr_db))\n",
    "\n",
    "        spec = iq_to_logspec_batch(iq)\n",
    "        _, logits = model(spec, None)\n",
    "        loss = ce(logits, y)\n",
    "\n",
    "        loss_sum += float(loss.item())\n",
    "        nb += 1\n",
    "\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "        if RETURN_CM:\n",
    "            all_y.append(y.detach().cpu().numpy())\n",
    "            all_p.append(pred.detach().cpu().numpy())\n",
    "\n",
    "    acc = 100.0 * correct / max(total, 1)\n",
    "    cm = None\n",
    "    if RETURN_CM and total > 0:\n",
    "        all_y = np.concatenate(all_y) if all_y else np.array([])\n",
    "        all_p = np.concatenate(all_p) if all_p else np.array([])\n",
    "        cm = confusion_matrix(all_y, all_p, labels=list(range(num_classes)))\n",
    "    return (loss_sum / max(nb, 1)), acc, cm\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Training (SimCLR positive pairs) + SNR sweep\n",
    "# ----------------------------\n",
    "def train_kfold_ltev_simclr_sample_split(data_path: str):\n",
    "    signals_by_file, label_str_by_file, file_paths, L = load_ltev_dataset(\n",
    "        data_path, recursive=RECURSIVE_GLOB, max_samples_per_file=MAX_SAMPLES_PER_FILE\n",
    "    )\n",
    "\n",
    "    label_list = sorted(list(set(label_str_by_file)))\n",
    "    label_to_idx = {lab: i for i, lab in enumerate(label_list)}\n",
    "    label_idx_by_file = [label_to_idx[s] for s in label_str_by_file]\n",
    "    num_classes = len(label_list)\n",
    "\n",
    "    # Forced sample-level split\n",
    "    train_samples, test_samples = split_train_test_by_samples(\n",
    "        signals_by_file, label_idx_by_file, test_size=TEST_SIZE\n",
    "    )\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    save_dir = f\"{timestamp}_{SCRIPT_NAME}\"\n",
    "    save_folder = os.path.join(SAVE_ROOT, save_dir)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    results_txt = os.path.join(save_folder, \"results.txt\")\n",
    "    fd_test = (TEST_V_KMH_FIXED / 3.6) / 3e8 * FC\n",
    "\n",
    "    # config\n",
    "    with open(os.path.join(save_folder, \"config.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"DEVICE={DEVICE}\\nAMP={USE_AMP}\\n\")\n",
    "        f.write(f\"DATA_PATH={data_path}\\nRECURSIVE_GLOB={RECURSIVE_GLOB}\\n\")\n",
    "        f.write(f\"MAX_SAMPLES_PER_FILE={MAX_SAMPLES_PER_FILE}\\n\")\n",
    "        f.write(f\"DMRS_LEN(L)={L}\\n\")\n",
    "        f.write(f\"num_classes={num_classes}\\n\")\n",
    "        f.write(f\"split_mode=sample_level_force\\n\")\n",
    "        f.write(f\"TEST_SIZE={TEST_SIZE}\\n\")\n",
    "        f.write(f\"BATCH_SIZE={BATCH_SIZE}, LR={LR}, WEIGHT_DECAY={WEIGHT_DECAY}\\n\")\n",
    "        f.write(f\"MAX_EPOCHS={MAX_EPOCHS}, N_SPLITS={N_SPLITS}\\n\")\n",
    "        f.write(f\"LR_PATIENCE={LR_PATIENCE}, LR_FACTOR={LR_FACTOR}, ES_PATIENCE={ES_PATIENCE}\\n\")\n",
    "        f.write(f\"TAU={TAU}, LAMBDA_CL={LAMBDA_CL}, LAMBDA_CE={LAMBDA_CE}\\n\")\n",
    "        f.write(f\"FS={FS}, FC={FC}\\n\")\n",
    "        f.write(f\"AUG_USE_MULTIPATH={AUG_USE_MULTIPATH}, RMS_DS_NS_RANGE={RMS_DS_NS_RANGE}, MAX_TAPS={MAX_TAPS}\\n\")\n",
    "        f.write(f\"AUG_USE_DOPPLER={AUG_USE_DOPPLER}, TRAIN_V_KMH_RANGE={TRAIN_V_KMH_RANGE}\\n\")\n",
    "        f.write(f\"AUG_USE_AWGN={AUG_USE_AWGN}, TRAIN_SNR_DB_RANGE={TRAIN_SNR_DB_RANGE}\\n\")\n",
    "        f.write(f\"TEST_V_KMH_FIXED={TEST_V_KMH_FIXED}, fd_test={fd_test}\\n\")\n",
    "        f.write(f\"TEST_USE_MULTIPATH={TEST_USE_MULTIPATH}, TEST_SNR_LIST={TEST_SNR_LIST}\\n\")\n",
    "        f.write(f\"SPEC_NFFT={SPEC_NFFT}, SPEC_WIN={SPEC_WIN}, SPEC_HOP={SPEC_HOP}, SPEC_SIZE={SPEC_SIZE}\\n\")\n",
    "        f.write(f\"workers(train/eval)={NUM_WORKERS_TRAIN}/{NUM_WORKERS_EVAL}\\n\")\n",
    "        f.write(f\"CV=StratifiedKFold(on train sample indices)\\n\")\n",
    "        f.write(f\"POS_PAIR=SimCLR(two random augmentations of same sample)\\n\")\n",
    "\n",
    "    print(f\"[INFO] DEVICE={DEVICE} | AMP={USE_AMP}\")\n",
    "    print(f\"[INFO] Classes={num_classes}, TrainSamples={len(train_samples)}, TestSamples={len(test_samples)}, L={L}\")\n",
    "    print(f\"[INFO] SaveFolder: {save_folder}\")\n",
    "\n",
    "    write_line(results_txt, f\"DEVICE={DEVICE}, AMP={USE_AMP}\")\n",
    "    write_line(results_txt, f\"Classes={num_classes}, TrainSamples={len(train_samples)}, TestSamples={len(test_samples)}, L={L}\")\n",
    "    write_line(results_txt, f\"split_mode=sample_level_force | TEST_SIZE={TEST_SIZE}\")\n",
    "    write_line(results_txt, f\"POS_PAIR=SimCLR(two random augmentations of same sample)\")\n",
    "    write_line(results_txt, f\"Train SNR~U{TRAIN_SNR_DB_RANGE}, v~{TRAIN_V_KMH_RANGE}, multipath={AUG_USE_MULTIPATH}\")\n",
    "    write_line(results_txt, f\"Test v={TEST_V_KMH_FIXED} (fd={fd_test:.2f}Hz), SNR sweep={TEST_SNR_LIST}, TEST_USE_MULTIPATH={TEST_USE_MULTIPATH}\")\n",
    "    write_line(results_txt, \"-\" * 80)\n",
    "\n",
    "    snr_to_accs = {snr: [] for snr in TEST_SNR_LIST}\n",
    "    scaler = make_grad_scaler(enabled=USE_AMP)\n",
    "\n",
    "    # fixed test loader (eval adds test-time augment)\n",
    "    test_ds = LTEVSingleIQDataset(signals_by_file, label_idx_by_file, test_samples)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS_EVAL, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # StratifiedKFold on train samples\n",
    "    y_train = np.array([label_idx_by_file[fi] for (fi, _) in train_samples], dtype=np.int64)\n",
    "    idx_all = np.arange(len(train_samples), dtype=np.int64)\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(idx_all, y_train), 1):\n",
    "        tr_samples_fold = [train_samples[i] for i in tr_idx]\n",
    "        va_samples_fold = [train_samples[i] for i in va_idx]\n",
    "\n",
    "        c_tr = Counter([label_idx_by_file[fi] for (fi, _) in tr_samples_fold])\n",
    "        c_va = Counter([label_idx_by_file[fi] for (fi, _) in va_samples_fold])\n",
    "        print(f\"\\n========== Fold {fold}/{N_SPLITS} ==========\")\n",
    "        print(f\"[FOLD {fold}] train_sample_counts:\", dict(sorted(c_tr.items())))\n",
    "        print(f\"[FOLD {fold}] val_sample_counts  :\", dict(sorted(c_va.items())))\n",
    "\n",
    "        _run_one_fold_ltev_simclr(\n",
    "            fold, save_folder, results_txt, num_classes,\n",
    "            signals_by_file, label_idx_by_file,\n",
    "            tr_samples_fold, va_samples_fold,\n",
    "            test_loader, snr_to_accs, scaler\n",
    "        )\n",
    "\n",
    "    # summarize sweep\n",
    "    rows = []\n",
    "    for snr in TEST_SNR_LIST:\n",
    "        arr = np.array(snr_to_accs[snr], dtype=np.float64)\n",
    "        mean = float(arr.mean()) if arr.size else 0.0\n",
    "        std  = float(arr.std()) if arr.size else 0.0\n",
    "        rows.append([snr, mean, std] + snr_to_accs[snr])\n",
    "\n",
    "    csv_path = os.path.join(save_folder, \"test_snr_sweep.csv\")\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        header = [\"snr_db\", \"acc_mean\", \"acc_std\"] + [f\"fold{i}\" for i in range(1, N_SPLITS + 1)]\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    write_line(results_txt, \"\\n========== Overall Test SNR Sweep (mean±std over folds) ==========\")\n",
    "    for snr in TEST_SNR_LIST:\n",
    "        arr = np.array(snr_to_accs[snr], dtype=np.float64)\n",
    "        mean = float(arr.mean()) if arr.size else 0.0\n",
    "        std  = float(arr.std()) if arr.size else 0.0\n",
    "        write_line(results_txt, f\"SNR {snr:>3} dB | Acc {mean:.2f} ± {std:.2f}\")\n",
    "\n",
    "    print(f\"\\n[INFO] All saved in: {save_folder}\")\n",
    "    print(f\"[INFO] SNR sweep CSV: {csv_path}\")\n",
    "    return save_folder\n",
    "\n",
    "def _run_one_fold_ltev_simclr(\n",
    "    fold: int,\n",
    "    save_folder: str,\n",
    "    results_txt: str,\n",
    "    num_classes: int,\n",
    "    signals_by_file,\n",
    "    label_idx_by_file,\n",
    "    tr_samples_fold,\n",
    "    va_samples_fold,\n",
    "    test_loader,\n",
    "    snr_to_accs: dict,\n",
    "    scaler\n",
    "):\n",
    "    write_line(results_txt, f\"\\n========== Fold {fold}/{N_SPLITS} ==========\")\n",
    "\n",
    "    tr_ds = LTEVSimCLRIQDataset(signals_by_file, label_idx_by_file, tr_samples_fold)\n",
    "    va_ds = LTEVSingleIQDataset(signals_by_file, label_idx_by_file, va_samples_fold)\n",
    "\n",
    "    tr_loader = DataLoader(\n",
    "        tr_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True,\n",
    "        num_workers=NUM_WORKERS_TRAIN, pin_memory=True\n",
    "    )\n",
    "    va_loader = DataLoader(\n",
    "        va_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS_EVAL, pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = SpecFeatureNet(num_classes=num_classes).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode=\"min\", factor=LR_FACTOR, patience=LR_PATIENCE\n",
    "    )\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "    es_count = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    fold_log_path = os.path.join(save_folder, f\"fold{fold}_trainlog.csv\")\n",
    "    fold_logger = CSVLogger(\n",
    "        fold_log_path,\n",
    "        header=[\"epoch\", \"lr\", \"train_loss\", \"val_loss\", \"val_acc\", \"best_val_loss\", \"es_count\", \"epoch_time_sec\"]\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        loss_sum, nb = 0.0, 0\n",
    "\n",
    "        for iq, y in tr_loader:\n",
    "            iq = iq.to(DEVICE, non_blocking=True)  # (B,L,2)\n",
    "            y  = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            # SimCLR positive pair: two random views of same sample\n",
    "            v1 = augment_train_batch(iq)\n",
    "            v2 = augment_train_batch(iq)\n",
    "\n",
    "            spec1 = iq_to_logspec_batch(v1)\n",
    "            spec2 = iq_to_logspec_batch(v2)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            with amp_autocast(enabled=USE_AMP):\n",
    "                z1, p1, z2, p2 = model(spec1, spec2)\n",
    "                loss_cl = nt_xent_loss(z1, z2, tau=TAU)\n",
    "\n",
    "                if LAMBDA_CE > 0.0:\n",
    "                    loss_ce = 0.5 * (ce(p1, y) + ce(p2, y))\n",
    "                else:\n",
    "                    loss_ce = torch.zeros((), device=DEVICE)\n",
    "\n",
    "                loss = LAMBDA_CL * loss_cl + LAMBDA_CE * loss_ce\n",
    "\n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            loss_sum += float(loss.item())\n",
    "            nb += 1\n",
    "\n",
    "        train_loss = loss_sum / max(nb, 1)\n",
    "        val_loss, val_acc, _ = eval_single_iq(model, va_loader, num_classes=num_classes, mode=\"val\")\n",
    "\n",
    "        prev_lr = opt.param_groups[0][\"lr\"]\n",
    "        scheduler.step(val_loss)\n",
    "        cur_lr = opt.param_groups[0][\"lr\"]\n",
    "        if cur_lr < prev_lr:\n",
    "            msg = f\"[LR DROP] {prev_lr:.2e} -> {cur_lr:.2e} (val_loss={val_loss:.4f})\"\n",
    "            print(msg)\n",
    "            write_line(results_txt, msg)\n",
    "\n",
    "        epoch_time = time.time() - t0\n",
    "        msg = (f\"Epoch {epoch:03d} | LR={cur_lr:.2e} | \"\n",
    "               f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | ValAcc={val_acc:.2f}% | \"\n",
    "               f\"BestValLoss={best_val_loss:.4f} | ES={es_count}/{ES_PATIENCE}\")\n",
    "        print(msg)\n",
    "        write_line(results_txt, msg)\n",
    "\n",
    "        fold_logger.log_row([epoch, cur_lr, train_loss, val_loss, val_acc, best_val_loss, es_count, epoch_time])\n",
    "\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            best_epoch = epoch\n",
    "            es_count = 0\n",
    "        else:\n",
    "            es_count += 1\n",
    "            if es_count >= ES_PATIENCE:\n",
    "                msg = \"[INFO] Early stopping triggered.\"\n",
    "                print(msg)\n",
    "                write_line(results_txt, msg)\n",
    "                break\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_folder, f\"model_fold{fold}.pth\"))\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        torch.save(model.state_dict(), os.path.join(save_folder, f\"best_model_fold{fold}.pth\"))\n",
    "\n",
    "    write_line(results_txt, f\"[FOLD {fold}] BestEpoch={best_epoch}, BestValLoss={best_val_loss:.6f}\")\n",
    "\n",
    "    # Test sweep for this fold\n",
    "    fold_test_csv = os.path.join(save_folder, f\"fold{fold}_test_snr.csv\")\n",
    "    fold_test_logger = CSVLogger(fold_test_csv, header=[\"snr_db\", \"test_loss\", \"test_acc\"])\n",
    "\n",
    "    fold_snr_acc = {}\n",
    "    for snr in TEST_SNR_LIST:\n",
    "        test_loss, test_acc, _ = eval_single_iq(model, test_loader, num_classes=num_classes, mode=\"test\", snr_db=float(snr))\n",
    "        snr_to_accs[snr].append(test_acc)\n",
    "        fold_snr_acc[snr] = test_acc\n",
    "        fold_test_logger.log_row([snr, test_loss, test_acc])\n",
    "\n",
    "    msg = \"[FOLD TEST] \" + \", \".join([f\"{snr}:{fold_snr_acc[snr]:.2f}%\" for snr in TEST_SNR_LIST])\n",
    "    print(msg)\n",
    "    write_line(results_txt, msg)\n",
    "\n",
    "# ----------------------------\n",
    "# 10) main\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_kfold_ltev_simclr_sample_split(DATA_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
