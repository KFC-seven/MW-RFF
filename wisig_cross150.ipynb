{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D CNN æ‰¹é‡ç‰ˆ\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from  data_utilities import *\n",
    "import cv2  # OpenCV ç”¨äºè°ƒæ•´å›¾åƒå¤§å°å’Œé¢œè‰²å¤„ç†\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gc  # å¼•å…¥åƒåœ¾å›æ”¶æ¨¡å—\n",
    "from tqdm.auto import tqdm  # è‡ªåŠ¨é€‚é…ç¯å¢ƒ å¯¼å…¥tqdmè¿›åº¦æ¡åº“\n",
    "from collections import defaultdict\n",
    "\n",
    "dataset_name = 'ManyTx'\n",
    "dataset_path='../ManyTx.pkl/'\n",
    "\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path,dataset_name)\n",
    "\n",
    "print(\"æ•°æ®é›†å‘å°„æœºæ•°é‡ï¼š\",len(compact_dataset['tx_list']),\"å…·ä½“ä¸ºï¼š\",compact_dataset['tx_list'])\n",
    "print(\"æ•°æ®é›†æ¥æ”¶æœºæ•°é‡ï¼š\",len(compact_dataset['rx_list']),\"å…·ä½“ä¸ºï¼š\",compact_dataset['rx_list'])\n",
    "print(\"æ•°æ®é›†é‡‡é›†å¤©æ•°ï¼š\",len(compact_dataset['capture_date_list']),\"å…·ä½“ä¸ºï¼š\",compact_dataset['capture_date_list'])\n",
    "tx_list = compact_dataset['tx_list']\n",
    "rx_list = compact_dataset['rx_list']\n",
    "equalized = 0\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "\n",
    "\n",
    "n_tx = len(tx_list)\n",
    "n_rx = len(rx_list)\n",
    "print(n_tx,n_rx)\n",
    "\n",
    "\n",
    "# å‚æ•°è®¾ç½®\n",
    "max_sig = None          # æ¯ä¸ª TX-RX-æ—¥æœŸæœ€å¤šä½¿ç”¨çš„ä¿¡å·æ•°\n",
    "block_size = 240        # æ¯ä¸ª block çš„ä¿¡å·æ•°\n",
    "y = 5                  # æ‹¼æ¥æ—¶æ¯ç»„å¤šå°‘æ¡ä¿¡å·\n",
    "test_ratio = 0.25        # æµ‹è¯•é›†æ¯”ä¾‹\n",
    "\n",
    "# è°ƒç”¨å‡½æ•°\n",
    "X_train, y_train, X_test, y_test = preprocess_dataset_cross_IQ_blocks_all_mix_random(\n",
    "    compact_dataset=compact_dataset,\n",
    "    tx_list=tx_list,\n",
    "    max_sig=max_sig,\n",
    "    equalized=equalized,\n",
    "    block_size=block_size,\n",
    "    y=y,\n",
    "    test_ratio=test_ratio,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test  shape:\", X_test.shape) \n",
    "print(\"y_test  shape:\", y_test.shape)\n",
    "\n",
    "# ---------- PyTorch & è®­ç»ƒé…ç½® ----------\n",
    "import torch\n",
    "torch._dynamo.disable()\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pywt  # ä¿ç•™ï¼Œè‹¥éœ€æ‰©å±•\n",
    "import math\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# æ•°æ®å¢å¼º / ä¿¡é“è®¾ç½®ï¼ˆç›®å‰é»˜è®¤å…³é—­ï¼‰\n",
    "USE_LOG = True\n",
    "FS = 20e6\n",
    "FC = 2.4e9\n",
    "SNR_DB = 20\n",
    "VELOCITY_KMH = 120\n",
    "ADD_NOISE = True\n",
    "ADD_DOPPLER = True\n",
    "\n",
    "# è®­ç»ƒå‚æ•°\n",
    "BATCH_SIZE = 256   # æ›´å¤§ batch æ›´ç¨³å®šï¼ˆä½ æ•°æ®å¾ˆå¤šï¼‰\n",
    "EPOCHS = 200\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "N_SPLITS = 5\n",
    "PATIENCE = 8\n",
    "MIN_DELTA = 0.1   # early stopping çš„æœ€å°è¿›æ­¥ï¼ˆ%ï¼‰\n",
    "\n",
    "SAVE_ROOT = \"./training_results\"\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "# ---------- å¯é€‰ï¼šAWGN / Dopplerï¼ˆä¿ç•™ï¼‰ ----------\n",
    "def compute_doppler_shift(v_kmh, fc_hz):\n",
    "    if not v_kmh:\n",
    "        return 0.0\n",
    "    c = 3e8\n",
    "    v = v_kmh / 3.6\n",
    "    return fc_hz * v / c\n",
    "\n",
    "def add_complex_awgn(signal, snr_db):\n",
    "    if snr_db is None:\n",
    "        return signal\n",
    "    power = np.mean(np.abs(signal)**2)\n",
    "    noise_power = power / (10**(snr_db/10))\n",
    "    noise_std = np.sqrt(noise_power/2)\n",
    "    noise = noise_std * (np.random.randn(*signal.shape) + 1j*np.random.randn(*signal.shape))\n",
    "    return signal + noise\n",
    "\n",
    "def apply_doppler_shift(signal, fd_hz, fs_hz):\n",
    "    if fd_hz is None or fd_hz == 0:\n",
    "        return signal\n",
    "    t = np.arange(len(signal)) / fs_hz\n",
    "    return signal * np.exp(1j * 2 * np.pi * fd_hz * t)\n",
    "\n",
    "# ---------- æ•°æ®é¢„å¤„ç†ï¼šé’ˆå¯¹ä½ çš„äº¤é”™æ ·æœ¬ï¼Œç”¨ per-sample æ ‡å‡†åŒ–ï¼Œä¸åš FFT ----------\n",
    "def preprocess_for_pointcloud_cnn(data_real_imag, add_noise=False, snr_db=None,\n",
    "                                  add_doppler=False, fd_hz=None, fs_hz=FS):\n",
    "    \"\"\"\n",
    "    è¾“å…¥ data_real_imag: np.array [N, L, 2] (I,Q)\n",
    "    è¾“å‡º torch.tensor [N, L, 2] (float32)ï¼Œæ¯ä¸ªæ ·æœ¬åš zero-mean unit-std æ ‡å‡†åŒ–ï¼ˆæŒ‰æ ·æœ¬ï¼‰\n",
    "    è¯´æ˜ï¼šä¸åš FFTï¼Œå› ä¸ºæ ·æœ¬ä¸æ˜¯è¿ç»­æ—¶é—´åºåˆ—ï¼ˆå·²äº¤é”™ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    data = data_real_imag.astype(np.float32).copy()\n",
    "    N, L, C = data.shape\n",
    "    out = np.empty_like(data, dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        iq = data[i]  # (L,2)\n",
    "        # å¯é€‰ï¼šå…ˆæŠŠ (I,Q) ç»„åˆæˆ complex åš channel çº§å¢å¼º (ä¿ç•™)\n",
    "        if add_noise:\n",
    "            # ä»¥ complex å½¢å¼æ·»åŠ å™ªå£°\n",
    "            sigc = iq[...,0] + 1j * iq[...,1]\n",
    "            sigc = add_complex_awgn(sigc, snr_db)\n",
    "            if add_doppler:\n",
    "                sigc = apply_doppler_shift(sigc, fd_hz, fs_hz)\n",
    "            iq = np.stack([np.real(sigc), np.imag(sigc)], axis=-1).astype(np.float32)\n",
    "\n",
    "        # per-sample æ ‡å‡†åŒ–ï¼ˆå¯¹ä¸¤ä¸ªé€šé“ä¸€èµ·ï¼‰\n",
    "        mu = iq.mean(axis=(0,))    # shape (2,)\n",
    "        sigma = iq.std(axis=(0,))  # shape (2,)\n",
    "        # é˜²æ­¢é™¤é›¶\n",
    "        sigma[sigma < 1e-8] = 1.0\n",
    "        iq_norm = (iq - mu) / sigma\n",
    "        out[i] = iq_norm\n",
    "\n",
    "    return torch.tensor(out, dtype=torch.float32)\n",
    "\n",
    "# ---------- æ–°çš„ CNNï¼šIQPointCloudCNNï¼ˆæŠŠæ¯ä¸ªæ ·æœ¬å½“ä½œ [1,240,2] å›¾åƒï¼‰ ----------\n",
    "class IQPointCloudCNN(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        # è¾“å…¥: [B, 1, L, 2]\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(7,2), stride=1, padding=(3,0), bias=False),  # -> [B,32,L,1]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),  # -> [B,32,L/2,1]\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=(5,1), padding=(2,0), bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),  # -> [B,64,L/4,1]\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=(3,1), padding=(1,0), bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),  # -> [B,128,L/8,1]\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=(3,1), padding=(1,0), bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),  # -> [B,256,L/16,1]\n",
    "        )\n",
    "        # è®¡ç®— flatten å¤§å°ï¼šL ä¸º 240 (ä½ çš„è®¾ç½®)ï¼ŒL//16 = 15\n",
    "        # 256 * 15 * 1 = 3840\n",
    "        self.flatten_dim = 256 * (240 // 16) * 1\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flatten_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, L, 2]\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)  # -> [B,1,L,2]\n",
    "        x = self.features(x)   # -> [B, 256, L/16, 1]\n",
    "        out = self.classifier(x)\n",
    "        return out\n",
    "\n",
    "# ---------- è¾…åŠ©ç»˜å›¾/è¯„ä¼°å‡½æ•° ----------\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            _, p = torch.max(out, 1)\n",
    "            correct += (p == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "            all_preds.extend(p.cpu().numpy())\n",
    "    acc = 100.0 * correct / total if total>0 else 0.0\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "    return acc, cm\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, fold, save_folder, dataset_type='Test'):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{dataset_type} Confusion Matrix Fold{fold}')\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.savefig(os.path.join(save_folder, f'{dataset_type.lower()}_cm_fold{fold}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_curves(train_losses, val_losses, train_acc, val_acc, fold, save_folder):\n",
    "    plt.figure(); plt.plot(train_losses, label='Train Loss'); plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title(f'Fold {fold} Loss'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_folder, f'loss_fold{fold}.png')); plt.close()\n",
    "    plt.figure(); plt.plot(train_acc, label='Train Acc'); plt.plot(val_acc, label='Val Acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)'); plt.title(f'Fold {fold} Accuracy'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_folder, f'acc_fold{fold}.png')); plt.close()\n",
    "\n",
    "# ---------- K-Fold è®­ç»ƒä¸»å‡½æ•°ï¼ˆä½¿ç”¨ IQPointCloudCNNï¼‰ ----------\n",
    "def train_kfold_pointcloud(X_train, y_train, X_test, y_test, num_classes, device=DEVICE,\n",
    "                           batch_size=BATCH_SIZE, epochs=EPOCHS, lr=LR, weight_decay=WEIGHT_DECAY,\n",
    "                           n_splits=N_SPLITS, patience=PATIENCE, min_delta=MIN_DELTA,\n",
    "                           script_name=\"wisig_LED_cross\"):\n",
    "    fd = int(compute_doppler_shift(VELOCITY_KMH, FC))\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    save_dir = f\"{timestamp}_{script_name}_SNR{SNR_DB}_fd{fd}_classes_{num_classes}_CNN\"\n",
    "    save_folder = os.path.join(SAVE_ROOT, save_dir)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    results_file = os.path.join(save_folder, \"results.txt\")\n",
    "\n",
    "    # ä¿å­˜å‚æ•°\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(\"=== Experiment Parameters ===\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"SNR_dB: {SNR_DB}, ADD_NOISE: {ADD_NOISE}, ADD_DOPPLER: {ADD_DOPPLER}\\n\")\n",
    "        f.write(f\"FS: {FS}, FC: {FC}, Velocity_kmh: {VELOCITY_KMH}\\n\")\n",
    "        f.write(f\"Batch: {batch_size}, Epochs: {epochs}, LR: {lr}, WD: {weight_decay}\\n\")\n",
    "        f.write(f\"Num classes: {num_classes}, K-Fold: {n_splits}, Patience: {patience}, MinDelta: {min_delta}\\n\")\n",
    "        f.write(\"============================\\n\\n\")\n",
    "\n",
    "    # test loader\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # full dataset for K-Fold\n",
    "    full_dataset = TensorDataset(X_train, y_train)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    indices = np.arange(len(full_dataset))\n",
    "\n",
    "    val_scores, test_scores = [], []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{n_splits} ===\")\n",
    "        tr_sub = Subset(full_dataset, tr_idx)\n",
    "        va_sub = Subset(full_dataset, va_idx)\n",
    "        tr_loader = DataLoader(tr_sub, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        va_loader = DataLoader(va_sub, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        model = IQPointCloudCNN(num_classes=num_classes, dropout=0.3).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        best_val = 0.0\n",
    "        best_wts = None\n",
    "        patience_cnt = 0\n",
    "\n",
    "        train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "        avg_grad_list = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss, correct, total = 0.0, 0, 0\n",
    "            total_grad, cnt_grad = 0.0, 0\n",
    "            for xb, yb in tr_loader:\n",
    "                xb = xb.to(device); yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                # grad norm\n",
    "                grad_norms = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]\n",
    "                if grad_norms:\n",
    "                    total_grad += np.mean(grad_norms); cnt_grad += 1\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                _, p = torch.max(out, 1)\n",
    "                correct += (p == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "            avg_grad = total_grad / max(cnt_grad, 1)\n",
    "            avg_grad_list.append(avg_grad)\n",
    "\n",
    "            train_loss = running_loss / max(1, len(tr_loader))\n",
    "            train_acc = 100.0 * correct / max(1, total)\n",
    "            train_losses.append(train_loss); train_accs.append(train_acc)\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            vloss, vcorrect, vtotal = 0.0, 0, 0\n",
    "            all_labels, all_preds = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in va_loader:\n",
    "                    xb = xb.to(device); yb = yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    loss = criterion(out, yb)\n",
    "                    vloss += loss.item()\n",
    "                    _, p = torch.max(out, 1)\n",
    "                    vcorrect += (p == yb).sum().item()\n",
    "                    vtotal += yb.size(0)\n",
    "                    all_labels.extend(yb.cpu().numpy()); all_preds.extend(p.cpu().numpy())\n",
    "            val_loss = vloss / max(1, len(va_loader))\n",
    "            val_acc = 100.0 * vcorrect / max(1, vtotal)\n",
    "            val_losses.append(val_loss); val_accs.append(val_acc)\n",
    "            val_cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "            np.save(os.path.join(save_folder, f'val_cm_fold{fold+1}.npy'), val_cm)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | TrainAcc={train_acc:.2f}% | ValAcc={val_acc:.2f}% | \"\n",
    "                  f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | AvgGrad={avg_grad:.4f}\")\n",
    "            with open(results_file, 'a') as f:\n",
    "                f.write(f\"Fold{fold+1} Epoch{epoch+1} | TrainAcc={train_acc:.2f}% | ValAcc={val_acc:.2f}% | \"\n",
    "                        f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | AvgGrad={avg_grad:.4f}\\n\")\n",
    "\n",
    "            # Early stopping on validation accuracy with min_delta (percentage points)\n",
    "            if val_acc > best_val + min_delta:\n",
    "                best_val = val_acc\n",
    "                best_wts = model.state_dict()\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "                if patience_cnt >= patience:\n",
    "                    print(\"Early stopping.\")\n",
    "                    break\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # restore best\n",
    "        if best_wts is not None:\n",
    "            model.load_state_dict(best_wts)\n",
    "\n",
    "        # ä¿å­˜ train/val æ··æ·†çŸ©é˜µ\n",
    "        train_acc, train_cm = evaluate_model(model, tr_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'train_cm_fold{fold+1}.npy'), train_cm)\n",
    "        plot_confusion_matrix(train_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Train')\n",
    "\n",
    "        val_acc, val_cm = evaluate_model(model, va_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'val_cm_fold{fold+1}.npy'), val_cm)\n",
    "        plot_confusion_matrix(val_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Val')\n",
    "\n",
    "        # test eval\n",
    "        test_acc, test_cm = evaluate_model(model, test_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'test_cm_fold{fold+1}.npy'), test_cm)\n",
    "        plot_confusion_matrix(test_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Test')\n",
    "        print(f\"Fold {fold+1} Test Accuracy: {test_acc:.2f}%\")\n",
    "        with open(results_file, 'a') as f:\n",
    "            f.write(f\"Fold{fold+1} TestAcc={test_acc:.2f}%\\n\")\n",
    "\n",
    "        # ä¿å­˜æ›²çº¿ & æ¨¡å‹\n",
    "        plot_curves(train_losses, val_losses, train_accs, val_accs, fold+1, save_folder)\n",
    "        torch.save(model.state_dict(), os.path.join(save_folder, f'model_fold{fold+1}.pth'))\n",
    "\n",
    "        val_scores.append(val_acc)\n",
    "        test_scores.append(test_acc)\n",
    "\n",
    "    # summary\n",
    "    print(\"\\n=== Overall Summary ===\")\n",
    "    print(f\"Val Acc: {np.mean(val_scores):.2f} Â± {np.std(val_scores):.2f}\")\n",
    "    print(f\"Test Acc: {np.mean(test_scores):.2f} Â± {np.std(test_scores):.2f}\")\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(f\"\\n=== Overall Summary ===\\nVal Acc: {np.mean(val_scores):.2f} Â± {np.std(val_scores):.2f}\\nTest Acc: {np.mean(test_scores):.2f} Â± {np.std(test_scores):.2f}\\n\")\n",
    "\n",
    "    print(f\"\\nAll results saved in {save_folder}\")\n",
    "    return save_folder\n",
    "\n",
    "# ---------- è¿è¡Œå‰çš„å‡†å¤‡ï¼ˆé¢„å¤„ç†å¹¶è½¬æ¢ä¸ºå¼ é‡ï¼‰ ----------\n",
    "# X_train, X_test ç›®å‰ shape: [N, 240, 2] (numpy)\n",
    "# æˆ‘ä»¬å¯¹æ¯ä¸ªæ ·æœ¬åš per-sample æ ‡å‡†åŒ–ï¼ˆzero-mean unit-stdï¼‰\n",
    "print(\"Preprocessing (per-sample normalization)...\")\n",
    "# ---------- è½¬ä¸º torch.tensor å¹¶åš per-sample æ ‡å‡†åŒ– ----------\n",
    "\n",
    "X_train_torch = preprocess_for_pointcloud_cnn(X_train, add_noise=False)\n",
    "X_test_torch  = preprocess_for_pointcloud_cnn(X_test, add_noise=False)\n",
    "\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_torch  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "num_classes = len(torch.unique(y_train_torch))\n",
    "print(\"Prepared tensors:\", X_train_torch.shape, X_test_torch.shape, \"num_classes=\", num_classes)\n",
    "\n",
    "# ---------- K-Fold è®­ç»ƒ ----------\n",
    "# train_kfold_pointcloud å†…éƒ¨ä¼šæŠŠè®­ç»ƒé›†å†åˆ’åˆ†ä¸ºè®­ç»ƒ+éªŒè¯é›†\n",
    "#save_folder = train_kfold_pointcloud(\n",
    "#    X_train_torch, \n",
    "#    y_train_torch,\n",
    "#    X_test_torch, \n",
    "#    y_test_torch,\n",
    "#    num_classes=num_classes\n",
    "#)\n",
    "\n",
    "#print(\"Done. Results in:\", save_folder)\n",
    "\n",
    "###########################################################\n",
    "# === å®šä¹‰ä¸€ä¸ªç»Ÿä¸€å…¥å£å‡½æ•°ï¼Œç”¨äºè·‘ä¸€æ¬¡ SNR å®éªŒ ===\n",
    "###########################################################\n",
    "def run_experiment_with_snr(snr_db):\n",
    "    global SNR_DB, ADD_NOISE\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸš€ Running experiment with SNR = {snr_db} dB\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # è®¾ç½®å½“å‰å®éªŒ SNR\n",
    "    SNR_DB = snr_db\n",
    "    ADD_NOISE = True   # æ‰“å¼€ AWGN\n",
    "    \n",
    "    # --- é‡æ–°é¢„å¤„ç†æ•°æ®ï¼ˆä¼šè°ƒç”¨ add_noise=Trueï¼‰ ---\n",
    "    X_train_torch_snr = preprocess_for_pointcloud_cnn(\n",
    "        X_train, add_noise=True, add_doppler = ADD_DOPPLER, snr_db=snr_db\n",
    "    )\n",
    "    X_test_torch_snr = preprocess_for_pointcloud_cnn(\n",
    "        X_test, add_noise=True, add_doppler = ADD_DOPPLER, snr_db=snr_db\n",
    "    )\n",
    "\n",
    "    y_train_torch_snr = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test_torch_snr  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "    num_classes = len(torch.unique(y_train_torch_snr))\n",
    "\n",
    "    # --- è°ƒç”¨ä½ çš„ K-Fold è®­ç»ƒ ---\n",
    "    save_folder = train_kfold_pointcloud(\n",
    "        X_train_torch_snr,\n",
    "        y_train_torch_snr,\n",
    "        X_test_torch_snr,\n",
    "        y_test_torch_snr,\n",
    "        num_classes=num_classes,\n",
    "        script_name=f\"SNR_{snr_db}dB\"\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸ‰ Finished SNR={snr_db} dB, results saved in: {save_folder}\")\n",
    "    return save_folder\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# === æ‰¹é‡æ‰§è¡Œ SNR Sweep: 20, 10, 0, -10, ... -40 dB ===\n",
    "###########################################################\n",
    "snr_list = list(range(20, -41, -5))\n",
    "# [20, 10, 0, -10, -20, -30, -40]\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for snr in snr_list:\n",
    "    folder = run_experiment_with_snr(snr)\n",
    "    all_results[snr] = folder\n",
    "\n",
    "print(\"\\n\\n================ FINAL SUMMARY ================\")\n",
    "for snr, folder in all_results.items():\n",
    "    print(f\"SNR {snr:>3} dB â†’ results in: {folder}\")\n",
    "print(\"==============================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174625f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet æ‰¹é‡ç‰ˆ\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from  data_utilities import *\n",
    "import cv2  # OpenCV ç”¨äºè°ƒæ•´å›¾åƒå¤§å°å’Œé¢œè‰²å¤„ç†\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gc  # å¼•å…¥åƒåœ¾å›æ”¶æ¨¡å—\n",
    "from tqdm.auto import tqdm  # è‡ªåŠ¨é€‚é…ç¯å¢ƒ å¯¼å…¥tqdmè¿›åº¦æ¡åº“\n",
    "from collections import defaultdict\n",
    "\n",
    "dataset_name = 'ManyTx'\n",
    "dataset_path='../ManyTx.pkl/'\n",
    "\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path,dataset_name)\n",
    "\n",
    "print(\"æ•°æ®é›†å‘å°„æœºæ•°é‡ï¼š\",len(compact_dataset['tx_list']),\"å…·ä½“ä¸ºï¼š\",compact_dataset['tx_list'])\n",
    "print(\"æ•°æ®é›†æ¥æ”¶æœºæ•°é‡ï¼š\",len(compact_dataset['rx_list']),\"å…·ä½“ä¸ºï¼š\",compact_dataset['rx_list'])\n",
    "print(\"æ•°æ®é›†é‡‡é›†å¤©æ•°ï¼š\",len(compact_dataset['capture_date_list']),\"å…·ä½“ä¸ºï¼š\",compact_dataset['capture_date_list'])\n",
    "tx_list = compact_dataset['tx_list']\n",
    "rx_list = compact_dataset['rx_list']\n",
    "equalized = 0\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "\n",
    "\n",
    "n_tx = len(tx_list)\n",
    "n_rx = len(rx_list)\n",
    "print(n_tx,n_rx)\n",
    "\n",
    "\n",
    "# å‚æ•°è®¾ç½®\n",
    "max_sig = None          # æ¯ä¸ª TX-RX-æ—¥æœŸæœ€å¤šä½¿ç”¨çš„ä¿¡å·æ•°\n",
    "block_size = 240        # æ¯ä¸ª block çš„ä¿¡å·æ•°\n",
    "y = 5                   # æ‹¼æ¥æ—¶æ¯ç»„å¤šå°‘æ¡ä¿¡å·\n",
    "test_ratio = 0.25       # æµ‹è¯•é›†æ¯”ä¾‹\n",
    "\n",
    "# è°ƒç”¨å‡½æ•°\n",
    "X_train, y_train, X_test, y_test = preprocess_dataset_cross_IQ_blocks_all_mix_random(\n",
    "    compact_dataset=compact_dataset,\n",
    "    tx_list=tx_list,\n",
    "    max_sig=max_sig,\n",
    "    equalized=equalized,\n",
    "    block_size=block_size,\n",
    "    y=y,\n",
    "    test_ratio=test_ratio,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test  shape:\", X_test.shape) \n",
    "print(\"y_test  shape:\", y_test.shape)\n",
    "\n",
    "# ---------- PyTorch & è®­ç»ƒé…ç½® ----------\n",
    "import torch\n",
    "torch._dynamo.disable()\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pywt  # ä¿ç•™ï¼Œè‹¥éœ€æ‰©å±•\n",
    "import math\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# æ•°æ®å¢å¼º / ä¿¡é“è®¾ç½®ï¼ˆç›®å‰é»˜è®¤å…³é—­ï¼‰\n",
    "USE_LOG = True\n",
    "FS = 20e6\n",
    "FC = 2.4e9\n",
    "SNR_DB = 20\n",
    "VELOCITY_KMH = 120\n",
    "ADD_NOISE = True\n",
    "ADD_DOPPLER = True\n",
    "\n",
    "# è®­ç»ƒå‚æ•°\n",
    "BATCH_SIZE = 256   # æ›´å¤§ batch æ›´ç¨³å®šï¼ˆä½ æ•°æ®å¾ˆå¤šï¼‰\n",
    "EPOCHS = 200\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "N_SPLITS = 5\n",
    "PATIENCE = 8\n",
    "MIN_DELTA = 0.1   # early stopping çš„æœ€å°è¿›æ­¥ï¼ˆ%ï¼‰\n",
    "\n",
    "SAVE_ROOT = \"./training_results\"\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "# ---------- å¯é€‰ï¼šAWGN / Dopplerï¼ˆä¿ç•™ï¼‰ ----------\n",
    "def compute_doppler_shift(v_kmh, fc_hz):\n",
    "    if not v_kmh:\n",
    "        return 0.0\n",
    "    c = 3e8\n",
    "    v = v_kmh / 3.6\n",
    "    return fc_hz * v / c\n",
    "\n",
    "def add_complex_awgn(signal, snr_db):\n",
    "    if snr_db is None:\n",
    "        return signal\n",
    "    power = np.mean(np.abs(signal)**2)\n",
    "    noise_power = power / (10**(snr_db/10))\n",
    "    noise_std = np.sqrt(noise_power/2)\n",
    "    noise = noise_std * (np.random.randn(*signal.shape) + 1j*np.random.randn(*signal.shape))\n",
    "    return signal + noise\n",
    "\n",
    "def apply_doppler_shift(signal, fd_hz, fs_hz):\n",
    "    if fd_hz is None or fd_hz == 0:\n",
    "        return signal\n",
    "    t = np.arange(len(signal)) / fs_hz\n",
    "    return signal * np.exp(1j * 2 * np.pi * fd_hz * t)\n",
    "\n",
    "# ---------- æ•°æ®é¢„å¤„ç†ï¼šé’ˆå¯¹ä½ çš„äº¤é”™æ ·æœ¬ï¼Œç”¨ per-sample æ ‡å‡†åŒ–ï¼Œä¸åš FFT ----------\n",
    "def preprocess_for_pointcloud_cnn(data_real_imag, add_noise=False, snr_db=None,\n",
    "                                  add_doppler=False, fd_hz=None, fs_hz=FS):\n",
    "    \"\"\"\n",
    "    è¾“å…¥ data_real_imag: np.array [N, L, 2] (I,Q)\n",
    "    è¾“å‡º torch.tensor [N, L, 2] (float32)ï¼Œæ¯ä¸ªæ ·æœ¬åš zero-mean unit-std æ ‡å‡†åŒ–ï¼ˆæŒ‰æ ·æœ¬ï¼‰\n",
    "    è¯´æ˜ï¼šä¸åš FFTï¼Œå› ä¸ºæ ·æœ¬ä¸æ˜¯è¿ç»­æ—¶é—´åºåˆ—ï¼ˆå·²äº¤é”™ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    data = data_real_imag.astype(np.float32).copy()\n",
    "    N, L, C = data.shape\n",
    "    out = np.empty_like(data, dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        iq = data[i]  # (L,2)\n",
    "        # å¯é€‰ï¼šå…ˆæŠŠ (I,Q) ç»„åˆæˆ complex åš channel çº§å¢å¼º (ä¿ç•™)\n",
    "        if add_noise:\n",
    "            # ä»¥ complex å½¢å¼æ·»åŠ å™ªå£°\n",
    "            sigc = iq[...,0] + 1j * iq[...,1]\n",
    "            sigc = add_complex_awgn(sigc, snr_db)\n",
    "            if add_doppler:\n",
    "                sigc = apply_doppler_shift(sigc, fd_hz, fs_hz)\n",
    "            iq = np.stack([np.real(sigc), np.imag(sigc)], axis=-1).astype(np.float32)\n",
    "\n",
    "        # per-sample æ ‡å‡†åŒ–ï¼ˆå¯¹ä¸¤ä¸ªé€šé“ä¸€èµ·ï¼‰\n",
    "        mu = iq.mean(axis=(0,))    # shape (2,)\n",
    "        sigma = iq.std(axis=(0,))  # shape (2,)\n",
    "        # é˜²æ­¢é™¤é›¶\n",
    "        sigma[sigma < 1e-8] = 1.0\n",
    "        iq_norm = (iq - mu) / sigma\n",
    "        out[i] = iq_norm\n",
    "\n",
    "    return torch.tensor(out, dtype=torch.float32)\n",
    "\n",
    "# ---------- æ–°çš„ CNNï¼šIQPointCloudCNNï¼ˆæŠŠæ¯ä¸ªæ ·æœ¬å½“ä½œ [1,240,2] å›¾åƒï¼‰ ----------\n",
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.downsample = None\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RF1DCNN(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = ResidualBlock1D(2, 32, kernel_size=7)\n",
    "        self.pool1 = nn.MaxPool1d(2)  # 240 â†’ 120\n",
    "\n",
    "        self.layer2 = ResidualBlock1D(32, 64, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool1d(2)  # 120 â†’ 60\n",
    "\n",
    "        self.layer3 = ResidualBlock1D(64, 128, kernel_size=5)\n",
    "        self.pool3 = nn.MaxPool1d(2)  # 60 â†’ 30\n",
    "\n",
    "        self.layer4 = ResidualBlock1D(128, 256, kernel_size=3)\n",
    "        self.pool4 = nn.MaxPool1d(2)  # 30 â†’ 15\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 15, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 240, 2)\n",
    "        x = x.permute(0, 2, 1)  # â†’ (B, 2, 240)\n",
    "\n",
    "        x = self.layer1(x); x = self.pool1(x)\n",
    "        x = self.layer2(x); x = self.pool2(x)\n",
    "        x = self.layer3(x); x = self.pool3(x)\n",
    "        x = self.layer4(x); x = self.pool4(x)\n",
    "\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ---------- è¾…åŠ©ç»˜å›¾/è¯„ä¼°å‡½æ•° ----------\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            _, p = torch.max(out, 1)\n",
    "            correct += (p == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "            all_preds.extend(p.cpu().numpy())\n",
    "    acc = 100.0 * correct / total if total>0 else 0.0\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "    return acc, cm\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, fold, save_folder, dataset_type='Test'):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{dataset_type} Confusion Matrix Fold{fold}')\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.savefig(os.path.join(save_folder, f'{dataset_type.lower()}_cm_fold{fold}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_curves(train_losses, val_losses, train_acc, val_acc, fold, save_folder):\n",
    "    plt.figure(); plt.plot(train_losses, label='Train Loss'); plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title(f'Fold {fold} Loss'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_folder, f'loss_fold{fold}.png')); plt.close()\n",
    "    plt.figure(); plt.plot(train_acc, label='Train Acc'); plt.plot(val_acc, label='Val Acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)'); plt.title(f'Fold {fold} Accuracy'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_folder, f'acc_fold{fold}.png')); plt.close()\n",
    "\n",
    "# ---------- K-Fold è®­ç»ƒä¸»å‡½æ•°ï¼ˆä½¿ç”¨ IQPointCloudCNNï¼‰ ----------\n",
    "def train_kfold_pointcloud(X_train, y_train, X_test, y_test, num_classes, device=DEVICE,\n",
    "                           batch_size=BATCH_SIZE, epochs=EPOCHS, lr=LR, weight_decay=WEIGHT_DECAY,\n",
    "                           n_splits=N_SPLITS, patience=PATIENCE, min_delta=MIN_DELTA,\n",
    "                           script_name=\"wisig_LED_cross\"):\n",
    "    fd = int(compute_doppler_shift(VELOCITY_KMH, FC))\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    save_dir = f\"{timestamp}_{script_name}_SNR{SNR_DB}_fd{fd}_classes_{num_classes}_CNN\"\n",
    "    save_folder = os.path.join(SAVE_ROOT, save_dir)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    results_file = os.path.join(save_folder, \"results.txt\")\n",
    "\n",
    "    # ä¿å­˜å‚æ•°\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(\"=== Experiment Parameters ===\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"SNR_dB: {SNR_DB}, ADD_NOISE: {ADD_NOISE}, ADD_DOPPLER: {ADD_DOPPLER}\\n\")\n",
    "        f.write(f\"FS: {FS}, FC: {FC}, Velocity_kmh: {VELOCITY_KMH}\\n\")\n",
    "        f.write(f\"Batch: {batch_size}, Epochs: {epochs}, LR: {lr}, WD: {weight_decay}\\n\")\n",
    "        f.write(f\"Num classes: {num_classes}, K-Fold: {n_splits}, Patience: {patience}, MinDelta: {min_delta}\\n\")\n",
    "        f.write(\"============================\\n\\n\")\n",
    "\n",
    "    # test loader\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # full dataset for K-Fold\n",
    "    full_dataset = TensorDataset(X_train, y_train)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    indices = np.arange(len(full_dataset))\n",
    "\n",
    "    val_scores, test_scores = [], []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{n_splits} ===\")\n",
    "        tr_sub = Subset(full_dataset, tr_idx)\n",
    "        va_sub = Subset(full_dataset, va_idx)\n",
    "        tr_loader = DataLoader(tr_sub, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        va_loader = DataLoader(va_sub, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        model = RF1DCNN(num_classes=num_classes, dropout=0.3).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        best_val = 0.0\n",
    "        best_wts = None\n",
    "        patience_cnt = 0\n",
    "\n",
    "        train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "        avg_grad_list = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss, correct, total = 0.0, 0, 0\n",
    "            total_grad, cnt_grad = 0.0, 0\n",
    "            for xb, yb in tr_loader:\n",
    "                xb = xb.to(device); yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                # grad norm\n",
    "                grad_norms = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]\n",
    "                if grad_norms:\n",
    "                    total_grad += np.mean(grad_norms); cnt_grad += 1\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                _, p = torch.max(out, 1)\n",
    "                correct += (p == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "            avg_grad = total_grad / max(cnt_grad, 1)\n",
    "            avg_grad_list.append(avg_grad)\n",
    "\n",
    "            train_loss = running_loss / max(1, len(tr_loader))\n",
    "            train_acc = 100.0 * correct / max(1, total)\n",
    "            train_losses.append(train_loss); train_accs.append(train_acc)\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            vloss, vcorrect, vtotal = 0.0, 0, 0\n",
    "            all_labels, all_preds = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in va_loader:\n",
    "                    xb = xb.to(device); yb = yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    loss = criterion(out, yb)\n",
    "                    vloss += loss.item()\n",
    "                    _, p = torch.max(out, 1)\n",
    "                    vcorrect += (p == yb).sum().item()\n",
    "                    vtotal += yb.size(0)\n",
    "                    all_labels.extend(yb.cpu().numpy()); all_preds.extend(p.cpu().numpy())\n",
    "            val_loss = vloss / max(1, len(va_loader))\n",
    "            val_acc = 100.0 * vcorrect / max(1, vtotal)\n",
    "            val_losses.append(val_loss); val_accs.append(val_acc)\n",
    "            val_cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "            np.save(os.path.join(save_folder, f'val_cm_fold{fold+1}.npy'), val_cm)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | TrainAcc={train_acc:.2f}% | ValAcc={val_acc:.2f}% | \"\n",
    "                  f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | AvgGrad={avg_grad:.4f}\")\n",
    "            with open(results_file, 'a') as f:\n",
    "                f.write(f\"Fold{fold+1} Epoch{epoch+1} | TrainAcc={train_acc:.2f}% | ValAcc={val_acc:.2f}% | \"\n",
    "                        f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | AvgGrad={avg_grad:.4f}\\n\")\n",
    "\n",
    "            # Early stopping on validation accuracy with min_delta (percentage points)\n",
    "            if val_acc > best_val + min_delta:\n",
    "                best_val = val_acc\n",
    "                best_wts = model.state_dict()\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "                if patience_cnt >= patience:\n",
    "                    print(\"Early stopping.\")\n",
    "                    break\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # restore best\n",
    "        if best_wts is not None:\n",
    "            model.load_state_dict(best_wts)\n",
    "\n",
    "        # ä¿å­˜ train/val æ··æ·†çŸ©é˜µ\n",
    "        train_acc, train_cm = evaluate_model(model, tr_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'train_cm_fold{fold+1}.npy'), train_cm)\n",
    "        plot_confusion_matrix(train_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Train')\n",
    "\n",
    "        val_acc, val_cm = evaluate_model(model, va_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'val_cm_fold{fold+1}.npy'), val_cm)\n",
    "        plot_confusion_matrix(val_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Val')\n",
    "\n",
    "        # test eval\n",
    "        test_acc, test_cm = evaluate_model(model, test_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'test_cm_fold{fold+1}.npy'), test_cm)\n",
    "        plot_confusion_matrix(test_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Test')\n",
    "        print(f\"Fold {fold+1} Test Accuracy: {test_acc:.2f}%\")\n",
    "        with open(results_file, 'a') as f:\n",
    "            f.write(f\"Fold{fold+1} TestAcc={test_acc:.2f}%\\n\")\n",
    "\n",
    "        # ä¿å­˜æ›²çº¿ & æ¨¡å‹\n",
    "        plot_curves(train_losses, val_losses, train_accs, val_accs, fold+1, save_folder)\n",
    "        torch.save(model.state_dict(), os.path.join(save_folder, f'model_fold{fold+1}.pth'))\n",
    "\n",
    "        val_scores.append(val_acc)\n",
    "        test_scores.append(test_acc)\n",
    "\n",
    "    # summary\n",
    "    print(\"\\n=== Overall Summary ===\")\n",
    "    print(f\"Val Acc: {np.mean(val_scores):.2f} Â± {np.std(val_scores):.2f}\")\n",
    "    print(f\"Test Acc: {np.mean(test_scores):.2f} Â± {np.std(test_scores):.2f}\")\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(f\"\\n=== Overall Summary ===\\nVal Acc: {np.mean(val_scores):.2f} Â± {np.std(val_scores):.2f}\\nTest Acc: {np.mean(test_scores):.2f} Â± {np.std(test_scores):.2f}\\n\")\n",
    "\n",
    "    print(f\"\\nAll results saved in {save_folder}\")\n",
    "    return save_folder\n",
    "\n",
    "# ---------- è¿è¡Œå‰çš„å‡†å¤‡ï¼ˆé¢„å¤„ç†å¹¶è½¬æ¢ä¸ºå¼ é‡ï¼‰ ----------\n",
    "# X_train, X_test ç›®å‰ shape: [N, 240, 2] (numpy)\n",
    "# æˆ‘ä»¬å¯¹æ¯ä¸ªæ ·æœ¬åš per-sample æ ‡å‡†åŒ–ï¼ˆzero-mean unit-stdï¼‰\n",
    "print(\"Preprocessing (per-sample normalization)...\")\n",
    "# ---------- è½¬ä¸º torch.tensor å¹¶åš per-sample æ ‡å‡†åŒ– ----------\n",
    "\n",
    "X_train_torch = preprocess_for_pointcloud_cnn(X_train, add_noise=False)\n",
    "X_test_torch  = preprocess_for_pointcloud_cnn(X_test, add_noise=False)\n",
    "\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_torch  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "num_classes = len(torch.unique(y_train_torch))\n",
    "print(\"Prepared tensors:\", X_train_torch.shape, X_test_torch.shape, \"num_classes=\", num_classes)\n",
    "\n",
    "# ---------- K-Fold è®­ç»ƒ ----------\n",
    "# train_kfold_pointcloud å†…éƒ¨ä¼šæŠŠè®­ç»ƒé›†å†åˆ’åˆ†ä¸ºè®­ç»ƒ+éªŒè¯é›†\n",
    "#save_folder = train_kfold_pointcloud(\n",
    "#    X_train_torch, \n",
    "#    y_train_torch,\n",
    "#    X_test_torch, \n",
    "#    y_test_torch,\n",
    "#    num_classes=num_classes\n",
    "#)\n",
    "\n",
    "#print(\"Done. Results in:\", save_folder)\n",
    "\n",
    "###########################################################\n",
    "# === å®šä¹‰ä¸€ä¸ªç»Ÿä¸€å…¥å£å‡½æ•°ï¼Œç”¨äºè·‘ä¸€æ¬¡ SNR å®éªŒ ===\n",
    "###########################################################\n",
    "def run_experiment_with_snr(snr_db):\n",
    "    global SNR_DB, ADD_NOISE\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸš€ Running experiment with SNR = {snr_db} dB\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # è®¾ç½®å½“å‰å®éªŒ SNR\n",
    "    SNR_DB = snr_db\n",
    "    ADD_NOISE = True   # æ‰“å¼€ AWGN\n",
    "    \n",
    "    # --- é‡æ–°é¢„å¤„ç†æ•°æ®ï¼ˆä¼šè°ƒç”¨ add_noise=Trueï¼‰ ---\n",
    "    X_train_torch_snr = preprocess_for_pointcloud_cnn(\n",
    "        X_train, add_noise=True, add_doppler = ADD_DOPPLER, snr_db=snr_db\n",
    "    )\n",
    "    X_test_torch_snr = preprocess_for_pointcloud_cnn(\n",
    "        X_test, add_noise=True, add_doppler = ADD_DOPPLER, snr_db=snr_db\n",
    "    )\n",
    "\n",
    "    y_train_torch_snr = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test_torch_snr  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "    num_classes = len(torch.unique(y_train_torch_snr))\n",
    "\n",
    "    # --- è°ƒç”¨ä½ çš„ K-Fold è®­ç»ƒ ---\n",
    "    save_folder = train_kfold_pointcloud(\n",
    "        X_train_torch_snr,\n",
    "        y_train_torch_snr,\n",
    "        X_test_torch_snr,\n",
    "        y_test_torch_snr,\n",
    "        num_classes=num_classes,\n",
    "        script_name=f\"SNR_{snr_db}dB\"\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸ‰ Finished SNR={snr_db} dB, results saved in: {save_folder}\")\n",
    "    return save_folder\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# === æ‰¹é‡æ‰§è¡Œ SNR Sweep: 20, 10, 0, -10, ... -40 dB ===\n",
    "###########################################################\n",
    "snr_list = list(range(20, -41, -5))\n",
    "# [20, 10, 0, -10, -20, -30, -40]\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for snr in snr_list:\n",
    "    folder = run_experiment_with_snr(snr)\n",
    "    all_results[snr] = folder\n",
    "\n",
    "print(\"\\n\\n================ FINAL SUMMARY ================\")\n",
    "for snr, folder in all_results.items():\n",
    "    print(f\"SNR {snr:>3} dB â†’ results in: {folder}\")\n",
    "print(\"==============================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18 å¾ªç¯SNR å…¨æ—¥æœŸé—­é›†\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from  data_utilities import *\n",
    "import cv2  # OpenCV ç”¨äºè°ƒæ•´å›¾åƒå¤§å°å’Œé¢œè‰²å¤„ç†\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gc  # å¼•å…¥åƒåœ¾å›æ”¶æ¨¡å—\n",
    "from tqdm.auto import tqdm  # è‡ªåŠ¨é€‚é…ç¯å¢ƒ å¯¼å…¥tqdmè¿›åº¦æ¡åº“\n",
    "from collections import defaultdict\n",
    "\n",
    "dataset_name = 'ManyTx'\n",
    "dataset_path='../ManyTx.pkl/'\n",
    "\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path,dataset_name)\n",
    "\n",
    "print(\"æ•°æ®é›†å‘å°„æœºæ•°é‡ï¼š\",len(compact_dataset['tx_list']),\"å…·ä½“ä¸ºï¼š\",compact_dataset['tx_list'])\n",
    "print(\"æ•°æ®é›†æ¥æ”¶æœºæ•°é‡ï¼š\",len(compact_dataset['rx_list']),\"å…·ä½“ä¸ºï¼š\",compact_dataset['rx_list'])\n",
    "print(\"æ•°æ®é›†é‡‡é›†å¤©æ•°ï¼š\",len(compact_dataset['capture_date_list']),\"å…·ä½“ä¸ºï¼š\",compact_dataset['capture_date_list'])\n",
    "tx_list = compact_dataset['tx_list']\n",
    "rx_list = compact_dataset['rx_list']\n",
    "equalized = 0\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "\n",
    "\n",
    "n_tx = len(tx_list)\n",
    "n_rx = len(rx_list)\n",
    "print(n_tx,n_rx)\n",
    "\n",
    "\n",
    "# å‚æ•°è®¾ç½®\n",
    "max_sig = None          # æ¯ä¸ª TX-RX-æ—¥æœŸæœ€å¤šä½¿ç”¨çš„ä¿¡å·æ•°\n",
    "block_size = 240        # æ¯ä¸ª block çš„ä¿¡å·æ•°\n",
    "y = 5                  # æ‹¼æ¥æ—¶æ¯ç»„å¤šå°‘æ¡ä¿¡å·\n",
    "test_ratio = 0.25        # æµ‹è¯•é›†æ¯”ä¾‹\n",
    "\n",
    "# è°ƒç”¨å‡½æ•°\n",
    "X_train, y_train, X_test, y_test = preprocess_dataset_cross_IQ_blocks_all_mix_random(\n",
    "    compact_dataset=compact_dataset,\n",
    "    tx_list=tx_list,\n",
    "    max_sig=max_sig,\n",
    "    equalized=equalized,\n",
    "    block_size=block_size,\n",
    "    y=y,\n",
    "    test_ratio=test_ratio,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test  shape:\", X_test.shape) \n",
    "print(\"y_test  shape:\", y_test.shape)\n",
    "\n",
    "# ---------- PyTorch & è®­ç»ƒé…ç½® ----------\n",
    "import torch\n",
    "torch._dynamo.disable()\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pywt  # ä¿ç•™ï¼Œè‹¥éœ€æ‰©å±•\n",
    "import math\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# æ•°æ®å¢å¼º / ä¿¡é“è®¾ç½®ï¼ˆç›®å‰é»˜è®¤å…³é—­ï¼‰\n",
    "USE_LOG = True\n",
    "FS = 20e6\n",
    "FC = 2.4e9\n",
    "SNR_DB = -10\n",
    "VELOCITY_KMH = 120\n",
    "ADD_NOISE = True\n",
    "ADD_DOPPLER = True\n",
    "\n",
    "# è®­ç»ƒå‚æ•°\n",
    "BATCH_SIZE = 256   # æ›´å¤§ batch æ›´ç¨³å®šï¼ˆä½ æ•°æ®å¾ˆå¤šï¼‰\n",
    "EPOCHS = 200\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "N_SPLITS = 5\n",
    "PATIENCE = 8\n",
    "MIN_DELTA = 0.1   # early stopping çš„æœ€å°è¿›æ­¥ï¼ˆ%ï¼‰\n",
    "\n",
    "SAVE_ROOT = \"./training_results\"\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "# ---------- å¯é€‰ï¼šAWGN / Dopplerï¼ˆä¿ç•™ï¼‰ ----------\n",
    "def compute_doppler_shift(v_kmh, fc_hz):\n",
    "    if not v_kmh:\n",
    "        return 0.0\n",
    "    c = 3e8\n",
    "    v = v_kmh / 3.6\n",
    "    return fc_hz * v / c\n",
    "\n",
    "def add_complex_awgn(signal, snr_db):\n",
    "    if snr_db is None:\n",
    "        return signal\n",
    "    power = np.mean(np.abs(signal)**2)\n",
    "    noise_power = power / (10**(snr_db/10))\n",
    "    noise_std = np.sqrt(noise_power/2)\n",
    "    noise = noise_std * (np.random.randn(*signal.shape) + 1j*np.random.randn(*signal.shape))\n",
    "    return signal + noise\n",
    "\n",
    "def apply_doppler_shift(signal, fd_hz, fs_hz):\n",
    "    if fd_hz is None or fd_hz == 0:\n",
    "        return signal\n",
    "    t = np.arange(len(signal)) / fs_hz\n",
    "    return signal * np.exp(1j * 2 * np.pi * fd_hz * t)\n",
    "\n",
    "# ---------- æ•°æ®é¢„å¤„ç†ï¼šé’ˆå¯¹ä½ çš„äº¤é”™æ ·æœ¬ï¼Œç”¨ per-sample æ ‡å‡†åŒ–ï¼Œä¸åš FFT ----------\n",
    "def preprocess_for_pointcloud_cnn(data_real_imag, add_noise=False, snr_db=None,\n",
    "                                  add_doppler=False, fd_hz=None, fs_hz=FS):\n",
    "    \"\"\"\n",
    "    è¾“å…¥ data_real_imag: np.array [N, L, 2] (I,Q)\n",
    "    è¾“å‡º torch.tensor [N, L, 2] (float32)ï¼Œæ¯ä¸ªæ ·æœ¬åš zero-mean unit-std æ ‡å‡†åŒ–ï¼ˆæŒ‰æ ·æœ¬ï¼‰\n",
    "    è¯´æ˜ï¼šä¸åš FFTï¼Œå› ä¸ºæ ·æœ¬ä¸æ˜¯è¿ç»­æ—¶é—´åºåˆ—ï¼ˆå·²äº¤é”™ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    data = data_real_imag.astype(np.float32).copy()\n",
    "    N, L, C = data.shape\n",
    "    out = np.empty_like(data, dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        iq = data[i]  # (L,2)\n",
    "        # å¯é€‰ï¼šå…ˆæŠŠ (I,Q) ç»„åˆæˆ complex åš channel çº§å¢å¼º (ä¿ç•™)\n",
    "        if add_noise:\n",
    "            # ä»¥ complex å½¢å¼æ·»åŠ å™ªå£°\n",
    "            sigc = iq[...,0] + 1j * iq[...,1]\n",
    "            sigc = add_complex_awgn(sigc, snr_db)\n",
    "            if add_doppler:\n",
    "                sigc = apply_doppler_shift(sigc, fd_hz, fs_hz)\n",
    "            iq = np.stack([np.real(sigc), np.imag(sigc)], axis=-1).astype(np.float32)\n",
    "\n",
    "        # per-sample æ ‡å‡†åŒ–ï¼ˆå¯¹ä¸¤ä¸ªé€šé“ä¸€èµ·ï¼‰\n",
    "        mu = iq.mean(axis=(0,))    # shape (2,)\n",
    "        sigma = iq.std(axis=(0,))  # shape (2,)\n",
    "        # é˜²æ­¢é™¤é›¶\n",
    "        sigma[sigma < 1e-8] = 1.0\n",
    "        iq_norm = (iq - mu) / sigma\n",
    "        out[i] = iq_norm\n",
    "\n",
    "    return torch.tensor(out, dtype=torch.float32)\n",
    "\n",
    "# ---------- æ–°çš„ RF1DCNN ----------\n",
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.downsample = None\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RF1DCNN(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = ResidualBlock1D(2, 32, kernel_size=7)\n",
    "        self.pool1 = nn.MaxPool1d(2)  # 240 â†’ 120\n",
    "\n",
    "        self.layer2 = ResidualBlock1D(32, 64, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool1d(2)  # 120 â†’ 60\n",
    "\n",
    "        self.layer3 = ResidualBlock1D(64, 128, kernel_size=5)\n",
    "        self.pool3 = nn.MaxPool1d(2)  # 60 â†’ 30\n",
    "\n",
    "        self.layer4 = ResidualBlock1D(128, 256, kernel_size=3)\n",
    "        self.pool4 = nn.MaxPool1d(2)  # 30 â†’ 15\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 15, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 240, 2)\n",
    "        x = x.permute(0, 2, 1)  # â†’ (B, 2, 240)\n",
    "\n",
    "        x = self.layer1(x); x = self.pool1(x)\n",
    "        x = self.layer2(x); x = self.pool2(x)\n",
    "        x = self.layer3(x); x = self.pool3(x)\n",
    "        x = self.layer4(x); x = self.pool4(x)\n",
    "\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ---------- è¾…åŠ©ç»˜å›¾/è¯„ä¼°å‡½æ•° ----------\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            _, p = torch.max(out, 1)\n",
    "            correct += (p == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "            all_preds.extend(p.cpu().numpy())\n",
    "    acc = 100.0 * correct / total if total>0 else 0.0\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "    return acc, cm\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, fold, save_folder, dataset_type='Test'):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{dataset_type} Confusion Matrix Fold{fold}')\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.savefig(os.path.join(save_folder, f'{dataset_type.lower()}_cm_fold{fold}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_curves(train_losses, val_losses, train_acc, val_acc, fold, save_folder):\n",
    "    plt.figure(); plt.plot(train_losses, label='Train Loss'); plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title(f'Fold {fold} Loss'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_folder, f'loss_fold{fold}.png')); plt.close()\n",
    "    plt.figure(); plt.plot(train_acc, label='Train Acc'); plt.plot(val_acc, label='Val Acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)'); plt.title(f'Fold {fold} Accuracy'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_folder, f'acc_fold{fold}.png')); plt.close()\n",
    "\n",
    "# ---------- K-Fold è®­ç»ƒä¸»å‡½æ•°ï¼ˆä½¿ç”¨ IQPointCloudCNNï¼‰ ----------\n",
    "def train_kfold_pointcloud(X_train, y_train, X_test, y_test, num_classes, device=DEVICE,\n",
    "                           batch_size=BATCH_SIZE, epochs=EPOCHS, lr=LR, weight_decay=WEIGHT_DECAY,\n",
    "                           n_splits=N_SPLITS, patience=PATIENCE, min_delta=MIN_DELTA,\n",
    "                           script_name=\"wisig_cross\"):\n",
    "    fd = int(compute_doppler_shift(VELOCITY_KMH, FC))\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    save_dir = f\"{timestamp}_{script_name}_SNR{SNR_DB}dB_fd{fd}_classes_{num_classes}_ResNet\"\n",
    "    save_folder = os.path.join(SAVE_ROOT, save_dir)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    results_file = os.path.join(save_folder, \"results.txt\")\n",
    "\n",
    "    # ä¿å­˜å‚æ•°\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(\"=== Experiment Parameters ===\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"SNR_dB: {SNR_DB}, ADD_NOISE: {ADD_NOISE}, ADD_DOPPLER: {ADD_DOPPLER}\\n\")\n",
    "        f.write(f\"FS: {FS}, FC: {FC}, Velocity_kmh: {VELOCITY_KMH}\\n\")\n",
    "        f.write(f\"Batch: {batch_size}, Epochs: {epochs}, LR: {lr}, WD: {weight_decay}\\n\")\n",
    "        f.write(f\"Num classes: {num_classes}, K-Fold: {n_splits}, Patience: {patience}, MinDelta: {min_delta}\\n\")\n",
    "        f.write(\"============================\\n\\n\")\n",
    "\n",
    "    # test loader\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # full dataset for K-Fold\n",
    "    full_dataset = TensorDataset(X_train, y_train)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    indices = np.arange(len(full_dataset))\n",
    "\n",
    "    val_scores, test_scores = [], []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{n_splits} ===\")\n",
    "        tr_sub = Subset(full_dataset, tr_idx)\n",
    "        va_sub = Subset(full_dataset, va_idx)\n",
    "        tr_loader = DataLoader(tr_sub, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        va_loader = DataLoader(va_sub, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        model = RF1DCNN(num_classes=num_classes, dropout=0.3).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        best_val = 0.0\n",
    "        best_wts = None\n",
    "        patience_cnt = 0\n",
    "\n",
    "        train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "        avg_grad_list = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss, correct, total = 0.0, 0, 0\n",
    "            total_grad, cnt_grad = 0.0, 0\n",
    "            for xb, yb in tr_loader:\n",
    "                xb = xb.to(device); yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                # grad norm\n",
    "                grad_norms = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]\n",
    "                if grad_norms:\n",
    "                    total_grad += np.mean(grad_norms); cnt_grad += 1\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                _, p = torch.max(out, 1)\n",
    "                correct += (p == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "            avg_grad = total_grad / max(cnt_grad, 1)\n",
    "            avg_grad_list.append(avg_grad)\n",
    "\n",
    "            train_loss = running_loss / max(1, len(tr_loader))\n",
    "            train_acc = 100.0 * correct / max(1, total)\n",
    "            train_losses.append(train_loss); train_accs.append(train_acc)\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            vloss, vcorrect, vtotal = 0.0, 0, 0\n",
    "            all_labels, all_preds = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in va_loader:\n",
    "                    xb = xb.to(device); yb = yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    loss = criterion(out, yb)\n",
    "                    vloss += loss.item()\n",
    "                    _, p = torch.max(out, 1)\n",
    "                    vcorrect += (p == yb).sum().item()\n",
    "                    vtotal += yb.size(0)\n",
    "                    all_labels.extend(yb.cpu().numpy()); all_preds.extend(p.cpu().numpy())\n",
    "            val_loss = vloss / max(1, len(va_loader))\n",
    "            val_acc = 100.0 * vcorrect / max(1, vtotal)\n",
    "            val_losses.append(val_loss); val_accs.append(val_acc)\n",
    "            val_cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "            np.save(os.path.join(save_folder, f'val_cm_fold{fold+1}.npy'), val_cm)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | TrainAcc={train_acc:.2f}% | ValAcc={val_acc:.2f}% | \"\n",
    "                  f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | AvgGrad={avg_grad:.4f}\")\n",
    "            with open(results_file, 'a') as f:\n",
    "                f.write(f\"Fold{fold+1} Epoch{epoch+1} | TrainAcc={train_acc:.2f}% | ValAcc={val_acc:.2f}% | \"\n",
    "                        f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | AvgGrad={avg_grad:.4f}\\n\")\n",
    "\n",
    "            # Early stopping on validation accuracy with min_delta (percentage points)\n",
    "            if val_acc > best_val + min_delta:\n",
    "                best_val = val_acc\n",
    "                best_wts = model.state_dict()\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "                if patience_cnt >= patience:\n",
    "                    print(\"Early stopping.\")\n",
    "                    break\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # restore best\n",
    "        if best_wts is not None:\n",
    "            model.load_state_dict(best_wts)\n",
    "\n",
    "        # ä¿å­˜ train/val æ··æ·†çŸ©é˜µ\n",
    "        train_acc, train_cm = evaluate_model(model, tr_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'train_cm_fold{fold+1}.npy'), train_cm)\n",
    "        plot_confusion_matrix(train_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Train')\n",
    "\n",
    "        val_acc, val_cm = evaluate_model(model, va_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'val_cm_fold{fold+1}.npy'), val_cm)\n",
    "        plot_confusion_matrix(val_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Val')\n",
    "\n",
    "        # test eval\n",
    "        test_acc, test_cm = evaluate_model(model, test_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'test_cm_fold{fold+1}.npy'), test_cm)\n",
    "        plot_confusion_matrix(test_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Test')\n",
    "        print(f\"Fold {fold+1} Test Accuracy: {test_acc:.2f}%\")\n",
    "        with open(results_file, 'a') as f:\n",
    "            f.write(f\"Fold{fold+1} TestAcc={test_acc:.2f}%\\n\")\n",
    "\n",
    "        # ä¿å­˜æ›²çº¿ & æ¨¡å‹\n",
    "        plot_curves(train_losses, val_losses, train_accs, val_accs, fold+1, save_folder)\n",
    "        torch.save(model.state_dict(), os.path.join(save_folder, f'model_fold{fold+1}.pth'))\n",
    "\n",
    "        val_scores.append(val_acc)\n",
    "        test_scores.append(test_acc)\n",
    "\n",
    "    # summary\n",
    "    print(\"\\n=== Overall Summary ===\")\n",
    "    print(f\"Val Acc: {np.mean(val_scores):.2f} Â± {np.std(val_scores):.2f}\")\n",
    "    print(f\"Test Acc: {np.mean(test_scores):.2f} Â± {np.std(test_scores):.2f}\")\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(f\"\\n=== Overall Summary ===\\nVal Acc: {np.mean(val_scores):.2f} Â± {np.std(val_scores):.2f}\\nTest Acc: {np.mean(test_scores):.2f} Â± {np.std(test_scores):.2f}\\n\")\n",
    "\n",
    "    print(f\"\\nAll results saved in {save_folder}\")\n",
    "    return save_folder\n",
    "\n",
    "# ---------- è¿è¡Œå‰çš„å‡†å¤‡ï¼ˆé¢„å¤„ç†å¹¶è½¬æ¢ä¸ºå¼ é‡ï¼‰ ----------\n",
    "# X_train, X_test ç›®å‰ shape: [N, 240, 2] (numpy)\n",
    "# æˆ‘ä»¬å¯¹æ¯ä¸ªæ ·æœ¬åš per-sample æ ‡å‡†åŒ–ï¼ˆzero-mean unit-stdï¼‰\n",
    "print(\"Preprocessing (per-sample normalization)...\")\n",
    "# ---------- è½¬ä¸º torch.tensor å¹¶åš per-sample æ ‡å‡†åŒ– ----------\n",
    "\n",
    "X_train_torch = preprocess_for_pointcloud_cnn(X_train, add_noise=False)\n",
    "X_test_torch  = preprocess_for_pointcloud_cnn(X_test, add_noise=False)\n",
    "\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_torch  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "num_classes = len(torch.unique(y_train_torch))\n",
    "print(\"Prepared tensors:\", X_train_torch.shape, X_test_torch.shape, \"num_classes=\", num_classes)\n",
    "\n",
    "# ---------- K-Fold è®­ç»ƒ ----------\n",
    "# train_kfold_pointcloud å†…éƒ¨ä¼šæŠŠè®­ç»ƒé›†å†åˆ’åˆ†ä¸ºè®­ç»ƒ+éªŒè¯é›†\n",
    "# save_folder = train_kfold_pointcloud(\n",
    "#    X_train_torch, \n",
    "#    y_train_torch,\n",
    "#    X_test_torch, \n",
    "#    y_test_torch,\n",
    "#    num_classes=num_classes\n",
    "# )\n",
    "\n",
    "#print(\"Done. Results in:\", save_folder)\n",
    "\n",
    "##########################################################\n",
    "#=== å®šä¹‰ä¸€ä¸ªç»Ÿä¸€å…¥å£å‡½æ•°ï¼Œç”¨äºè·‘ä¸€æ¬¡ SNR å®éªŒ ===\n",
    "##########################################################\n",
    "def run_experiment_with_snr(snr_db):\n",
    "    global SNR_DB, ADD_NOISE\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸš€ Running experiment with SNR = {snr_db} dB\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # è®¾ç½®å½“å‰å®éªŒ SNR\n",
    "    SNR_DB = snr_db\n",
    "    ADD_NOISE = True   # æ‰“å¼€ AWGN\n",
    "    \n",
    "    # --- é‡æ–°é¢„å¤„ç†æ•°æ®ï¼ˆä¼šè°ƒç”¨ add_noise=Trueï¼‰ ---\n",
    "    X_train_torch_snr = preprocess_for_pointcloud_cnn(\n",
    "        X_train, add_noise=True, add_doppler = ADD_DOPPLER, snr_db=snr_db\n",
    "    )\n",
    "    X_test_torch_snr = preprocess_for_pointcloud_cnn(\n",
    "        X_test, add_noise=True, add_doppler = ADD_DOPPLER, snr_db=snr_db\n",
    "    )\n",
    "\n",
    "    y_train_torch_snr = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test_torch_snr  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "    num_classes = len(torch.unique(y_train_torch_snr))\n",
    "\n",
    "    # --- è°ƒç”¨ä½ çš„ K-Fold è®­ç»ƒ ---\n",
    "    save_folder = train_kfold_pointcloud(\n",
    "        X_train_torch_snr,\n",
    "        y_train_torch_snr,\n",
    "        X_test_torch_snr,\n",
    "        y_test_torch_snr,\n",
    "        num_classes=num_classes,\n",
    "        script_name=f\"SNR_{snr_db}dB\"\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸ‰ Finished SNR={snr_db} dB, results saved in: {save_folder}\")\n",
    "    return save_folder\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# === æ‰¹é‡æ‰§è¡Œ SNR Sweep: 20, 10, 0, -10, ... -40 dB ===\n",
    "###########################################################\n",
    "snr_list = list(range(20, -41, -5))\n",
    "# [20, 15, 10, 5, 0, -5, -10, -15, -20, -25, -30, -35, -40]\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for snr in snr_list:\n",
    "    folder = run_experiment_with_snr(snr)\n",
    "    all_results[snr] = folder\n",
    "\n",
    "print(\"\\n\\n================ FINAL SUMMARY ================\")\n",
    "for snr, folder in all_results.items():\n",
    "    print(f\"SNR {snr:>3} dB â†’ results in: {folder}\")\n",
    "print(\"==============================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee287311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®é›†å‘å°„æœºæ•°é‡ï¼š 6 å…·ä½“ä¸ºï¼š ['14-10', '14-7', '20-15', '20-19', '6-15', '8-20']\n",
      "æ•°æ®é›†æ¥æ”¶æœºæ•°é‡ï¼š 12 å…·ä½“ä¸ºï¼š ['1-1', '1-19', '14-7', '18-2', '19-2', '2-1', '2-19', '20-1', '3-19', '7-14', '7-7', '8-8']\n",
      "æ•°æ®é›†é‡‡é›†å¤©æ•°ï¼š 4 å…·ä½“ä¸ºï¼š ['2021_03_01', '2021_03_08', '2021_03_15', '2021_03_23']\n",
      "6 12\n",
      "X_train shape: (76800, 240, 2)\n",
      "y_train shape: (76800,)\n",
      "X_test  shape: (76800, 240, 2)\n",
      "y_test  shape: (76800,)\n",
      "Preprocessing (per-sample normalization)...\n",
      "Prepared tensors: torch.Size([76800, 240, 2]) torch.Size([76800, 240, 2]) num_classes= 6\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running experiment with SNR = 20 dB\n",
      "============================================================\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 1/200 | TrainAcc=97.12% | ValAcc=99.80% | TrainLoss=0.0825 | ValLoss=0.0061 | AvgGrad=0.0734\n",
      "Epoch 2/200 | TrainAcc=99.86% | ValAcc=99.84% | TrainLoss=0.0042 | ValLoss=0.0044 | AvgGrad=0.0174\n",
      "Epoch 3/200 | TrainAcc=99.89% | ValAcc=98.22% | TrainLoss=0.0037 | ValLoss=0.0605 | AvgGrad=0.0159\n",
      "Epoch 4/200 | TrainAcc=99.81% | ValAcc=99.92% | TrainLoss=0.0054 | ValLoss=0.0031 | AvgGrad=0.0179\n",
      "Epoch 5/200 | TrainAcc=99.96% | ValAcc=99.82% | TrainLoss=0.0011 | ValLoss=0.0066 | AvgGrad=0.0057\n",
      "Epoch 6/200 | TrainAcc=99.94% | ValAcc=99.85% | TrainLoss=0.0020 | ValLoss=0.0058 | AvgGrad=0.0083\n",
      "Epoch 7/200 | TrainAcc=99.86% | ValAcc=99.89% | TrainLoss=0.0044 | ValLoss=0.0035 | AvgGrad=0.0146\n",
      "Epoch 8/200 | TrainAcc=99.97% | ValAcc=99.96% | TrainLoss=0.0011 | ValLoss=0.0020 | AvgGrad=0.0047\n",
      "Epoch 9/200 | TrainAcc=99.95% | ValAcc=99.97% | TrainLoss=0.0013 | ValLoss=0.0017 | AvgGrad=0.0065\n",
      "Epoch 10/200 | TrainAcc=99.96% | ValAcc=99.94% | TrainLoss=0.0013 | ValLoss=0.0026 | AvgGrad=0.0061\n",
      "Epoch 11/200 | TrainAcc=99.98% | ValAcc=99.95% | TrainLoss=0.0012 | ValLoss=0.0031 | AvgGrad=0.0044\n",
      "Epoch 12/200 | TrainAcc=100.00% | ValAcc=99.95% | TrainLoss=0.0001 | ValLoss=0.0022 | AvgGrad=0.0008\n",
      "Early stopping.\n",
      "Fold 1 Test Accuracy: 96.96%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 1/200 | TrainAcc=96.43% | ValAcc=99.84% | TrainLoss=0.1178 | ValLoss=0.0054 | AvgGrad=0.0819\n",
      "Epoch 2/200 | TrainAcc=99.85% | ValAcc=99.85% | TrainLoss=0.0050 | ValLoss=0.0046 | AvgGrad=0.0193\n",
      "Epoch 3/200 | TrainAcc=99.94% | ValAcc=99.95% | TrainLoss=0.0017 | ValLoss=0.0015 | AvgGrad=0.0086\n",
      "Epoch 4/200 | TrainAcc=99.89% | ValAcc=99.69% | TrainLoss=0.0032 | ValLoss=0.0081 | AvgGrad=0.0143\n",
      "Epoch 5/200 | TrainAcc=99.94% | ValAcc=99.96% | TrainLoss=0.0019 | ValLoss=0.0011 | AvgGrad=0.0082\n",
      "Epoch 6/200 | TrainAcc=99.97% | ValAcc=99.65% | TrainLoss=0.0011 | ValLoss=0.0116 | AvgGrad=0.0061\n",
      "Epoch 7/200 | TrainAcc=99.85% | ValAcc=99.95% | TrainLoss=0.0042 | ValLoss=0.0017 | AvgGrad=0.0159\n",
      "Epoch 8/200 | TrainAcc=99.96% | ValAcc=99.95% | TrainLoss=0.0010 | ValLoss=0.0023 | AvgGrad=0.0048\n",
      "Epoch 9/200 | TrainAcc=99.96% | ValAcc=99.94% | TrainLoss=0.0014 | ValLoss=0.0015 | AvgGrad=0.0057\n",
      "Epoch 10/200 | TrainAcc=99.93% | ValAcc=99.93% | TrainLoss=0.0023 | ValLoss=0.0026 | AvgGrad=0.0084\n",
      "Epoch 11/200 | TrainAcc=99.98% | ValAcc=99.97% | TrainLoss=0.0006 | ValLoss=0.0008 | AvgGrad=0.0032\n",
      "Early stopping.\n",
      "Fold 2 Test Accuracy: 96.67%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 1/200 | TrainAcc=96.75% | ValAcc=99.75% | TrainLoss=0.0994 | ValLoss=0.0083 | AvgGrad=0.0767\n",
      "Epoch 2/200 | TrainAcc=99.83% | ValAcc=99.89% | TrainLoss=0.0051 | ValLoss=0.0038 | AvgGrad=0.0204\n",
      "Epoch 3/200 | TrainAcc=99.91% | ValAcc=99.86% | TrainLoss=0.0030 | ValLoss=0.0031 | AvgGrad=0.0134\n",
      "Epoch 4/200 | TrainAcc=99.92% | ValAcc=99.62% | TrainLoss=0.0026 | ValLoss=0.0110 | AvgGrad=0.0113\n",
      "Epoch 5/200 | TrainAcc=99.91% | ValAcc=99.82% | TrainLoss=0.0032 | ValLoss=0.0067 | AvgGrad=0.0121\n",
      "Epoch 6/200 | TrainAcc=99.94% | ValAcc=99.96% | TrainLoss=0.0020 | ValLoss=0.0018 | AvgGrad=0.0084\n",
      "Epoch 7/200 | TrainAcc=99.96% | ValAcc=99.93% | TrainLoss=0.0014 | ValLoss=0.0019 | AvgGrad=0.0063\n",
      "Epoch 8/200 | TrainAcc=99.93% | ValAcc=99.95% | TrainLoss=0.0020 | ValLoss=0.0015 | AvgGrad=0.0083\n",
      "Epoch 9/200 | TrainAcc=99.91% | ValAcc=99.91% | TrainLoss=0.0027 | ValLoss=0.0036 | AvgGrad=0.0103\n",
      "Epoch 10/200 | TrainAcc=99.95% | ValAcc=99.95% | TrainLoss=0.0015 | ValLoss=0.0018 | AvgGrad=0.0064\n",
      "Early stopping.\n",
      "Fold 3 Test Accuracy: 97.79%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 1/200 | TrainAcc=96.30% | ValAcc=99.77% | TrainLoss=0.1378 | ValLoss=0.0070 | AvgGrad=0.0748\n",
      "Epoch 2/200 | TrainAcc=99.82% | ValAcc=99.97% | TrainLoss=0.0059 | ValLoss=0.0026 | AvgGrad=0.0197\n",
      "Epoch 3/200 | TrainAcc=99.93% | ValAcc=99.82% | TrainLoss=0.0026 | ValLoss=0.0058 | AvgGrad=0.0106\n",
      "Epoch 4/200 | TrainAcc=99.88% | ValAcc=99.88% | TrainLoss=0.0033 | ValLoss=0.0037 | AvgGrad=0.0133\n",
      "Epoch 5/200 | TrainAcc=99.96% | ValAcc=99.96% | TrainLoss=0.0011 | ValLoss=0.0014 | AvgGrad=0.0059\n",
      "Epoch 6/200 | TrainAcc=99.82% | ValAcc=99.72% | TrainLoss=0.0052 | ValLoss=0.0095 | AvgGrad=0.0172\n",
      "Epoch 7/200 | TrainAcc=99.96% | ValAcc=99.96% | TrainLoss=0.0012 | ValLoss=0.0016 | AvgGrad=0.0055\n",
      "Epoch 8/200 | TrainAcc=99.99% | ValAcc=99.97% | TrainLoss=0.0003 | ValLoss=0.0009 | AvgGrad=0.0020\n",
      "Epoch 9/200 | TrainAcc=99.93% | ValAcc=99.93% | TrainLoss=0.0024 | ValLoss=0.0028 | AvgGrad=0.0092\n",
      "Epoch 10/200 | TrainAcc=99.86% | ValAcc=99.91% | TrainLoss=0.0038 | ValLoss=0.0043 | AvgGrad=0.0136\n",
      "Early stopping.\n",
      "Fold 4 Test Accuracy: 96.89%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 1/200 | TrainAcc=96.95% | ValAcc=99.69% | TrainLoss=0.1049 | ValLoss=0.0085 | AvgGrad=0.0746\n",
      "Epoch 2/200 | TrainAcc=99.80% | ValAcc=99.92% | TrainLoss=0.0059 | ValLoss=0.0023 | AvgGrad=0.0221\n",
      "Epoch 3/200 | TrainAcc=99.90% | ValAcc=99.71% | TrainLoss=0.0036 | ValLoss=0.0078 | AvgGrad=0.0138\n",
      "Epoch 4/200 | TrainAcc=99.98% | ValAcc=99.98% | TrainLoss=0.0008 | ValLoss=0.0011 | AvgGrad=0.0048\n",
      "Epoch 5/200 | TrainAcc=99.87% | ValAcc=99.77% | TrainLoss=0.0038 | ValLoss=0.0074 | AvgGrad=0.0156\n",
      "Epoch 6/200 | TrainAcc=99.92% | ValAcc=99.95% | TrainLoss=0.0027 | ValLoss=0.0019 | AvgGrad=0.0105\n",
      "Epoch 7/200 | TrainAcc=99.95% | ValAcc=98.87% | TrainLoss=0.0020 | ValLoss=0.0372 | AvgGrad=0.0076\n",
      "Epoch 8/200 | TrainAcc=99.80% | ValAcc=99.93% | TrainLoss=0.0076 | ValLoss=0.0028 | AvgGrad=0.0183\n",
      "Epoch 9/200 | TrainAcc=99.94% | ValAcc=99.85% | TrainLoss=0.0019 | ValLoss=0.0045 | AvgGrad=0.0063\n",
      "Epoch 10/200 | TrainAcc=99.96% | ValAcc=99.97% | TrainLoss=0.0017 | ValLoss=0.0011 | AvgGrad=0.0051\n",
      "Early stopping.\n",
      "Fold 5 Test Accuracy: 97.86%\n",
      "\n",
      "=== Overall Summary ===\n",
      "Val Acc: 99.95 Â± 0.02\n",
      "Test Acc: 97.23 Â± 0.49\n",
      "\n",
      "All results saved in ./training_results\\2025-11-25_18-05-13_wisig_cross_SNR20dB_fd266_classes_6_ResNet\n",
      "ğŸ‰ Finished SNR=20 dB, results saved in: ./training_results\\2025-11-25_18-05-13_wisig_cross_SNR20dB_fd266_classes_6_ResNet\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running experiment with SNR = 15 dB\n",
      "============================================================\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 1/200 | TrainAcc=96.48% | ValAcc=99.71% | TrainLoss=0.1264 | ValLoss=0.0090 | AvgGrad=0.0710\n",
      "Epoch 2/200 | TrainAcc=99.84% | ValAcc=99.71% | TrainLoss=0.0053 | ValLoss=0.0092 | AvgGrad=0.0186\n",
      "Epoch 3/200 | TrainAcc=99.78% | ValAcc=99.80% | TrainLoss=0.0069 | ValLoss=0.0071 | AvgGrad=0.0207\n",
      "Epoch 4/200 | TrainAcc=99.91% | ValAcc=99.92% | TrainLoss=0.0028 | ValLoss=0.0031 | AvgGrad=0.0103\n",
      "Epoch 5/200 | TrainAcc=100.00% | ValAcc=99.94% | TrainLoss=0.0004 | ValLoss=0.0020 | AvgGrad=0.0023\n",
      "Epoch 6/200 | TrainAcc=99.97% | ValAcc=99.90% | TrainLoss=0.0012 | ValLoss=0.0030 | AvgGrad=0.0052\n",
      "Epoch 7/200 | TrainAcc=99.76% | ValAcc=99.92% | TrainLoss=0.0091 | ValLoss=0.0031 | AvgGrad=0.0185\n",
      "Epoch 8/200 | TrainAcc=99.93% | ValAcc=99.93% | TrainLoss=0.0020 | ValLoss=0.0025 | AvgGrad=0.0076\n",
      "Epoch 9/200 | TrainAcc=99.95% | ValAcc=99.50% | TrainLoss=0.0017 | ValLoss=0.0172 | AvgGrad=0.0068\n",
      "Epoch 10/200 | TrainAcc=99.98% | ValAcc=99.96% | TrainLoss=0.0009 | ValLoss=0.0021 | AvgGrad=0.0037\n",
      "Epoch 11/200 | TrainAcc=100.00% | ValAcc=99.96% | TrainLoss=0.0001 | ValLoss=0.0021 | AvgGrad=0.0006\n",
      "Epoch 12/200 | TrainAcc=100.00% | ValAcc=99.96% | TrainLoss=0.0000 | ValLoss=0.0024 | AvgGrad=0.0003\n",
      "Early stopping.\n",
      "Fold 1 Test Accuracy: 97.13%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 1/200 | TrainAcc=97.07% | ValAcc=99.68% | TrainLoss=0.0879 | ValLoss=0.0088 | AvgGrad=0.0767\n",
      "Epoch 2/200 | TrainAcc=99.87% | ValAcc=99.80% | TrainLoss=0.0043 | ValLoss=0.0080 | AvgGrad=0.0177\n",
      "Epoch 3/200 | TrainAcc=99.90% | ValAcc=99.95% | TrainLoss=0.0034 | ValLoss=0.0018 | AvgGrad=0.0150\n",
      "Epoch 4/200 | TrainAcc=99.85% | ValAcc=99.88% | TrainLoss=0.0040 | ValLoss=0.0040 | AvgGrad=0.0183\n",
      "Epoch 5/200 | TrainAcc=99.94% | ValAcc=99.93% | TrainLoss=0.0020 | ValLoss=0.0033 | AvgGrad=0.0095\n",
      "Epoch 6/200 | TrainAcc=99.92% | ValAcc=99.93% | TrainLoss=0.0022 | ValLoss=0.0013 | AvgGrad=0.0103\n",
      "Epoch 7/200 | TrainAcc=99.90% | ValAcc=99.90% | TrainLoss=0.0034 | ValLoss=0.0032 | AvgGrad=0.0142\n",
      "Epoch 8/200 | TrainAcc=99.89% | ValAcc=99.96% | TrainLoss=0.0042 | ValLoss=0.0017 | AvgGrad=0.0125\n",
      "Epoch 9/200 | TrainAcc=99.93% | ValAcc=99.93% | TrainLoss=0.0024 | ValLoss=0.0023 | AvgGrad=0.0082\n",
      "Epoch 10/200 | TrainAcc=99.92% | ValAcc=99.64% | TrainLoss=0.0032 | ValLoss=0.0196 | AvgGrad=0.0099\n",
      "Epoch 11/200 | TrainAcc=99.97% | ValAcc=99.95% | TrainLoss=0.0010 | ValLoss=0.0014 | AvgGrad=0.0042\n",
      "Early stopping.\n",
      "Fold 2 Test Accuracy: 97.49%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 1/200 | TrainAcc=96.87% | ValAcc=99.81% | TrainLoss=0.1011 | ValLoss=0.0060 | AvgGrad=0.0774\n",
      "Epoch 2/200 | TrainAcc=99.88% | ValAcc=99.83% | TrainLoss=0.0042 | ValLoss=0.0046 | AvgGrad=0.0160\n",
      "Epoch 3/200 | TrainAcc=99.84% | ValAcc=99.88% | TrainLoss=0.0043 | ValLoss=0.0036 | AvgGrad=0.0173\n",
      "Epoch 4/200 | TrainAcc=99.94% | ValAcc=99.82% | TrainLoss=0.0018 | ValLoss=0.0046 | AvgGrad=0.0089\n",
      "Epoch 5/200 | TrainAcc=99.73% | ValAcc=99.84% | TrainLoss=0.0080 | ValLoss=0.0033 | AvgGrad=0.0209\n",
      "Epoch 6/200 | TrainAcc=99.94% | ValAcc=99.91% | TrainLoss=0.0016 | ValLoss=0.0028 | AvgGrad=0.0069\n",
      "Epoch 7/200 | TrainAcc=99.94% | ValAcc=99.96% | TrainLoss=0.0016 | ValLoss=0.0015 | AvgGrad=0.0075\n",
      "Epoch 8/200 | TrainAcc=99.96% | ValAcc=99.96% | TrainLoss=0.0010 | ValLoss=0.0014 | AvgGrad=0.0057\n",
      "Epoch 9/200 | TrainAcc=99.97% | ValAcc=99.88% | TrainLoss=0.0008 | ValLoss=0.0035 | AvgGrad=0.0037\n",
      "Epoch 10/200 | TrainAcc=99.82% | ValAcc=99.86% | TrainLoss=0.0062 | ValLoss=0.0053 | AvgGrad=0.0161\n",
      "Epoch 11/200 | TrainAcc=99.99% | ValAcc=99.95% | TrainLoss=0.0003 | ValLoss=0.0024 | AvgGrad=0.0018\n",
      "Epoch 12/200 | TrainAcc=100.00% | ValAcc=99.95% | TrainLoss=0.0002 | ValLoss=0.0021 | AvgGrad=0.0012\n",
      "Epoch 13/200 | TrainAcc=100.00% | ValAcc=99.95% | TrainLoss=0.0000 | ValLoss=0.0020 | AvgGrad=0.0002\n",
      "Epoch 14/200 | TrainAcc=100.00% | ValAcc=99.95% | TrainLoss=0.0001 | ValLoss=0.0022 | AvgGrad=0.0006\n",
      "Epoch 15/200 | TrainAcc=100.00% | ValAcc=99.96% | TrainLoss=0.0000 | ValLoss=0.0023 | AvgGrad=0.0002\n",
      "Early stopping.\n",
      "Fold 3 Test Accuracy: 96.77%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 1/200 | TrainAcc=96.41% | ValAcc=99.75% | TrainLoss=0.1329 | ValLoss=0.0068 | AvgGrad=0.0785\n",
      "Epoch 2/200 | TrainAcc=99.85% | ValAcc=99.77% | TrainLoss=0.0052 | ValLoss=0.0056 | AvgGrad=0.0201\n",
      "Epoch 3/200 | TrainAcc=99.77% | ValAcc=99.82% | TrainLoss=0.0077 | ValLoss=0.0050 | AvgGrad=0.0237\n",
      "Epoch 4/200 | TrainAcc=99.91% | ValAcc=99.84% | TrainLoss=0.0026 | ValLoss=0.0053 | AvgGrad=0.0103\n",
      "Epoch 5/200 | TrainAcc=99.94% | ValAcc=99.90% | TrainLoss=0.0016 | ValLoss=0.0018 | AvgGrad=0.0082\n",
      "Epoch 6/200 | TrainAcc=99.95% | ValAcc=99.77% | TrainLoss=0.0016 | ValLoss=0.0070 | AvgGrad=0.0070\n",
      "Epoch 7/200 | TrainAcc=99.93% | ValAcc=99.86% | TrainLoss=0.0022 | ValLoss=0.0048 | AvgGrad=0.0091\n",
      "Epoch 8/200 | TrainAcc=99.85% | ValAcc=99.95% | TrainLoss=0.0048 | ValLoss=0.0023 | AvgGrad=0.0162\n",
      "Epoch 9/200 | TrainAcc=99.96% | ValAcc=99.92% | TrainLoss=0.0011 | ValLoss=0.0022 | AvgGrad=0.0055\n",
      "Epoch 10/200 | TrainAcc=99.92% | ValAcc=99.79% | TrainLoss=0.0026 | ValLoss=0.0090 | AvgGrad=0.0093\n",
      "Epoch 11/200 | TrainAcc=99.97% | ValAcc=99.97% | TrainLoss=0.0009 | ValLoss=0.0012 | AvgGrad=0.0037\n",
      "Epoch 12/200 | TrainAcc=100.00% | ValAcc=99.96% | TrainLoss=0.0001 | ValLoss=0.0015 | AvgGrad=0.0007\n",
      "Epoch 13/200 | TrainAcc=99.99% | ValAcc=99.97% | TrainLoss=0.0002 | ValLoss=0.0007 | AvgGrad=0.0011\n",
      "Early stopping.\n",
      "Fold 4 Test Accuracy: 96.47%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 1/200 | TrainAcc=95.94% | ValAcc=99.66% | TrainLoss=0.1402 | ValLoss=0.0095 | AvgGrad=0.0837\n",
      "Epoch 2/200 | TrainAcc=99.75% | ValAcc=99.77% | TrainLoss=0.0080 | ValLoss=0.0063 | AvgGrad=0.0248\n",
      "Epoch 3/200 | TrainAcc=99.91% | ValAcc=99.86% | TrainLoss=0.0026 | ValLoss=0.0048 | AvgGrad=0.0120\n",
      "Epoch 4/200 | TrainAcc=99.91% | ValAcc=99.86% | TrainLoss=0.0029 | ValLoss=0.0042 | AvgGrad=0.0120\n",
      "Epoch 5/200 | TrainAcc=99.92% | ValAcc=99.86% | TrainLoss=0.0025 | ValLoss=0.0051 | AvgGrad=0.0109\n",
      "Epoch 6/200 | TrainAcc=99.90% | ValAcc=99.89% | TrainLoss=0.0029 | ValLoss=0.0030 | AvgGrad=0.0134\n",
      "Epoch 7/200 | TrainAcc=99.94% | ValAcc=99.85% | TrainLoss=0.0022 | ValLoss=0.0047 | AvgGrad=0.0083\n",
      "Epoch 8/200 | TrainAcc=99.91% | ValAcc=99.76% | TrainLoss=0.0029 | ValLoss=0.0076 | AvgGrad=0.0118\n",
      "Epoch 9/200 | TrainAcc=99.95% | ValAcc=99.80% | TrainLoss=0.0020 | ValLoss=0.0072 | AvgGrad=0.0079\n",
      "Epoch 10/200 | TrainAcc=99.91% | ValAcc=99.90% | TrainLoss=0.0026 | ValLoss=0.0026 | AvgGrad=0.0091\n",
      "Epoch 11/200 | TrainAcc=99.99% | ValAcc=99.92% | TrainLoss=0.0004 | ValLoss=0.0029 | AvgGrad=0.0022\n",
      "Epoch 12/200 | TrainAcc=99.99% | ValAcc=99.95% | TrainLoss=0.0002 | ValLoss=0.0013 | AvgGrad=0.0012\n",
      "Epoch 13/200 | TrainAcc=100.00% | ValAcc=99.97% | TrainLoss=0.0000 | ValLoss=0.0012 | AvgGrad=0.0003\n",
      "Epoch 14/200 | TrainAcc=100.00% | ValAcc=99.97% | TrainLoss=0.0000 | ValLoss=0.0010 | AvgGrad=0.0003\n",
      "Early stopping.\n",
      "Fold 5 Test Accuracy: 96.79%\n",
      "\n",
      "=== Overall Summary ===\n",
      "Val Acc: 99.96 Â± 0.01\n",
      "Test Acc: 96.93 Â± 0.35\n",
      "\n",
      "All results saved in ./training_results\\2025-11-25_18-17-07_wisig_cross_SNR15dB_fd266_classes_6_ResNet\n",
      "ğŸ‰ Finished SNR=15 dB, results saved in: ./training_results\\2025-11-25_18-17-07_wisig_cross_SNR15dB_fd266_classes_6_ResNet\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running experiment with SNR = 10 dB\n",
      "============================================================\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 1/200 | TrainAcc=96.70% | ValAcc=99.52% | TrainLoss=0.1013 | ValLoss=0.0124 | AvgGrad=0.0835\n",
      "Epoch 2/200 | TrainAcc=99.77% | ValAcc=99.69% | TrainLoss=0.0073 | ValLoss=0.0113 | AvgGrad=0.0244\n",
      "Epoch 3/200 | TrainAcc=99.86% | ValAcc=99.66% | TrainLoss=0.0043 | ValLoss=0.0123 | AvgGrad=0.0176\n",
      "Epoch 4/200 | TrainAcc=99.82% | ValAcc=99.87% | TrainLoss=0.0052 | ValLoss=0.0048 | AvgGrad=0.0197\n",
      "Epoch 5/200 | TrainAcc=99.84% | ValAcc=99.78% | TrainLoss=0.0047 | ValLoss=0.0070 | AvgGrad=0.0174\n",
      "Epoch 6/200 | TrainAcc=99.87% | ValAcc=99.84% | TrainLoss=0.0038 | ValLoss=0.0048 | AvgGrad=0.0149\n",
      "Epoch 7/200 | TrainAcc=99.84% | ValAcc=99.80% | TrainLoss=0.0058 | ValLoss=0.0071 | AvgGrad=0.0182\n",
      "Epoch 8/200 | TrainAcc=99.93% | ValAcc=99.84% | TrainLoss=0.0029 | ValLoss=0.0051 | AvgGrad=0.0098\n",
      "Epoch 9/200 | TrainAcc=99.90% | ValAcc=99.91% | TrainLoss=0.0035 | ValLoss=0.0034 | AvgGrad=0.0123\n",
      "Epoch 10/200 | TrainAcc=99.86% | ValAcc=99.87% | TrainLoss=0.0045 | ValLoss=0.0042 | AvgGrad=0.0138\n",
      "Epoch 11/200 | TrainAcc=99.98% | ValAcc=99.92% | TrainLoss=0.0007 | ValLoss=0.0025 | AvgGrad=0.0031\n",
      "Epoch 12/200 | TrainAcc=100.00% | ValAcc=99.94% | TrainLoss=0.0001 | ValLoss=0.0026 | AvgGrad=0.0005\n",
      "Early stopping.\n",
      "Fold 1 Test Accuracy: 97.30%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 1/200 | TrainAcc=95.64% | ValAcc=99.70% | TrainLoss=0.1449 | ValLoss=0.0095 | AvgGrad=0.0847\n",
      "Epoch 2/200 | TrainAcc=99.65% | ValAcc=99.54% | TrainLoss=0.0101 | ValLoss=0.0135 | AvgGrad=0.0278\n",
      "Epoch 3/200 | TrainAcc=99.81% | ValAcc=99.88% | TrainLoss=0.0062 | ValLoss=0.0048 | AvgGrad=0.0194\n",
      "Epoch 4/200 | TrainAcc=99.92% | ValAcc=99.75% | TrainLoss=0.0025 | ValLoss=0.0095 | AvgGrad=0.0113\n",
      "Epoch 5/200 | TrainAcc=99.87% | ValAcc=99.63% | TrainLoss=0.0040 | ValLoss=0.0111 | AvgGrad=0.0150\n",
      "Epoch 6/200 | TrainAcc=99.82% | ValAcc=99.90% | TrainLoss=0.0056 | ValLoss=0.0027 | AvgGrad=0.0179\n",
      "Epoch 7/200 | TrainAcc=99.94% | ValAcc=99.94% | TrainLoss=0.0017 | ValLoss=0.0016 | AvgGrad=0.0081\n",
      "Epoch 8/200 | TrainAcc=99.88% | ValAcc=99.89% | TrainLoss=0.0038 | ValLoss=0.0039 | AvgGrad=0.0130\n",
      "Epoch 9/200 | TrainAcc=99.94% | ValAcc=99.93% | TrainLoss=0.0023 | ValLoss=0.0026 | AvgGrad=0.0083\n",
      "Epoch 10/200 | TrainAcc=99.94% | ValAcc=99.84% | TrainLoss=0.0016 | ValLoss=0.0056 | AvgGrad=0.0072\n",
      "Epoch 11/200 | TrainAcc=99.97% | ValAcc=99.93% | TrainLoss=0.0008 | ValLoss=0.0029 | AvgGrad=0.0040\n",
      "Early stopping.\n",
      "Fold 2 Test Accuracy: 97.06%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 1/200 | TrainAcc=96.08% | ValAcc=99.37% | TrainLoss=0.1345 | ValLoss=0.0182 | AvgGrad=0.0826\n",
      "Epoch 2/200 | TrainAcc=99.73% | ValAcc=99.76% | TrainLoss=0.0084 | ValLoss=0.0072 | AvgGrad=0.0254\n",
      "Epoch 3/200 | TrainAcc=99.82% | ValAcc=99.50% | TrainLoss=0.0053 | ValLoss=0.0182 | AvgGrad=0.0199\n",
      "Epoch 4/200 | TrainAcc=99.84% | ValAcc=98.22% | TrainLoss=0.0048 | ValLoss=0.0594 | AvgGrad=0.0179\n",
      "Epoch 5/200 | TrainAcc=99.84% | ValAcc=99.81% | TrainLoss=0.0049 | ValLoss=0.0059 | AvgGrad=0.0168\n",
      "Epoch 6/200 | TrainAcc=99.91% | ValAcc=99.80% | TrainLoss=0.0029 | ValLoss=0.0066 | AvgGrad=0.0111\n",
      "Epoch 7/200 | TrainAcc=99.92% | ValAcc=99.89% | TrainLoss=0.0026 | ValLoss=0.0036 | AvgGrad=0.0106\n",
      "Epoch 8/200 | TrainAcc=99.93% | ValAcc=99.71% | TrainLoss=0.0020 | ValLoss=0.0103 | AvgGrad=0.0086\n",
      "Epoch 9/200 | TrainAcc=99.87% | ValAcc=99.78% | TrainLoss=0.0044 | ValLoss=0.0078 | AvgGrad=0.0141\n",
      "Epoch 10/200 | TrainAcc=99.91% | ValAcc=99.62% | TrainLoss=0.0027 | ValLoss=0.0153 | AvgGrad=0.0104\n",
      "Epoch 11/200 | TrainAcc=99.97% | ValAcc=99.90% | TrainLoss=0.0008 | ValLoss=0.0035 | AvgGrad=0.0038\n",
      "Epoch 12/200 | TrainAcc=100.00% | ValAcc=99.95% | TrainLoss=0.0001 | ValLoss=0.0020 | AvgGrad=0.0006\n",
      "Epoch 13/200 | TrainAcc=100.00% | ValAcc=99.95% | TrainLoss=0.0000 | ValLoss=0.0020 | AvgGrad=0.0002\n",
      "Epoch 14/200 | TrainAcc=100.00% | ValAcc=99.95% | TrainLoss=0.0000 | ValLoss=0.0022 | AvgGrad=0.0004\n",
      "Epoch 15/200 | TrainAcc=100.00% | ValAcc=99.94% | TrainLoss=0.0000 | ValLoss=0.0021 | AvgGrad=0.0002\n",
      "Early stopping.\n",
      "Fold 3 Test Accuracy: 97.13%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 1/200 | TrainAcc=96.53% | ValAcc=98.40% | TrainLoss=0.1071 | ValLoss=0.0524 | AvgGrad=0.0815\n",
      "Epoch 2/200 | TrainAcc=99.73% | ValAcc=99.72% | TrainLoss=0.0088 | ValLoss=0.0083 | AvgGrad=0.0274\n",
      "Epoch 3/200 | TrainAcc=99.86% | ValAcc=99.69% | TrainLoss=0.0041 | ValLoss=0.0103 | AvgGrad=0.0176\n",
      "Epoch 4/200 | TrainAcc=99.80% | ValAcc=99.75% | TrainLoss=0.0062 | ValLoss=0.0088 | AvgGrad=0.0215\n",
      "Epoch 5/200 | TrainAcc=99.88% | ValAcc=99.69% | TrainLoss=0.0041 | ValLoss=0.0113 | AvgGrad=0.0164\n",
      "Epoch 6/200 | TrainAcc=99.88% | ValAcc=99.80% | TrainLoss=0.0034 | ValLoss=0.0060 | AvgGrad=0.0147\n",
      "Epoch 7/200 | TrainAcc=99.85% | ValAcc=99.67% | TrainLoss=0.0053 | ValLoss=0.0093 | AvgGrad=0.0183\n",
      "Epoch 8/200 | TrainAcc=99.90% | ValAcc=99.70% | TrainLoss=0.0035 | ValLoss=0.0124 | AvgGrad=0.0127\n",
      "Epoch 9/200 | TrainAcc=99.90% | ValAcc=99.90% | TrainLoss=0.0029 | ValLoss=0.0034 | AvgGrad=0.0114\n",
      "Epoch 10/200 | TrainAcc=99.90% | ValAcc=99.90% | TrainLoss=0.0025 | ValLoss=0.0041 | AvgGrad=0.0109\n",
      "Epoch 11/200 | TrainAcc=99.98% | ValAcc=99.94% | TrainLoss=0.0006 | ValLoss=0.0024 | AvgGrad=0.0037\n",
      "Epoch 12/200 | TrainAcc=100.00% | ValAcc=99.92% | TrainLoss=0.0002 | ValLoss=0.0025 | AvgGrad=0.0014\n",
      "Epoch 13/200 | TrainAcc=100.00% | ValAcc=99.95% | TrainLoss=0.0001 | ValLoss=0.0016 | AvgGrad=0.0008\n",
      "Epoch 14/200 | TrainAcc=100.00% | ValAcc=99.95% | TrainLoss=0.0001 | ValLoss=0.0020 | AvgGrad=0.0007\n",
      "Epoch 15/200 | TrainAcc=100.00% | ValAcc=99.95% | TrainLoss=0.0000 | ValLoss=0.0018 | AvgGrad=0.0001\n",
      "Epoch 16/200 | TrainAcc=100.00% | ValAcc=99.95% | TrainLoss=0.0000 | ValLoss=0.0019 | AvgGrad=0.0001\n",
      "Epoch 17/200 | TrainAcc=100.00% | ValAcc=99.95% | TrainLoss=0.0000 | ValLoss=0.0018 | AvgGrad=0.0001\n",
      "Early stopping.\n",
      "Fold 4 Test Accuracy: 97.48%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 1/200 | TrainAcc=96.25% | ValAcc=99.69% | TrainLoss=0.1246 | ValLoss=0.0092 | AvgGrad=0.0858\n",
      "Epoch 2/200 | TrainAcc=99.70% | ValAcc=99.71% | TrainLoss=0.0087 | ValLoss=0.0096 | AvgGrad=0.0288\n",
      "Epoch 3/200 | TrainAcc=99.82% | ValAcc=99.75% | TrainLoss=0.0053 | ValLoss=0.0079 | AvgGrad=0.0208\n",
      "Epoch 4/200 | TrainAcc=99.83% | ValAcc=99.87% | TrainLoss=0.0056 | ValLoss=0.0054 | AvgGrad=0.0189\n",
      "Epoch 5/200 | TrainAcc=99.92% | ValAcc=99.87% | TrainLoss=0.0025 | ValLoss=0.0057 | AvgGrad=0.0103\n",
      "Epoch 6/200 | TrainAcc=99.95% | ValAcc=99.87% | TrainLoss=0.0016 | ValLoss=0.0040 | AvgGrad=0.0080\n",
      "Epoch 7/200 | TrainAcc=99.89% | ValAcc=99.65% | TrainLoss=0.0034 | ValLoss=0.0109 | AvgGrad=0.0136\n",
      "Epoch 8/200 | TrainAcc=99.84% | ValAcc=99.81% | TrainLoss=0.0049 | ValLoss=0.0069 | AvgGrad=0.0160\n",
      "Epoch 9/200 | TrainAcc=99.96% | ValAcc=99.88% | TrainLoss=0.0012 | ValLoss=0.0052 | AvgGrad=0.0062\n",
      "Epoch 10/200 | TrainAcc=99.93% | ValAcc=99.26% | TrainLoss=0.0018 | ValLoss=0.0351 | AvgGrad=0.0085\n",
      "Epoch 11/200 | TrainAcc=99.96% | ValAcc=99.85% | TrainLoss=0.0014 | ValLoss=0.0073 | AvgGrad=0.0064\n",
      "Epoch 12/200 | TrainAcc=100.00% | ValAcc=99.91% | TrainLoss=0.0002 | ValLoss=0.0048 | AvgGrad=0.0016\n",
      "Early stopping.\n",
      "Fold 5 Test Accuracy: 97.22%\n",
      "\n",
      "=== Overall Summary ===\n",
      "Val Acc: 99.93 Â± 0.02\n",
      "Test Acc: 97.24 Â± 0.15\n",
      "\n",
      "All results saved in ./training_results\\2025-11-25_18-31-16_wisig_cross_SNR10dB_fd266_classes_6_ResNet\n",
      "ğŸ‰ Finished SNR=10 dB, results saved in: ./training_results\\2025-11-25_18-31-16_wisig_cross_SNR10dB_fd266_classes_6_ResNet\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running experiment with SNR = 5 dB\n",
      "============================================================\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 1/200 | TrainAcc=95.34% | ValAcc=99.38% | TrainLoss=0.1365 | ValLoss=0.0192 | AvgGrad=0.1061\n",
      "Epoch 2/200 | TrainAcc=99.38% | ValAcc=99.31% | TrainLoss=0.0182 | ValLoss=0.0201 | AvgGrad=0.0426\n",
      "Epoch 3/200 | TrainAcc=99.38% | ValAcc=99.56% | TrainLoss=0.0168 | ValLoss=0.0130 | AvgGrad=0.0429\n",
      "Epoch 4/200 | TrainAcc=99.69% | ValAcc=99.42% | TrainLoss=0.0096 | ValLoss=0.0182 | AvgGrad=0.0289\n",
      "Epoch 5/200 | TrainAcc=99.76% | ValAcc=99.58% | TrainLoss=0.0071 | ValLoss=0.0134 | AvgGrad=0.0246\n",
      "Epoch 6/200 | TrainAcc=99.75% | ValAcc=99.50% | TrainLoss=0.0070 | ValLoss=0.0156 | AvgGrad=0.0260\n",
      "Epoch 7/200 | TrainAcc=99.62% | ValAcc=99.49% | TrainLoss=0.0114 | ValLoss=0.0184 | AvgGrad=0.0330\n",
      "Epoch 8/200 | TrainAcc=99.73% | ValAcc=99.54% | TrainLoss=0.0072 | ValLoss=0.0149 | AvgGrad=0.0241\n",
      "Epoch 9/200 | TrainAcc=99.79% | ValAcc=99.47% | TrainLoss=0.0060 | ValLoss=0.0222 | AvgGrad=0.0220\n",
      "Epoch 10/200 | TrainAcc=99.84% | ValAcc=99.58% | TrainLoss=0.0047 | ValLoss=0.0162 | AvgGrad=0.0190\n",
      "Epoch 11/200 | TrainAcc=99.96% | ValAcc=99.71% | TrainLoss=0.0012 | ValLoss=0.0131 | AvgGrad=0.0069\n",
      "Epoch 12/200 | TrainAcc=99.99% | ValAcc=99.66% | TrainLoss=0.0003 | ValLoss=0.0145 | AvgGrad=0.0021\n",
      "Epoch 13/200 | TrainAcc=99.99% | ValAcc=99.63% | TrainLoss=0.0002 | ValLoss=0.0154 | AvgGrad=0.0018\n",
      "Epoch 14/200 | TrainAcc=100.00% | ValAcc=99.69% | TrainLoss=0.0001 | ValLoss=0.0143 | AvgGrad=0.0006\n",
      "Epoch 15/200 | TrainAcc=100.00% | ValAcc=99.69% | TrainLoss=0.0001 | ValLoss=0.0146 | AvgGrad=0.0006\n",
      "Epoch 16/200 | TrainAcc=100.00% | ValAcc=99.70% | TrainLoss=0.0000 | ValLoss=0.0149 | AvgGrad=0.0003\n",
      "Epoch 17/200 | TrainAcc=100.00% | ValAcc=99.70% | TrainLoss=0.0000 | ValLoss=0.0149 | AvgGrad=0.0002\n",
      "Epoch 18/200 | TrainAcc=100.00% | ValAcc=99.71% | TrainLoss=0.0000 | ValLoss=0.0151 | AvgGrad=0.0001\n",
      "Epoch 19/200 | TrainAcc=99.73% | ValAcc=99.54% | TrainLoss=0.0087 | ValLoss=0.0151 | AvgGrad=0.0229\n",
      "Early stopping.\n",
      "Fold 1 Test Accuracy: 94.42%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 1/200 | TrainAcc=94.51% | ValAcc=99.21% | TrainLoss=0.1812 | ValLoss=0.0237 | AvgGrad=0.1023\n",
      "Epoch 2/200 | TrainAcc=99.23% | ValAcc=99.47% | TrainLoss=0.0218 | ValLoss=0.0173 | AvgGrad=0.0441\n",
      "Epoch 3/200 | TrainAcc=99.50% | ValAcc=99.42% | TrainLoss=0.0143 | ValLoss=0.0156 | AvgGrad=0.0366\n",
      "Epoch 4/200 | TrainAcc=99.62% | ValAcc=99.52% | TrainLoss=0.0110 | ValLoss=0.0136 | AvgGrad=0.0315\n",
      "Epoch 5/200 | TrainAcc=99.66% | ValAcc=99.41% | TrainLoss=0.0098 | ValLoss=0.0181 | AvgGrad=0.0291\n",
      "Epoch 6/200 | TrainAcc=99.76% | ValAcc=99.57% | TrainLoss=0.0069 | ValLoss=0.0165 | AvgGrad=0.0231\n",
      "Epoch 7/200 | TrainAcc=99.67% | ValAcc=99.35% | TrainLoss=0.0091 | ValLoss=0.0221 | AvgGrad=0.0295\n",
      "Epoch 8/200 | TrainAcc=99.71% | ValAcc=99.58% | TrainLoss=0.0073 | ValLoss=0.0160 | AvgGrad=0.0244\n",
      "Epoch 9/200 | TrainAcc=99.76% | ValAcc=99.65% | TrainLoss=0.0070 | ValLoss=0.0150 | AvgGrad=0.0234\n",
      "Epoch 10/200 | TrainAcc=99.87% | ValAcc=99.65% | TrainLoss=0.0040 | ValLoss=0.0136 | AvgGrad=0.0147\n",
      "Epoch 11/200 | TrainAcc=99.95% | ValAcc=99.69% | TrainLoss=0.0012 | ValLoss=0.0125 | AvgGrad=0.0068\n",
      "Epoch 12/200 | TrainAcc=99.98% | ValAcc=99.71% | TrainLoss=0.0007 | ValLoss=0.0126 | AvgGrad=0.0040\n",
      "Epoch 13/200 | TrainAcc=99.99% | ValAcc=99.72% | TrainLoss=0.0003 | ValLoss=0.0128 | AvgGrad=0.0025\n",
      "Epoch 14/200 | TrainAcc=99.99% | ValAcc=99.72% | TrainLoss=0.0002 | ValLoss=0.0124 | AvgGrad=0.0016\n",
      "Epoch 15/200 | TrainAcc=100.00% | ValAcc=99.74% | TrainLoss=0.0001 | ValLoss=0.0127 | AvgGrad=0.0010\n",
      "Epoch 16/200 | TrainAcc=99.99% | ValAcc=99.70% | TrainLoss=0.0003 | ValLoss=0.0148 | AvgGrad=0.0026\n",
      "Epoch 17/200 | TrainAcc=99.92% | ValAcc=99.54% | TrainLoss=0.0026 | ValLoss=0.0190 | AvgGrad=0.0114\n",
      "Epoch 18/200 | TrainAcc=99.87% | ValAcc=99.54% | TrainLoss=0.0037 | ValLoss=0.0186 | AvgGrad=0.0156\n",
      "Epoch 19/200 | TrainAcc=99.91% | ValAcc=99.64% | TrainLoss=0.0025 | ValLoss=0.0164 | AvgGrad=0.0117\n",
      "Early stopping.\n",
      "Fold 2 Test Accuracy: 95.52%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 1/200 | TrainAcc=93.44% | ValAcc=98.69% | TrainLoss=0.2077 | ValLoss=0.0367 | AvgGrad=0.1124\n",
      "Epoch 2/200 | TrainAcc=99.27% | ValAcc=99.38% | TrainLoss=0.0214 | ValLoss=0.0199 | AvgGrad=0.0415\n",
      "Epoch 3/200 | TrainAcc=99.51% | ValAcc=99.32% | TrainLoss=0.0148 | ValLoss=0.0187 | AvgGrad=0.0370\n",
      "Epoch 4/200 | TrainAcc=99.64% | ValAcc=99.33% | TrainLoss=0.0106 | ValLoss=0.0220 | AvgGrad=0.0300\n",
      "Epoch 5/200 | TrainAcc=99.68% | ValAcc=99.28% | TrainLoss=0.0099 | ValLoss=0.0294 | AvgGrad=0.0303\n",
      "Epoch 6/200 | TrainAcc=99.71% | ValAcc=99.47% | TrainLoss=0.0089 | ValLoss=0.0182 | AvgGrad=0.0278\n",
      "Epoch 7/200 | TrainAcc=99.80% | ValAcc=98.28% | TrainLoss=0.0061 | ValLoss=0.0681 | AvgGrad=0.0201\n",
      "Epoch 8/200 | TrainAcc=99.74% | ValAcc=99.54% | TrainLoss=0.0076 | ValLoss=0.0160 | AvgGrad=0.0252\n",
      "Epoch 9/200 | TrainAcc=99.83% | ValAcc=99.48% | TrainLoss=0.0053 | ValLoss=0.0190 | AvgGrad=0.0188\n",
      "Epoch 10/200 | TrainAcc=99.67% | ValAcc=99.29% | TrainLoss=0.0102 | ValLoss=0.0247 | AvgGrad=0.0288\n",
      "Epoch 11/200 | TrainAcc=99.96% | ValAcc=99.70% | TrainLoss=0.0014 | ValLoss=0.0131 | AvgGrad=0.0068\n",
      "Epoch 12/200 | TrainAcc=99.99% | ValAcc=99.66% | TrainLoss=0.0004 | ValLoss=0.0152 | AvgGrad=0.0026\n",
      "Epoch 13/200 | TrainAcc=99.99% | ValAcc=99.67% | TrainLoss=0.0003 | ValLoss=0.0153 | AvgGrad=0.0020\n",
      "Epoch 14/200 | TrainAcc=100.00% | ValAcc=99.67% | TrainLoss=0.0003 | ValLoss=0.0153 | AvgGrad=0.0019\n",
      "Epoch 15/200 | TrainAcc=100.00% | ValAcc=99.67% | TrainLoss=0.0001 | ValLoss=0.0154 | AvgGrad=0.0007\n",
      "Epoch 16/200 | TrainAcc=99.99% | ValAcc=99.63% | TrainLoss=0.0004 | ValLoss=0.0171 | AvgGrad=0.0032\n",
      "Epoch 17/200 | TrainAcc=99.95% | ValAcc=99.39% | TrainLoss=0.0014 | ValLoss=0.0250 | AvgGrad=0.0068\n",
      "Epoch 18/200 | TrainAcc=99.85% | ValAcc=99.29% | TrainLoss=0.0043 | ValLoss=0.0332 | AvgGrad=0.0186\n",
      "Epoch 19/200 | TrainAcc=99.91% | ValAcc=99.64% | TrainLoss=0.0030 | ValLoss=0.0179 | AvgGrad=0.0126\n",
      "Early stopping.\n",
      "Fold 3 Test Accuracy: 95.55%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 1/200 | TrainAcc=94.69% | ValAcc=99.34% | TrainLoss=0.1681 | ValLoss=0.0197 | AvgGrad=0.0997\n",
      "Epoch 2/200 | TrainAcc=99.26% | ValAcc=99.11% | TrainLoss=0.0218 | ValLoss=0.0281 | AvgGrad=0.0418\n",
      "Epoch 3/200 | TrainAcc=99.55% | ValAcc=99.10% | TrainLoss=0.0126 | ValLoss=0.0275 | AvgGrad=0.0323\n",
      "Epoch 4/200 | TrainAcc=99.65% | ValAcc=99.29% | TrainLoss=0.0101 | ValLoss=0.0238 | AvgGrad=0.0302\n",
      "Epoch 5/200 | TrainAcc=99.63% | ValAcc=99.41% | TrainLoss=0.0103 | ValLoss=0.0177 | AvgGrad=0.0299\n",
      "Epoch 6/200 | TrainAcc=99.67% | ValAcc=99.40% | TrainLoss=0.0100 | ValLoss=0.0232 | AvgGrad=0.0276\n",
      "Epoch 7/200 | TrainAcc=99.73% | ValAcc=99.11% | TrainLoss=0.0079 | ValLoss=0.0310 | AvgGrad=0.0255\n",
      "Epoch 8/200 | TrainAcc=99.65% | ValAcc=99.52% | TrainLoss=0.0109 | ValLoss=0.0169 | AvgGrad=0.0286\n",
      "Epoch 9/200 | TrainAcc=99.86% | ValAcc=99.57% | TrainLoss=0.0043 | ValLoss=0.0181 | AvgGrad=0.0153\n",
      "Epoch 10/200 | TrainAcc=99.91% | ValAcc=99.52% | TrainLoss=0.0028 | ValLoss=0.0220 | AvgGrad=0.0117\n",
      "Epoch 11/200 | TrainAcc=99.97% | ValAcc=99.64% | TrainLoss=0.0008 | ValLoss=0.0168 | AvgGrad=0.0051\n",
      "Epoch 12/200 | TrainAcc=99.99% | ValAcc=99.68% | TrainLoss=0.0003 | ValLoss=0.0146 | AvgGrad=0.0023\n",
      "Epoch 13/200 | TrainAcc=99.99% | ValAcc=99.72% | TrainLoss=0.0003 | ValLoss=0.0154 | AvgGrad=0.0021\n",
      "Epoch 14/200 | TrainAcc=100.00% | ValAcc=99.63% | TrainLoss=0.0001 | ValLoss=0.0161 | AvgGrad=0.0010\n",
      "Epoch 15/200 | TrainAcc=100.00% | ValAcc=99.70% | TrainLoss=0.0001 | ValLoss=0.0149 | AvgGrad=0.0009\n",
      "Epoch 16/200 | TrainAcc=100.00% | ValAcc=99.71% | TrainLoss=0.0001 | ValLoss=0.0145 | AvgGrad=0.0006\n",
      "Epoch 17/200 | TrainAcc=99.86% | ValAcc=99.71% | TrainLoss=0.0049 | ValLoss=0.0129 | AvgGrad=0.0154\n",
      "Epoch 18/200 | TrainAcc=99.93% | ValAcc=99.58% | TrainLoss=0.0019 | ValLoss=0.0195 | AvgGrad=0.0096\n",
      "Epoch 19/200 | TrainAcc=99.97% | ValAcc=99.60% | TrainLoss=0.0010 | ValLoss=0.0209 | AvgGrad=0.0059\n",
      "Early stopping.\n",
      "Fold 4 Test Accuracy: 95.17%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 1/200 | TrainAcc=94.66% | ValAcc=98.93% | TrainLoss=0.1685 | ValLoss=0.0303 | AvgGrad=0.1030\n",
      "Epoch 2/200 | TrainAcc=99.21% | ValAcc=99.41% | TrainLoss=0.0226 | ValLoss=0.0168 | AvgGrad=0.0468\n",
      "Epoch 3/200 | TrainAcc=99.49% | ValAcc=99.19% | TrainLoss=0.0149 | ValLoss=0.0269 | AvgGrad=0.0354\n",
      "Epoch 4/200 | TrainAcc=99.62% | ValAcc=99.50% | TrainLoss=0.0111 | ValLoss=0.0154 | AvgGrad=0.0313\n",
      "Epoch 5/200 | TrainAcc=99.70% | ValAcc=99.18% | TrainLoss=0.0087 | ValLoss=0.0254 | AvgGrad=0.0284\n",
      "Epoch 6/200 | TrainAcc=99.70% | ValAcc=99.49% | TrainLoss=0.0085 | ValLoss=0.0160 | AvgGrad=0.0279\n",
      "Epoch 7/200 | TrainAcc=99.78% | ValAcc=98.50% | TrainLoss=0.0074 | ValLoss=0.0513 | AvgGrad=0.0233\n",
      "Epoch 8/200 | TrainAcc=99.72% | ValAcc=99.19% | TrainLoss=0.0084 | ValLoss=0.0252 | AvgGrad=0.0260\n",
      "Epoch 9/200 | TrainAcc=99.77% | ValAcc=99.47% | TrainLoss=0.0066 | ValLoss=0.0187 | AvgGrad=0.0226\n",
      "Epoch 10/200 | TrainAcc=99.76% | ValAcc=99.41% | TrainLoss=0.0068 | ValLoss=0.0225 | AvgGrad=0.0224\n",
      "Early stopping.\n",
      "Fold 5 Test Accuracy: 94.51%\n",
      "\n",
      "=== Overall Summary ===\n",
      "Val Acc: 99.57 Â± 0.08\n",
      "Test Acc: 95.04 Â± 0.48\n",
      "\n",
      "All results saved in ./training_results\\2025-11-25_18-45-27_wisig_cross_SNR5dB_fd266_classes_6_ResNet\n",
      "ğŸ‰ Finished SNR=5 dB, results saved in: ./training_results\\2025-11-25_18-45-27_wisig_cross_SNR5dB_fd266_classes_6_ResNet\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running experiment with SNR = 0 dB\n",
      "============================================================\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 1/200 | TrainAcc=86.87% | ValAcc=94.75% | TrainLoss=0.3994 | ValLoss=0.1430 | AvgGrad=0.1406\n",
      "Epoch 2/200 | TrainAcc=95.52% | ValAcc=95.51% | TrainLoss=0.1225 | ValLoss=0.1199 | AvgGrad=0.0920\n",
      "Epoch 3/200 | TrainAcc=96.62% | ValAcc=96.21% | TrainLoss=0.0957 | ValLoss=0.1050 | AvgGrad=0.0873\n",
      "Epoch 4/200 | TrainAcc=97.30% | ValAcc=96.19% | TrainLoss=0.0748 | ValLoss=0.1134 | AvgGrad=0.0802\n",
      "Epoch 5/200 | TrainAcc=97.72% | ValAcc=96.52% | TrainLoss=0.0641 | ValLoss=0.1019 | AvgGrad=0.0805\n",
      "Epoch 6/200 | TrainAcc=98.04% | ValAcc=96.07% | TrainLoss=0.0530 | ValLoss=0.1206 | AvgGrad=0.0790\n",
      "Epoch 7/200 | TrainAcc=98.41% | ValAcc=96.01% | TrainLoss=0.0413 | ValLoss=0.1212 | AvgGrad=0.0724\n",
      "Epoch 8/200 | TrainAcc=98.57% | ValAcc=96.61% | TrainLoss=0.0389 | ValLoss=0.1167 | AvgGrad=0.0731\n",
      "Epoch 9/200 | TrainAcc=98.87% | ValAcc=96.26% | TrainLoss=0.0306 | ValLoss=0.1377 | AvgGrad=0.0674\n",
      "Epoch 10/200 | TrainAcc=99.05% | ValAcc=96.15% | TrainLoss=0.0259 | ValLoss=0.1424 | AvgGrad=0.0651\n",
      "Epoch 11/200 | TrainAcc=99.59% | ValAcc=97.02% | TrainLoss=0.0118 | ValLoss=0.1422 | AvgGrad=0.0417\n",
      "Epoch 12/200 | TrainAcc=99.86% | ValAcc=96.71% | TrainLoss=0.0044 | ValLoss=0.1593 | AvgGrad=0.0257\n",
      "Epoch 13/200 | TrainAcc=99.89% | ValAcc=96.61% | TrainLoss=0.0034 | ValLoss=0.1727 | AvgGrad=0.0225\n",
      "Epoch 14/200 | TrainAcc=99.94% | ValAcc=96.36% | TrainLoss=0.0024 | ValLoss=0.2072 | AvgGrad=0.0186\n",
      "Epoch 15/200 | TrainAcc=99.85% | ValAcc=96.73% | TrainLoss=0.0046 | ValLoss=0.1765 | AvgGrad=0.0304\n",
      "Epoch 16/200 | TrainAcc=99.74% | ValAcc=96.03% | TrainLoss=0.0069 | ValLoss=0.2163 | AvgGrad=0.0392\n",
      "Epoch 17/200 | TrainAcc=99.77% | ValAcc=96.73% | TrainLoss=0.0061 | ValLoss=0.1908 | AvgGrad=0.0351\n",
      "Epoch 18/200 | TrainAcc=99.69% | ValAcc=96.58% | TrainLoss=0.0093 | ValLoss=0.1766 | AvgGrad=0.0471\n",
      "Epoch 19/200 | TrainAcc=99.59% | ValAcc=96.64% | TrainLoss=0.0117 | ValLoss=0.1840 | AvgGrad=0.0505\n",
      "Early stopping.\n",
      "Fold 1 Test Accuracy: 90.04%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 1/200 | TrainAcc=88.86% | ValAcc=95.92% | TrainLoss=0.3203 | ValLoss=0.1143 | AvgGrad=0.1384\n",
      "Epoch 2/200 | TrainAcc=95.71% | ValAcc=96.54% | TrainLoss=0.1186 | ValLoss=0.0925 | AvgGrad=0.0947\n",
      "Epoch 3/200 | TrainAcc=96.66% | ValAcc=95.74% | TrainLoss=0.0920 | ValLoss=0.1204 | AvgGrad=0.0847\n",
      "Epoch 4/200 | TrainAcc=97.21% | ValAcc=96.47% | TrainLoss=0.0766 | ValLoss=0.0960 | AvgGrad=0.0824\n",
      "Epoch 5/200 | TrainAcc=97.79% | ValAcc=96.25% | TrainLoss=0.0606 | ValLoss=0.1115 | AvgGrad=0.0795\n",
      "Epoch 6/200 | TrainAcc=97.98% | ValAcc=96.79% | TrainLoss=0.0538 | ValLoss=0.0974 | AvgGrad=0.0794\n",
      "Epoch 7/200 | TrainAcc=98.52% | ValAcc=96.67% | TrainLoss=0.0405 | ValLoss=0.1079 | AvgGrad=0.0691\n",
      "Epoch 8/200 | TrainAcc=98.61% | ValAcc=96.61% | TrainLoss=0.0369 | ValLoss=0.1063 | AvgGrad=0.0715\n",
      "Epoch 9/200 | TrainAcc=98.85% | ValAcc=95.96% | TrainLoss=0.0308 | ValLoss=0.1593 | AvgGrad=0.0690\n",
      "Epoch 10/200 | TrainAcc=99.16% | ValAcc=96.43% | TrainLoss=0.0228 | ValLoss=0.1389 | AvgGrad=0.0605\n",
      "Epoch 11/200 | TrainAcc=99.64% | ValAcc=96.93% | TrainLoss=0.0098 | ValLoss=0.1341 | AvgGrad=0.0378\n",
      "Epoch 12/200 | TrainAcc=99.88% | ValAcc=97.10% | TrainLoss=0.0037 | ValLoss=0.1402 | AvgGrad=0.0246\n",
      "Epoch 13/200 | TrainAcc=99.95% | ValAcc=97.26% | TrainLoss=0.0021 | ValLoss=0.1454 | AvgGrad=0.0155\n",
      "Epoch 14/200 | TrainAcc=99.87% | ValAcc=96.95% | TrainLoss=0.0036 | ValLoss=0.1534 | AvgGrad=0.0249\n",
      "Epoch 15/200 | TrainAcc=99.88% | ValAcc=96.84% | TrainLoss=0.0033 | ValLoss=0.1819 | AvgGrad=0.0238\n",
      "Epoch 16/200 | TrainAcc=99.66% | ValAcc=96.71% | TrainLoss=0.0094 | ValLoss=0.1753 | AvgGrad=0.0466\n",
      "Epoch 17/200 | TrainAcc=99.77% | ValAcc=96.99% | TrainLoss=0.0072 | ValLoss=0.1616 | AvgGrad=0.0359\n",
      "Epoch 18/200 | TrainAcc=99.81% | ValAcc=96.66% | TrainLoss=0.0060 | ValLoss=0.1897 | AvgGrad=0.0354\n",
      "Epoch 19/200 | TrainAcc=99.74% | ValAcc=96.91% | TrainLoss=0.0083 | ValLoss=0.1459 | AvgGrad=0.0405\n",
      "Epoch 20/200 | TrainAcc=99.64% | ValAcc=97.11% | TrainLoss=0.0101 | ValLoss=0.1650 | AvgGrad=0.0476\n",
      "Epoch 21/200 | TrainAcc=99.89% | ValAcc=97.25% | TrainLoss=0.0031 | ValLoss=0.1573 | AvgGrad=0.0202\n",
      "Early stopping.\n",
      "Fold 2 Test Accuracy: 90.51%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 1/200 | TrainAcc=87.97% | ValAcc=93.96% | TrainLoss=0.3573 | ValLoss=0.1595 | AvgGrad=0.1462\n",
      "Epoch 2/200 | TrainAcc=95.89% | ValAcc=95.98% | TrainLoss=0.1170 | ValLoss=0.1120 | AvgGrad=0.0918\n",
      "Epoch 3/200 | TrainAcc=96.84% | ValAcc=95.75% | TrainLoss=0.0894 | ValLoss=0.1192 | AvgGrad=0.0860\n",
      "Epoch 4/200 | TrainAcc=97.39% | ValAcc=95.70% | TrainLoss=0.0730 | ValLoss=0.1208 | AvgGrad=0.0803\n",
      "Epoch 5/200 | TrainAcc=97.76% | ValAcc=96.12% | TrainLoss=0.0607 | ValLoss=0.1154 | AvgGrad=0.0802\n",
      "Epoch 6/200 | TrainAcc=98.02% | ValAcc=96.54% | TrainLoss=0.0541 | ValLoss=0.1109 | AvgGrad=0.0797\n",
      "Epoch 7/200 | TrainAcc=98.48% | ValAcc=96.22% | TrainLoss=0.0415 | ValLoss=0.1241 | AvgGrad=0.0733\n",
      "Epoch 8/200 | TrainAcc=98.80% | ValAcc=96.39% | TrainLoss=0.0330 | ValLoss=0.1201 | AvgGrad=0.0676\n",
      "Epoch 9/200 | TrainAcc=98.99% | ValAcc=96.17% | TrainLoss=0.0275 | ValLoss=0.1455 | AvgGrad=0.0623\n",
      "Epoch 10/200 | TrainAcc=99.13% | ValAcc=95.94% | TrainLoss=0.0250 | ValLoss=0.1595 | AvgGrad=0.0639\n",
      "Epoch 11/200 | TrainAcc=99.66% | ValAcc=96.58% | TrainLoss=0.0101 | ValLoss=0.1543 | AvgGrad=0.0392\n",
      "Epoch 12/200 | TrainAcc=99.87% | ValAcc=96.52% | TrainLoss=0.0044 | ValLoss=0.1713 | AvgGrad=0.0244\n",
      "Epoch 13/200 | TrainAcc=99.91% | ValAcc=96.58% | TrainLoss=0.0028 | ValLoss=0.1785 | AvgGrad=0.0190\n",
      "Epoch 14/200 | TrainAcc=99.88% | ValAcc=96.17% | TrainLoss=0.0038 | ValLoss=0.2170 | AvgGrad=0.0256\n",
      "Early stopping.\n",
      "Fold 3 Test Accuracy: 89.85%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 1/200 | TrainAcc=88.86% | ValAcc=95.87% | TrainLoss=0.3136 | ValLoss=0.1151 | AvgGrad=0.1415\n",
      "Epoch 2/200 | TrainAcc=95.98% | ValAcc=96.56% | TrainLoss=0.1122 | ValLoss=0.0937 | AvgGrad=0.0935\n",
      "Epoch 3/200 | TrainAcc=96.73% | ValAcc=96.61% | TrainLoss=0.0891 | ValLoss=0.0962 | AvgGrad=0.0872\n",
      "Epoch 4/200 | TrainAcc=97.27% | ValAcc=96.51% | TrainLoss=0.0737 | ValLoss=0.0986 | AvgGrad=0.0841\n",
      "Epoch 5/200 | TrainAcc=97.87% | ValAcc=96.04% | TrainLoss=0.0600 | ValLoss=0.1225 | AvgGrad=0.0805\n",
      "Epoch 6/200 | TrainAcc=98.24% | ValAcc=96.64% | TrainLoss=0.0488 | ValLoss=0.1050 | AvgGrad=0.0767\n",
      "Epoch 7/200 | TrainAcc=98.51% | ValAcc=96.53% | TrainLoss=0.0400 | ValLoss=0.1127 | AvgGrad=0.0728\n",
      "Epoch 8/200 | TrainAcc=98.74% | ValAcc=96.35% | TrainLoss=0.0344 | ValLoss=0.1222 | AvgGrad=0.0707\n",
      "Epoch 9/200 | TrainAcc=98.92% | ValAcc=96.65% | TrainLoss=0.0280 | ValLoss=0.1371 | AvgGrad=0.0668\n",
      "Epoch 10/200 | TrainAcc=98.97% | ValAcc=96.44% | TrainLoss=0.0273 | ValLoss=0.1486 | AvgGrad=0.0665\n",
      "Early stopping.\n",
      "Fold 4 Test Accuracy: 89.97%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 1/200 | TrainAcc=88.95% | ValAcc=95.51% | TrainLoss=0.3184 | ValLoss=0.1204 | AvgGrad=0.1423\n",
      "Epoch 2/200 | TrainAcc=95.78% | ValAcc=95.93% | TrainLoss=0.1177 | ValLoss=0.1088 | AvgGrad=0.0945\n",
      "Epoch 3/200 | TrainAcc=96.81% | ValAcc=96.54% | TrainLoss=0.0887 | ValLoss=0.0971 | AvgGrad=0.0821\n",
      "Epoch 4/200 | TrainAcc=97.35% | ValAcc=96.34% | TrainLoss=0.0722 | ValLoss=0.1031 | AvgGrad=0.0775\n",
      "Epoch 5/200 | TrainAcc=97.89% | ValAcc=96.35% | TrainLoss=0.0585 | ValLoss=0.1063 | AvgGrad=0.0757\n",
      "Epoch 6/200 | TrainAcc=98.13% | ValAcc=96.46% | TrainLoss=0.0499 | ValLoss=0.1139 | AvgGrad=0.0752\n",
      "Epoch 7/200 | TrainAcc=98.51% | ValAcc=96.27% | TrainLoss=0.0412 | ValLoss=0.1279 | AvgGrad=0.0713\n",
      "Epoch 8/200 | TrainAcc=98.60% | ValAcc=96.29% | TrainLoss=0.0383 | ValLoss=0.1304 | AvgGrad=0.0702\n",
      "Epoch 9/200 | TrainAcc=98.99% | ValAcc=96.18% | TrainLoss=0.0272 | ValLoss=0.1397 | AvgGrad=0.0622\n",
      "Epoch 10/200 | TrainAcc=99.03% | ValAcc=96.56% | TrainLoss=0.0270 | ValLoss=0.1246 | AvgGrad=0.0636\n",
      "Epoch 11/200 | TrainAcc=99.64% | ValAcc=96.86% | TrainLoss=0.0102 | ValLoss=0.1423 | AvgGrad=0.0361\n",
      "Epoch 12/200 | TrainAcc=99.91% | ValAcc=96.92% | TrainLoss=0.0032 | ValLoss=0.1614 | AvgGrad=0.0194\n",
      "Epoch 13/200 | TrainAcc=99.93% | ValAcc=96.89% | TrainLoss=0.0021 | ValLoss=0.1750 | AvgGrad=0.0150\n",
      "Epoch 14/200 | TrainAcc=99.93% | ValAcc=96.79% | TrainLoss=0.0025 | ValLoss=0.1816 | AvgGrad=0.0193\n",
      "Epoch 15/200 | TrainAcc=99.85% | ValAcc=96.86% | TrainLoss=0.0048 | ValLoss=0.1705 | AvgGrad=0.0285\n",
      "Epoch 16/200 | TrainAcc=99.80% | ValAcc=96.00% | TrainLoss=0.0060 | ValLoss=0.2008 | AvgGrad=0.0339\n",
      "Epoch 17/200 | TrainAcc=99.80% | ValAcc=96.37% | TrainLoss=0.0058 | ValLoss=0.1959 | AvgGrad=0.0340\n",
      "Epoch 18/200 | TrainAcc=99.70% | ValAcc=96.61% | TrainLoss=0.0083 | ValLoss=0.1773 | AvgGrad=0.0418\n",
      "Epoch 19/200 | TrainAcc=99.59% | ValAcc=96.72% | TrainLoss=0.0115 | ValLoss=0.1746 | AvgGrad=0.0530\n",
      "Early stopping.\n",
      "Fold 5 Test Accuracy: 90.92%\n",
      "\n",
      "=== Overall Summary ===\n",
      "Val Acc: 96.64 Â± 0.36\n",
      "Test Acc: 90.26 Â± 0.40\n",
      "\n",
      "All results saved in ./training_results\\2025-11-25_19-01-49_wisig_cross_SNR0dB_fd266_classes_6_ResNet\n",
      "ğŸ‰ Finished SNR=0 dB, results saved in: ./training_results\\2025-11-25_19-01-49_wisig_cross_SNR0dB_fd266_classes_6_ResNet\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running experiment with SNR = -5 dB\n",
      "============================================================\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 1/200 | TrainAcc=65.60% | ValAcc=75.40% | TrainLoss=0.9206 | ValLoss=0.6190 | AvgGrad=0.1488\n",
      "Epoch 2/200 | TrainAcc=76.19% | ValAcc=77.36% | TrainLoss=0.6073 | ValLoss=0.5727 | AvgGrad=0.1229\n",
      "Epoch 3/200 | TrainAcc=77.96% | ValAcc=77.47% | TrainLoss=0.5614 | ValLoss=0.5579 | AvgGrad=0.1235\n",
      "Epoch 4/200 | TrainAcc=79.39% | ValAcc=78.02% | TrainLoss=0.5264 | ValLoss=0.5517 | AvgGrad=0.1229\n",
      "Epoch 5/200 | TrainAcc=80.50% | ValAcc=78.02% | TrainLoss=0.4988 | ValLoss=0.5643 | AvgGrad=0.1260\n",
      "Epoch 6/200 | TrainAcc=82.25% | ValAcc=78.20% | TrainLoss=0.4597 | ValLoss=0.5780 | AvgGrad=0.1294\n",
      "Epoch 7/200 | TrainAcc=83.18% | ValAcc=77.01% | TrainLoss=0.4288 | ValLoss=0.6140 | AvgGrad=0.1385\n",
      "Epoch 8/200 | TrainAcc=85.21% | ValAcc=76.26% | TrainLoss=0.3831 | ValLoss=0.6396 | AvgGrad=0.1454\n",
      "Epoch 9/200 | TrainAcc=86.77% | ValAcc=76.11% | TrainLoss=0.3369 | ValLoss=0.6539 | AvgGrad=0.1503\n",
      "Epoch 10/200 | TrainAcc=88.87% | ValAcc=76.33% | TrainLoss=0.2883 | ValLoss=0.7348 | AvgGrad=0.1573\n",
      "Epoch 11/200 | TrainAcc=93.60% | ValAcc=76.15% | TrainLoss=0.1711 | ValLoss=0.8879 | AvgGrad=0.1432\n",
      "Epoch 12/200 | TrainAcc=95.78% | ValAcc=75.21% | TrainLoss=0.1152 | ValLoss=1.0116 | AvgGrad=0.1474\n",
      "Epoch 13/200 | TrainAcc=97.00% | ValAcc=76.05% | TrainLoss=0.0824 | ValLoss=1.1590 | AvgGrad=0.1427\n",
      "Epoch 14/200 | TrainAcc=97.72% | ValAcc=75.20% | TrainLoss=0.0652 | ValLoss=1.2698 | AvgGrad=0.1415\n",
      "Early stopping.\n",
      "Fold 1 Test Accuracy: 68.75%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 1/200 | TrainAcc=67.42% | ValAcc=76.29% | TrainLoss=0.8624 | ValLoss=0.5994 | AvgGrad=0.1504\n",
      "Epoch 2/200 | TrainAcc=76.42% | ValAcc=77.79% | TrainLoss=0.6013 | ValLoss=0.5492 | AvgGrad=0.1294\n",
      "Epoch 3/200 | TrainAcc=78.01% | ValAcc=78.26% | TrainLoss=0.5589 | ValLoss=0.5485 | AvgGrad=0.1266\n",
      "Epoch 4/200 | TrainAcc=79.63% | ValAcc=77.89% | TrainLoss=0.5205 | ValLoss=0.5650 | AvgGrad=0.1259\n",
      "Epoch 5/200 | TrainAcc=80.76% | ValAcc=77.38% | TrainLoss=0.4928 | ValLoss=0.5710 | AvgGrad=0.1346\n",
      "Epoch 6/200 | TrainAcc=82.00% | ValAcc=78.37% | TrainLoss=0.4561 | ValLoss=0.5616 | AvgGrad=0.1367\n",
      "Epoch 7/200 | TrainAcc=83.69% | ValAcc=78.28% | TrainLoss=0.4164 | ValLoss=0.5817 | AvgGrad=0.1456\n",
      "Epoch 8/200 | TrainAcc=85.29% | ValAcc=77.55% | TrainLoss=0.3725 | ValLoss=0.6454 | AvgGrad=0.1512\n",
      "Epoch 9/200 | TrainAcc=87.32% | ValAcc=77.33% | TrainLoss=0.3244 | ValLoss=0.6881 | AvgGrad=0.1603\n",
      "Epoch 10/200 | TrainAcc=89.29% | ValAcc=76.68% | TrainLoss=0.2748 | ValLoss=0.7464 | AvgGrad=0.1643\n",
      "Epoch 11/200 | TrainAcc=94.17% | ValAcc=76.65% | TrainLoss=0.1570 | ValLoss=0.9008 | AvgGrad=0.1485\n",
      "Epoch 12/200 | TrainAcc=96.51% | ValAcc=76.05% | TrainLoss=0.0979 | ValLoss=1.0501 | AvgGrad=0.1406\n",
      "Epoch 13/200 | TrainAcc=97.46% | ValAcc=76.37% | TrainLoss=0.0718 | ValLoss=1.2086 | AvgGrad=0.1413\n",
      "Epoch 14/200 | TrainAcc=97.80% | ValAcc=76.13% | TrainLoss=0.0605 | ValLoss=1.2724 | AvgGrad=0.1455\n",
      "Early stopping.\n",
      "Fold 2 Test Accuracy: 69.22%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 1/200 | TrainAcc=63.12% | ValAcc=74.27% | TrainLoss=1.0009 | ValLoss=0.6373 | AvgGrad=0.1460\n",
      "Epoch 2/200 | TrainAcc=75.21% | ValAcc=76.41% | TrainLoss=0.6324 | ValLoss=0.5923 | AvgGrad=0.1258\n",
      "Epoch 3/200 | TrainAcc=77.39% | ValAcc=77.51% | TrainLoss=0.5809 | ValLoss=0.5709 | AvgGrad=0.1239\n",
      "Epoch 4/200 | TrainAcc=78.92% | ValAcc=77.45% | TrainLoss=0.5504 | ValLoss=0.5665 | AvgGrad=0.1282\n",
      "Epoch 5/200 | TrainAcc=80.00% | ValAcc=77.29% | TrainLoss=0.5178 | ValLoss=0.5803 | AvgGrad=0.1294\n",
      "Epoch 6/200 | TrainAcc=81.32% | ValAcc=77.19% | TrainLoss=0.4847 | ValLoss=0.5755 | AvgGrad=0.1361\n",
      "Epoch 7/200 | TrainAcc=82.47% | ValAcc=77.10% | TrainLoss=0.4537 | ValLoss=0.6033 | AvgGrad=0.1410\n",
      "Epoch 8/200 | TrainAcc=83.96% | ValAcc=77.22% | TrainLoss=0.4138 | ValLoss=0.6372 | AvgGrad=0.1485\n",
      "Epoch 9/200 | TrainAcc=85.72% | ValAcc=76.73% | TrainLoss=0.3720 | ValLoss=0.6505 | AvgGrad=0.1543\n",
      "Epoch 10/200 | TrainAcc=87.54% | ValAcc=75.76% | TrainLoss=0.3246 | ValLoss=0.7647 | AvgGrad=0.1582\n",
      "Epoch 11/200 | TrainAcc=91.74% | ValAcc=76.65% | TrainLoss=0.2188 | ValLoss=0.8253 | AvgGrad=0.1499\n",
      "Early stopping.\n",
      "Fold 3 Test Accuracy: 70.23%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 1/200 | TrainAcc=65.80% | ValAcc=75.33% | TrainLoss=0.8989 | ValLoss=0.6225 | AvgGrad=0.1425\n",
      "Epoch 2/200 | TrainAcc=76.14% | ValAcc=77.84% | TrainLoss=0.6082 | ValLoss=0.5634 | AvgGrad=0.1247\n",
      "Epoch 3/200 | TrainAcc=78.24% | ValAcc=77.99% | TrainLoss=0.5554 | ValLoss=0.5631 | AvgGrad=0.1236\n",
      "Epoch 4/200 | TrainAcc=79.74% | ValAcc=78.03% | TrainLoss=0.5235 | ValLoss=0.5604 | AvgGrad=0.1252\n",
      "Epoch 5/200 | TrainAcc=80.97% | ValAcc=78.24% | TrainLoss=0.4897 | ValLoss=0.5647 | AvgGrad=0.1318\n",
      "Epoch 6/200 | TrainAcc=82.05% | ValAcc=76.73% | TrainLoss=0.4581 | ValLoss=0.6126 | AvgGrad=0.1387\n",
      "Epoch 7/200 | TrainAcc=83.75% | ValAcc=77.71% | TrainLoss=0.4135 | ValLoss=0.6118 | AvgGrad=0.1424\n",
      "Epoch 8/200 | TrainAcc=85.54% | ValAcc=77.18% | TrainLoss=0.3680 | ValLoss=0.6434 | AvgGrad=0.1517\n",
      "Epoch 9/200 | TrainAcc=87.39% | ValAcc=76.65% | TrainLoss=0.3247 | ValLoss=0.6912 | AvgGrad=0.1580\n",
      "Epoch 10/200 | TrainAcc=89.44% | ValAcc=76.34% | TrainLoss=0.2731 | ValLoss=0.8171 | AvgGrad=0.1634\n",
      "Epoch 11/200 | TrainAcc=93.93% | ValAcc=76.10% | TrainLoss=0.1626 | ValLoss=0.8779 | AvgGrad=0.1495\n",
      "Epoch 12/200 | TrainAcc=96.38% | ValAcc=75.96% | TrainLoss=0.1009 | ValLoss=1.1193 | AvgGrad=0.1396\n",
      "Epoch 13/200 | TrainAcc=97.33% | ValAcc=76.24% | TrainLoss=0.0748 | ValLoss=1.1979 | AvgGrad=0.1396\n",
      "Early stopping.\n",
      "Fold 4 Test Accuracy: 69.47%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 1/200 | TrainAcc=65.51% | ValAcc=75.41% | TrainLoss=0.9270 | ValLoss=0.6230 | AvgGrad=0.1483\n",
      "Epoch 2/200 | TrainAcc=75.82% | ValAcc=76.69% | TrainLoss=0.6177 | ValLoss=0.5820 | AvgGrad=0.1308\n",
      "Epoch 3/200 | TrainAcc=77.99% | ValAcc=77.00% | TrainLoss=0.5655 | ValLoss=0.5947 | AvgGrad=0.1251\n",
      "Epoch 4/200 | TrainAcc=79.26% | ValAcc=77.98% | TrainLoss=0.5334 | ValLoss=0.5582 | AvgGrad=0.1289\n",
      "Epoch 5/200 | TrainAcc=80.61% | ValAcc=77.02% | TrainLoss=0.5020 | ValLoss=0.5778 | AvgGrad=0.1293\n",
      "Epoch 6/200 | TrainAcc=81.76% | ValAcc=77.62% | TrainLoss=0.4709 | ValLoss=0.5789 | AvgGrad=0.1363\n",
      "Epoch 7/200 | TrainAcc=83.35% | ValAcc=76.58% | TrainLoss=0.4303 | ValLoss=0.6224 | AvgGrad=0.1387\n",
      "Epoch 8/200 | TrainAcc=84.61% | ValAcc=77.21% | TrainLoss=0.3980 | ValLoss=0.6093 | AvgGrad=0.1516\n",
      "Epoch 9/200 | TrainAcc=86.66% | ValAcc=76.05% | TrainLoss=0.3456 | ValLoss=0.6714 | AvgGrad=0.1529\n",
      "Epoch 10/200 | TrainAcc=88.48% | ValAcc=76.63% | TrainLoss=0.2983 | ValLoss=0.7247 | AvgGrad=0.1569\n",
      "Epoch 11/200 | TrainAcc=93.21% | ValAcc=76.16% | TrainLoss=0.1843 | ValLoss=0.8820 | AvgGrad=0.1498\n",
      "Epoch 12/200 | TrainAcc=95.39% | ValAcc=76.00% | TrainLoss=0.1257 | ValLoss=0.9950 | AvgGrad=0.1490\n",
      "Early stopping.\n",
      "Fold 5 Test Accuracy: 69.71%\n",
      "\n",
      "=== Overall Summary ===\n",
      "Val Acc: 76.04 Â± 0.47\n",
      "Test Acc: 69.48 Â± 0.49\n",
      "\n",
      "All results saved in ./training_results\\2025-11-25_19-17-41_wisig_cross_SNR-5dB_fd266_classes_6_ResNet\n",
      "ğŸ‰ Finished SNR=-5 dB, results saved in: ./training_results\\2025-11-25_19-17-41_wisig_cross_SNR-5dB_fd266_classes_6_ResNet\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running experiment with SNR = -10 dB\n",
      "============================================================\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 1/200 | TrainAcc=34.76% | ValAcc=39.76% | TrainLoss=1.6380 | ValLoss=1.4615 | AvgGrad=0.0951\n",
      "Epoch 2/200 | TrainAcc=41.51% | ValAcc=43.48% | TrainLoss=1.4415 | ValLoss=1.3984 | AvgGrad=0.0846\n",
      "Epoch 3/200 | TrainAcc=44.07% | ValAcc=43.50% | TrainLoss=1.3885 | ValLoss=1.3916 | AvgGrad=0.0876\n",
      "Epoch 4/200 | TrainAcc=45.60% | ValAcc=43.67% | TrainLoss=1.3583 | ValLoss=1.3845 | AvgGrad=0.0914\n",
      "Epoch 5/200 | TrainAcc=46.53% | ValAcc=43.65% | TrainLoss=1.3295 | ValLoss=1.3898 | AvgGrad=0.0970\n",
      "Epoch 6/200 | TrainAcc=47.94% | ValAcc=43.41% | TrainLoss=1.2967 | ValLoss=1.4231 | AvgGrad=0.1050\n",
      "Epoch 7/200 | TrainAcc=49.75% | ValAcc=43.42% | TrainLoss=1.2573 | ValLoss=1.4180 | AvgGrad=0.1150\n",
      "Epoch 8/200 | TrainAcc=51.52% | ValAcc=42.96% | TrainLoss=1.2098 | ValLoss=1.4513 | AvgGrad=0.1291\n",
      "Epoch 9/200 | TrainAcc=53.90% | ValAcc=42.70% | TrainLoss=1.1509 | ValLoss=1.4689 | AvgGrad=0.1455\n",
      "Epoch 10/200 | TrainAcc=56.86% | ValAcc=41.92% | TrainLoss=1.0752 | ValLoss=1.6316 | AvgGrad=0.1641\n",
      "Epoch 11/200 | TrainAcc=64.89% | ValAcc=40.62% | TrainLoss=0.8799 | ValLoss=1.7501 | AvgGrad=0.1928\n",
      "Epoch 12/200 | TrainAcc=69.88% | ValAcc=39.47% | TrainLoss=0.7495 | ValLoss=1.9749 | AvgGrad=0.2254\n",
      "Early stopping.\n",
      "Fold 1 Test Accuracy: 36.77%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 1/200 | TrainAcc=34.32% | ValAcc=40.03% | TrainLoss=1.6448 | ValLoss=1.4636 | AvgGrad=0.0930\n",
      "Epoch 2/200 | TrainAcc=41.06% | ValAcc=42.55% | TrainLoss=1.4541 | ValLoss=1.4090 | AvgGrad=0.0829\n",
      "Epoch 3/200 | TrainAcc=43.20% | ValAcc=43.33% | TrainLoss=1.4070 | ValLoss=1.3908 | AvgGrad=0.0857\n",
      "Epoch 4/200 | TrainAcc=44.86% | ValAcc=43.86% | TrainLoss=1.3749 | ValLoss=1.3846 | AvgGrad=0.0892\n",
      "Epoch 5/200 | TrainAcc=45.72% | ValAcc=43.89% | TrainLoss=1.3506 | ValLoss=1.3881 | AvgGrad=0.0932\n",
      "Epoch 6/200 | TrainAcc=47.10% | ValAcc=43.78% | TrainLoss=1.3210 | ValLoss=1.4110 | AvgGrad=0.1000\n",
      "Epoch 7/200 | TrainAcc=48.32% | ValAcc=43.45% | TrainLoss=1.2935 | ValLoss=1.4120 | AvgGrad=0.1086\n",
      "Epoch 8/200 | TrainAcc=49.69% | ValAcc=42.92% | TrainLoss=1.2505 | ValLoss=1.4194 | AvgGrad=0.1203\n",
      "Epoch 9/200 | TrainAcc=51.71% | ValAcc=42.45% | TrainLoss=1.2011 | ValLoss=1.4536 | AvgGrad=0.1333\n",
      "Epoch 10/200 | TrainAcc=53.91% | ValAcc=42.35% | TrainLoss=1.1446 | ValLoss=1.5142 | AvgGrad=0.1500\n",
      "Epoch 11/200 | TrainAcc=60.88% | ValAcc=40.96% | TrainLoss=0.9742 | ValLoss=1.6923 | AvgGrad=0.1783\n",
      "Epoch 12/200 | TrainAcc=65.04% | ValAcc=40.38% | TrainLoss=0.8633 | ValLoss=1.8807 | AvgGrad=0.2092\n",
      "Early stopping.\n",
      "Fold 2 Test Accuracy: 37.59%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 1/200 | TrainAcc=35.27% | ValAcc=41.58% | TrainLoss=1.5955 | ValLoss=1.4424 | AvgGrad=0.0970\n",
      "Epoch 2/200 | TrainAcc=41.90% | ValAcc=43.48% | TrainLoss=1.4400 | ValLoss=1.3967 | AvgGrad=0.0895\n",
      "Epoch 3/200 | TrainAcc=44.00% | ValAcc=44.00% | TrainLoss=1.3940 | ValLoss=1.3818 | AvgGrad=0.0931\n",
      "Epoch 4/200 | TrainAcc=45.09% | ValAcc=44.27% | TrainLoss=1.3689 | ValLoss=1.3831 | AvgGrad=0.0975\n",
      "Epoch 5/200 | TrainAcc=46.30% | ValAcc=43.95% | TrainLoss=1.3369 | ValLoss=1.3820 | AvgGrad=0.1018\n",
      "Epoch 6/200 | TrainAcc=47.61% | ValAcc=43.14% | TrainLoss=1.3059 | ValLoss=1.4041 | AvgGrad=0.1088\n",
      "Epoch 7/200 | TrainAcc=49.31% | ValAcc=43.54% | TrainLoss=1.2677 | ValLoss=1.4205 | AvgGrad=0.1196\n",
      "Epoch 8/200 | TrainAcc=50.92% | ValAcc=42.06% | TrainLoss=1.2199 | ValLoss=1.4504 | AvgGrad=0.1341\n",
      "Epoch 9/200 | TrainAcc=53.50% | ValAcc=41.97% | TrainLoss=1.1592 | ValLoss=1.5030 | AvgGrad=0.1510\n",
      "Epoch 10/200 | TrainAcc=56.29% | ValAcc=40.70% | TrainLoss=1.0882 | ValLoss=1.5631 | AvgGrad=0.1689\n",
      "Epoch 11/200 | TrainAcc=64.06% | ValAcc=39.87% | TrainLoss=0.8934 | ValLoss=1.7821 | AvgGrad=0.1934\n",
      "Epoch 12/200 | TrainAcc=68.89% | ValAcc=38.89% | TrainLoss=0.7706 | ValLoss=2.0522 | AvgGrad=0.2247\n",
      "Early stopping.\n",
      "Fold 3 Test Accuracy: 37.17%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 1/200 | TrainAcc=34.17% | ValAcc=41.71% | TrainLoss=1.6303 | ValLoss=1.4414 | AvgGrad=0.0929\n",
      "Epoch 2/200 | TrainAcc=41.64% | ValAcc=44.20% | TrainLoss=1.4421 | ValLoss=1.3956 | AvgGrad=0.0804\n",
      "Epoch 3/200 | TrainAcc=43.75% | ValAcc=44.84% | TrainLoss=1.3950 | ValLoss=1.3763 | AvgGrad=0.0846\n",
      "Epoch 4/200 | TrainAcc=45.00% | ValAcc=45.47% | TrainLoss=1.3661 | ValLoss=1.3684 | AvgGrad=0.0886\n",
      "Epoch 5/200 | TrainAcc=46.32% | ValAcc=44.78% | TrainLoss=1.3406 | ValLoss=1.3761 | AvgGrad=0.0926\n",
      "Epoch 6/200 | TrainAcc=47.55% | ValAcc=45.04% | TrainLoss=1.3083 | ValLoss=1.3777 | AvgGrad=0.1001\n",
      "Epoch 7/200 | TrainAcc=49.03% | ValAcc=44.62% | TrainLoss=1.2728 | ValLoss=1.4017 | AvgGrad=0.1096\n",
      "Epoch 8/200 | TrainAcc=50.95% | ValAcc=43.74% | TrainLoss=1.2273 | ValLoss=1.4257 | AvgGrad=0.1233\n",
      "Epoch 9/200 | TrainAcc=53.31% | ValAcc=43.73% | TrainLoss=1.1689 | ValLoss=1.4441 | AvgGrad=0.1380\n",
      "Epoch 10/200 | TrainAcc=56.37% | ValAcc=42.48% | TrainLoss=1.0943 | ValLoss=1.5179 | AvgGrad=0.1558\n",
      "Epoch 11/200 | TrainAcc=63.55% | ValAcc=42.32% | TrainLoss=0.9071 | ValLoss=1.7173 | AvgGrad=0.1861\n",
      "Epoch 12/200 | TrainAcc=68.70% | ValAcc=39.92% | TrainLoss=0.7793 | ValLoss=1.8917 | AvgGrad=0.2186\n",
      "Early stopping.\n",
      "Fold 4 Test Accuracy: 37.10%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 1/200 | TrainAcc=33.31% | ValAcc=38.78% | TrainLoss=1.6772 | ValLoss=1.4828 | AvgGrad=0.0944\n",
      "Epoch 2/200 | TrainAcc=40.33% | ValAcc=42.82% | TrainLoss=1.4658 | ValLoss=1.4121 | AvgGrad=0.0766\n",
      "Epoch 3/200 | TrainAcc=42.99% | ValAcc=43.07% | TrainLoss=1.4175 | ValLoss=1.3926 | AvgGrad=0.0833\n",
      "Epoch 4/200 | TrainAcc=44.50% | ValAcc=43.40% | TrainLoss=1.3835 | ValLoss=1.3839 | AvgGrad=0.0849\n",
      "Epoch 5/200 | TrainAcc=45.50% | ValAcc=44.11% | TrainLoss=1.3612 | ValLoss=1.3809 | AvgGrad=0.0897\n",
      "Epoch 6/200 | TrainAcc=46.48% | ValAcc=44.26% | TrainLoss=1.3346 | ValLoss=1.3768 | AvgGrad=0.0951\n",
      "Epoch 7/200 | TrainAcc=47.73% | ValAcc=43.66% | TrainLoss=1.3044 | ValLoss=1.3861 | AvgGrad=0.1030\n",
      "Epoch 8/200 | TrainAcc=49.29% | ValAcc=43.36% | TrainLoss=1.2704 | ValLoss=1.4007 | AvgGrad=0.1139\n",
      "Epoch 9/200 | TrainAcc=51.16% | ValAcc=42.74% | TrainLoss=1.2236 | ValLoss=1.4356 | AvgGrad=0.1266\n",
      "Epoch 10/200 | TrainAcc=53.61% | ValAcc=41.97% | TrainLoss=1.1631 | ValLoss=1.4997 | AvgGrad=0.1418\n",
      "Epoch 11/200 | TrainAcc=59.99% | ValAcc=41.15% | TrainLoss=1.0050 | ValLoss=1.5938 | AvgGrad=0.1686\n",
      "Epoch 12/200 | TrainAcc=64.34% | ValAcc=40.16% | TrainLoss=0.8929 | ValLoss=1.7735 | AvgGrad=0.2015\n",
      "Epoch 13/200 | TrainAcc=68.47% | ValAcc=39.27% | TrainLoss=0.7900 | ValLoss=2.0097 | AvgGrad=0.2280\n",
      "Epoch 14/200 | TrainAcc=72.31% | ValAcc=39.20% | TrainLoss=0.6898 | ValLoss=2.1746 | AvgGrad=0.2518\n",
      "Early stopping.\n",
      "Fold 5 Test Accuracy: 36.21%\n",
      "\n",
      "=== Overall Summary ===\n",
      "Val Acc: 39.57 Â± 0.53\n",
      "Test Acc: 36.97 Â± 0.46\n",
      "\n",
      "All results saved in ./training_results\\2025-11-25_19-30-18_wisig_cross_SNR-10dB_fd266_classes_6_ResNet\n",
      "ğŸ‰ Finished SNR=-10 dB, results saved in: ./training_results\\2025-11-25_19-30-18_wisig_cross_SNR-10dB_fd266_classes_6_ResNet\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running experiment with SNR = -15 dB\n",
      "============================================================\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 1/200 | TrainAcc=19.24% | ValAcc=21.73% | TrainLoss=1.8569 | ValLoss=1.7700 | AvgGrad=0.0433\n",
      "Epoch 2/200 | TrainAcc=22.06% | ValAcc=22.84% | TrainLoss=1.7669 | ValLoss=1.7560 | AvgGrad=0.0295\n",
      "Epoch 3/200 | TrainAcc=22.94% | ValAcc=23.27% | TrainLoss=1.7584 | ValLoss=1.7501 | AvgGrad=0.0329\n",
      "Epoch 4/200 | TrainAcc=23.41% | ValAcc=23.28% | TrainLoss=1.7530 | ValLoss=1.7507 | AvgGrad=0.0368\n",
      "Epoch 5/200 | TrainAcc=23.61% | ValAcc=23.43% | TrainLoss=1.7476 | ValLoss=1.7509 | AvgGrad=0.0404\n",
      "Epoch 6/200 | TrainAcc=24.28% | ValAcc=22.94% | TrainLoss=1.7399 | ValLoss=1.7515 | AvgGrad=0.0451\n",
      "Epoch 7/200 | TrainAcc=25.17% | ValAcc=23.76% | TrainLoss=1.7295 | ValLoss=1.7509 | AvgGrad=0.0511\n",
      "Epoch 8/200 | TrainAcc=25.94% | ValAcc=22.99% | TrainLoss=1.7152 | ValLoss=1.7573 | AvgGrad=0.0601\n",
      "Epoch 9/200 | TrainAcc=27.31% | ValAcc=23.10% | TrainLoss=1.6945 | ValLoss=1.7744 | AvgGrad=0.0694\n",
      "Epoch 10/200 | TrainAcc=28.56% | ValAcc=21.46% | TrainLoss=1.6663 | ValLoss=1.7949 | AvgGrad=0.0830\n",
      "Epoch 11/200 | TrainAcc=32.69% | ValAcc=21.80% | TrainLoss=1.5773 | ValLoss=1.8641 | AvgGrad=0.1118\n",
      "Epoch 12/200 | TrainAcc=36.23% | ValAcc=20.72% | TrainLoss=1.5000 | ValLoss=1.9317 | AvgGrad=0.1445\n",
      "Epoch 13/200 | TrainAcc=38.69% | ValAcc=20.83% | TrainLoss=1.4279 | ValLoss=2.0537 | AvgGrad=0.1727\n",
      "Epoch 14/200 | TrainAcc=41.66% | ValAcc=20.73% | TrainLoss=1.3506 | ValLoss=2.2892 | AvgGrad=0.1983\n",
      "Epoch 15/200 | TrainAcc=44.46% | ValAcc=20.23% | TrainLoss=1.2789 | ValLoss=2.3796 | AvgGrad=0.2226\n",
      "Early stopping.\n",
      "Fold 1 Test Accuracy: 19.60%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 1/200 | TrainAcc=19.79% | ValAcc=22.89% | TrainLoss=1.8519 | ValLoss=1.7627 | AvgGrad=0.0527\n",
      "Epoch 2/200 | TrainAcc=22.78% | ValAcc=23.09% | TrainLoss=1.7615 | ValLoss=1.7605 | AvgGrad=0.0340\n",
      "Epoch 3/200 | TrainAcc=23.42% | ValAcc=23.83% | TrainLoss=1.7547 | ValLoss=1.7502 | AvgGrad=0.0372\n",
      "Epoch 4/200 | TrainAcc=24.10% | ValAcc=23.65% | TrainLoss=1.7472 | ValLoss=1.7530 | AvgGrad=0.0407\n",
      "Epoch 5/200 | TrainAcc=24.54% | ValAcc=23.48% | TrainLoss=1.7399 | ValLoss=1.7534 | AvgGrad=0.0454\n",
      "Epoch 6/200 | TrainAcc=25.42% | ValAcc=23.35% | TrainLoss=1.7290 | ValLoss=1.7545 | AvgGrad=0.0527\n",
      "Epoch 7/200 | TrainAcc=26.63% | ValAcc=23.14% | TrainLoss=1.7131 | ValLoss=1.7661 | AvgGrad=0.0608\n",
      "Epoch 8/200 | TrainAcc=27.85% | ValAcc=22.82% | TrainLoss=1.6911 | ValLoss=1.7746 | AvgGrad=0.0712\n",
      "Epoch 9/200 | TrainAcc=29.63% | ValAcc=22.21% | TrainLoss=1.6599 | ValLoss=1.8052 | AvgGrad=0.0851\n",
      "Epoch 10/200 | TrainAcc=31.79% | ValAcc=21.41% | TrainLoss=1.6204 | ValLoss=1.8395 | AvgGrad=0.1030\n",
      "Epoch 11/200 | TrainAcc=37.31% | ValAcc=21.15% | TrainLoss=1.4949 | ValLoss=1.9862 | AvgGrad=0.1348\n",
      "Early stopping.\n",
      "Fold 2 Test Accuracy: 21.02%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 1/200 | TrainAcc=19.18% | ValAcc=22.63% | TrainLoss=1.8312 | ValLoss=1.7658 | AvgGrad=0.0448\n",
      "Epoch 2/200 | TrainAcc=21.78% | ValAcc=23.02% | TrainLoss=1.7681 | ValLoss=1.7582 | AvgGrad=0.0314\n",
      "Epoch 3/200 | TrainAcc=22.82% | ValAcc=23.54% | TrainLoss=1.7588 | ValLoss=1.7545 | AvgGrad=0.0351\n",
      "Epoch 4/200 | TrainAcc=23.22% | ValAcc=24.13% | TrainLoss=1.7536 | ValLoss=1.7478 | AvgGrad=0.0389\n",
      "Epoch 5/200 | TrainAcc=23.64% | ValAcc=23.95% | TrainLoss=1.7464 | ValLoss=1.7466 | AvgGrad=0.0425\n",
      "Epoch 6/200 | TrainAcc=24.54% | ValAcc=23.85% | TrainLoss=1.7377 | ValLoss=1.7472 | AvgGrad=0.0479\n",
      "Epoch 7/200 | TrainAcc=25.07% | ValAcc=23.74% | TrainLoss=1.7256 | ValLoss=1.7535 | AvgGrad=0.0563\n",
      "Epoch 8/200 | TrainAcc=26.22% | ValAcc=23.03% | TrainLoss=1.7090 | ValLoss=1.7585 | AvgGrad=0.0650\n",
      "Epoch 9/200 | TrainAcc=27.55% | ValAcc=22.97% | TrainLoss=1.6866 | ValLoss=1.7812 | AvgGrad=0.0752\n",
      "Epoch 10/200 | TrainAcc=29.25% | ValAcc=22.76% | TrainLoss=1.6539 | ValLoss=1.8037 | AvgGrad=0.0903\n",
      "Epoch 11/200 | TrainAcc=33.72% | ValAcc=22.12% | TrainLoss=1.5582 | ValLoss=1.8688 | AvgGrad=0.1161\n",
      "Epoch 12/200 | TrainAcc=36.94% | ValAcc=21.24% | TrainLoss=1.4785 | ValLoss=1.9264 | AvgGrad=0.1447\n",
      "Early stopping.\n",
      "Fold 3 Test Accuracy: 20.54%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 1/200 | TrainAcc=18.61% | ValAcc=22.26% | TrainLoss=1.8473 | ValLoss=1.7670 | AvgGrad=0.0390\n",
      "Epoch 2/200 | TrainAcc=21.60% | ValAcc=22.82% | TrainLoss=1.7688 | ValLoss=1.7621 | AvgGrad=0.0265\n",
      "Epoch 3/200 | TrainAcc=22.01% | ValAcc=23.25% | TrainLoss=1.7618 | ValLoss=1.7586 | AvgGrad=0.0303\n",
      "Epoch 4/200 | TrainAcc=22.24% | ValAcc=23.52% | TrainLoss=1.7580 | ValLoss=1.7545 | AvgGrad=0.0329\n",
      "Epoch 5/200 | TrainAcc=22.85% | ValAcc=23.77% | TrainLoss=1.7516 | ValLoss=1.7569 | AvgGrad=0.0362\n",
      "Epoch 6/200 | TrainAcc=23.07% | ValAcc=23.78% | TrainLoss=1.7448 | ValLoss=1.7528 | AvgGrad=0.0402\n",
      "Epoch 7/200 | TrainAcc=23.82% | ValAcc=23.21% | TrainLoss=1.7342 | ValLoss=1.7560 | AvgGrad=0.0461\n",
      "Epoch 8/200 | TrainAcc=24.42% | ValAcc=23.16% | TrainLoss=1.7227 | ValLoss=1.7608 | AvgGrad=0.0535\n",
      "Epoch 9/200 | TrainAcc=25.10% | ValAcc=22.92% | TrainLoss=1.7059 | ValLoss=1.7680 | AvgGrad=0.0627\n",
      "Epoch 10/200 | TrainAcc=26.41% | ValAcc=22.56% | TrainLoss=1.6839 | ValLoss=1.7820 | AvgGrad=0.0728\n",
      "Epoch 11/200 | TrainAcc=29.48% | ValAcc=22.25% | TrainLoss=1.6136 | ValLoss=1.8548 | AvgGrad=0.0957\n",
      "Epoch 12/200 | TrainAcc=31.79% | ValAcc=22.15% | TrainLoss=1.5546 | ValLoss=1.9176 | AvgGrad=0.1235\n",
      "Epoch 13/200 | TrainAcc=34.05% | ValAcc=21.51% | TrainLoss=1.4950 | ValLoss=1.9916 | AvgGrad=0.1469\n",
      "Early stopping.\n",
      "Fold 4 Test Accuracy: 20.55%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 1/200 | TrainAcc=20.29% | ValAcc=22.79% | TrainLoss=1.8472 | ValLoss=1.7606 | AvgGrad=0.0511\n",
      "Epoch 2/200 | TrainAcc=23.07% | ValAcc=22.71% | TrainLoss=1.7584 | ValLoss=1.7592 | AvgGrad=0.0335\n",
      "Epoch 3/200 | TrainAcc=23.86% | ValAcc=23.75% | TrainLoss=1.7508 | ValLoss=1.7494 | AvgGrad=0.0365\n",
      "Epoch 4/200 | TrainAcc=24.51% | ValAcc=23.95% | TrainLoss=1.7437 | ValLoss=1.7497 | AvgGrad=0.0409\n",
      "Epoch 5/200 | TrainAcc=25.10% | ValAcc=24.60% | TrainLoss=1.7350 | ValLoss=1.7453 | AvgGrad=0.0452\n",
      "Epoch 6/200 | TrainAcc=25.75% | ValAcc=23.79% | TrainLoss=1.7246 | ValLoss=1.7554 | AvgGrad=0.0512\n",
      "Epoch 7/200 | TrainAcc=26.85% | ValAcc=23.27% | TrainLoss=1.7079 | ValLoss=1.7603 | AvgGrad=0.0600\n",
      "Epoch 8/200 | TrainAcc=28.17% | ValAcc=22.70% | TrainLoss=1.6872 | ValLoss=1.7819 | AvgGrad=0.0702\n",
      "Epoch 9/200 | TrainAcc=30.11% | ValAcc=22.47% | TrainLoss=1.6545 | ValLoss=1.7842 | AvgGrad=0.0854\n",
      "Epoch 10/200 | TrainAcc=32.30% | ValAcc=23.00% | TrainLoss=1.6106 | ValLoss=1.8241 | AvgGrad=0.1024\n",
      "Epoch 11/200 | TrainAcc=38.77% | ValAcc=21.95% | TrainLoss=1.4704 | ValLoss=1.9761 | AvgGrad=0.1435\n",
      "Epoch 12/200 | TrainAcc=43.81% | ValAcc=20.78% | TrainLoss=1.3569 | ValLoss=2.1038 | AvgGrad=0.1873\n",
      "Epoch 13/200 | TrainAcc=48.71% | ValAcc=20.89% | TrainLoss=1.2384 | ValLoss=2.3744 | AvgGrad=0.2247\n",
      "Early stopping.\n",
      "Fold 5 Test Accuracy: 20.37%\n",
      "\n",
      "=== Overall Summary ===\n",
      "Val Acc: 21.00 Â± 0.43\n",
      "Test Acc: 20.42 Â± 0.46\n",
      "\n",
      "All results saved in ./training_results\\2025-11-25_19-42-26_wisig_cross_SNR-15dB_fd266_classes_6_ResNet\n",
      "ğŸ‰ Finished SNR=-15 dB, results saved in: ./training_results\\2025-11-25_19-42-26_wisig_cross_SNR-15dB_fd266_classes_6_ResNet\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running experiment with SNR = -20 dB\n",
      "============================================================\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.96% | ValAcc=17.18% | TrainLoss=1.8541 | ValLoss=1.7912 | AvgGrad=0.0421\n",
      "Epoch 2/200 | TrainAcc=17.78% | ValAcc=17.55% | TrainLoss=1.7905 | ValLoss=1.7898 | AvgGrad=0.0117\n",
      "Epoch 3/200 | TrainAcc=17.73% | ValAcc=17.49% | TrainLoss=1.7898 | ValLoss=1.7908 | AvgGrad=0.0142\n",
      "Epoch 4/200 | TrainAcc=18.09% | ValAcc=17.27% | TrainLoss=1.7884 | ValLoss=1.7922 | AvgGrad=0.0166\n",
      "Epoch 5/200 | TrainAcc=18.04% | ValAcc=17.37% | TrainLoss=1.7881 | ValLoss=1.7903 | AvgGrad=0.0177\n",
      "Epoch 6/200 | TrainAcc=18.08% | ValAcc=16.05% | TrainLoss=1.7872 | ValLoss=1.7917 | AvgGrad=0.0196\n",
      "Epoch 7/200 | TrainAcc=18.34% | ValAcc=17.66% | TrainLoss=1.7856 | ValLoss=1.7910 | AvgGrad=0.0219\n",
      "Epoch 8/200 | TrainAcc=18.84% | ValAcc=17.83% | TrainLoss=1.7836 | ValLoss=1.7942 | AvgGrad=0.0251\n",
      "Epoch 9/200 | TrainAcc=18.96% | ValAcc=17.34% | TrainLoss=1.7797 | ValLoss=1.7934 | AvgGrad=0.0297\n",
      "Epoch 10/200 | TrainAcc=19.65% | ValAcc=16.92% | TrainLoss=1.7739 | ValLoss=1.7944 | AvgGrad=0.0348\n",
      "Epoch 11/200 | TrainAcc=21.24% | ValAcc=17.49% | TrainLoss=1.7484 | ValLoss=1.8118 | AvgGrad=0.0513\n",
      "Epoch 12/200 | TrainAcc=22.72% | ValAcc=17.19% | TrainLoss=1.7218 | ValLoss=1.8426 | AvgGrad=0.0682\n",
      "Epoch 13/200 | TrainAcc=24.05% | ValAcc=17.17% | TrainLoss=1.6897 | ValLoss=1.8477 | AvgGrad=0.0851\n",
      "Epoch 14/200 | TrainAcc=25.43% | ValAcc=17.15% | TrainLoss=1.6532 | ValLoss=1.9169 | AvgGrad=0.1006\n",
      "Epoch 15/200 | TrainAcc=26.80% | ValAcc=17.24% | TrainLoss=1.6120 | ValLoss=1.9476 | AvgGrad=0.1187\n",
      "Epoch 16/200 | TrainAcc=28.15% | ValAcc=17.23% | TrainLoss=1.5713 | ValLoss=2.0423 | AvgGrad=0.1314\n",
      "Early stopping.\n",
      "Fold 1 Test Accuracy: 16.94%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.45% | ValAcc=16.88% | TrainLoss=1.8552 | ValLoss=1.7917 | AvgGrad=0.0312\n",
      "Epoch 2/200 | TrainAcc=16.53% | ValAcc=16.24% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0038\n",
      "Epoch 3/200 | TrainAcc=16.52% | ValAcc=16.50% | TrainLoss=1.7919 | ValLoss=1.7919 | AvgGrad=0.0026\n",
      "Epoch 4/200 | TrainAcc=16.63% | ValAcc=16.15% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0024\n",
      "Epoch 5/200 | TrainAcc=16.65% | ValAcc=16.16% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0023\n",
      "Epoch 6/200 | TrainAcc=16.98% | ValAcc=16.20% | TrainLoss=1.7916 | ValLoss=1.7918 | AvgGrad=0.0039\n",
      "Epoch 7/200 | TrainAcc=16.98% | ValAcc=16.09% | TrainLoss=1.7918 | ValLoss=1.7920 | AvgGrad=0.0038\n",
      "Epoch 8/200 | TrainAcc=16.71% | ValAcc=16.09% | TrainLoss=1.7918 | ValLoss=1.7920 | AvgGrad=0.0013\n",
      "Epoch 9/200 | TrainAcc=16.55% | ValAcc=16.09% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0014\n",
      "Early stopping.\n",
      "Fold 2 Test Accuracy: 16.67%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.57% | ValAcc=17.31% | TrainLoss=1.8447 | ValLoss=1.7915 | AvgGrad=0.0297\n",
      "Epoch 2/200 | TrainAcc=17.21% | ValAcc=17.68% | TrainLoss=1.7915 | ValLoss=1.7915 | AvgGrad=0.0072\n",
      "Epoch 3/200 | TrainAcc=17.36% | ValAcc=17.53% | TrainLoss=1.7911 | ValLoss=1.7915 | AvgGrad=0.0076\n",
      "Epoch 4/200 | TrainAcc=17.85% | ValAcc=17.29% | TrainLoss=1.7904 | ValLoss=1.7911 | AvgGrad=0.0100\n",
      "Epoch 5/200 | TrainAcc=17.67% | ValAcc=17.02% | TrainLoss=1.7903 | ValLoss=1.7906 | AvgGrad=0.0116\n",
      "Epoch 6/200 | TrainAcc=17.91% | ValAcc=16.52% | TrainLoss=1.7896 | ValLoss=1.7914 | AvgGrad=0.0133\n",
      "Epoch 7/200 | TrainAcc=17.97% | ValAcc=16.71% | TrainLoss=1.7889 | ValLoss=1.7913 | AvgGrad=0.0148\n",
      "Epoch 8/200 | TrainAcc=18.21% | ValAcc=17.14% | TrainLoss=1.7877 | ValLoss=1.7906 | AvgGrad=0.0166\n",
      "Epoch 9/200 | TrainAcc=18.47% | ValAcc=17.05% | TrainLoss=1.7859 | ValLoss=1.7915 | AvgGrad=0.0194\n",
      "Epoch 10/200 | TrainAcc=18.84% | ValAcc=16.93% | TrainLoss=1.7839 | ValLoss=1.7923 | AvgGrad=0.0225\n",
      "Early stopping.\n",
      "Fold 3 Test Accuracy: 17.04%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.76% | ValAcc=18.18% | TrainLoss=1.8613 | ValLoss=1.7914 | AvgGrad=0.0342\n",
      "Epoch 2/200 | TrainAcc=17.11% | ValAcc=17.26% | TrainLoss=1.7916 | ValLoss=1.7916 | AvgGrad=0.0077\n",
      "Epoch 3/200 | TrainAcc=17.48% | ValAcc=17.38% | TrainLoss=1.7912 | ValLoss=1.7911 | AvgGrad=0.0082\n",
      "Epoch 4/200 | TrainAcc=17.73% | ValAcc=17.40% | TrainLoss=1.7902 | ValLoss=1.7907 | AvgGrad=0.0102\n",
      "Epoch 5/200 | TrainAcc=17.80% | ValAcc=17.21% | TrainLoss=1.7901 | ValLoss=1.7914 | AvgGrad=0.0119\n",
      "Epoch 6/200 | TrainAcc=17.63% | ValAcc=18.53% | TrainLoss=1.7895 | ValLoss=1.7925 | AvgGrad=0.0122\n",
      "Epoch 7/200 | TrainAcc=17.77% | ValAcc=17.40% | TrainLoss=1.7892 | ValLoss=1.7912 | AvgGrad=0.0141\n",
      "Epoch 8/200 | TrainAcc=18.12% | ValAcc=17.43% | TrainLoss=1.7879 | ValLoss=1.7958 | AvgGrad=0.0158\n",
      "Epoch 9/200 | TrainAcc=18.29% | ValAcc=17.48% | TrainLoss=1.7865 | ValLoss=1.7919 | AvgGrad=0.0185\n",
      "Epoch 10/200 | TrainAcc=18.86% | ValAcc=17.47% | TrainLoss=1.7840 | ValLoss=1.7936 | AvgGrad=0.0209\n",
      "Epoch 11/200 | TrainAcc=19.91% | ValAcc=18.09% | TrainLoss=1.7745 | ValLoss=1.7994 | AvgGrad=0.0285\n",
      "Epoch 12/200 | TrainAcc=20.42% | ValAcc=17.29% | TrainLoss=1.7656 | ValLoss=1.7994 | AvgGrad=0.0384\n",
      "Epoch 13/200 | TrainAcc=21.59% | ValAcc=17.45% | TrainLoss=1.7518 | ValLoss=1.8187 | AvgGrad=0.0492\n",
      "Epoch 14/200 | TrainAcc=22.52% | ValAcc=18.01% | TrainLoss=1.7359 | ValLoss=1.8600 | AvgGrad=0.0621\n",
      "Early stopping.\n",
      "Fold 4 Test Accuracy: 17.22%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.74% | ValAcc=16.30% | TrainLoss=1.8591 | ValLoss=1.7921 | AvgGrad=0.0300\n",
      "Epoch 2/200 | TrainAcc=16.87% | ValAcc=17.04% | TrainLoss=1.7915 | ValLoss=1.7914 | AvgGrad=0.0056\n",
      "Epoch 3/200 | TrainAcc=17.22% | ValAcc=16.30% | TrainLoss=1.7910 | ValLoss=1.7916 | AvgGrad=0.0089\n",
      "Epoch 4/200 | TrainAcc=17.34% | ValAcc=16.65% | TrainLoss=1.7905 | ValLoss=1.7911 | AvgGrad=0.0102\n",
      "Epoch 5/200 | TrainAcc=17.42% | ValAcc=17.15% | TrainLoss=1.7902 | ValLoss=1.7912 | AvgGrad=0.0111\n",
      "Epoch 6/200 | TrainAcc=17.76% | ValAcc=16.80% | TrainLoss=1.7895 | ValLoss=1.7935 | AvgGrad=0.0140\n",
      "Epoch 7/200 | TrainAcc=17.62% | ValAcc=17.73% | TrainLoss=1.7886 | ValLoss=1.7915 | AvgGrad=0.0153\n",
      "Epoch 8/200 | TrainAcc=17.78% | ValAcc=17.75% | TrainLoss=1.7878 | ValLoss=1.7930 | AvgGrad=0.0164\n",
      "Epoch 9/200 | TrainAcc=17.99% | ValAcc=18.09% | TrainLoss=1.7871 | ValLoss=1.7922 | AvgGrad=0.0172\n",
      "Epoch 10/200 | TrainAcc=18.24% | ValAcc=17.85% | TrainLoss=1.7853 | ValLoss=1.7924 | AvgGrad=0.0201\n",
      "Epoch 11/200 | TrainAcc=19.13% | ValAcc=18.06% | TrainLoss=1.7757 | ValLoss=1.7973 | AvgGrad=0.0274\n",
      "Epoch 12/200 | TrainAcc=19.38% | ValAcc=17.53% | TrainLoss=1.7686 | ValLoss=1.7986 | AvgGrad=0.0356\n",
      "Epoch 13/200 | TrainAcc=20.12% | ValAcc=17.58% | TrainLoss=1.7572 | ValLoss=1.8099 | AvgGrad=0.0437\n",
      "Epoch 14/200 | TrainAcc=20.72% | ValAcc=17.99% | TrainLoss=1.7455 | ValLoss=1.8303 | AvgGrad=0.0515\n",
      "Epoch 15/200 | TrainAcc=21.19% | ValAcc=17.47% | TrainLoss=1.7348 | ValLoss=1.8229 | AvgGrad=0.0587\n",
      "Epoch 16/200 | TrainAcc=21.39% | ValAcc=17.43% | TrainLoss=1.7219 | ValLoss=1.8469 | AvgGrad=0.0644\n",
      "Epoch 17/200 | TrainAcc=21.94% | ValAcc=17.69% | TrainLoss=1.7110 | ValLoss=1.8597 | AvgGrad=0.0712\n",
      "Early stopping.\n",
      "Fold 5 Test Accuracy: 17.00%\n",
      "\n",
      "=== Overall Summary ===\n",
      "Val Acc: 17.19 Â± 0.67\n",
      "Test Acc: 16.97 Â± 0.18\n",
      "\n",
      "All results saved in ./training_results\\2025-11-25_19-54-55_wisig_cross_SNR-20dB_fd266_classes_6_ResNet\n",
      "ğŸ‰ Finished SNR=-20 dB, results saved in: ./training_results\\2025-11-25_19-54-55_wisig_cross_SNR-20dB_fd266_classes_6_ResNet\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running experiment with SNR = -25 dB\n",
      "============================================================\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.53% | ValAcc=16.72% | TrainLoss=1.8824 | ValLoss=1.7920 | AvgGrad=0.0387\n",
      "Epoch 2/200 | TrainAcc=16.84% | ValAcc=16.38% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0045\n",
      "Epoch 3/200 | TrainAcc=16.82% | ValAcc=16.64% | TrainLoss=1.7916 | ValLoss=1.7919 | AvgGrad=0.0051\n",
      "Epoch 4/200 | TrainAcc=16.88% | ValAcc=16.75% | TrainLoss=1.7915 | ValLoss=1.7919 | AvgGrad=0.0058\n",
      "Epoch 5/200 | TrainAcc=17.05% | ValAcc=16.67% | TrainLoss=1.7915 | ValLoss=1.7919 | AvgGrad=0.0070\n",
      "Epoch 6/200 | TrainAcc=17.29% | ValAcc=16.73% | TrainLoss=1.7910 | ValLoss=1.7919 | AvgGrad=0.0080\n",
      "Epoch 7/200 | TrainAcc=17.32% | ValAcc=16.19% | TrainLoss=1.7910 | ValLoss=1.7920 | AvgGrad=0.0089\n",
      "Epoch 8/200 | TrainAcc=16.88% | ValAcc=15.96% | TrainLoss=1.7913 | ValLoss=1.7919 | AvgGrad=0.0063\n",
      "Epoch 9/200 | TrainAcc=16.68% | ValAcc=16.05% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0028\n",
      "Early stopping.\n",
      "Fold 1 Test Accuracy: 16.64%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.36% | ValAcc=16.63% | TrainLoss=1.8568 | ValLoss=1.7920 | AvgGrad=0.0372\n",
      "Epoch 2/200 | TrainAcc=16.74% | ValAcc=16.61% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0054\n",
      "Epoch 3/200 | TrainAcc=16.76% | ValAcc=17.12% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0039\n",
      "Epoch 4/200 | TrainAcc=16.87% | ValAcc=16.05% | TrainLoss=1.7917 | ValLoss=1.7921 | AvgGrad=0.0063\n",
      "Epoch 5/200 | TrainAcc=16.86% | ValAcc=16.98% | TrainLoss=1.7915 | ValLoss=1.7921 | AvgGrad=0.0063\n",
      "Epoch 6/200 | TrainAcc=16.85% | ValAcc=16.84% | TrainLoss=1.7914 | ValLoss=1.7926 | AvgGrad=0.0075\n",
      "Epoch 7/200 | TrainAcc=16.88% | ValAcc=16.10% | TrainLoss=1.7914 | ValLoss=1.7920 | AvgGrad=0.0045\n",
      "Epoch 8/200 | TrainAcc=16.62% | ValAcc=16.13% | TrainLoss=1.7916 | ValLoss=1.7920 | AvgGrad=0.0036\n",
      "Epoch 9/200 | TrainAcc=16.68% | ValAcc=16.65% | TrainLoss=1.7915 | ValLoss=1.7920 | AvgGrad=0.0059\n",
      "Epoch 10/200 | TrainAcc=16.75% | ValAcc=16.25% | TrainLoss=1.7915 | ValLoss=1.7924 | AvgGrad=0.0065\n",
      "Epoch 11/200 | TrainAcc=17.17% | ValAcc=16.67% | TrainLoss=1.7902 | ValLoss=1.7929 | AvgGrad=0.0104\n",
      "Early stopping.\n",
      "Fold 2 Test Accuracy: 16.56%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.56% | ValAcc=16.47% | TrainLoss=1.8561 | ValLoss=1.7919 | AvgGrad=0.0360\n",
      "Epoch 2/200 | TrainAcc=16.66% | ValAcc=16.49% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0036\n",
      "Epoch 3/200 | TrainAcc=16.59% | ValAcc=16.50% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0041\n",
      "Epoch 4/200 | TrainAcc=16.71% | ValAcc=16.03% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0014\n",
      "Epoch 5/200 | TrainAcc=16.69% | ValAcc=16.03% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0014\n",
      "Epoch 6/200 | TrainAcc=16.61% | ValAcc=16.03% | TrainLoss=1.7917 | ValLoss=1.7919 | AvgGrad=0.0026\n",
      "Epoch 7/200 | TrainAcc=16.80% | ValAcc=16.46% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0026\n",
      "Epoch 8/200 | TrainAcc=16.48% | ValAcc=16.03% | TrainLoss=1.7917 | ValLoss=1.7919 | AvgGrad=0.0038\n",
      "Epoch 9/200 | TrainAcc=16.74% | ValAcc=16.50% | TrainLoss=1.7917 | ValLoss=1.7919 | AvgGrad=0.0024\n",
      "Early stopping.\n",
      "Fold 3 Test Accuracy: 16.67%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.62% | ValAcc=16.12% | TrainLoss=1.8515 | ValLoss=1.7921 | AvgGrad=0.0366\n",
      "Epoch 2/200 | TrainAcc=16.79% | ValAcc=15.73% | TrainLoss=1.7918 | ValLoss=1.7920 | AvgGrad=0.0026\n",
      "Epoch 3/200 | TrainAcc=16.86% | ValAcc=16.41% | TrainLoss=1.7918 | ValLoss=1.7920 | AvgGrad=0.0027\n",
      "Epoch 4/200 | TrainAcc=16.77% | ValAcc=15.89% | TrainLoss=1.7918 | ValLoss=1.7920 | AvgGrad=0.0025\n",
      "Epoch 5/200 | TrainAcc=16.92% | ValAcc=16.17% | TrainLoss=1.7917 | ValLoss=1.7922 | AvgGrad=0.0038\n",
      "Epoch 6/200 | TrainAcc=16.89% | ValAcc=15.90% | TrainLoss=1.7919 | ValLoss=1.7918 | AvgGrad=0.0050\n",
      "Epoch 7/200 | TrainAcc=17.12% | ValAcc=16.13% | TrainLoss=1.7915 | ValLoss=1.7918 | AvgGrad=0.0054\n",
      "Epoch 8/200 | TrainAcc=16.87% | ValAcc=15.81% | TrainLoss=1.7916 | ValLoss=1.7919 | AvgGrad=0.0055\n",
      "Epoch 9/200 | TrainAcc=17.25% | ValAcc=15.94% | TrainLoss=1.7913 | ValLoss=1.7919 | AvgGrad=0.0075\n",
      "Epoch 10/200 | TrainAcc=17.22% | ValAcc=16.12% | TrainLoss=1.7912 | ValLoss=1.7921 | AvgGrad=0.0077\n",
      "Epoch 11/200 | TrainAcc=17.53% | ValAcc=16.11% | TrainLoss=1.7902 | ValLoss=1.7940 | AvgGrad=0.0103\n",
      "Early stopping.\n",
      "Fold 4 Test Accuracy: 16.74%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.72% | ValAcc=16.50% | TrainLoss=1.8560 | ValLoss=1.7917 | AvgGrad=0.0356\n",
      "Epoch 2/200 | TrainAcc=16.70% | ValAcc=16.49% | TrainLoss=1.7918 | ValLoss=1.7918 | AvgGrad=0.0027\n",
      "Epoch 3/200 | TrainAcc=16.40% | ValAcc=16.49% | TrainLoss=1.7918 | ValLoss=1.7918 | AvgGrad=0.0028\n",
      "Epoch 4/200 | TrainAcc=16.69% | ValAcc=16.52% | TrainLoss=1.7917 | ValLoss=1.7918 | AvgGrad=0.0042\n",
      "Epoch 5/200 | TrainAcc=16.75% | ValAcc=16.41% | TrainLoss=1.7917 | ValLoss=1.7918 | AvgGrad=0.0040\n",
      "Epoch 6/200 | TrainAcc=16.94% | ValAcc=16.58% | TrainLoss=1.7916 | ValLoss=1.7917 | AvgGrad=0.0050\n",
      "Epoch 7/200 | TrainAcc=16.91% | ValAcc=17.34% | TrainLoss=1.7913 | ValLoss=1.7917 | AvgGrad=0.0062\n",
      "Epoch 8/200 | TrainAcc=16.88% | ValAcc=16.85% | TrainLoss=1.7914 | ValLoss=1.7918 | AvgGrad=0.0056\n",
      "Epoch 9/200 | TrainAcc=16.86% | ValAcc=16.56% | TrainLoss=1.7914 | ValLoss=1.7917 | AvgGrad=0.0060\n",
      "Epoch 10/200 | TrainAcc=17.02% | ValAcc=16.39% | TrainLoss=1.7913 | ValLoss=1.7919 | AvgGrad=0.0059\n",
      "Epoch 11/200 | TrainAcc=16.84% | ValAcc=16.32% | TrainLoss=1.7915 | ValLoss=1.7921 | AvgGrad=0.0040\n",
      "Epoch 12/200 | TrainAcc=16.82% | ValAcc=16.31% | TrainLoss=1.7915 | ValLoss=1.7921 | AvgGrad=0.0051\n",
      "Epoch 13/200 | TrainAcc=17.01% | ValAcc=16.37% | TrainLoss=1.7910 | ValLoss=1.7928 | AvgGrad=0.0065\n",
      "Epoch 14/200 | TrainAcc=16.92% | ValAcc=16.18% | TrainLoss=1.7909 | ValLoss=1.7922 | AvgGrad=0.0071\n",
      "Epoch 15/200 | TrainAcc=17.09% | ValAcc=16.33% | TrainLoss=1.7903 | ValLoss=1.7935 | AvgGrad=0.0087\n",
      "Early stopping.\n",
      "Fold 5 Test Accuracy: 16.69%\n",
      "\n",
      "=== Overall Summary ===\n",
      "Val Acc: 16.33 Â± 0.23\n",
      "Test Acc: 16.66 Â± 0.06\n",
      "\n",
      "All results saved in ./training_results\\2025-11-25_20-07-43_wisig_cross_SNR-25dB_fd266_classes_6_ResNet\n",
      "ğŸ‰ Finished SNR=-25 dB, results saved in: ./training_results\\2025-11-25_20-07-43_wisig_cross_SNR-25dB_fd266_classes_6_ResNet\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running experiment with SNR = -30 dB\n",
      "============================================================\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.90% | ValAcc=16.05% | TrainLoss=1.8422 | ValLoss=1.7919 | AvgGrad=0.0359\n",
      "Epoch 2/200 | TrainAcc=16.91% | ValAcc=16.99% | TrainLoss=1.7918 | ValLoss=1.7921 | AvgGrad=0.0067\n",
      "Epoch 3/200 | TrainAcc=16.89% | ValAcc=16.76% | TrainLoss=1.7916 | ValLoss=1.7918 | AvgGrad=0.0065\n",
      "Epoch 4/200 | TrainAcc=17.26% | ValAcc=16.67% | TrainLoss=1.7914 | ValLoss=1.7921 | AvgGrad=0.0076\n",
      "Epoch 5/200 | TrainAcc=17.41% | ValAcc=17.17% | TrainLoss=1.7913 | ValLoss=1.7922 | AvgGrad=0.0092\n",
      "Epoch 6/200 | TrainAcc=17.37% | ValAcc=16.54% | TrainLoss=1.7912 | ValLoss=1.7922 | AvgGrad=0.0082\n",
      "Epoch 7/200 | TrainAcc=17.46% | ValAcc=16.93% | TrainLoss=1.7910 | ValLoss=1.7922 | AvgGrad=0.0099\n",
      "Epoch 8/200 | TrainAcc=17.65% | ValAcc=16.89% | TrainLoss=1.7906 | ValLoss=1.7927 | AvgGrad=0.0111\n",
      "Epoch 9/200 | TrainAcc=17.57% | ValAcc=16.64% | TrainLoss=1.7905 | ValLoss=1.7924 | AvgGrad=0.0123\n",
      "Epoch 10/200 | TrainAcc=17.84% | ValAcc=16.84% | TrainLoss=1.7894 | ValLoss=1.7929 | AvgGrad=0.0141\n",
      "Epoch 11/200 | TrainAcc=18.79% | ValAcc=16.88% | TrainLoss=1.7843 | ValLoss=1.7951 | AvgGrad=0.0203\n",
      "Epoch 12/200 | TrainAcc=19.78% | ValAcc=16.83% | TrainLoss=1.7771 | ValLoss=1.7937 | AvgGrad=0.0300\n",
      "Epoch 13/200 | TrainAcc=20.58% | ValAcc=16.69% | TrainLoss=1.7668 | ValLoss=1.8177 | AvgGrad=0.0392\n",
      "Early stopping.\n",
      "Fold 1 Test Accuracy: 16.67%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.72% | ValAcc=16.59% | TrainLoss=1.8583 | ValLoss=1.7918 | AvgGrad=0.0360\n",
      "Epoch 2/200 | TrainAcc=16.66% | ValAcc=16.11% | TrainLoss=1.7918 | ValLoss=1.7918 | AvgGrad=0.0040\n",
      "Epoch 3/200 | TrainAcc=16.86% | ValAcc=16.89% | TrainLoss=1.7918 | ValLoss=1.7921 | AvgGrad=0.0042\n",
      "Epoch 4/200 | TrainAcc=16.82% | ValAcc=16.06% | TrainLoss=1.7917 | ValLoss=1.7919 | AvgGrad=0.0035\n",
      "Epoch 5/200 | TrainAcc=16.73% | ValAcc=16.08% | TrainLoss=1.7917 | ValLoss=1.7919 | AvgGrad=0.0045\n",
      "Epoch 6/200 | TrainAcc=17.01% | ValAcc=16.28% | TrainLoss=1.7915 | ValLoss=1.7926 | AvgGrad=0.0046\n",
      "Epoch 7/200 | TrainAcc=16.94% | ValAcc=16.01% | TrainLoss=1.7916 | ValLoss=1.7920 | AvgGrad=0.0059\n",
      "Epoch 8/200 | TrainAcc=16.87% | ValAcc=15.68% | TrainLoss=1.7915 | ValLoss=1.7926 | AvgGrad=0.0051\n",
      "Epoch 9/200 | TrainAcc=16.98% | ValAcc=16.09% | TrainLoss=1.7913 | ValLoss=1.7920 | AvgGrad=0.0059\n",
      "Epoch 10/200 | TrainAcc=16.86% | ValAcc=16.21% | TrainLoss=1.7917 | ValLoss=1.7921 | AvgGrad=0.0031\n",
      "Epoch 11/200 | TrainAcc=17.29% | ValAcc=16.12% | TrainLoss=1.7907 | ValLoss=1.7930 | AvgGrad=0.0088\n",
      "Early stopping.\n",
      "Fold 2 Test Accuracy: 16.68%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.59% | ValAcc=16.61% | TrainLoss=1.8330 | ValLoss=1.7920 | AvgGrad=0.0341\n",
      "Epoch 2/200 | TrainAcc=16.82% | ValAcc=16.64% | TrainLoss=1.7917 | ValLoss=1.7920 | AvgGrad=0.0067\n",
      "Epoch 3/200 | TrainAcc=16.99% | ValAcc=16.50% | TrainLoss=1.7915 | ValLoss=1.7920 | AvgGrad=0.0068\n",
      "Epoch 4/200 | TrainAcc=16.99% | ValAcc=16.52% | TrainLoss=1.7914 | ValLoss=1.7920 | AvgGrad=0.0070\n",
      "Epoch 5/200 | TrainAcc=17.16% | ValAcc=16.27% | TrainLoss=1.7913 | ValLoss=1.7922 | AvgGrad=0.0081\n",
      "Epoch 6/200 | TrainAcc=17.01% | ValAcc=16.54% | TrainLoss=1.7913 | ValLoss=1.7921 | AvgGrad=0.0086\n",
      "Epoch 7/200 | TrainAcc=17.08% | ValAcc=16.59% | TrainLoss=1.7910 | ValLoss=1.7920 | AvgGrad=0.0086\n",
      "Epoch 8/200 | TrainAcc=17.16% | ValAcc=16.51% | TrainLoss=1.7908 | ValLoss=1.7927 | AvgGrad=0.0103\n",
      "Epoch 9/200 | TrainAcc=17.54% | ValAcc=16.74% | TrainLoss=1.7901 | ValLoss=1.7936 | AvgGrad=0.0122\n",
      "Epoch 10/200 | TrainAcc=17.53% | ValAcc=16.62% | TrainLoss=1.7891 | ValLoss=1.7925 | AvgGrad=0.0147\n",
      "Epoch 11/200 | TrainAcc=18.25% | ValAcc=16.99% | TrainLoss=1.7842 | ValLoss=1.8017 | AvgGrad=0.0209\n",
      "Epoch 12/200 | TrainAcc=18.79% | ValAcc=16.60% | TrainLoss=1.7783 | ValLoss=1.8029 | AvgGrad=0.0284\n",
      "Epoch 13/200 | TrainAcc=19.36% | ValAcc=16.58% | TrainLoss=1.7703 | ValLoss=1.8036 | AvgGrad=0.0366\n",
      "Epoch 14/200 | TrainAcc=19.79% | ValAcc=16.84% | TrainLoss=1.7600 | ValLoss=1.8114 | AvgGrad=0.0449\n",
      "Epoch 15/200 | TrainAcc=20.52% | ValAcc=16.61% | TrainLoss=1.7464 | ValLoss=1.8122 | AvgGrad=0.0536\n",
      "Epoch 16/200 | TrainAcc=21.17% | ValAcc=16.74% | TrainLoss=1.7327 | ValLoss=1.8112 | AvgGrad=0.0633\n",
      "Epoch 17/200 | TrainAcc=21.60% | ValAcc=16.59% | TrainLoss=1.7210 | ValLoss=1.8229 | AvgGrad=0.0685\n",
      "Epoch 18/200 | TrainAcc=22.01% | ValAcc=16.66% | TrainLoss=1.7070 | ValLoss=1.8819 | AvgGrad=0.0734\n",
      "Epoch 19/200 | TrainAcc=22.30% | ValAcc=16.54% | TrainLoss=1.6983 | ValLoss=1.8956 | AvgGrad=0.0796\n",
      "Early stopping.\n",
      "Fold 3 Test Accuracy: 16.73%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.64% | ValAcc=17.12% | TrainLoss=1.8655 | ValLoss=1.7917 | AvgGrad=0.0378\n",
      "Epoch 2/200 | TrainAcc=16.61% | ValAcc=16.74% | TrainLoss=1.7919 | ValLoss=1.7922 | AvgGrad=0.0045\n",
      "Epoch 3/200 | TrainAcc=16.86% | ValAcc=16.67% | TrainLoss=1.7917 | ValLoss=1.7926 | AvgGrad=0.0050\n",
      "Epoch 4/200 | TrainAcc=16.71% | ValAcc=15.88% | TrainLoss=1.7914 | ValLoss=1.7924 | AvgGrad=0.0053\n",
      "Epoch 5/200 | TrainAcc=16.78% | ValAcc=15.79% | TrainLoss=1.7916 | ValLoss=1.7921 | AvgGrad=0.0049\n",
      "Epoch 6/200 | TrainAcc=17.00% | ValAcc=15.76% | TrainLoss=1.7915 | ValLoss=1.7922 | AvgGrad=0.0042\n",
      "Epoch 7/200 | TrainAcc=17.05% | ValAcc=15.82% | TrainLoss=1.7913 | ValLoss=1.7924 | AvgGrad=0.0054\n",
      "Epoch 8/200 | TrainAcc=16.98% | ValAcc=16.03% | TrainLoss=1.7911 | ValLoss=1.7926 | AvgGrad=0.0074\n",
      "Epoch 9/200 | TrainAcc=16.99% | ValAcc=15.70% | TrainLoss=1.7912 | ValLoss=1.7922 | AvgGrad=0.0064\n",
      "Early stopping.\n",
      "Fold 4 Test Accuracy: 16.60%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.76% | ValAcc=16.65% | TrainLoss=1.8546 | ValLoss=1.7920 | AvgGrad=0.0279\n",
      "Epoch 2/200 | TrainAcc=16.61% | ValAcc=16.77% | TrainLoss=1.7918 | ValLoss=1.7920 | AvgGrad=0.0032\n",
      "Epoch 3/200 | TrainAcc=16.46% | ValAcc=16.49% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0014\n",
      "Epoch 4/200 | TrainAcc=16.54% | ValAcc=16.50% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0015\n",
      "Epoch 5/200 | TrainAcc=16.35% | ValAcc=16.31% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0015\n",
      "Epoch 6/200 | TrainAcc=16.55% | ValAcc=16.30% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0019\n",
      "Epoch 7/200 | TrainAcc=16.50% | ValAcc=16.33% | TrainLoss=1.7918 | ValLoss=1.7920 | AvgGrad=0.0015\n",
      "Epoch 8/200 | TrainAcc=16.55% | ValAcc=16.30% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0013\n",
      "Epoch 9/200 | TrainAcc=16.62% | ValAcc=16.33% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0018\n",
      "Epoch 10/200 | TrainAcc=16.75% | ValAcc=16.54% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0018\n",
      "Early stopping.\n",
      "Fold 5 Test Accuracy: 16.65%\n",
      "\n",
      "=== Overall Summary ===\n",
      "Val Acc: 16.32 Â± 0.36\n",
      "Test Acc: 16.67 Â± 0.04\n",
      "\n",
      "All results saved in ./training_results\\2025-11-25_20-18-32_wisig_cross_SNR-30dB_fd266_classes_6_ResNet\n",
      "ğŸ‰ Finished SNR=-30 dB, results saved in: ./training_results\\2025-11-25_20-18-32_wisig_cross_SNR-30dB_fd266_classes_6_ResNet\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running experiment with SNR = -35 dB\n",
      "============================================================\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.61% | ValAcc=16.09% | TrainLoss=1.8610 | ValLoss=1.7922 | AvgGrad=0.0371\n",
      "Epoch 2/200 | TrainAcc=16.69% | ValAcc=16.05% | TrainLoss=1.7918 | ValLoss=1.7922 | AvgGrad=0.0037\n",
      "Epoch 3/200 | TrainAcc=16.80% | ValAcc=16.17% | TrainLoss=1.7916 | ValLoss=1.7919 | AvgGrad=0.0055\n",
      "Epoch 4/200 | TrainAcc=16.93% | ValAcc=16.74% | TrainLoss=1.7915 | ValLoss=1.7921 | AvgGrad=0.0055\n",
      "Epoch 5/200 | TrainAcc=16.99% | ValAcc=16.75% | TrainLoss=1.7915 | ValLoss=1.7921 | AvgGrad=0.0064\n",
      "Epoch 6/200 | TrainAcc=17.29% | ValAcc=16.48% | TrainLoss=1.7909 | ValLoss=1.7930 | AvgGrad=0.0079\n",
      "Epoch 7/200 | TrainAcc=16.77% | ValAcc=16.79% | TrainLoss=1.7912 | ValLoss=1.7919 | AvgGrad=0.0068\n",
      "Epoch 8/200 | TrainAcc=16.98% | ValAcc=16.11% | TrainLoss=1.7910 | ValLoss=1.7926 | AvgGrad=0.0074\n",
      "Epoch 9/200 | TrainAcc=17.08% | ValAcc=16.84% | TrainLoss=1.7907 | ValLoss=1.7931 | AvgGrad=0.0092\n",
      "Epoch 10/200 | TrainAcc=17.01% | ValAcc=16.68% | TrainLoss=1.7904 | ValLoss=1.7923 | AvgGrad=0.0095\n",
      "Epoch 11/200 | TrainAcc=17.08% | ValAcc=17.03% | TrainLoss=1.7886 | ValLoss=1.7937 | AvgGrad=0.0126\n",
      "Epoch 12/200 | TrainAcc=17.61% | ValAcc=16.93% | TrainLoss=1.7863 | ValLoss=1.7952 | AvgGrad=0.0178\n",
      "Epoch 13/200 | TrainAcc=17.94% | ValAcc=16.74% | TrainLoss=1.7842 | ValLoss=1.7953 | AvgGrad=0.0219\n",
      "Epoch 14/200 | TrainAcc=18.46% | ValAcc=16.76% | TrainLoss=1.7792 | ValLoss=1.7969 | AvgGrad=0.0264\n",
      "Epoch 15/200 | TrainAcc=18.71% | ValAcc=16.67% | TrainLoss=1.7745 | ValLoss=1.8004 | AvgGrad=0.0314\n",
      "Epoch 16/200 | TrainAcc=19.07% | ValAcc=16.78% | TrainLoss=1.7667 | ValLoss=1.8029 | AvgGrad=0.0369\n",
      "Epoch 17/200 | TrainAcc=19.28% | ValAcc=16.72% | TrainLoss=1.7610 | ValLoss=1.8042 | AvgGrad=0.0427\n",
      "Epoch 18/200 | TrainAcc=19.58% | ValAcc=16.60% | TrainLoss=1.7525 | ValLoss=1.8131 | AvgGrad=0.0480\n",
      "Epoch 19/200 | TrainAcc=19.94% | ValAcc=16.58% | TrainLoss=1.7438 | ValLoss=1.8199 | AvgGrad=0.0511\n",
      "Early stopping.\n",
      "Fold 1 Test Accuracy: 16.68%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.79% | ValAcc=16.46% | TrainLoss=1.8732 | ValLoss=1.7918 | AvgGrad=0.0419\n",
      "Epoch 2/200 | TrainAcc=16.69% | ValAcc=16.74% | TrainLoss=1.7918 | ValLoss=1.7918 | AvgGrad=0.0050\n",
      "Epoch 3/200 | TrainAcc=16.57% | ValAcc=16.42% | TrainLoss=1.7917 | ValLoss=1.7920 | AvgGrad=0.0052\n",
      "Epoch 4/200 | TrainAcc=17.16% | ValAcc=16.60% | TrainLoss=1.7916 | ValLoss=1.7922 | AvgGrad=0.0065\n",
      "Epoch 5/200 | TrainAcc=17.08% | ValAcc=16.50% | TrainLoss=1.7915 | ValLoss=1.7919 | AvgGrad=0.0060\n",
      "Epoch 6/200 | TrainAcc=17.03% | ValAcc=16.21% | TrainLoss=1.7913 | ValLoss=1.7924 | AvgGrad=0.0072\n",
      "Epoch 7/200 | TrainAcc=17.25% | ValAcc=16.79% | TrainLoss=1.7911 | ValLoss=1.7921 | AvgGrad=0.0077\n",
      "Epoch 8/200 | TrainAcc=17.18% | ValAcc=16.43% | TrainLoss=1.7909 | ValLoss=1.7922 | AvgGrad=0.0085\n",
      "Epoch 9/200 | TrainAcc=17.21% | ValAcc=16.38% | TrainLoss=1.7907 | ValLoss=1.7926 | AvgGrad=0.0100\n",
      "Epoch 10/200 | TrainAcc=16.98% | ValAcc=16.41% | TrainLoss=1.7910 | ValLoss=1.7922 | AvgGrad=0.0089\n",
      "Early stopping.\n",
      "Fold 2 Test Accuracy: 16.60%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.61% | ValAcc=16.94% | TrainLoss=1.8615 | ValLoss=1.7920 | AvgGrad=0.0340\n",
      "Epoch 2/200 | TrainAcc=16.67% | ValAcc=16.99% | TrainLoss=1.7918 | ValLoss=1.7918 | AvgGrad=0.0045\n",
      "Epoch 3/200 | TrainAcc=16.95% | ValAcc=16.07% | TrainLoss=1.7917 | ValLoss=1.7921 | AvgGrad=0.0044\n",
      "Epoch 4/200 | TrainAcc=17.04% | ValAcc=16.07% | TrainLoss=1.7917 | ValLoss=1.7922 | AvgGrad=0.0049\n",
      "Epoch 5/200 | TrainAcc=16.84% | ValAcc=16.03% | TrainLoss=1.7917 | ValLoss=1.7921 | AvgGrad=0.0047\n",
      "Epoch 6/200 | TrainAcc=16.56% | ValAcc=16.05% | TrainLoss=1.7917 | ValLoss=1.7919 | AvgGrad=0.0039\n",
      "Epoch 7/200 | TrainAcc=16.57% | ValAcc=16.89% | TrainLoss=1.7915 | ValLoss=1.7920 | AvgGrad=0.0048\n",
      "Epoch 8/200 | TrainAcc=16.91% | ValAcc=16.04% | TrainLoss=1.7916 | ValLoss=1.7922 | AvgGrad=0.0051\n",
      "Epoch 9/200 | TrainAcc=16.46% | ValAcc=16.03% | TrainLoss=1.7916 | ValLoss=1.7921 | AvgGrad=0.0052\n",
      "Early stopping.\n",
      "Fold 3 Test Accuracy: 16.67%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.64% | ValAcc=16.95% | TrainLoss=1.8726 | ValLoss=1.7918 | AvgGrad=0.0379\n",
      "Epoch 2/200 | TrainAcc=16.77% | ValAcc=16.73% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0037\n",
      "Epoch 3/200 | TrainAcc=16.95% | ValAcc=16.30% | TrainLoss=1.7917 | ValLoss=1.7919 | AvgGrad=0.0043\n",
      "Epoch 4/200 | TrainAcc=17.07% | ValAcc=16.40% | TrainLoss=1.7916 | ValLoss=1.7919 | AvgGrad=0.0046\n",
      "Epoch 5/200 | TrainAcc=17.06% | ValAcc=16.27% | TrainLoss=1.7916 | ValLoss=1.7923 | AvgGrad=0.0052\n",
      "Epoch 6/200 | TrainAcc=17.01% | ValAcc=16.40% | TrainLoss=1.7914 | ValLoss=1.7922 | AvgGrad=0.0062\n",
      "Epoch 7/200 | TrainAcc=17.03% | ValAcc=16.36% | TrainLoss=1.7912 | ValLoss=1.7920 | AvgGrad=0.0077\n",
      "Epoch 8/200 | TrainAcc=16.99% | ValAcc=16.39% | TrainLoss=1.7914 | ValLoss=1.7921 | AvgGrad=0.0057\n",
      "Epoch 9/200 | TrainAcc=17.03% | ValAcc=16.44% | TrainLoss=1.7912 | ValLoss=1.7928 | AvgGrad=0.0072\n",
      "Early stopping.\n",
      "Fold 4 Test Accuracy: 16.77%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.61% | ValAcc=16.86% | TrainLoss=1.8667 | ValLoss=1.7917 | AvgGrad=0.0355\n",
      "Epoch 2/200 | TrainAcc=16.55% | ValAcc=16.34% | TrainLoss=1.7917 | ValLoss=1.7919 | AvgGrad=0.0048\n",
      "Epoch 3/200 | TrainAcc=17.09% | ValAcc=16.48% | TrainLoss=1.7916 | ValLoss=1.7921 | AvgGrad=0.0060\n",
      "Epoch 4/200 | TrainAcc=16.68% | ValAcc=16.45% | TrainLoss=1.7918 | ValLoss=1.7918 | AvgGrad=0.0050\n",
      "Epoch 5/200 | TrainAcc=16.66% | ValAcc=16.76% | TrainLoss=1.7917 | ValLoss=1.7918 | AvgGrad=0.0035\n",
      "Epoch 6/200 | TrainAcc=16.74% | ValAcc=16.62% | TrainLoss=1.7917 | ValLoss=1.7920 | AvgGrad=0.0050\n",
      "Epoch 7/200 | TrainAcc=16.75% | ValAcc=16.48% | TrainLoss=1.7916 | ValLoss=1.7920 | AvgGrad=0.0051\n",
      "Epoch 8/200 | TrainAcc=16.97% | ValAcc=16.37% | TrainLoss=1.7912 | ValLoss=1.7923 | AvgGrad=0.0068\n",
      "Epoch 9/200 | TrainAcc=17.05% | ValAcc=16.28% | TrainLoss=1.7914 | ValLoss=1.7919 | AvgGrad=0.0059\n",
      "Early stopping.\n",
      "Fold 5 Test Accuracy: 16.70%\n",
      "\n",
      "=== Overall Summary ===\n",
      "Val Acc: 16.35 Â± 0.19\n",
      "Test Acc: 16.68 Â± 0.06\n",
      "\n",
      "All results saved in ./training_results\\2025-11-25_20-30-36_wisig_cross_SNR-35dB_fd266_classes_6_ResNet\n",
      "ğŸ‰ Finished SNR=-35 dB, results saved in: ./training_results\\2025-11-25_20-30-36_wisig_cross_SNR-35dB_fd266_classes_6_ResNet\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running experiment with SNR = -40 dB\n",
      "============================================================\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.63% | ValAcc=16.80% | TrainLoss=1.8578 | ValLoss=1.7918 | AvgGrad=0.0346\n",
      "Epoch 2/200 | TrainAcc=16.46% | ValAcc=16.94% | TrainLoss=1.7918 | ValLoss=1.7918 | AvgGrad=0.0027\n",
      "Epoch 3/200 | TrainAcc=16.63% | ValAcc=16.72% | TrainLoss=1.7918 | ValLoss=1.7918 | AvgGrad=0.0040\n",
      "Epoch 4/200 | TrainAcc=16.78% | ValAcc=16.11% | TrainLoss=1.7918 | ValLoss=1.7918 | AvgGrad=0.0032\n",
      "Epoch 5/200 | TrainAcc=16.61% | ValAcc=16.28% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0035\n",
      "Epoch 6/200 | TrainAcc=16.89% | ValAcc=16.52% | TrainLoss=1.7916 | ValLoss=1.7918 | AvgGrad=0.0051\n",
      "Epoch 7/200 | TrainAcc=16.90% | ValAcc=16.48% | TrainLoss=1.7914 | ValLoss=1.7921 | AvgGrad=0.0054\n",
      "Epoch 8/200 | TrainAcc=17.01% | ValAcc=16.63% | TrainLoss=1.7915 | ValLoss=1.7919 | AvgGrad=0.0062\n",
      "Epoch 9/200 | TrainAcc=16.92% | ValAcc=16.71% | TrainLoss=1.7914 | ValLoss=1.7921 | AvgGrad=0.0064\n",
      "Epoch 10/200 | TrainAcc=16.86% | ValAcc=16.05% | TrainLoss=1.7915 | ValLoss=1.7918 | AvgGrad=0.0052\n",
      "Early stopping.\n",
      "Fold 1 Test Accuracy: 16.67%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.88% | ValAcc=16.50% | TrainLoss=1.8596 | ValLoss=1.7918 | AvgGrad=0.0293\n",
      "Epoch 2/200 | TrainAcc=16.58% | ValAcc=16.50% | TrainLoss=1.7918 | ValLoss=1.7918 | AvgGrad=0.0032\n",
      "Epoch 3/200 | TrainAcc=16.76% | ValAcc=16.09% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0029\n",
      "Epoch 4/200 | TrainAcc=16.92% | ValAcc=15.94% | TrainLoss=1.7917 | ValLoss=1.7921 | AvgGrad=0.0042\n",
      "Epoch 5/200 | TrainAcc=16.76% | ValAcc=16.48% | TrainLoss=1.7916 | ValLoss=1.7919 | AvgGrad=0.0050\n",
      "Epoch 6/200 | TrainAcc=16.87% | ValAcc=16.09% | TrainLoss=1.7915 | ValLoss=1.7920 | AvgGrad=0.0054\n",
      "Epoch 7/200 | TrainAcc=16.83% | ValAcc=16.48% | TrainLoss=1.7915 | ValLoss=1.7919 | AvgGrad=0.0056\n",
      "Epoch 8/200 | TrainAcc=16.75% | ValAcc=16.65% | TrainLoss=1.7917 | ValLoss=1.7930 | AvgGrad=0.0020\n",
      "Epoch 9/200 | TrainAcc=16.83% | ValAcc=16.49% | TrainLoss=1.7914 | ValLoss=1.7927 | AvgGrad=0.0077\n",
      "Epoch 10/200 | TrainAcc=16.94% | ValAcc=16.56% | TrainLoss=1.7914 | ValLoss=1.7923 | AvgGrad=0.0084\n",
      "Epoch 11/200 | TrainAcc=17.03% | ValAcc=16.59% | TrainLoss=1.7905 | ValLoss=1.7923 | AvgGrad=0.0096\n",
      "Epoch 12/200 | TrainAcc=17.37% | ValAcc=16.74% | TrainLoss=1.7893 | ValLoss=1.7931 | AvgGrad=0.0136\n",
      "Epoch 13/200 | TrainAcc=17.58% | ValAcc=16.74% | TrainLoss=1.7880 | ValLoss=1.7932 | AvgGrad=0.0162\n",
      "Epoch 14/200 | TrainAcc=17.73% | ValAcc=16.54% | TrainLoss=1.7868 | ValLoss=1.7936 | AvgGrad=0.0173\n",
      "Epoch 15/200 | TrainAcc=18.01% | ValAcc=16.62% | TrainLoss=1.7834 | ValLoss=1.7993 | AvgGrad=0.0220\n",
      "Epoch 16/200 | TrainAcc=18.23% | ValAcc=16.54% | TrainLoss=1.7794 | ValLoss=1.8024 | AvgGrad=0.0271\n",
      "Early stopping.\n",
      "Fold 2 Test Accuracy: 16.60%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.57% | ValAcc=16.76% | TrainLoss=1.8555 | ValLoss=1.7918 | AvgGrad=0.0353\n",
      "Epoch 2/200 | TrainAcc=16.84% | ValAcc=16.82% | TrainLoss=1.7919 | ValLoss=1.7917 | AvgGrad=0.0055\n",
      "Epoch 3/200 | TrainAcc=16.75% | ValAcc=16.19% | TrainLoss=1.7917 | ValLoss=1.7919 | AvgGrad=0.0051\n",
      "Epoch 4/200 | TrainAcc=16.97% | ValAcc=15.92% | TrainLoss=1.7915 | ValLoss=1.7927 | AvgGrad=0.0059\n",
      "Epoch 5/200 | TrainAcc=17.02% | ValAcc=16.34% | TrainLoss=1.7913 | ValLoss=1.7924 | AvgGrad=0.0070\n",
      "Epoch 6/200 | TrainAcc=16.96% | ValAcc=16.57% | TrainLoss=1.7912 | ValLoss=1.7922 | AvgGrad=0.0060\n",
      "Epoch 7/200 | TrainAcc=16.84% | ValAcc=16.31% | TrainLoss=1.7913 | ValLoss=1.7924 | AvgGrad=0.0059\n",
      "Epoch 8/200 | TrainAcc=17.05% | ValAcc=16.59% | TrainLoss=1.7909 | ValLoss=1.7924 | AvgGrad=0.0083\n",
      "Epoch 9/200 | TrainAcc=16.83% | ValAcc=16.78% | TrainLoss=1.7909 | ValLoss=1.7920 | AvgGrad=0.0082\n",
      "Early stopping.\n",
      "Fold 3 Test Accuracy: 16.66%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.48% | ValAcc=16.15% | TrainLoss=1.8784 | ValLoss=1.7923 | AvgGrad=0.0379\n",
      "Epoch 2/200 | TrainAcc=16.86% | ValAcc=16.08% | TrainLoss=1.7918 | ValLoss=1.7919 | AvgGrad=0.0049\n",
      "Epoch 3/200 | TrainAcc=16.95% | ValAcc=16.15% | TrainLoss=1.7917 | ValLoss=1.7918 | AvgGrad=0.0045\n",
      "Epoch 4/200 | TrainAcc=17.11% | ValAcc=16.39% | TrainLoss=1.7916 | ValLoss=1.7921 | AvgGrad=0.0063\n",
      "Epoch 5/200 | TrainAcc=17.09% | ValAcc=15.81% | TrainLoss=1.7914 | ValLoss=1.7921 | AvgGrad=0.0067\n",
      "Epoch 6/200 | TrainAcc=17.56% | ValAcc=16.13% | TrainLoss=1.7911 | ValLoss=1.7920 | AvgGrad=0.0088\n",
      "Epoch 7/200 | TrainAcc=17.36% | ValAcc=16.62% | TrainLoss=1.7912 | ValLoss=1.7922 | AvgGrad=0.0091\n",
      "Epoch 8/200 | TrainAcc=17.53% | ValAcc=15.87% | TrainLoss=1.7906 | ValLoss=1.7928 | AvgGrad=0.0104\n",
      "Epoch 9/200 | TrainAcc=17.59% | ValAcc=17.35% | TrainLoss=1.7901 | ValLoss=1.7935 | AvgGrad=0.0123\n",
      "Epoch 10/200 | TrainAcc=17.30% | ValAcc=17.12% | TrainLoss=1.7902 | ValLoss=1.7918 | AvgGrad=0.0123\n",
      "Epoch 11/200 | TrainAcc=18.36% | ValAcc=16.95% | TrainLoss=1.7870 | ValLoss=1.7938 | AvgGrad=0.0167\n",
      "Epoch 12/200 | TrainAcc=19.18% | ValAcc=16.46% | TrainLoss=1.7819 | ValLoss=1.8018 | AvgGrad=0.0259\n",
      "Epoch 13/200 | TrainAcc=19.73% | ValAcc=17.17% | TrainLoss=1.7734 | ValLoss=1.7988 | AvgGrad=0.0353\n",
      "Epoch 14/200 | TrainAcc=20.82% | ValAcc=16.88% | TrainLoss=1.7613 | ValLoss=1.8044 | AvgGrad=0.0475\n",
      "Epoch 15/200 | TrainAcc=22.00% | ValAcc=16.69% | TrainLoss=1.7412 | ValLoss=1.8196 | AvgGrad=0.0606\n",
      "Epoch 16/200 | TrainAcc=23.18% | ValAcc=16.63% | TrainLoss=1.7172 | ValLoss=1.8501 | AvgGrad=0.0767\n",
      "Epoch 17/200 | TrainAcc=24.62% | ValAcc=16.82% | TrainLoss=1.6847 | ValLoss=1.8961 | AvgGrad=0.0947\n",
      "Early stopping.\n",
      "Fold 4 Test Accuracy: 16.66%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 1/200 | TrainAcc=16.75% | ValAcc=16.35% | TrainLoss=1.8487 | ValLoss=1.7919 | AvgGrad=0.0371\n",
      "Epoch 2/200 | TrainAcc=17.11% | ValAcc=16.70% | TrainLoss=1.7916 | ValLoss=1.7919 | AvgGrad=0.0055\n",
      "Epoch 3/200 | TrainAcc=17.23% | ValAcc=16.53% | TrainLoss=1.7915 | ValLoss=1.7925 | AvgGrad=0.0068\n",
      "Epoch 4/200 | TrainAcc=17.26% | ValAcc=16.23% | TrainLoss=1.7915 | ValLoss=1.7922 | AvgGrad=0.0074\n",
      "Epoch 5/200 | TrainAcc=17.24% | ValAcc=16.37% | TrainLoss=1.7913 | ValLoss=1.7920 | AvgGrad=0.0060\n",
      "Epoch 6/200 | TrainAcc=17.27% | ValAcc=16.53% | TrainLoss=1.7913 | ValLoss=1.7924 | AvgGrad=0.0078\n",
      "Epoch 7/200 | TrainAcc=17.35% | ValAcc=16.61% | TrainLoss=1.7910 | ValLoss=1.7928 | AvgGrad=0.0091\n",
      "Epoch 8/200 | TrainAcc=17.00% | ValAcc=16.24% | TrainLoss=1.7914 | ValLoss=1.7921 | AvgGrad=0.0051\n",
      "Epoch 9/200 | TrainAcc=17.25% | ValAcc=16.56% | TrainLoss=1.7911 | ValLoss=1.7923 | AvgGrad=0.0081\n",
      "Epoch 10/200 | TrainAcc=17.36% | ValAcc=16.48% | TrainLoss=1.7909 | ValLoss=1.7921 | AvgGrad=0.0098\n",
      "Early stopping.\n",
      "Fold 5 Test Accuracy: 16.61%\n",
      "\n",
      "=== Overall Summary ===\n",
      "Val Acc: 16.54 Â± 0.28\n",
      "Test Acc: 16.64 Â± 0.03\n",
      "\n",
      "All results saved in ./training_results\\2025-11-25_20-41-37_wisig_cross_SNR-40dB_fd266_classes_6_ResNet\n",
      "ğŸ‰ Finished SNR=-40 dB, results saved in: ./training_results\\2025-11-25_20-41-37_wisig_cross_SNR-40dB_fd266_classes_6_ResNet\n",
      "\n",
      "\n",
      "================ FINAL SUMMARY ================\n",
      "SNR  20 dB â†’ results in: ./training_results\\2025-11-25_18-05-13_wisig_cross_SNR20dB_fd266_classes_6_ResNet\n",
      "SNR  15 dB â†’ results in: ./training_results\\2025-11-25_18-17-07_wisig_cross_SNR15dB_fd266_classes_6_ResNet\n",
      "SNR  10 dB â†’ results in: ./training_results\\2025-11-25_18-31-16_wisig_cross_SNR10dB_fd266_classes_6_ResNet\n",
      "SNR   5 dB â†’ results in: ./training_results\\2025-11-25_18-45-27_wisig_cross_SNR5dB_fd266_classes_6_ResNet\n",
      "SNR   0 dB â†’ results in: ./training_results\\2025-11-25_19-01-49_wisig_cross_SNR0dB_fd266_classes_6_ResNet\n",
      "SNR  -5 dB â†’ results in: ./training_results\\2025-11-25_19-17-41_wisig_cross_SNR-5dB_fd266_classes_6_ResNet\n",
      "SNR -10 dB â†’ results in: ./training_results\\2025-11-25_19-30-18_wisig_cross_SNR-10dB_fd266_classes_6_ResNet\n",
      "SNR -15 dB â†’ results in: ./training_results\\2025-11-25_19-42-26_wisig_cross_SNR-15dB_fd266_classes_6_ResNet\n",
      "SNR -20 dB â†’ results in: ./training_results\\2025-11-25_19-54-55_wisig_cross_SNR-20dB_fd266_classes_6_ResNet\n",
      "SNR -25 dB â†’ results in: ./training_results\\2025-11-25_20-07-43_wisig_cross_SNR-25dB_fd266_classes_6_ResNet\n",
      "SNR -30 dB â†’ results in: ./training_results\\2025-11-25_20-18-32_wisig_cross_SNR-30dB_fd266_classes_6_ResNet\n",
      "SNR -35 dB â†’ results in: ./training_results\\2025-11-25_20-30-36_wisig_cross_SNR-35dB_fd266_classes_6_ResNet\n",
      "SNR -40 dB â†’ results in: ./training_results\\2025-11-25_20-41-37_wisig_cross_SNR-40dB_fd266_classes_6_ResNet\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "# è·¨æ—¥æœŸ ResNet18 å¾ªç¯SNR 6ä¸ªè®¾å¤‡\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from  data_utilities import *\n",
    "import cv2  # OpenCV ç”¨äºè°ƒæ•´å›¾åƒå¤§å°å’Œé¢œè‰²å¤„ç†\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gc  # å¼•å…¥åƒåœ¾å›æ”¶æ¨¡å—\n",
    "from tqdm.auto import tqdm  # è‡ªåŠ¨é€‚é…ç¯å¢ƒ å¯¼å…¥tqdmè¿›åº¦æ¡åº“\n",
    "from collections import defaultdict\n",
    "\n",
    "dataset_name = 'ManySig'\n",
    "dataset_path='../ManySig.pkl/'\n",
    "\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path,dataset_name)\n",
    "\n",
    "print(\"æ•°æ®é›†å‘å°„æœºæ•°é‡ï¼š\",len(compact_dataset['tx_list']),\"å…·ä½“ä¸ºï¼š\",compact_dataset['tx_list'])\n",
    "print(\"æ•°æ®é›†æ¥æ”¶æœºæ•°é‡ï¼š\",len(compact_dataset['rx_list']),\"å…·ä½“ä¸ºï¼š\",compact_dataset['rx_list'])\n",
    "print(\"æ•°æ®é›†é‡‡é›†å¤©æ•°ï¼š\",len(compact_dataset['capture_date_list']),\"å…·ä½“ä¸ºï¼š\",compact_dataset['capture_date_list'])\n",
    "tx_list = compact_dataset['tx_list']\n",
    "rx_list = compact_dataset['rx_list']\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "\n",
    "\n",
    "n_tx = len(tx_list)\n",
    "n_rx = len(rx_list)\n",
    "print(n_tx,n_rx)\n",
    "\n",
    "\n",
    "# å‚æ•°è®¾ç½®\n",
    "equalized = 0\n",
    "max_sig = None          # æ¯ä¸ª TX-RX-æ—¥æœŸæœ€å¤šä½¿ç”¨çš„ä¿¡å·æ•°\n",
    "block_size = 240        # æ¯ä¸ª block çš„ä¿¡å·æ•°\n",
    "y = 5                  # æ‹¼æ¥æ—¶æ¯ç»„å¤šå°‘æ¡ä¿¡å·\n",
    "\n",
    "train_dates = ['2021_03_15']  # è®¾å®šè®­ç»ƒæ—¥æœŸ\n",
    "test_dates  = ['2021_03_01']  # è®¾å®šæµ‹è¯•æ—¥æœŸ\n",
    "# è°ƒç”¨å‡½æ•°\n",
    "# è°ƒç”¨å‡½æ•°\n",
    "X_train, y_train, X_test, y_test = preprocess_dataset_cross_IQ_blocks_single_date_per_rx_cyclic(\n",
    "    compact_dataset=compact_dataset,\n",
    "    tx_list=tx_list,\n",
    "    train_dates=train_dates,\n",
    "    test_dates=test_dates,\n",
    "    max_sig=max_sig,        # æ¯ä¸ª RX æœ€å¤šå–å¤šå°‘æ¡ä¿¡å·ï¼Œå¯é€‰\n",
    "    equalized=equalized,        # æ˜¯å¦ä½¿ç”¨å‡è¡¡åŒ–ä¿¡å·\n",
    "    block_size=block_size,     # æ¯ä¸ª block çš„æ ·æœ¬æ•°\n",
    "    y=y                # æ¯æ¬¡å¾ªç¯ä» RX æŠ½å–çš„ä¿¡å·æ•°\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test  shape:\", X_test.shape) \n",
    "print(\"y_test  shape:\", y_test.shape)\n",
    "\n",
    "# ---------- PyTorch & è®­ç»ƒé…ç½® ----------\n",
    "import torch\n",
    "torch._dynamo.disable()\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pywt  # ä¿ç•™ï¼Œè‹¥éœ€æ‰©å±•\n",
    "import math\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# æ•°æ®å¢å¼º / ä¿¡é“è®¾ç½®ï¼ˆç›®å‰é»˜è®¤å…³é—­ï¼‰\n",
    "USE_LOG = True\n",
    "FS = 20e6\n",
    "FC = 2.4e9\n",
    "SNR_DB = 20\n",
    "VELOCITY_KMH = 120\n",
    "ADD_NOISE = True\n",
    "ADD_DOPPLER = True\n",
    "\n",
    "# è®­ç»ƒå‚æ•°\n",
    "BATCH_SIZE = 256   # æ›´å¤§ batch æ›´ç¨³å®šï¼ˆä½ æ•°æ®å¾ˆå¤šï¼‰\n",
    "EPOCHS = 200\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "N_SPLITS = 5\n",
    "PATIENCE = 8\n",
    "MIN_DELTA = 0.1   # early stopping çš„æœ€å°è¿›æ­¥ï¼ˆ%ï¼‰\n",
    "\n",
    "SAVE_ROOT = \"./training_results\"\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "# ---------- å¯é€‰ï¼šAWGN / Dopplerï¼ˆä¿ç•™ï¼‰ ----------\n",
    "def compute_doppler_shift(v_kmh, fc_hz):\n",
    "    if not v_kmh:\n",
    "        return 0.0\n",
    "    c = 3e8\n",
    "    v = v_kmh / 3.6\n",
    "    return fc_hz * v / c\n",
    "\n",
    "def add_complex_awgn(signal, snr_db):\n",
    "    if snr_db is None:\n",
    "        return signal\n",
    "    power = np.mean(np.abs(signal)**2)\n",
    "    noise_power = power / (10**(snr_db/10))\n",
    "    noise_std = np.sqrt(noise_power/2)\n",
    "    noise = noise_std * (np.random.randn(*signal.shape) + 1j*np.random.randn(*signal.shape))\n",
    "    return signal + noise\n",
    "\n",
    "def apply_doppler_shift(signal, fd_hz, fs_hz):\n",
    "    if fd_hz is None or fd_hz == 0:\n",
    "        return signal\n",
    "    t = np.arange(len(signal)) / fs_hz\n",
    "    return signal * np.exp(1j * 2 * np.pi * fd_hz * t)\n",
    "\n",
    "# ---------- æ•°æ®é¢„å¤„ç†ï¼šé’ˆå¯¹ä½ çš„äº¤é”™æ ·æœ¬ï¼Œç”¨ per-sample æ ‡å‡†åŒ–ï¼Œä¸åš FFT ----------\n",
    "def preprocess_for_pointcloud_cnn(data_real_imag, add_noise=False, snr_db=None,\n",
    "                                  add_doppler=False, fd_hz=None, fs_hz=FS):\n",
    "    \"\"\"\n",
    "    è¾“å…¥ data_real_imag: np.array [N, L, 2] (I,Q)\n",
    "    è¾“å‡º torch.tensor [N, L, 2] (float32)ï¼Œæ¯ä¸ªæ ·æœ¬åš zero-mean unit-std æ ‡å‡†åŒ–ï¼ˆæŒ‰æ ·æœ¬ï¼‰\n",
    "    è¯´æ˜ï¼šä¸åš FFTï¼Œå› ä¸ºæ ·æœ¬ä¸æ˜¯è¿ç»­æ—¶é—´åºåˆ—ï¼ˆå·²äº¤é”™ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    data = data_real_imag.astype(np.float32).copy()\n",
    "    N, L, C = data.shape\n",
    "    out = np.empty_like(data, dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        iq = data[i]  # (L,2)\n",
    "        # å¯é€‰ï¼šå…ˆæŠŠ (I,Q) ç»„åˆæˆ complex åš channel çº§å¢å¼º (ä¿ç•™)\n",
    "        if add_noise:\n",
    "            # ä»¥ complex å½¢å¼æ·»åŠ å™ªå£°\n",
    "            sigc = iq[...,0] + 1j * iq[...,1]\n",
    "            sigc = add_complex_awgn(sigc, snr_db)\n",
    "            if add_doppler:\n",
    "                sigc = apply_doppler_shift(sigc, fd_hz, fs_hz)\n",
    "            iq = np.stack([np.real(sigc), np.imag(sigc)], axis=-1).astype(np.float32)\n",
    "\n",
    "        # per-sample æ ‡å‡†åŒ–ï¼ˆå¯¹ä¸¤ä¸ªé€šé“ä¸€èµ·ï¼‰\n",
    "        mu = iq.mean(axis=(0,))    # shape (2,)\n",
    "        sigma = iq.std(axis=(0,))  # shape (2,)\n",
    "        # é˜²æ­¢é™¤é›¶\n",
    "        sigma[sigma < 1e-8] = 1.0\n",
    "        iq_norm = (iq - mu) / sigma\n",
    "        out[i] = iq_norm\n",
    "\n",
    "    return torch.tensor(out, dtype=torch.float32)\n",
    "\n",
    "# ---------- æ–°çš„ RF1DCNN ----------\n",
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.downsample = None\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RF1DCNN(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = ResidualBlock1D(2, 32, kernel_size=7)\n",
    "        self.pool1 = nn.MaxPool1d(2)  # 240 â†’ 120\n",
    "\n",
    "        self.layer2 = ResidualBlock1D(32, 64, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool1d(2)  # 120 â†’ 60\n",
    "\n",
    "        self.layer3 = ResidualBlock1D(64, 128, kernel_size=5)\n",
    "        self.pool3 = nn.MaxPool1d(2)  # 60 â†’ 30\n",
    "\n",
    "        self.layer4 = ResidualBlock1D(128, 256, kernel_size=3)\n",
    "        self.pool4 = nn.MaxPool1d(2)  # 30 â†’ 15\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 15, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 240, 2)\n",
    "        x = x.permute(0, 2, 1)  # â†’ (B, 2, 240)\n",
    "\n",
    "        x = self.layer1(x); x = self.pool1(x)\n",
    "        x = self.layer2(x); x = self.pool2(x)\n",
    "        x = self.layer3(x); x = self.pool3(x)\n",
    "        x = self.layer4(x); x = self.pool4(x)\n",
    "\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ---------- è¾…åŠ©ç»˜å›¾/è¯„ä¼°å‡½æ•° ----------\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            _, p = torch.max(out, 1)\n",
    "            correct += (p == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "            all_preds.extend(p.cpu().numpy())\n",
    "    acc = 100.0 * correct / total if total>0 else 0.0\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "    return acc, cm\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, fold, save_folder, dataset_type='Test'):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{dataset_type} Confusion Matrix Fold{fold}')\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.savefig(os.path.join(save_folder, f'{dataset_type.lower()}_cm_fold{fold}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_curves(train_losses, val_losses, train_acc, val_acc, fold, save_folder):\n",
    "    plt.figure(); plt.plot(train_losses, label='Train Loss'); plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title(f'Fold {fold} Loss'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_folder, f'loss_fold{fold}.png')); plt.close()\n",
    "    plt.figure(); plt.plot(train_acc, label='Train Acc'); plt.plot(val_acc, label='Val Acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)'); plt.title(f'Fold {fold} Accuracy'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_folder, f'acc_fold{fold}.png')); plt.close()\n",
    "\n",
    "# ---------- K-Fold è®­ç»ƒä¸»å‡½æ•°ï¼ˆä½¿ç”¨ RF1DCNNï¼‰ ----------\n",
    "def train_kfold_pointcloud(X_train, y_train, X_test, y_test, num_classes, device=DEVICE,\n",
    "                           batch_size=BATCH_SIZE, epochs=EPOCHS, lr=LR, weight_decay=WEIGHT_DECAY,\n",
    "                           n_splits=N_SPLITS, patience=PATIENCE, min_delta=MIN_DELTA,\n",
    "                           script_name=\"wisig_cross\"):\n",
    "    fd = int(compute_doppler_shift(VELOCITY_KMH, FC))\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    save_dir = f\"{timestamp}_{script_name}_SNR{SNR_DB}dB_fd{fd}_classes_{num_classes}_ResNet\"\n",
    "    save_folder = os.path.join(SAVE_ROOT, save_dir)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    results_file = os.path.join(save_folder, \"results.txt\")\n",
    "\n",
    "    # ä¿å­˜å‚æ•°\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(\"=== Experiment Parameters ===\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"SNR_dB: {SNR_DB}, ADD_NOISE: {ADD_NOISE}, ADD_DOPPLER: {ADD_DOPPLER}\\n\")\n",
    "        f.write(f\"FS: {FS}, FC: {FC}, Velocity_kmh: {VELOCITY_KMH}\\n\")\n",
    "        f.write(f\"Batch: {batch_size}, Epochs: {epochs}, LR: {lr}, WD: {weight_decay}\\n\")\n",
    "        f.write(f\"Num classes: {num_classes}, K-Fold: {n_splits}, Patience: {patience}, MinDelta: {min_delta}\\n\")\n",
    "        f.write(\"============================\\n\\n\")\n",
    "\n",
    "    # test loader\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # full dataset for K-Fold\n",
    "    full_dataset = TensorDataset(X_train, y_train)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    indices = np.arange(len(full_dataset))\n",
    "\n",
    "    val_scores, test_scores = [], []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{n_splits} ===\")\n",
    "        tr_sub = Subset(full_dataset, tr_idx)\n",
    "        va_sub = Subset(full_dataset, va_idx)\n",
    "        tr_loader = DataLoader(tr_sub, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        va_loader = DataLoader(va_sub, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        model = RF1DCNN(num_classes=num_classes, dropout=0.3).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        best_val = 0.0\n",
    "        best_wts = None\n",
    "        patience_cnt = 0\n",
    "\n",
    "        train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "        avg_grad_list = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss, correct, total = 0.0, 0, 0\n",
    "            total_grad, cnt_grad = 0.0, 0\n",
    "            for xb, yb in tr_loader:\n",
    "                xb = xb.to(device); yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                # grad norm\n",
    "                grad_norms = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]\n",
    "                if grad_norms:\n",
    "                    total_grad += np.mean(grad_norms); cnt_grad += 1\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                _, p = torch.max(out, 1)\n",
    "                correct += (p == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "            avg_grad = total_grad / max(cnt_grad, 1)\n",
    "            avg_grad_list.append(avg_grad)\n",
    "\n",
    "            train_loss = running_loss / max(1, len(tr_loader))\n",
    "            train_acc = 100.0 * correct / max(1, total)\n",
    "            train_losses.append(train_loss); train_accs.append(train_acc)\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            vloss, vcorrect, vtotal = 0.0, 0, 0\n",
    "            all_labels, all_preds = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in va_loader:\n",
    "                    xb = xb.to(device); yb = yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    loss = criterion(out, yb)\n",
    "                    vloss += loss.item()\n",
    "                    _, p = torch.max(out, 1)\n",
    "                    vcorrect += (p == yb).sum().item()\n",
    "                    vtotal += yb.size(0)\n",
    "                    all_labels.extend(yb.cpu().numpy()); all_preds.extend(p.cpu().numpy())\n",
    "            val_loss = vloss / max(1, len(va_loader))\n",
    "            val_acc = 100.0 * vcorrect / max(1, vtotal)\n",
    "            val_losses.append(val_loss); val_accs.append(val_acc)\n",
    "            val_cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "            np.save(os.path.join(save_folder, f'val_cm_fold{fold+1}.npy'), val_cm)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | TrainAcc={train_acc:.2f}% | ValAcc={val_acc:.2f}% | \"\n",
    "                  f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | AvgGrad={avg_grad:.4f}\")\n",
    "            with open(results_file, 'a') as f:\n",
    "                f.write(f\"Fold{fold+1} Epoch{epoch+1} | TrainAcc={train_acc:.2f}% | ValAcc={val_acc:.2f}% | \"\n",
    "                        f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | AvgGrad={avg_grad:.4f}\\n\")\n",
    "\n",
    "            # Early stopping on validation accuracy with min_delta (percentage points)\n",
    "            if val_acc > best_val + min_delta:\n",
    "                best_val = val_acc\n",
    "                best_wts = model.state_dict()\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "                if patience_cnt >= patience:\n",
    "                    print(\"Early stopping.\")\n",
    "                    break\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # restore best\n",
    "        if best_wts is not None:\n",
    "            model.load_state_dict(best_wts)\n",
    "\n",
    "        # ä¿å­˜ train/val æ··æ·†çŸ©é˜µ\n",
    "        train_acc, train_cm = evaluate_model(model, tr_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'train_cm_fold{fold+1}.npy'), train_cm)\n",
    "        plot_confusion_matrix(train_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Train')\n",
    "\n",
    "        val_acc, val_cm = evaluate_model(model, va_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'val_cm_fold{fold+1}.npy'), val_cm)\n",
    "        plot_confusion_matrix(val_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Val')\n",
    "\n",
    "        # test eval\n",
    "        test_acc, test_cm = evaluate_model(model, test_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'test_cm_fold{fold+1}.npy'), test_cm)\n",
    "        plot_confusion_matrix(test_cm, classes=list(range(num_classes)), fold=fold+1, save_folder=save_folder, dataset_type='Test')\n",
    "        print(f\"Fold {fold+1} Test Accuracy: {test_acc:.2f}%\")\n",
    "        with open(results_file, 'a') as f:\n",
    "            f.write(f\"Fold{fold+1} TestAcc={test_acc:.2f}%\\n\")\n",
    "\n",
    "        # ä¿å­˜æ›²çº¿ & æ¨¡å‹\n",
    "        plot_curves(train_losses, val_losses, train_accs, val_accs, fold+1, save_folder)\n",
    "        torch.save(model.state_dict(), os.path.join(save_folder, f'model_fold{fold+1}.pth'))\n",
    "\n",
    "        val_scores.append(val_acc)\n",
    "        test_scores.append(test_acc)\n",
    "\n",
    "    # summary\n",
    "    print(\"\\n=== Overall Summary ===\")\n",
    "    print(f\"Val Acc: {np.mean(val_scores):.2f} Â± {np.std(val_scores):.2f}\")\n",
    "    print(f\"Test Acc: {np.mean(test_scores):.2f} Â± {np.std(test_scores):.2f}\")\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(f\"\\n=== Overall Summary ===\\nVal Acc: {np.mean(val_scores):.2f} Â± {np.std(val_scores):.2f}\\nTest Acc: {np.mean(test_scores):.2f} Â± {np.std(test_scores):.2f}\\n\")\n",
    "\n",
    "    print(f\"\\nAll results saved in {save_folder}\")\n",
    "    return save_folder\n",
    "\n",
    "# ---------- è¿è¡Œå‰çš„å‡†å¤‡ï¼ˆé¢„å¤„ç†å¹¶è½¬æ¢ä¸ºå¼ é‡ï¼‰ ----------\n",
    "# X_train, X_test ç›®å‰ shape: [N, 240, 2] (numpy)\n",
    "# æˆ‘ä»¬å¯¹æ¯ä¸ªæ ·æœ¬åš per-sample æ ‡å‡†åŒ–ï¼ˆzero-mean unit-stdï¼‰\n",
    "print(\"Preprocessing (per-sample normalization)...\")\n",
    "# ---------- è½¬ä¸º torch.tensor å¹¶åš per-sample æ ‡å‡†åŒ– ----------\n",
    "\n",
    "X_train_torch = preprocess_for_pointcloud_cnn(X_train, add_noise=False)\n",
    "X_test_torch  = preprocess_for_pointcloud_cnn(X_test, add_noise=False)\n",
    "\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_torch  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "num_classes = len(torch.unique(y_train_torch))\n",
    "print(\"Prepared tensors:\", X_train_torch.shape, X_test_torch.shape, \"num_classes=\", num_classes)\n",
    "\n",
    "# ---------- K-Fold è®­ç»ƒ ----------\n",
    "# train_kfold_pointcloud å†…éƒ¨ä¼šæŠŠè®­ç»ƒé›†å†åˆ’åˆ†ä¸ºè®­ç»ƒ+éªŒè¯é›†\n",
    "# save_folder = train_kfold_pointcloud(\n",
    "#    X_train_torch, \n",
    "#    y_train_torch,\n",
    "#    X_test_torch, \n",
    "#    y_test_torch,\n",
    "#    num_classes=num_classes\n",
    "# )\n",
    "\n",
    "#print(\"Done. Results in:\", save_folder)\n",
    "\n",
    "# ##########################################################\n",
    "# #=== å®šä¹‰ä¸€ä¸ªç»Ÿä¸€å…¥å£å‡½æ•°ï¼Œç”¨äºè·‘ä¸€æ¬¡ SNR å®éªŒ ===\n",
    "# ##########################################################\n",
    "def run_experiment_with_snr(snr_db):\n",
    "    global SNR_DB, ADD_NOISE\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸš€ Running experiment with SNR = {snr_db} dB\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # è®¾ç½®å½“å‰å®éªŒ SNR\n",
    "    SNR_DB = snr_db\n",
    "    ADD_NOISE = True   # æ‰“å¼€ AWGN\n",
    "    \n",
    "    # --- é‡æ–°é¢„å¤„ç†æ•°æ®ï¼ˆä¼šè°ƒç”¨ add_noise=Trueï¼‰ ---\n",
    "    X_train_torch_snr = preprocess_for_pointcloud_cnn(\n",
    "        X_train, add_noise=True, add_doppler = ADD_DOPPLER, snr_db=snr_db\n",
    "    )\n",
    "    X_test_torch_snr = preprocess_for_pointcloud_cnn(\n",
    "        X_test, add_noise=True, add_doppler = ADD_DOPPLER, snr_db=snr_db\n",
    "    )\n",
    "\n",
    "    y_train_torch_snr = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test_torch_snr  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "    num_classes = len(torch.unique(y_train_torch_snr))\n",
    "\n",
    "    # --- è°ƒç”¨ä½ çš„ K-Fold è®­ç»ƒ ---\n",
    "    save_folder = train_kfold_pointcloud(\n",
    "        X_train_torch_snr,\n",
    "        y_train_torch_snr,\n",
    "        X_test_torch_snr,\n",
    "        y_test_torch_snr,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸ‰ Finished SNR={snr_db} dB, results saved in: {save_folder}\")\n",
    "    return save_folder\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# === æ‰¹é‡æ‰§è¡Œ SNR Sweep: 20, 10, 0, -10, ... -40 dB ===\n",
    "###########################################################\n",
    "snr_list = list(range(20, -41, -5))\n",
    "# [20, 15, 10, 5, 0, -5, -10, -15, -20, -25, -30, -35, -40]\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for snr in snr_list:\n",
    "    folder = run_experiment_with_snr(snr)\n",
    "    all_results[snr] = folder\n",
    "\n",
    "print(\"\\n\\n================ FINAL SUMMARY ================\")\n",
    "for snr, folder in all_results.items():\n",
    "    print(f\"SNR {snr:>3} dB â†’ results in: {folder}\")\n",
    "print(\"==============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925873f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# run_date_cross_experiments.py\n",
    "# å®Œæ•´ï¼šæŒ‰å•æ—¥è®­ç»ƒ/å•æ—¥æµ‹è¯•ï¼ˆ4å¤© -> 16 å®éªŒï¼‰è‡ªåŠ¨åŒ–è„šæœ¬\n",
    "# 2025-xx-xx\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import load\n",
    "from data_utilities import *   # ä½ åŸæ¥çš„å·¥å…·å‡½æ•°\n",
    "import gc\n",
    "import h5py\n",
    "\n",
    "# -------------------- ç”¨æˆ·å¯é…ç½®é¡¹ --------------------\n",
    "dataset_name = 'ManySig'\n",
    "dataset_path = '../ManySig.pkl/'   # è¯·æ ¹æ®æœ¬åœ°è·¯å¾„ä¿®æ”¹\n",
    "# è‹¥ compact pickle çš„åŠ è½½å‡½æ•°ä¸åŒï¼Œè¯·è°ƒæ•´load_compact_pkl_datasetçš„è°ƒç”¨\n",
    "\n",
    "# è®­ç»ƒ/æ•°æ®å‚æ•°\n",
    "EQUALIZED = 0\n",
    "MAX_SIG = None         # æ¯ä¸ª TX-RX-æ—¥æœŸæœ€å¤šä½¿ç”¨ä¿¡å·æ•°ï¼ˆNone è¡¨ç¤ºä¸æˆªæ–­ï¼‰\n",
    "BLOCK_SIZE = 240       # æ¯ä¸ª block çš„æ ·æœ¬æ•°ï¼ˆæ¨¡å‹æœŸæœ›é•¿åº¦ï¼‰\n",
    "Y = 5                  # æ¯æ¬¡ä»åŒä¸€ä¸ª RX æŠ½å–çš„ä¿¡å·æ•°\n",
    "N_SPLITS = 5\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 5\n",
    "MIN_DELTA = 0.1  # early stopping çš„æœ€å°ç™¾åˆ†æ¯”ç‚¹æ•°æå‡\n",
    "\n",
    "SAVE_ROOT = \"./training_results_date_cross\"\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"[INFO] Device:\", DEVICE)\n",
    "\n",
    "# -------------------- åŠ è½½ compact dataset --------------------\n",
    "print(\"[INFO] Loading compact dataset ...\")\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path, dataset_name)\n",
    "tx_list = compact_dataset['tx_list']\n",
    "rx_list = compact_dataset['rx_list']\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "\n",
    "print(\"[INFO] tx_list:\", tx_list)\n",
    "print(\"[INFO] rx_list:\", rx_list)\n",
    "print(\"[INFO] capture_date_list:\", capture_date_list)\n",
    "\n",
    "# é€‰æ‹©è¦åšå•æ—¥äº¤å‰çš„å‰ 4 å¤©ï¼ˆå¦‚æœä¸è¶³ 4 å¤©ï¼Œåˆ™å–å…¨éƒ¨ï¼‰\n",
    "num_days = len(capture_date_list)\n",
    "dates_to_use = capture_date_list[:num_days]\n",
    "print(f\"[INFO] Using {len(dates_to_use)} dates for cross experiments:\", dates_to_use)\n",
    "\n",
    "# -------------------- æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼ˆå•æ—¥è®­ç»ƒ / å•æ—¥æµ‹è¯•ï¼‰ --------------------\n",
    "def preprocess_dataset_cross_IQ_blocks_single_date_per_rx_cyclic(compact_dataset, tx_list, train_dates, test_dates,\n",
    "                                                                 max_sig=None, equalized=0, block_size=240, y=5):\n",
    "    \"\"\"\n",
    "    ä» compact_dataset ä¸­æŒ‰æŒ‡å®š train_dates ä¸ test_dates åˆ†åˆ«æå–æ ·æœ¬ã€‚\n",
    "    åŒ RX å‘¨æœŸæ€§æŠ½æ ·æ‹¼æ¥æˆ blocksï¼ˆblock_sizeï¼‰ï¼Œç„¶åè½¬ç½®å¾—åˆ°æ¯ä¸ªæ ·æœ¬é•¿åº¦ä¸º block_sizeï¼Œé€šé“ä¸º I/Qã€‚\n",
    "    è¿”å›ï¼šX_train (N_train, block_size, 2), y_train (N_train,), X_test, y_test\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    def extract_samples_for_dates(dates):\n",
    "        X = []\n",
    "        y_labels = []\n",
    "        # tx_list çš„é¡ºåºå†³å®šç±»æ ‡ç­¾\n",
    "        for tx_idx, tx in enumerate(tx_list):\n",
    "            tx_i = compact_dataset['tx_list'].index(tx)\n",
    "            # equalized æ‰¾ index\n",
    "            if equalized in compact_dataset['equalized_list']:\n",
    "                eq_i = compact_dataset['equalized_list'].index(equalized)\n",
    "            else:\n",
    "                eq_i = 0\n",
    "            for date in dates:\n",
    "                if date not in compact_dataset['capture_date_list']:\n",
    "                    # è·³è¿‡æ²¡æœ‰çš„æ—¥æœŸ\n",
    "                    continue\n",
    "                date_i = compact_dataset['capture_date_list'].index(date)\n",
    "\n",
    "                # æ”¶é›†æ¯ä¸ª RX çš„ä¿¡å·åˆ—è¡¨ï¼ˆå¹¶æ‰“ä¹±é¡ºåºï¼‰\n",
    "                rx_signals = []\n",
    "                for rx_i in range(len(compact_dataset['rx_list'])):\n",
    "                    # å¯èƒ½æ•°æ®ä¸ºç©ºæˆ–å½¢çŠ¶ä¸åŒï¼Œéœ€å®‰å…¨å–\n",
    "                    sig_data = compact_dataset['data'][tx_i][rx_i][date_i][eq_i]\n",
    "                    if max_sig is not None:\n",
    "                        sig_data = sig_data[:max_sig]\n",
    "                    # å°† array è½¬ä¸º listï¼Œä¾¿äº pop\n",
    "                    sig_list = list(sig_data.copy())\n",
    "                    # éšæœºåŒ–é¡ºåº\n",
    "                    np.random.shuffle(sig_list)\n",
    "                    rx_signals.append(sig_list)\n",
    "\n",
    "                num_rx = len(rx_signals)\n",
    "                rx_pointer = 0\n",
    "                accum_block = []\n",
    "\n",
    "                # å¾ªç¯ç›´åˆ°æ‰€æœ‰ rx çš„ä¿¡å·è€—å°½\n",
    "                while any(len(sig_list) > 0 for sig_list in rx_signals):\n",
    "                    rx_idx = rx_pointer % num_rx\n",
    "                    sig_list = rx_signals[rx_idx]\n",
    "                    if len(sig_list) > 0:\n",
    "                        take_n = min(y, len(sig_list))\n",
    "                        # é¡ºåºå–å‡º take_n ä¸ª\n",
    "                        sampled = [sig_list.pop(0) for _ in range(take_n)]\n",
    "                        accum_block.extend(sampled)\n",
    "                    rx_pointer += 1\n",
    "\n",
    "                    # å½“ç´¯ç§¯å¤Ÿä¸€ä¸ª block æ—¶ï¼Œåˆ‡åˆ†å¹¶æŠŠæ¯ä¸ªé‡‡æ ·ç‚¹å½“æˆä¸€ä¸ªæ ·æœ¬ï¼ˆtransposeï¼‰\n",
    "                    while len(accum_block) >= block_size:\n",
    "                        block_chunk = accum_block[:block_size]\n",
    "                        accum_block = accum_block[block_size:]\n",
    "                        block_array = np.array(block_chunk)  # (block_size, sample_len, 2) â€” æ³¨æ„ sample_len æ˜¯åŸå•ä¸ªä¿¡å·é•¿åº¦\n",
    "                        # è¿™é‡ŒåŸå§‹ä¿¡å·é•¿åº¦ï¼ˆä¾‹å¦‚ 256ï¼‰å¯èƒ½åœ¨ç¬¬ 1 ç»´ï¼Œå› æ­¤éœ€è¦è½¬ç½®ä¸º (sample_len, block_size, 2)\n",
    "                        # åŸå§‹ä¿¡å·é€šå¸¸æ˜¯ (L,2) å•ä¿¡å·ã€‚ block_array -> (block_size, L, 2)\n",
    "                        # æˆ‘ä»¬è¦æŠŠæ¯ä¸ªé‡‡æ ·ç‚¹ï¼ˆ0..L-1ï¼‰å½“æˆæ ·æœ¬ï¼šshape -> (L, block_size, 2) -> ç„¶åæŠŠæ¯ä¸ª j å½“ä½œä¸€ä¸ªæ ·æœ¬\n",
    "                        block_transposed = block_array.transpose(1, 0, 2)  # (L, block_size, 2)\n",
    "                        # å¯¹äºæ¯ä¸ªé‡‡æ ·ç‚¹ j, æ·»åŠ æ ·æœ¬é•¿åº¦ä¸º block_size çš„æ ·æœ¬ï¼Œé€šé“ I/Q\n",
    "                        for j in range(block_transposed.shape[0]):\n",
    "                            X.append(block_transposed[j])\n",
    "                            y_labels.append(tx_idx)\n",
    "                # ä¸¢å¼ƒå‰©ä½™çš„ accum_blockï¼ˆä¸è¶³ block_sizeï¼‰\n",
    "                accum_block = []\n",
    "\n",
    "        if len(X) == 0:\n",
    "            return np.zeros((0, block_size, 2), dtype=np.float32), np.zeros((0,), dtype=np.int64)\n",
    "        return np.array(X, dtype=np.float32), np.array(y_labels, dtype=np.int64)\n",
    "\n",
    "    X_train, y_train = extract_samples_for_dates(train_dates)\n",
    "    X_test, y_test = extract_samples_for_dates(test_dates)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# -------------------- æ¨¡å‹å®šä¹‰ï¼šRF1DResidualï¼ˆä¸ä½ ä¹‹å‰çš„ RF1DCNN ç±»ä¼¼ï¼‰ --------------------\n",
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.downsample = None\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class RF1DResNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.3, input_length=BLOCK_SIZE):\n",
    "        super().__init__()\n",
    "        # è¾“å…¥æ˜¯ (B, L, 2) -> è½¬ä¸º (B, 2, L)\n",
    "        self.layer1 = ResidualBlock1D(2, 32, kernel_size=7)\n",
    "        self.pool1 = nn.MaxPool1d(2)  # L -> L/2\n",
    "\n",
    "        self.layer2 = ResidualBlock1D(32, 64, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool1d(2)  # L/4\n",
    "\n",
    "        self.layer3 = ResidualBlock1D(64, 128, kernel_size=5)\n",
    "        self.pool3 = nn.MaxPool1d(2)  # L/8\n",
    "\n",
    "        self.layer4 = ResidualBlock1D(128, 256, kernel_size=3)\n",
    "        self.pool4 = nn.MaxPool1d(2)  # L/16\n",
    "\n",
    "        # è®¡ç®— flatten å¤§å°ï¼ˆå‘ä¸‹å–æ•´ï¼‰\n",
    "        L_after = input_length\n",
    "        for _ in range(4):\n",
    "            L_after = (L_after + 1) // 2  # approximate for pool1d/2\n",
    "        self.flatten_dim = 256 * L_after\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flatten_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, 2)\n",
    "        x = x.permute(0, 2, 1)  # -> (B, 2, L)\n",
    "        x = self.layer1(x); x = self.pool1(x)\n",
    "        x = self.layer2(x); x = self.pool2(x)\n",
    "        x = self.layer3(x); x = self.pool3(x)\n",
    "        x = self.layer4(x); x = self.pool4(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# -------------------- è®­ç»ƒä¸è¯„ä¼°è¾…åŠ©å‡½æ•° --------------------\n",
    "def compute_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm += (p.grad.data.norm(2).item()) ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            _, p = torch.max(out, 1)\n",
    "            correct += (p == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "            all_preds.extend(p.cpu().numpy())\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "    return acc, cm\n",
    "\n",
    "def plot_confusion_matrix_save(cm, classes, save_path, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_curves_save(train_losses, val_losses, train_acc, val_acc, save_prefix):\n",
    "    # loss curve\n",
    "    plt.figure(); plt.plot(train_losses, label='Train Loss'); plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(save_prefix + '_loss.png'); plt.close()\n",
    "    # acc curve\n",
    "    plt.figure(); plt.plot(train_acc, label='Train Acc'); plt.plot(val_acc, label='Val Acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(save_prefix + '_acc.png'); plt.close()\n",
    "\n",
    "# -------------------- K-Fold è®­ç»ƒå‡½æ•°ï¼ˆå¹¶ä¿å­˜æ¯ fold çš„æ‰€æœ‰ artefactsï¼‰ --------------------\n",
    "def train_kfold_pointcloud(X_train, y_train, X_test, y_test, num_classes,\n",
    "                           device=DEVICE, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                           lr=LR, weight_decay=WEIGHT_DECAY, n_splits=N_SPLITS,\n",
    "                           patience=PATIENCE, min_delta=MIN_DELTA, save_folder=None,\n",
    "                           script_name=\"date_cross\"):\n",
    "    \"\"\"\n",
    "    X_train: torch.tensor or numpy array (N, L, 2)\n",
    "    y_train: torch.tensor or numpy (N,)\n",
    "    X_test, y_test: same form\n",
    "    save_folder: where to save per-experiment outputs\n",
    "    \"\"\"\n",
    "    if save_folder is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        save_folder = os.path.join(SAVE_ROOT, f\"{timestamp}_{script_name}\")\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    results_file = os.path.join(save_folder, \"results.txt\")\n",
    "\n",
    "    # å°†è¾“å…¥è½¬æ¢ä¸º TensorDatasetï¼ˆå¦‚æœè¿˜ä¸æ˜¯ tensorï¼‰\n",
    "    if not torch.is_tensor(X_train):\n",
    "        X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    else:\n",
    "        X_train_t = X_train\n",
    "    if not torch.is_tensor(y_train):\n",
    "        y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "    else:\n",
    "        y_train_t = y_train\n",
    "    if not torch.is_tensor(X_test):\n",
    "        X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "    else:\n",
    "        X_test_t = X_test\n",
    "    if not torch.is_tensor(y_test):\n",
    "        y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "    else:\n",
    "        y_test_t = y_test\n",
    "\n",
    "    test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # å†™å…¥å®éªŒå‚æ•°å¤´\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(\"=== Experiment Parameters ===\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Device: {DEVICE}\\n\")\n",
    "        f.write(f\"Batch: {batch_size}, Epochs: {epochs}, LR: {lr}, WD: {weight_decay}\\n\")\n",
    "        f.write(f\"K-Fold: {n_splits}, Patience: {patience}\\n\")\n",
    "        f.write(\"============================\\n\\n\")\n",
    "\n",
    "    full_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    indices = np.arange(len(full_dataset))\n",
    "\n",
    "    val_scores, test_scores = [], []\n",
    "\n",
    "    # K-Fold loop\n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"\\n[Experiment] Fold {fold+1}/{n_splits}\")\n",
    "        tr_sub = Subset(full_dataset, tr_idx)\n",
    "        va_sub = Subset(full_dataset, va_idx)\n",
    "        tr_loader = DataLoader(tr_sub, batch_size=batch_size, shuffle=True)\n",
    "        va_loader = DataLoader(va_sub, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = RF1DResNet(num_classes=num_classes, input_length=BLOCK_SIZE).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        best_val = 0.0\n",
    "        best_wts = None\n",
    "        patience_cnt = 0\n",
    "\n",
    "        train_losses, val_losses = [], []\n",
    "        train_accs, val_accs = [], []\n",
    "        avg_grad_list = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss, correct, total = 0.0, 0, 0\n",
    "            total_grad, cnt_grad = 0.0, 0\n",
    "            for xb, yb in tr_loader:\n",
    "                xb = xb.to(device); yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                # grad norm\n",
    "                grad_norms = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]\n",
    "                if grad_norms:\n",
    "                    total_grad += np.mean(grad_norms); cnt_grad += 1\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                _, p = torch.max(out, 1)\n",
    "                correct += (p == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "\n",
    "            avg_grad = total_grad / max(cnt_grad, 1)\n",
    "            avg_grad_list.append(avg_grad)\n",
    "\n",
    "            train_loss = running_loss / max(1, len(tr_loader))\n",
    "            train_acc = 100.0 * correct / max(1, total)\n",
    "            train_losses.append(train_loss); train_accs.append(train_acc)\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            vloss, vcorrect, vtotal = 0.0, 0, 0\n",
    "            all_labels, all_preds = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in va_loader:\n",
    "                    xb = xb.to(device); yb = yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    loss = criterion(out, yb)\n",
    "                    vloss += loss.item()\n",
    "                    _, p = torch.max(out, 1)\n",
    "                    vcorrect += (p == yb).sum().item()\n",
    "                    vtotal += yb.size(0)\n",
    "                    all_labels.extend(yb.cpu().numpy()); all_preds.extend(p.cpu().numpy())\n",
    "\n",
    "            val_loss = vloss / max(1, len(va_loader))\n",
    "            val_acc = 100.0 * vcorrect / max(1, vtotal)\n",
    "            val_losses.append(val_loss); val_accs.append(val_acc)\n",
    "\n",
    "            # æ‰“å°å¹¶å†™å…¥ results_file\n",
    "            line = f\"Fold{fold+1} Epoch{epoch+1} | TrainAcc={train_acc:.2f}% | ValAcc={val_acc:.2f}% | TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | AvgGrad={avg_grad:.4f}\"\n",
    "            print(line)\n",
    "            with open(results_file, 'a') as f:\n",
    "                f.write(line + \"\\n\")\n",
    "\n",
    "            # Early stopping on validation accuracy with min_delta (percentage points)\n",
    "            if val_acc > best_val + min_delta:\n",
    "                best_val = val_acc\n",
    "                best_wts = model.state_dict()\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "                if patience_cnt >= patience:\n",
    "                    print(\"[INFO] Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # restore best weights for this fold\n",
    "        if best_wts is not None:\n",
    "            model.load_state_dict(best_wts)\n",
    "\n",
    "        # ä¿å­˜ train/val æ··æ·†çŸ©é˜µå’Œæ›²çº¿ & æ¨¡å‹\n",
    "        train_acc_fold, train_cm = evaluate_model(model, tr_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'train_cm_fold{fold+1}.npy'), train_cm)\n",
    "        plot_confusion_matrix_save(train_cm, classes=list(range(num_classes)),\n",
    "                                   save_path=os.path.join(save_folder, f'train_cm_fold{fold+1}.png'),\n",
    "                                   title=f'Train CM Fold {fold+1}')\n",
    "\n",
    "        val_acc_fold, val_cm = evaluate_model(model, va_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'val_cm_fold{fold+1}.npy'), val_cm)\n",
    "        plot_confusion_matrix_save(val_cm, classes=list(range(num_classes)),\n",
    "                                   save_path=os.path.join(save_folder, f'val_cm_fold{fold+1}_val.png'),\n",
    "                                   title=f'Val CM Fold {fold+1}')\n",
    "\n",
    "        test_acc_fold, test_cm = evaluate_model(model, test_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'test_cm_fold{fold+1}.npy'), test_cm)\n",
    "        plot_confusion_matrix_save(test_cm, classes=list(range(num_classes)),\n",
    "                                   save_path=os.path.join(save_folder, f'test_cm_fold{fold+1}.png'),\n",
    "                                   title=f'Test CM Fold {fold+1}')\n",
    "\n",
    "        # curve plots per fold\n",
    "        plot_curves_save(train_losses, val_losses, train_accs, val_accs, save_prefix=os.path.join(save_folder, f'fold{fold+1}'))\n",
    "\n",
    "        # save model\n",
    "        torch.save(model.state_dict(), os.path.join(save_folder, f'model_fold{fold+1}.pth'))\n",
    "\n",
    "        # append fold metrics\n",
    "        val_scores.append(val_acc); test_scores.append(test_acc_fold)\n",
    "\n",
    "        print(f\"ğŸ”¥ Fold {fold+1} - TEST Accuracy: {test_acc_fold:.4f}\")\n",
    "        \n",
    "        # å†™ fold summary\n",
    "        with open(results_file, 'a') as f:\n",
    "            f.write(f\"\\n=== Fold {fold+1} Summary ===\\n\")\n",
    "            f.write(f\"TrainAcc (last) = {train_acc:.2f}%\\n\")\n",
    "            f.write(f\"ValAcc (best) = {best_val:.2f}%\\n\")\n",
    "            f.write(f\"TestAcc = {test_acc_fold:.2f}%\\n\")\n",
    "            f.write(\"===========================\\n\\n\")\n",
    "\n",
    "        # æ¸…ç†æ˜¾å­˜\n",
    "        del model; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    # overall summary\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(\"\\n=== Overall Summary ===\\n\")\n",
    "        f.write(f\"Val Acc: {np.mean(val_scores):.2f} Â± {np.std(val_scores):.2f}\\n\")\n",
    "        f.write(f\"Test Acc: {np.mean(test_scores):.2f} Â± {np.std(test_scores):.2f}\\n\")\n",
    "        f.write(\"=======================\\n\")\n",
    "    print(\"\\n=== Overall Summary ===\")\n",
    "    print(f\"Val Acc: {np.mean(val_scores):.2f} Â± {np.std(val_scores):.2f}\")\n",
    "    print(f\"Test Acc: {np.mean(test_scores):.2f} Â± {np.std(test_scores):.2f}\")\n",
    "\n",
    "    return os.path.abspath(save_folder)\n",
    "\n",
    "# -------------------- ä¸»æ§åˆ¶æµç¨‹ï¼š16 æ¬¡å®éªŒå¾ªç¯ --------------------\n",
    "def main_all_date_experiments():\n",
    "    # take first num_days dates\n",
    "    dates = dates_to_use\n",
    "    all_experiment_folders = []\n",
    "    for train_date in dates:\n",
    "        for test_date in dates:\n",
    "            # æ„é€  folder åç§°åŒ…å«è®­ç»ƒ/æµ‹è¯•æ—¥æœŸ\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            folder_name = f\"{timestamp}_train{train_date}_test{test_date}\"\n",
    "            save_folder = os.path.join(SAVE_ROOT, folder_name)\n",
    "            os.makedirs(save_folder, exist_ok=True)\n",
    "            results_file = os.path.join(save_folder, \"results.txt\")\n",
    "\n",
    "            with open(results_file, \"a\") as f:\n",
    "                f.write(f\"Experiment Timestamp: {timestamp}\\n\")\n",
    "                f.write(f\"Train Date: {train_date}\\n\")\n",
    "                f.write(f\"Test Date: {test_date}\\n\")\n",
    "                f.write(\"========================\\n\")\n",
    "\n",
    "            print(f\"\\n\\n===== Experiment: train={train_date}  test={test_date}  => folder: {save_folder} =====\\n\")\n",
    "            # é¢„å¤„ç†ï¼šåˆ†åˆ«æŒ‰ train_date å’Œ test_date æå–æ•°æ®\n",
    "            X_train, y_train, X_test, y_test = preprocess_dataset_cross_IQ_blocks_single_date_per_rx_cyclic(\n",
    "                compact_dataset=compact_dataset,\n",
    "                tx_list=tx_list,\n",
    "                train_dates=[train_date],\n",
    "                test_dates=[test_date],\n",
    "                max_sig=MAX_SIG,\n",
    "                equalized=EQUALIZED,\n",
    "                block_size=BLOCK_SIZE,\n",
    "                y=Y\n",
    "            )\n",
    "\n",
    "            # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡\n",
    "            if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "                print(f\"[WARN] Empty train/test for train={train_date} test={test_date}, skipping.\")\n",
    "                with open(results_file, \"a\") as f:\n",
    "                    f.write(\"SKIPPED: empty train/test\\n\")\n",
    "                continue\n",
    "\n",
    "            # æŠŠæ•°æ®æ ‡å‡†åŒ–ä¸º per-sample zero-mean unit-stdï¼ˆæŒ‰ sample çš„ä¸¤ä¸ªé€šé“ä¸€èµ·ï¼‰\n",
    "            def per_sample_normalize(X):\n",
    "                Xn = X.astype(np.float32).copy()\n",
    "                N = Xn.shape[0]\n",
    "                for i in range(N):\n",
    "                    mu = Xn[i].mean(axis=0)   # (2,)\n",
    "                    sigma = Xn[i].std(axis=0)\n",
    "                    sigma[sigma < 1e-8] = 1.0\n",
    "                    Xn[i] = (Xn[i] - mu) / sigma\n",
    "                return Xn\n",
    "            X_train_n = per_sample_normalize(X_train)\n",
    "            X_test_n = per_sample_normalize(X_test)\n",
    "\n",
    "            # è½¬ä¸º torch tensors\n",
    "            X_train_t = torch.tensor(X_train_n, dtype=torch.float32)\n",
    "            y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "            X_test_t = torch.tensor(X_test_n, dtype=torch.float32)\n",
    "            y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "            num_classes = len(np.unique(y_train))\n",
    "            # å†™å…¥ result æ–‡ä»¶ç®€è¦ä¿¡æ¯\n",
    "            with open(results_file, \"a\") as f:\n",
    "                f.write(f\"Num train samples: {len(X_train_t)}\\n\")\n",
    "                f.write(f\"Num test samples: {len(X_test_t)}\\n\")\n",
    "                f.write(f\"Num classes (from train): {num_classes}\\n\")\n",
    "                f.write(\"========================\\n\")\n",
    "\n",
    "            # è®­ç»ƒå¹¶ä¿å­˜\n",
    "            exp_folder = train_kfold_pointcloud(X_train_t, y_train_t, X_test_t, y_test_t,\n",
    "                                               num_classes=num_classes, save_folder=save_folder,\n",
    "                                               script_name=f\"train{train_date}_test{test_date}\")\n",
    "            all_experiment_folders.append(exp_folder)\n",
    "\n",
    "    # summary of all experiments\n",
    "    print(\"\\nAll experiments done. Results folders:\")\n",
    "    for p in all_experiment_folders:\n",
    "        print(p)\n",
    "    return all_experiment_folders\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folders = main_all_date_experiments()\n",
    "    print(\"\\nFinished. Saved experiment folders:\")\n",
    "    for fd in folders:\n",
    "        print(fd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3662a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# run_date_cross_experiments.py\n",
    "# ManyTxå®Œæ•´ï¼šæŒ‰å•æ—¥è®­ç»ƒ/å•æ—¥æµ‹è¯•ï¼ˆ4å¤© -> 16 å®éªŒï¼‰è‡ªåŠ¨åŒ–è„šæœ¬\n",
    "# 2025-xx-xx\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import load\n",
    "from data_utilities import *   # ä½ åŸæ¥çš„å·¥å…·å‡½æ•°\n",
    "import gc\n",
    "import h5py\n",
    "\n",
    "# -------------------- ç”¨æˆ·å¯é…ç½®é¡¹ --------------------\n",
    "dataset_name = 'ManyTx'\n",
    "dataset_path = '../ManyTx.pkl/'   # è¯·æ ¹æ®æœ¬åœ°è·¯å¾„ä¿®æ”¹\n",
    "# è‹¥ compact pickle çš„åŠ è½½å‡½æ•°ä¸åŒï¼Œè¯·è°ƒæ•´load_compact_pkl_datasetçš„è°ƒç”¨\n",
    "\n",
    "# è®­ç»ƒ/æ•°æ®å‚æ•°\n",
    "EQUALIZED = 0\n",
    "MAX_SIG = None         # æ¯ä¸ª TX-RX-æ—¥æœŸæœ€å¤šä½¿ç”¨ä¿¡å·æ•°ï¼ˆNone è¡¨ç¤ºä¸æˆªæ–­ï¼‰\n",
    "BLOCK_SIZE = 240       # æ¯ä¸ª block çš„æ ·æœ¬æ•°ï¼ˆæ¨¡å‹æœŸæœ›é•¿åº¦ï¼‰\n",
    "Y = 5                  # æ¯æ¬¡ä»åŒä¸€ä¸ª RX æŠ½å–çš„ä¿¡å·æ•°\n",
    "N_SPLITS = 5\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 5\n",
    "MIN_DELTA = 0.1  # early stopping çš„æœ€å°ç™¾åˆ†æ¯”ç‚¹æ•°æå‡\n",
    "\n",
    "SAVE_ROOT = \"./training_results_date_cross\"\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"[INFO] Device:\", DEVICE)\n",
    "\n",
    "# -------------------- åŠ è½½ compact dataset --------------------\n",
    "print(\"[INFO] Loading compact dataset ...\")\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path, dataset_name)\n",
    "tx_list = compact_dataset['tx_list']\n",
    "rx_list = compact_dataset['rx_list']\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "\n",
    "print(\"[INFO] tx_list:\", tx_list)\n",
    "print(\"[INFO] rx_list:\", rx_list)\n",
    "print(\"[INFO] capture_date_list:\", capture_date_list)\n",
    "\n",
    "# é€‰æ‹©è¦åšå•æ—¥äº¤å‰çš„å‰ 4 å¤©ï¼ˆå¦‚æœä¸è¶³ 4 å¤©ï¼Œåˆ™å–å…¨éƒ¨ï¼‰\n",
    "num_days = len(capture_date_list)\n",
    "dates_to_use = capture_date_list[:num_days]\n",
    "print(f\"[INFO] Using {len(dates_to_use)} dates for cross experiments:\", dates_to_use)\n",
    "\n",
    "# -------------------- æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼ˆå•æ—¥è®­ç»ƒ / å•æ—¥æµ‹è¯•ï¼‰ --------------------\n",
    "def preprocess_dataset_cross_IQ_blocks_single_date_per_rx_cyclic(compact_dataset, tx_list, train_dates, test_dates,\n",
    "                                                                 max_sig=None, equalized=0, block_size=240, y=5):\n",
    "    \"\"\"\n",
    "    ä» compact_dataset ä¸­æŒ‰æŒ‡å®š train_dates ä¸ test_dates åˆ†åˆ«æå–æ ·æœ¬ã€‚\n",
    "    åŒ RX å‘¨æœŸæ€§æŠ½æ ·æ‹¼æ¥æˆ blocksï¼ˆblock_sizeï¼‰ï¼Œç„¶åè½¬ç½®å¾—åˆ°æ¯ä¸ªæ ·æœ¬é•¿åº¦ä¸º block_sizeï¼Œé€šé“ä¸º I/Qã€‚\n",
    "    è¿”å›ï¼šX_train (N_train, block_size, 2), y_train (N_train,), X_test, y_test\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    def extract_samples_for_dates(dates):\n",
    "        X = []\n",
    "        y_labels = []\n",
    "        # tx_list çš„é¡ºåºå†³å®šç±»æ ‡ç­¾\n",
    "        for tx_idx, tx in enumerate(tx_list):\n",
    "            tx_i = compact_dataset['tx_list'].index(tx)\n",
    "            # equalized æ‰¾ index\n",
    "            if equalized in compact_dataset['equalized_list']:\n",
    "                eq_i = compact_dataset['equalized_list'].index(equalized)\n",
    "            else:\n",
    "                eq_i = 0\n",
    "            for date in dates:\n",
    "                if date not in compact_dataset['capture_date_list']:\n",
    "                    # è·³è¿‡æ²¡æœ‰çš„æ—¥æœŸ\n",
    "                    continue\n",
    "                date_i = compact_dataset['capture_date_list'].index(date)\n",
    "\n",
    "                # æ”¶é›†æ¯ä¸ª RX çš„ä¿¡å·åˆ—è¡¨ï¼ˆå¹¶æ‰“ä¹±é¡ºåºï¼‰\n",
    "                rx_signals = []\n",
    "                for rx_i in range(len(compact_dataset['rx_list'])):\n",
    "                    # å¯èƒ½æ•°æ®ä¸ºç©ºæˆ–å½¢çŠ¶ä¸åŒï¼Œéœ€å®‰å…¨å–\n",
    "                    sig_data = compact_dataset['data'][tx_i][rx_i][date_i][eq_i]\n",
    "                    if max_sig is not None:\n",
    "                        sig_data = sig_data[:max_sig]\n",
    "                    # å°† array è½¬ä¸º listï¼Œä¾¿äº pop\n",
    "                    sig_list = list(sig_data.copy())\n",
    "                    # éšæœºåŒ–é¡ºåº\n",
    "                    np.random.shuffle(sig_list)\n",
    "                    rx_signals.append(sig_list)\n",
    "\n",
    "                num_rx = len(rx_signals)\n",
    "                rx_pointer = 0\n",
    "                accum_block = []\n",
    "\n",
    "                # å¾ªç¯ç›´åˆ°æ‰€æœ‰ rx çš„ä¿¡å·è€—å°½\n",
    "                while any(len(sig_list) > 0 for sig_list in rx_signals):\n",
    "                    rx_idx = rx_pointer % num_rx\n",
    "                    sig_list = rx_signals[rx_idx]\n",
    "                    if len(sig_list) > 0:\n",
    "                        take_n = min(y, len(sig_list))\n",
    "                        # é¡ºåºå–å‡º take_n ä¸ª\n",
    "                        sampled = [sig_list.pop(0) for _ in range(take_n)]\n",
    "                        accum_block.extend(sampled)\n",
    "                    rx_pointer += 1\n",
    "\n",
    "                    # å½“ç´¯ç§¯å¤Ÿä¸€ä¸ª block æ—¶ï¼Œåˆ‡åˆ†å¹¶æŠŠæ¯ä¸ªé‡‡æ ·ç‚¹å½“æˆä¸€ä¸ªæ ·æœ¬ï¼ˆtransposeï¼‰\n",
    "                    while len(accum_block) >= block_size:\n",
    "                        block_chunk = accum_block[:block_size]\n",
    "                        accum_block = accum_block[block_size:]\n",
    "                        block_array = np.array(block_chunk)  # (block_size, sample_len, 2) â€” æ³¨æ„ sample_len æ˜¯åŸå•ä¸ªä¿¡å·é•¿åº¦\n",
    "                        # è¿™é‡ŒåŸå§‹ä¿¡å·é•¿åº¦ï¼ˆä¾‹å¦‚ 256ï¼‰å¯èƒ½åœ¨ç¬¬ 1 ç»´ï¼Œå› æ­¤éœ€è¦è½¬ç½®ä¸º (sample_len, block_size, 2)\n",
    "                        # åŸå§‹ä¿¡å·é€šå¸¸æ˜¯ (L,2) å•ä¿¡å·ã€‚ block_array -> (block_size, L, 2)\n",
    "                        # æˆ‘ä»¬è¦æŠŠæ¯ä¸ªé‡‡æ ·ç‚¹ï¼ˆ0..L-1ï¼‰å½“æˆæ ·æœ¬ï¼šshape -> (L, block_size, 2) -> ç„¶åæŠŠæ¯ä¸ª j å½“ä½œä¸€ä¸ªæ ·æœ¬\n",
    "                        block_transposed = block_array.transpose(1, 0, 2)  # (L, block_size, 2)\n",
    "                        # å¯¹äºæ¯ä¸ªé‡‡æ ·ç‚¹ j, æ·»åŠ æ ·æœ¬é•¿åº¦ä¸º block_size çš„æ ·æœ¬ï¼Œé€šé“ I/Q\n",
    "                        for j in range(block_transposed.shape[0]):\n",
    "                            X.append(block_transposed[j])\n",
    "                            y_labels.append(tx_idx)\n",
    "                # ä¸¢å¼ƒå‰©ä½™çš„ accum_blockï¼ˆä¸è¶³ block_sizeï¼‰\n",
    "                accum_block = []\n",
    "\n",
    "        if len(X) == 0:\n",
    "            return np.zeros((0, block_size, 2), dtype=np.float32), np.zeros((0,), dtype=np.int64)\n",
    "        return np.array(X, dtype=np.float32), np.array(y_labels, dtype=np.int64)\n",
    "\n",
    "    X_train, y_train = extract_samples_for_dates(train_dates)\n",
    "    X_test, y_test = extract_samples_for_dates(test_dates)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# -------------------- æ¨¡å‹å®šä¹‰ï¼šRF1DResidualï¼ˆä¸ä½ ä¹‹å‰çš„ RF1DCNN ç±»ä¼¼ï¼‰ --------------------\n",
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.downsample = None\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class RF1DResNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.3, input_length=BLOCK_SIZE):\n",
    "        super().__init__()\n",
    "        # è¾“å…¥æ˜¯ (B, L, 2) -> è½¬ä¸º (B, 2, L)\n",
    "        self.layer1 = ResidualBlock1D(2, 32, kernel_size=7)\n",
    "        self.pool1 = nn.MaxPool1d(2)  # L -> L/2\n",
    "\n",
    "        self.layer2 = ResidualBlock1D(32, 64, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool1d(2)  # L/4\n",
    "\n",
    "        self.layer3 = ResidualBlock1D(64, 128, kernel_size=5)\n",
    "        self.pool3 = nn.MaxPool1d(2)  # L/8\n",
    "\n",
    "        self.layer4 = ResidualBlock1D(128, 256, kernel_size=3)\n",
    "        self.pool4 = nn.MaxPool1d(2)  # L/16\n",
    "\n",
    "        # è®¡ç®— flatten å¤§å°ï¼ˆå‘ä¸‹å–æ•´ï¼‰\n",
    "        L_after = input_length\n",
    "        for _ in range(4):\n",
    "            L_after = (L_after + 1) // 2  # approximate for pool1d/2\n",
    "        self.flatten_dim = 256 * L_after\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flatten_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, 2)\n",
    "        x = x.permute(0, 2, 1)  # -> (B, 2, L)\n",
    "        x = self.layer1(x); x = self.pool1(x)\n",
    "        x = self.layer2(x); x = self.pool2(x)\n",
    "        x = self.layer3(x); x = self.pool3(x)\n",
    "        x = self.layer4(x); x = self.pool4(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# -------------------- è®­ç»ƒä¸è¯„ä¼°è¾…åŠ©å‡½æ•° --------------------\n",
    "def compute_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm += (p.grad.data.norm(2).item()) ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            _, p = torch.max(out, 1)\n",
    "            correct += (p == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "            all_preds.extend(p.cpu().numpy())\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "    return acc, cm\n",
    "\n",
    "def plot_confusion_matrix_save(cm, classes, save_path, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_curves_save(train_losses, val_losses, train_acc, val_acc, save_prefix):\n",
    "    # loss curve\n",
    "    plt.figure(); plt.plot(train_losses, label='Train Loss'); plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(save_prefix + '_loss.png'); plt.close()\n",
    "    # acc curve\n",
    "    plt.figure(); plt.plot(train_acc, label='Train Acc'); plt.plot(val_acc, label='Val Acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(save_prefix + '_acc.png'); plt.close()\n",
    "\n",
    "# -------------------- K-Fold è®­ç»ƒå‡½æ•°ï¼ˆå¹¶ä¿å­˜æ¯ fold çš„æ‰€æœ‰ artefactsï¼‰ --------------------\n",
    "def train_kfold_pointcloud(X_train, y_train, X_test, y_test, num_classes,\n",
    "                           device=DEVICE, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                           lr=LR, weight_decay=WEIGHT_DECAY, n_splits=N_SPLITS,\n",
    "                           patience=PATIENCE, min_delta=MIN_DELTA, save_folder=None,\n",
    "                           script_name=\"date_cross\"):\n",
    "    \"\"\"\n",
    "    X_train: torch.tensor or numpy array (N, L, 2)\n",
    "    y_train: torch.tensor or numpy (N,)\n",
    "    X_test, y_test: same form\n",
    "    save_folder: where to save per-experiment outputs\n",
    "    \"\"\"\n",
    "    if save_folder is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        save_folder = os.path.join(SAVE_ROOT, f\"{timestamp}_{script_name}\")\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    results_file = os.path.join(save_folder, \"results.txt\")\n",
    "\n",
    "    # å°†è¾“å…¥è½¬æ¢ä¸º TensorDatasetï¼ˆå¦‚æœè¿˜ä¸æ˜¯ tensorï¼‰\n",
    "    if not torch.is_tensor(X_train):\n",
    "        X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    else:\n",
    "        X_train_t = X_train\n",
    "    if not torch.is_tensor(y_train):\n",
    "        y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "    else:\n",
    "        y_train_t = y_train\n",
    "    if not torch.is_tensor(X_test):\n",
    "        X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "    else:\n",
    "        X_test_t = X_test\n",
    "    if not torch.is_tensor(y_test):\n",
    "        y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "    else:\n",
    "        y_test_t = y_test\n",
    "\n",
    "    test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # å†™å…¥å®éªŒå‚æ•°å¤´\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(\"=== Experiment Parameters ===\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Device: {DEVICE}\\n\")\n",
    "        f.write(f\"Batch: {batch_size}, Epochs: {epochs}, LR: {lr}, WD: {weight_decay}\\n\")\n",
    "        f.write(f\"K-Fold: {n_splits}, Patience: {patience}\\n\")\n",
    "        f.write(\"============================\\n\\n\")\n",
    "\n",
    "    full_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    indices = np.arange(len(full_dataset))\n",
    "\n",
    "    val_scores, test_scores = [], []\n",
    "\n",
    "    # K-Fold loop\n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"\\n[Experiment] Fold {fold+1}/{n_splits}\")\n",
    "        tr_sub = Subset(full_dataset, tr_idx)\n",
    "        va_sub = Subset(full_dataset, va_idx)\n",
    "        tr_loader = DataLoader(tr_sub, batch_size=batch_size, shuffle=True)\n",
    "        va_loader = DataLoader(va_sub, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = RF1DResNet(num_classes=num_classes, input_length=BLOCK_SIZE).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        best_val = 0.0\n",
    "        best_wts = None\n",
    "        patience_cnt = 0\n",
    "\n",
    "        train_losses, val_losses = [], []\n",
    "        train_accs, val_accs = [], []\n",
    "        avg_grad_list = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss, correct, total = 0.0, 0, 0\n",
    "            total_grad, cnt_grad = 0.0, 0\n",
    "            for xb, yb in tr_loader:\n",
    "                xb = xb.to(device); yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                # grad norm\n",
    "                grad_norms = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]\n",
    "                if grad_norms:\n",
    "                    total_grad += np.mean(grad_norms); cnt_grad += 1\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                _, p = torch.max(out, 1)\n",
    "                correct += (p == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "\n",
    "            avg_grad = total_grad / max(cnt_grad, 1)\n",
    "            avg_grad_list.append(avg_grad)\n",
    "\n",
    "            train_loss = running_loss / max(1, len(tr_loader))\n",
    "            train_acc = 100.0 * correct / max(1, total)\n",
    "            train_losses.append(train_loss); train_accs.append(train_acc)\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            vloss, vcorrect, vtotal = 0.0, 0, 0\n",
    "            all_labels, all_preds = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in va_loader:\n",
    "                    xb = xb.to(device); yb = yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    loss = criterion(out, yb)\n",
    "                    vloss += loss.item()\n",
    "                    _, p = torch.max(out, 1)\n",
    "                    vcorrect += (p == yb).sum().item()\n",
    "                    vtotal += yb.size(0)\n",
    "                    all_labels.extend(yb.cpu().numpy()); all_preds.extend(p.cpu().numpy())\n",
    "\n",
    "            val_loss = vloss / max(1, len(va_loader))\n",
    "            val_acc = 100.0 * vcorrect / max(1, vtotal)\n",
    "            val_losses.append(val_loss); val_accs.append(val_acc)\n",
    "\n",
    "            # æ‰“å°å¹¶å†™å…¥ results_file\n",
    "            line = f\"Fold{fold+1} Epoch{epoch+1} | TrainAcc={train_acc:.2f}% | ValAcc={val_acc:.2f}% | TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | AvgGrad={avg_grad:.4f}\"\n",
    "            print(line)\n",
    "            with open(results_file, 'a') as f:\n",
    "                f.write(line + \"\\n\")\n",
    "\n",
    "            # Early stopping on validation accuracy with min_delta (percentage points)\n",
    "            if val_acc > best_val + min_delta:\n",
    "                best_val = val_acc\n",
    "                best_wts = model.state_dict()\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "                if patience_cnt >= patience:\n",
    "                    print(\"[INFO] Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # restore best weights for this fold\n",
    "        if best_wts is not None:\n",
    "            model.load_state_dict(best_wts)\n",
    "\n",
    "        # ä¿å­˜ train/val æ··æ·†çŸ©é˜µå’Œæ›²çº¿ & æ¨¡å‹\n",
    "        train_acc_fold, train_cm = evaluate_model(model, tr_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'train_cm_fold{fold+1}.npy'), train_cm)\n",
    "        plot_confusion_matrix_save(train_cm, classes=list(range(num_classes)),\n",
    "                                   save_path=os.path.join(save_folder, f'train_cm_fold{fold+1}.png'),\n",
    "                                   title=f'Train CM Fold {fold+1}')\n",
    "\n",
    "        val_acc_fold, val_cm = evaluate_model(model, va_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'val_cm_fold{fold+1}.npy'), val_cm)\n",
    "        plot_confusion_matrix_save(val_cm, classes=list(range(num_classes)),\n",
    "                                   save_path=os.path.join(save_folder, f'val_cm_fold{fold+1}_val.png'),\n",
    "                                   title=f'Val CM Fold {fold+1}')\n",
    "\n",
    "        test_acc_fold, test_cm = evaluate_model(model, test_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'test_cm_fold{fold+1}.npy'), test_cm)\n",
    "        plot_confusion_matrix_save(test_cm, classes=list(range(num_classes)),\n",
    "                                   save_path=os.path.join(save_folder, f'test_cm_fold{fold+1}.png'),\n",
    "                                   title=f'Test CM Fold {fold+1}')\n",
    "\n",
    "        # curve plots per fold\n",
    "        plot_curves_save(train_losses, val_losses, train_accs, val_accs, save_prefix=os.path.join(save_folder, f'fold{fold+1}'))\n",
    "\n",
    "        # save model\n",
    "        torch.save(model.state_dict(), os.path.join(save_folder, f'model_fold{fold+1}.pth'))\n",
    "\n",
    "        # append fold metrics\n",
    "        val_scores.append(val_acc); test_scores.append(test_acc_fold)\n",
    "\n",
    "        print(f\"ğŸ”¥ Fold {fold+1} - TEST Accuracy: {test_acc_fold:.4f}\")\n",
    "        \n",
    "        # å†™ fold summary\n",
    "        with open(results_file, 'a') as f:\n",
    "            f.write(f\"\\n=== Fold {fold+1} Summary ===\\n\")\n",
    "            f.write(f\"TrainAcc (last) = {train_acc:.2f}%\\n\")\n",
    "            f.write(f\"ValAcc (best) = {best_val:.2f}%\\n\")\n",
    "            f.write(f\"TestAcc = {test_acc_fold:.2f}%\\n\")\n",
    "            f.write(\"===========================\\n\\n\")\n",
    "\n",
    "        # æ¸…ç†æ˜¾å­˜\n",
    "        del model; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    # overall summary\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(\"\\n=== Overall Summary ===\\n\")\n",
    "        f.write(f\"Val Acc: {np.mean(val_scores):.2f} Â± {np.std(val_scores):.2f}\\n\")\n",
    "        f.write(f\"Test Acc: {np.mean(test_scores):.2f} Â± {np.std(test_scores):.2f}\\n\")\n",
    "        f.write(\"=======================\\n\")\n",
    "    print(\"\\n=== Overall Summary ===\")\n",
    "    print(f\"Val Acc: {np.mean(val_scores):.2f} Â± {np.std(val_scores):.2f}\")\n",
    "    print(f\"Test Acc: {np.mean(test_scores):.2f} Â± {np.std(test_scores):.2f}\")\n",
    "\n",
    "    return os.path.abspath(save_folder)\n",
    "\n",
    "# -------------------- ä¸»æ§åˆ¶æµç¨‹ï¼š16 æ¬¡å®éªŒå¾ªç¯ --------------------\n",
    "def main_all_date_experiments():\n",
    "    # take first num_days dates\n",
    "    dates = dates_to_use\n",
    "    all_experiment_folders = []\n",
    "    for train_date in dates:\n",
    "        for test_date in dates:\n",
    "            # æ„é€  folder åç§°åŒ…å«è®­ç»ƒ/æµ‹è¯•æ—¥æœŸ\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            folder_name = f\"{timestamp}_train{train_date}_test{test_date}_ManyTx\"\n",
    "            save_folder = os.path.join(SAVE_ROOT, folder_name)\n",
    "            os.makedirs(save_folder, exist_ok=True)\n",
    "            results_file = os.path.join(save_folder, \"results.txt\")\n",
    "\n",
    "            with open(results_file, \"a\") as f:\n",
    "                f.write(f\"Experiment Timestamp: {timestamp}\\n\")\n",
    "                f.write(f\"Train Date: {train_date}\\n\")\n",
    "                f.write(f\"Test Date: {test_date}\\n\")\n",
    "                f.write(\"========================\\n\")\n",
    "\n",
    "            print(f\"\\n\\n===== Experiment: train={train_date}  test={test_date}  => folder: {save_folder} =====\\n\")\n",
    "            # é¢„å¤„ç†ï¼šåˆ†åˆ«æŒ‰ train_date å’Œ test_date æå–æ•°æ®\n",
    "            X_train, y_train, X_test, y_test = preprocess_dataset_cross_IQ_blocks_single_date_per_rx_cyclic(\n",
    "                compact_dataset=compact_dataset,\n",
    "                tx_list=tx_list,\n",
    "                train_dates=[train_date],\n",
    "                test_dates=[test_date],\n",
    "                max_sig=MAX_SIG,\n",
    "                equalized=EQUALIZED,\n",
    "                block_size=BLOCK_SIZE,\n",
    "                y=Y\n",
    "            )\n",
    "\n",
    "            # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡\n",
    "            if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "                print(f\"[WARN] Empty train/test for train={train_date} test={test_date}, skipping.\")\n",
    "                with open(results_file, \"a\") as f:\n",
    "                    f.write(\"SKIPPED: empty train/test\\n\")\n",
    "                continue\n",
    "\n",
    "            # æŠŠæ•°æ®æ ‡å‡†åŒ–ä¸º per-sample zero-mean unit-stdï¼ˆæŒ‰ sample çš„ä¸¤ä¸ªé€šé“ä¸€èµ·ï¼‰\n",
    "            def per_sample_normalize(X):\n",
    "                Xn = X.astype(np.float32).copy()\n",
    "                N = Xn.shape[0]\n",
    "                for i in range(N):\n",
    "                    mu = Xn[i].mean(axis=0)   # (2,)\n",
    "                    sigma = Xn[i].std(axis=0)\n",
    "                    sigma[sigma < 1e-8] = 1.0\n",
    "                    Xn[i] = (Xn[i] - mu) / sigma\n",
    "                return Xn\n",
    "            X_train_n = per_sample_normalize(X_train)\n",
    "            X_test_n = per_sample_normalize(X_test)\n",
    "\n",
    "            # è½¬ä¸º torch tensors\n",
    "            X_train_t = torch.tensor(X_train_n, dtype=torch.float32)\n",
    "            y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "            X_test_t = torch.tensor(X_test_n, dtype=torch.float32)\n",
    "            y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "            num_classes = len(np.unique(y_train))\n",
    "            # å†™å…¥ result æ–‡ä»¶ç®€è¦ä¿¡æ¯\n",
    "            with open(results_file, \"a\") as f:\n",
    "                f.write(f\"Num train samples: {len(X_train_t)}\\n\")\n",
    "                f.write(f\"Num test samples: {len(X_test_t)}\\n\")\n",
    "                f.write(f\"Num classes (from train): {num_classes}\\n\")\n",
    "                f.write(\"========================\\n\")\n",
    "\n",
    "            # è®­ç»ƒå¹¶ä¿å­˜\n",
    "            exp_folder = train_kfold_pointcloud(X_train_t, y_train_t, X_test_t, y_test_t,\n",
    "                                               num_classes=num_classes, save_folder=save_folder,\n",
    "                                               script_name=f\"train{train_date}_test{test_date}\")\n",
    "            all_experiment_folders.append(exp_folder)\n",
    "\n",
    "    # summary of all experiments\n",
    "    print(\"\\nAll experiments done. Results folders:\")\n",
    "    for p in all_experiment_folders:\n",
    "        print(p)\n",
    "    return all_experiment_folders\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folders = main_all_date_experiments()\n",
    "    print(\"\\nFinished. Saved experiment folders:\")\n",
    "    for fd in folders:\n",
    "        print(fd)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
