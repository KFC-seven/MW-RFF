{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca05955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DEVICE: cuda\n",
      "[INFO] Found 72 .mat files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading clean (.mat): 100%|██████████| 72/72 [00:05<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Domains: 8  | Labels: 9  | Sample_len: 256\n",
      "[INFO] Domain preview: [('rx1', 10), ('rx1', 20), ('rx2', 10), ('rx2', 20), ('rx3', 10), ('rx3', 20), ('rx4', 10), ('rx4', 20)]\n",
      "\n",
      "================================================================================\n",
      "[RUN] SNR = 20 dB\n",
      "================================================================================\n",
      "[INFO] Train blocks: 491\n",
      "[INFO] Val domains RAW=8 XFR=8 | Test domains RAW=8 XFR=8\n",
      "Epoch 001/200 | TrainLoss=2.0778 | ValXFR_mean=10.16% | ValRAW_mean=10.73%\n",
      "Epoch 002/200 | TrainLoss=1.1047 | ValXFR_mean=9.52% | ValRAW_mean=11.35%\n",
      "Epoch 003/200 | TrainLoss=0.5930 | ValXFR_mean=11.07% | ValRAW_mean=11.90%\n",
      "Epoch 004/200 | TrainLoss=0.3610 | ValXFR_mean=11.24% | ValRAW_mean=12.17%\n",
      "Epoch 005/200 | TrainLoss=0.2049 | ValXFR_mean=11.40% | ValRAW_mean=11.87%\n",
      "Epoch 006/200 | TrainLoss=0.1250 | ValXFR_mean=11.39% | ValRAW_mean=12.26%\n",
      "Epoch 007/200 | TrainLoss=0.0834 | ValXFR_mean=11.65% | ValRAW_mean=12.05%\n",
      "Epoch 008/200 | TrainLoss=0.0619 | ValXFR_mean=11.40% | ValRAW_mean=12.30%\n",
      "Epoch 009/200 | TrainLoss=0.0487 | ValXFR_mean=11.46% | ValRAW_mean=11.68%\n",
      "Epoch 010/200 | TrainLoss=0.0420 | ValXFR_mean=11.43% | ValRAW_mean=11.64%\n",
      "Epoch 011/200 | TrainLoss=0.0246 | ValXFR_mean=11.24% | ValRAW_mean=11.57%\n",
      "Epoch 012/200 | TrainLoss=0.0226 | ValXFR_mean=11.42% | ValRAW_mean=11.97%\n",
      "Epoch 013/200 | TrainLoss=0.0210 | ValXFR_mean=11.39% | ValRAW_mean=11.88%\n",
      "Epoch 014/200 | TrainLoss=0.0188 | ValXFR_mean=10.35% | ValRAW_mean=12.32%\n",
      "Epoch 015/200 | TrainLoss=0.0180 | ValXFR_mean=9.88% | ValRAW_mean=12.17%\n",
      "[INFO] Early stopping.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 972\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Plots & logs saved to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, SAVE_ROOT)\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 972\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 915\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    912\u001b[0m save_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SAVE_ROOT, run_name)\n\u001b[0;32m    913\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 915\u001b[0m test_raw_mean, test_xfr_mean \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_snr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnr_db\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msnr_db\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_xfr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_xfr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_xfr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_xfr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_folder\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    929\u001b[0m snr_test_raw_means\u001b[38;5;241m.\u001b[39mappend(test_raw_mean)\n\u001b[0;32m    930\u001b[0m snr_test_xfr_means\u001b[38;5;241m.\u001b[39mappend(test_xfr_mean)\n",
      "Cell \u001b[1;32mIn[1], line 763\u001b[0m, in \u001b[0;36mtrain_one_snr\u001b[1;34m(snr_db, X_train_blocks, y_train_blocks, val_raw, val_xfr, test_raw, test_xfr, num_classes, sample_len, block_size, save_folder)\u001b[0m\n\u001b[0;32m    760\u001b[0m test_raw_mean, (test_raw_accs, test_raw_cms) \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m, ({}, {}))\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(test_xfr_loaders) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     test_xfr_mean, (test_xfr_accs, test_xfr_cms) \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mlambda\u001b[39;00m loaders: (\n\u001b[0;32m    764\u001b[0m         \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean([evaluate_model(model, ld, DEVICE, num_classes)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ld \u001b[38;5;129;01min\u001b[39;00m loaders\u001b[38;5;241m.\u001b[39mvalues()])),\n\u001b[0;32m    765\u001b[0m         {dom: evaluate_model(model, ld, DEVICE, num_classes) \u001b[38;5;28;01mfor\u001b[39;00m dom, ld \u001b[38;5;129;01min\u001b[39;00m loaders\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    766\u001b[0m     ))(test_xfr_loaders)\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;66;03m# 拆出来\u001b[39;00m\n\u001b[0;32m    769\u001b[0m     txfr_accs \u001b[38;5;241m=\u001b[39m {dom: v[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m dom, v \u001b[38;5;129;01min\u001b[39;00m test_xfr_accs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LTE-V  多域训练 + 单域评估（RAW vs XFR） + SNR Sweep\n",
    "#\n",
    "# 训练（multi-domain）：\n",
    "#   - domain = (rx, v)\n",
    "#   - 对每个 TX 类别：从所有可用 domain 中“顺序轮询”抽样，拼成 block_size 个 frame\n",
    "#   - 对 block 做 XFR 转置： (block_size, sample_len, 2) -> (sample_len, block_size, 2)\n",
    "#   - 展开为 row-samples：每个 row 是一个样本，shape=(block_size,2)\n",
    "#\n",
    "# 验证/测试（single-domain, per (rx,v)）两套版本：\n",
    "#   1) RAW：原始 frame 作为样本，shape=(sample_len,2)\n",
    "#   2) XFR：在该单域内部拼 block -> 转置 -> 展开 row-samples，shape=(block_size,2)\n",
    "#\n",
    "# SNR sweep:\n",
    "#   - 从“干净（仅功率归一化）”信号缓存出发，对每个 SNR 重新加 AWGN（可选额外 Doppler）\n",
    "#   - 每个 SNR 独立训练并保存结果与曲线\n",
    "#\n",
    "# 你需要保证数据路径下能解析出 rx 与 v（速度）：\n",
    "#   默认解析策略：从路径组件里找 rx\\d+ / v\\d+ / \\d+kmh 等。\n",
    "#   若你数据命名不同，修改 parse_domain_from_path() 即可。\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import gc\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "matplotlib.use(\"agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 全局参数（按需改）\n",
    "# =========================\n",
    "DATA_ROOT = r\"E:/rf_datasets_IQ_raw/\"          # LTE-V 根目录（含 .mat）\n",
    "SAVE_ROOT = \"./training_results\"\n",
    "\n",
    "# SNR sweep\n",
    "SNR_LIST = list(range(20, -45, -5))           # 20,15,...,-40\n",
    "APPLY_AWGN = True\n",
    "APPLY_EXTRA_DOPPLER = True                   # 通常 LTE-V 本身已含速度效应；默认 False\n",
    "FS = 5e6\n",
    "FC = 5.9e9\n",
    "\n",
    "# 多域/单域 XFR 参数\n",
    "BLOCK_SIZE = 256                              # XFR 的 block_size（每个 row sample 的长度）\n",
    "Y_TAKE = 1                                    # 轮询时每个 domain 一次取 y 条 frame\n",
    "MAX_SIG_PER_DOMAIN_LABEL = None               # 可限制每个(domain,label)最多用多少帧（None=不限制）\n",
    "\n",
    "# 数据集划分（按每个 domain & label 的时序切分）\n",
    "TRAIN_RATIO = 0.6\n",
    "VAL_RATIO = 0.2\n",
    "TEST_RATIO = 0.2\n",
    "assert abs(TRAIN_RATIO + VAL_RATIO + TEST_RATIO - 1.0) < 1e-6\n",
    "\n",
    "# 训练超参\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-3\n",
    "DROPOUT = 0.5\n",
    "IN_PLANES = 64\n",
    "PATIENCE = 8\n",
    "MIN_DELTA = 0.05                              # val acc 至少提升 MIN_DELTA 才算进步（百分点）\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 复现\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 信号处理：Doppler / AWGN\n",
    "# =========================\n",
    "def compute_doppler_shift(v_kmh, fc_hz):\n",
    "    c = 3e8\n",
    "    v = v_kmh / 3.6\n",
    "    return (v / c) * fc_hz\n",
    "\n",
    "def apply_doppler_shift(sig_complex_1d, fd_hz, fs_hz):\n",
    "    #ival = np.arange(sig_complex_1d.shape[0], dtype=np.float64)\n",
    "    t = np.arange(sig_complex_1d.shape[0], dtype=np.float64) / fs_hz\n",
    "    return sig_complex_1d * np.exp(1j * 2 * np.pi * fd_hz * t)\n",
    "\n",
    "def add_awgn_unit_power(sig_complex_1d, snr_db, rng: np.random.Generator):\n",
    "    \"\"\"\n",
    "    假设 sig_complex 已做单位功率归一化（E|s|^2 = 1）\n",
    "    则噪声功率 = 10^(-snr/10)\n",
    "    complex noise: real/imag 方差 = noise_power/2\n",
    "    \"\"\"\n",
    "    if snr_db is None:\n",
    "        return sig_complex_1d\n",
    "    noise_power = 10.0 ** (-snr_db / 10.0)\n",
    "    noise_std = math.sqrt(noise_power / 2.0)\n",
    "    noise = noise_std * (rng.standard_normal(sig_complex_1d.shape) + 1j * rng.standard_normal(sig_complex_1d.shape))\n",
    "    return sig_complex_1d + noise\n",
    "\n",
    "\n",
    "# =========================\n",
    "# domain 解析（你数据命名不同就改这里）\n",
    "# =========================\n",
    "_rx_pat = re.compile(r\"(?:^|[^a-z0-9])rx\\s*[_-]?\\s*(\\d+)\", re.IGNORECASE)\n",
    "_v_pat1 = re.compile(r\"(?:^|[^a-z0-9])v(?:el|speed)?\\s*[_-]?\\s*(\\d+)\", re.IGNORECASE)\n",
    "_v_pat2 = re.compile(r\"(\\d+)\\s*kmh\", re.IGNORECASE)\n",
    "\n",
    "def parse_domain_from_path(path):\n",
    "    \"\"\"\n",
    "    从文件路径解析 domain = (rx, v_kmh)\n",
    "    默认策略：\n",
    "      - 在路径各级组件中找 rx\\d+\n",
    "      - 找 v\\d+ 或 vel\\d+ 或 \\d+kmh\n",
    "    解析失败会抛异常（避免把所有数据都归为 unknown 导致“假多域”）\n",
    "    \"\"\"\n",
    "    parts = re.split(r\"[\\\\/]+\", path)\n",
    "    rx_val = None\n",
    "    v_val = None\n",
    "\n",
    "    for p in parts:\n",
    "        m = _rx_pat.search(p.lower())\n",
    "        if m:\n",
    "            rx_val = f\"rx{int(m.group(1))}\"\n",
    "        m = _v_pat1.search(p.lower())\n",
    "        if m:\n",
    "            v_val = int(m.group(1))\n",
    "        m = _v_pat2.search(p.lower())\n",
    "        if m and v_val is None:\n",
    "            v_val = int(m.group(1))\n",
    "\n",
    "    if rx_val is None or v_val is None:\n",
    "        raise ValueError(\n",
    "            f\"Cannot parse (rx,v) from path:\\n  {path}\\n\"\n",
    "            f\"Please modify parse_domain_from_path() to match your folder/file naming.\"\n",
    "        )\n",
    "    return (rx_val, v_val)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# .mat 读取：dmrs + txID\n",
    "# =========================\n",
    "def read_ltev_mat(file_path):\n",
    "    \"\"\"\n",
    "    读取单个 .mat（HDF5 mat）：\n",
    "      - rfDataset/dmrs: complex (num_frames, sample_len)\n",
    "      - rfDataset/txID: uint16 char array (label string)\n",
    "    返回：\n",
    "      tx_label(str), dmrs_complex(np.complex64 [N,L])\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, \"r\") as f:\n",
    "        if \"rfDataset\" not in f:\n",
    "            raise KeyError(f\"{file_path}: missing group 'rfDataset'\")\n",
    "        rf = f[\"rfDataset\"]\n",
    "\n",
    "        if \"dmrs\" not in rf:\n",
    "            raise KeyError(f\"{file_path}: missing 'rfDataset/dmrs'\")\n",
    "        dmrs = rf[\"dmrs\"]\n",
    "\n",
    "        # dmrs 可能是复结构体：fields real/imag\n",
    "        if hasattr(dmrs, \"dtype\") and dmrs.dtype.fields is not None and (\"real\" in dmrs.dtype.fields and \"imag\" in dmrs.dtype.fields):\n",
    "            dmrs_struct = dmrs[:]\n",
    "            dmrs_complex = dmrs_struct[\"real\"] + 1j * dmrs_struct[\"imag\"]\n",
    "        else:\n",
    "            # 兜底：若已是复数（少见），或是引用\n",
    "            dmrs_arr = dmrs[:]\n",
    "            if np.iscomplexobj(dmrs_arr):\n",
    "                dmrs_complex = dmrs_arr\n",
    "            else:\n",
    "                raise ValueError(f\"{file_path}: dmrs dtype not supported: {dmrs_arr.dtype}\")\n",
    "\n",
    "        if \"txID\" not in rf:\n",
    "            raise KeyError(f\"{file_path}: missing 'rfDataset/txID'\")\n",
    "        txID_uint16 = rf[\"txID\"][:].flatten().astype(np.uint16)\n",
    "        tx_label = \"\".join(chr(c) for c in txID_uint16 if c != 0).strip()\n",
    "\n",
    "    dmrs_complex = np.asarray(dmrs_complex, dtype=np.complex64)\n",
    "    if dmrs_complex.ndim != 2:\n",
    "        raise ValueError(f\"{file_path}: dmrs_complex must be 2D [N,L], got {dmrs_complex.shape}\")\n",
    "    return tx_label, dmrs_complex\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 缓存：读取干净（仅功率归一化）的多域数据\n",
    "# 结构：clean_data[domain][label] = list of np.float32 IQ frames (L,2)\n",
    "# =========================\n",
    "def load_clean_multidomain_dataset(data_root):\n",
    "    mat_files = glob.glob(os.path.join(data_root, \"**\", \"*.mat\"), recursive=True)\n",
    "    if len(mat_files) == 0:\n",
    "        raise RuntimeError(f\"No .mat found under: {data_root}\")\n",
    "\n",
    "    clean_data = {}      # domain -> label -> list[frame IQ]\n",
    "    label_set = set()\n",
    "    sample_len_ref = None\n",
    "\n",
    "    print(f\"[INFO] Found {len(mat_files)} .mat files\")\n",
    "    for fp in tqdm(sorted(mat_files), desc=\"Loading clean (.mat)\"):\n",
    "        domain = parse_domain_from_path(fp)  # (rx, v)\n",
    "        tx_label, dmrs_complex = read_ltev_mat(fp)  # [N,L] complex\n",
    "\n",
    "        if sample_len_ref is None:\n",
    "            sample_len_ref = dmrs_complex.shape[1]\n",
    "        else:\n",
    "            if dmrs_complex.shape[1] != sample_len_ref:\n",
    "                raise ValueError(f\"Sample length mismatch: {fp} has L={dmrs_complex.shape[1]}, expected {sample_len_ref}\")\n",
    "\n",
    "        # 每条帧做功率归一化，再转 IQ\n",
    "        frames_iq = []\n",
    "        for i in range(dmrs_complex.shape[0]):\n",
    "            sig = dmrs_complex[i, :]\n",
    "            sig = sig / (np.sqrt(np.mean(np.abs(sig) ** 2)) + 1e-12)  # unit power\n",
    "            iq = np.stack([sig.real, sig.imag], axis=-1).astype(np.float32)  # (L,2)\n",
    "            frames_iq.append(iq)\n",
    "\n",
    "        clean_data.setdefault(domain, {}).setdefault(tx_label, []).extend(frames_iq)\n",
    "        label_set.add(tx_label)\n",
    "\n",
    "    label_list = sorted(list(label_set))\n",
    "    label_to_idx = {lab: i for i, lab in enumerate(label_list)}\n",
    "\n",
    "    # 可选：限制每个(domain,label)最多帧数（保序截断）\n",
    "    if MAX_SIG_PER_DOMAIN_LABEL is not None:\n",
    "        for d in list(clean_data.keys()):\n",
    "            for lab in list(clean_data[d].keys()):\n",
    "                clean_data[d][lab] = clean_data[d][lab][:MAX_SIG_PER_DOMAIN_LABEL]\n",
    "\n",
    "    print(f\"[INFO] Domains: {len(clean_data)}  | Labels: {len(label_list)}  | Sample_len: {sample_len_ref}\")\n",
    "    # 打印部分 domain 例子\n",
    "    dom_preview = list(clean_data.keys())[:10]\n",
    "    print(f\"[INFO] Domain preview: {dom_preview}\")\n",
    "    return clean_data, label_to_idx, sample_len_ref\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 给定“干净 IQ”，按 SNR 生成“带噪 IQ”（并可加额外 Doppler）\n",
    "# noisy_data[domain][label] = list of IQ frames (L,2)\n",
    "# =========================\n",
    "def make_noisy_dataset(clean_data, label_to_idx, snr_db, seed_base=1234,\n",
    "                       apply_awgn=True, apply_extra_doppler=False):\n",
    "    rng = np.random.default_rng(seed_base + int(snr_db * 10) if snr_db is not None else seed_base)\n",
    "    noisy_data = {}\n",
    "\n",
    "    for domain, lab_dict in clean_data.items():\n",
    "        rx, v_kmh = domain\n",
    "        fd = compute_doppler_shift(v_kmh, FC) if apply_extra_doppler else 0.0\n",
    "\n",
    "        for lab, frames_iq in lab_dict.items():\n",
    "            out_frames = []\n",
    "            for iq in frames_iq:\n",
    "                sigc = iq[:, 0].astype(np.float32) + 1j * iq[:, 1].astype(np.float32)\n",
    "\n",
    "                # 额外 Doppler（通常关闭）\n",
    "                if apply_extra_doppler and fd != 0.0:\n",
    "                    sigc = apply_doppler_shift(sigc, fd, FS)\n",
    "\n",
    "                # AWGN（SNR sweep）\n",
    "                if apply_awgn:\n",
    "                    sigc = add_awgn_unit_power(sigc, snr_db, rng)\n",
    "\n",
    "                out_iq = np.stack([sigc.real, sigc.imag], axis=-1).astype(np.float32)\n",
    "                out_frames.append(out_iq)\n",
    "\n",
    "            noisy_data.setdefault(domain, {})[lab] = out_frames\n",
    "\n",
    "    return noisy_data\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 划分：每个(domain,label)按时序切 train/val/test\n",
    "# =========================\n",
    "def split_by_domain_label_ordered(noisy_data, train_ratio, val_ratio, test_ratio):\n",
    "    train_dict, val_dict, test_dict = {}, {}, {}\n",
    "    for domain, lab_dict in noisy_data.items():\n",
    "        for lab, frames in lab_dict.items():\n",
    "            n = len(frames)\n",
    "            if n < 3:\n",
    "                continue\n",
    "            n_train = int(n * train_ratio)\n",
    "            n_val = int(n * val_ratio)\n",
    "            n_test = n - n_train - n_val\n",
    "            if n_train <= 0 or n_val <= 0 or n_test <= 0:\n",
    "                # 太少则跳过这个(domain,label)\n",
    "                continue\n",
    "\n",
    "            tr = frames[:n_train]\n",
    "            va = frames[n_train:n_train + n_val]\n",
    "            te = frames[n_train + n_val:]\n",
    "\n",
    "            train_dict.setdefault(domain, {})[lab] = tr\n",
    "            val_dict.setdefault(domain, {})[lab] = va\n",
    "            test_dict.setdefault(domain, {})[lab] = te\n",
    "    return train_dict, val_dict, test_dict\n",
    "\n",
    "\n",
    "# =========================\n",
    "# XFR：把 frame list -> blocks（转置后的 block）\n",
    "# block_transposed shape = (sample_len, block_size, 2)\n",
    "# =========================\n",
    "def build_xfr_blocks_single_domain(frames_iq_list, block_size, y_take=1):\n",
    "    \"\"\"\n",
    "    单域：按顺序取 frame，凑足 block_size -> 转置 -> 存 block\n",
    "    y_take 对单域影响不大，保留接口与多域一致\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    acc = []\n",
    "    i = 0\n",
    "    while i < len(frames_iq_list):\n",
    "        take_n = min(y_take, len(frames_iq_list) - i)\n",
    "        acc.extend(frames_iq_list[i:i+take_n])\n",
    "        i += take_n\n",
    "\n",
    "        while len(acc) >= block_size:\n",
    "            chunk = acc[:block_size]\n",
    "            acc = acc[block_size:]\n",
    "\n",
    "            block = np.array(chunk, dtype=np.float32)          # (block_size, sample_len, 2)\n",
    "            block_t = np.transpose(block, (1, 0, 2))           # (sample_len, block_size, 2)\n",
    "            blocks.append(block_t)\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def build_xfr_blocks_multidomain_for_label(train_dict, label, domains, block_size, y_take=1):\n",
    "    \"\"\"\n",
    "    多域：对某个 label，从多个 domain 轮询抽样，拼成 block_size -> 转置 -> 存 block\n",
    "    train_dict[domain][label] = list of frames (ordered)\n",
    "    \"\"\"\n",
    "    # 收集各 domain 的该 label 帧列表（保序）\n",
    "    dom_lists = []\n",
    "    dom_keys = []\n",
    "    for d in domains:\n",
    "        if d in train_dict and label in train_dict[d] and len(train_dict[d][label]) > 0:\n",
    "            dom_lists.append(list(train_dict[d][label]))  # copy，后面会 pop(0)\n",
    "            dom_keys.append(d)\n",
    "\n",
    "    if len(dom_lists) == 0:\n",
    "        return []\n",
    "\n",
    "    blocks = []\n",
    "    acc = []\n",
    "    ptr = 0\n",
    "    num_dom = len(dom_lists)\n",
    "\n",
    "    while any(len(lst) > 0 for lst in dom_lists):\n",
    "        di = ptr % num_dom\n",
    "        lst = dom_lists[di]\n",
    "        if len(lst) > 0:\n",
    "            take_n = min(y_take, len(lst))\n",
    "            # 顺序取 y_take\n",
    "            for _ in range(take_n):\n",
    "                acc.append(lst.pop(0))\n",
    "        ptr += 1\n",
    "\n",
    "        while len(acc) >= block_size:\n",
    "            chunk = acc[:block_size]\n",
    "            acc = acc[block_size:]\n",
    "\n",
    "            block = np.array(chunk, dtype=np.float32)          # (block_size, sample_len, 2)\n",
    "            block_t = np.transpose(block, (1, 0, 2))           # (sample_len, block_size, 2)\n",
    "            blocks.append(block_t)\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def expand_blocks_to_rows(x_blocks_t, y_blocks):\n",
    "    \"\"\"\n",
    "    x_blocks_t: np.array (num_blocks, sample_len, block_size, 2)\n",
    "    y_blocks:   np.array (num_blocks,)\n",
    "    返回：\n",
    "      X_rows: (num_blocks*sample_len, block_size, 2)\n",
    "      y_rows: (num_blocks*sample_len,)\n",
    "    \"\"\"\n",
    "    num_blocks, sample_len, block_size, _ = x_blocks_t.shape\n",
    "    X_rows = x_blocks_t.reshape(-1, block_size, 2)\n",
    "    y_rows = np.repeat(y_blocks, sample_len)\n",
    "    return X_rows, y_rows\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RAW：把单域 frames list -> (N, sample_len,2)\n",
    "# =========================\n",
    "def build_raw_rows_single_domain(domain_dict, label_to_idx):\n",
    "    X_list, y_list = [], []\n",
    "    for lab, frames in domain_dict.items():\n",
    "        if lab not in label_to_idx:\n",
    "            continue\n",
    "        yi = label_to_idx[lab]\n",
    "        for f in frames:\n",
    "            X_list.append(f)\n",
    "            y_list.append(yi)\n",
    "    if len(X_list) == 0:\n",
    "        return None, None\n",
    "    X = np.stack(X_list, axis=0).astype(np.float32)  # (N, sample_len,2)\n",
    "    y = np.array(y_list, dtype=np.int64)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 模型：1D ResNet18（接受可变长度）\n",
    "# =========================\n",
    "class BasicBlock1D(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "class ResNet18_1D(nn.Module):\n",
    "    def __init__(self, num_classes, in_planes=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.conv1 = nn.Conv1d(2, in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1, dropout=dropout)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2, dropout=dropout)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2, dropout=dropout)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2, dropout=dropout)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride, dropout):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes)\n",
    "            )\n",
    "        layers = [BasicBlock1D(self.in_planes, planes, stride, downsample, dropout)]\n",
    "        self.in_planes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock1D(self.in_planes, planes, dropout=dropout))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, 2) -> (B, 2, L)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 评估（acc + confusion）\n",
    "# =========================\n",
    "def evaluate_model(model, loader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_y, all_p = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            _, pred = torch.max(out, 1)\n",
    "            correct += (pred == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "            all_y.extend(yb.cpu().numpy().tolist())\n",
    "            all_p.extend(pred.cpu().numpy().tolist())\n",
    "    acc = 100.0 * correct / max(1, total)\n",
    "    cm = confusion_matrix(all_y, all_p, labels=list(range(num_classes)))\n",
    "    return acc, cm\n",
    "\n",
    "\n",
    "def plot_confusion(cm, title, save_path):\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_curve(x, y, xlabel, ylabel, title, save_path):\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(x, y, marker=\"o\")\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 构建训练 blocks（多域） + 构建 val/test（单域 RAW & XFR）\n",
    "# =========================\n",
    "def build_all_datasets_for_snr(noisy_train, noisy_val, noisy_test, label_to_idx,\n",
    "                               sample_len, block_size, y_take):\n",
    "    domains = sorted(list(set(list(noisy_train.keys()) + list(noisy_val.keys()) + list(noisy_test.keys()))))\n",
    "\n",
    "    # ===== 训练 blocks（多域混合）：按 label 分别轮询 domains 拼 block\n",
    "    idx_to_label = {i: lab for lab, i in label_to_idx.items()}\n",
    "    label_list = [idx_to_label[i] for i in range(len(label_to_idx))]\n",
    "\n",
    "    train_blocks = []\n",
    "    train_block_labels = []\n",
    "\n",
    "    for lab in label_list:\n",
    "        blocks_lab = build_xfr_blocks_multidomain_for_label(\n",
    "            train_dict=noisy_train,\n",
    "            label=lab,\n",
    "            domains=domains,\n",
    "            block_size=block_size,\n",
    "            y_take=y_take\n",
    "        )\n",
    "        if len(blocks_lab) == 0:\n",
    "            continue\n",
    "        yi = label_to_idx[lab]\n",
    "        train_blocks.extend(blocks_lab)\n",
    "        train_block_labels.extend([yi] * len(blocks_lab))\n",
    "\n",
    "    if len(train_blocks) == 0:\n",
    "        raise RuntimeError(\"No training blocks were generated. Check BLOCK_SIZE/Y_TAKE and data volume per domain/label.\")\n",
    "\n",
    "    X_train_blocks = np.stack(train_blocks, axis=0).astype(np.float32)          # (nb, sample_len, block_size,2)\n",
    "    y_train_blocks = np.array(train_block_labels, dtype=np.int64)\n",
    "\n",
    "    # ===== 单域 val/test：每个 domain 两套版本\n",
    "    # val_raw[domain] = (X,y) raw frames\n",
    "    # val_xfr[domain] = (X_rows,y_rows) XFR rows\n",
    "    val_raw, val_xfr = {}, {}\n",
    "    test_raw, test_xfr = {}, {}\n",
    "\n",
    "    for domain in domains:\n",
    "        # ---- val RAW\n",
    "        if domain in noisy_val:\n",
    "            Xv_raw, yv_raw = build_raw_rows_single_domain(noisy_val[domain], label_to_idx)\n",
    "            if Xv_raw is not None:\n",
    "                val_raw[domain] = (Xv_raw, yv_raw)\n",
    "\n",
    "        # ---- test RAW\n",
    "        if domain in noisy_test:\n",
    "            Xt_raw, yt_raw = build_raw_rows_single_domain(noisy_test[domain], label_to_idx)\n",
    "            if Xt_raw is not None:\n",
    "                test_raw[domain] = (Xt_raw, yt_raw)\n",
    "\n",
    "        # ---- val XFR (single-domain internal blocks)\n",
    "        if domain in noisy_val:\n",
    "            blocks_d, yb_d = [], []\n",
    "            for lab, frames in noisy_val[domain].items():\n",
    "                if lab not in label_to_idx:\n",
    "                    continue\n",
    "                blocks = build_xfr_blocks_single_domain(frames, block_size=block_size, y_take=y_take)\n",
    "                if len(blocks) == 0:\n",
    "                    continue\n",
    "                yi = label_to_idx[lab]\n",
    "                blocks_d.extend(blocks)\n",
    "                yb_d.extend([yi] * len(blocks))\n",
    "\n",
    "            if len(blocks_d) > 0:\n",
    "                Xb = np.stack(blocks_d, axis=0).astype(np.float32)\n",
    "                yb = np.array(yb_d, dtype=np.int64)\n",
    "                X_rows, y_rows = expand_blocks_to_rows(Xb, yb)\n",
    "                val_xfr[domain] = (X_rows, y_rows)\n",
    "\n",
    "        # ---- test XFR (single-domain internal blocks)\n",
    "        if domain in noisy_test:\n",
    "            blocks_d, yb_d = [], []\n",
    "            for lab, frames in noisy_test[domain].items():\n",
    "                if lab not in label_to_idx:\n",
    "                    continue\n",
    "                blocks = build_xfr_blocks_single_domain(frames, block_size=block_size, y_take=y_take)\n",
    "                if len(blocks) == 0:\n",
    "                    continue\n",
    "                yi = label_to_idx[lab]\n",
    "                blocks_d.extend(blocks)\n",
    "                yb_d.extend([yi] * len(blocks))\n",
    "\n",
    "            if len(blocks_d) > 0:\n",
    "                Xb = np.stack(blocks_d, axis=0).astype(np.float32)\n",
    "                yb = np.array(yb_d, dtype=np.int64)\n",
    "                X_rows, y_rows = expand_blocks_to_rows(Xb, yb)\n",
    "                test_xfr[domain] = (X_rows, y_rows)\n",
    "\n",
    "    return (X_train_blocks, y_train_blocks), (val_raw, val_xfr), (test_raw, test_xfr), domains\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 训练（单模型）：训练用 XFR rows（从多域 blocks 展开）\n",
    "# 验证：每个 domain 两套版本都评估（RAW/XFR）\n",
    "# 早停：默认用 “val_xfr 各 domain 平均 acc”\n",
    "# =========================\n",
    "def train_one_snr(\n",
    "        snr_db,\n",
    "        X_train_blocks, y_train_blocks,\n",
    "        val_raw, val_xfr,\n",
    "        test_raw, test_xfr,\n",
    "        num_classes,\n",
    "        sample_len,\n",
    "        block_size,\n",
    "        save_folder):\n",
    "\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    results_file = os.path.join(save_folder, \"results.txt\")\n",
    "\n",
    "    # 展开训练 blocks 为 row-samples（XFR rows）\n",
    "    X_train_rows, y_train_rows = expand_blocks_to_rows(X_train_blocks, y_train_blocks)\n",
    "    train_ds = TensorDataset(torch.tensor(X_train_rows, dtype=torch.float32),\n",
    "                             torch.tensor(y_train_rows, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    # 为 val/test 构造 DataLoader（按 domain）\n",
    "    def make_loaders(domain_dict, batch_size):\n",
    "        loaders = {}\n",
    "        for dom, (X, y) in domain_dict.items():\n",
    "            ds = TensorDataset(torch.tensor(X, dtype=torch.float32),\n",
    "                               torch.tensor(y, dtype=torch.long))\n",
    "            loaders[dom] = DataLoader(ds, batch_size=batch_size, shuffle=False,\n",
    "                                      num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        return loaders\n",
    "\n",
    "    val_raw_loaders = make_loaders(val_raw, BATCH_SIZE)\n",
    "    val_xfr_loaders = make_loaders(val_xfr, BATCH_SIZE)\n",
    "    test_raw_loaders = make_loaders(test_raw, BATCH_SIZE)\n",
    "    test_xfr_loaders = make_loaders(test_xfr, BATCH_SIZE)\n",
    "\n",
    "    # 记录参数\n",
    "    with open(results_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== LTE-V Multi-domain Train / Single-domain Eval (RAW vs XFR) ===\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"SNR_dB: {snr_db}\\n\")\n",
    "        f.write(f\"APPLY_AWGN: {APPLY_AWGN}, APPLY_EXTRA_DOPPLER: {APPLY_EXTRA_DOPPLER}\\n\")\n",
    "        f.write(f\"BLOCK_SIZE: {block_size}, Y_TAKE: {Y_TAKE}\\n\")\n",
    "        f.write(f\"Sample_len (raw): {sample_len}\\n\")\n",
    "        f.write(f\"Train rows: {len(train_ds)} (from blocks={len(X_train_blocks)})\\n\")\n",
    "        f.write(f\"Val domains RAW: {len(val_raw_loaders)}, XFR: {len(val_xfr_loaders)}\\n\")\n",
    "        f.write(f\"Test domains RAW: {len(test_raw_loaders)}, XFR: {len(test_xfr_loaders)}\\n\")\n",
    "        f.write(f\"Model: ResNet18_1D, in_planes={IN_PLANES}, dropout={DROPOUT}\\n\")\n",
    "        f.write(f\"Batch={BATCH_SIZE}, Epochs={EPOCHS}, LR={LR}, WD={WEIGHT_DECAY}, Patience={PATIENCE}, MinDelta={MIN_DELTA}\\n\")\n",
    "        f.write(\"===============================================================\\n\\n\")\n",
    "\n",
    "    # 模型\n",
    "    model = ResNet18_1D(num_classes=num_classes, in_planes=IN_PLANES, dropout=DROPOUT).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    best_val = -1.0\n",
    "    best_wts = None\n",
    "    patience_cnt = 0\n",
    "\n",
    "    train_loss_hist = []\n",
    "    val_xfr_mean_hist = []\n",
    "    val_raw_mean_hist = []\n",
    "\n",
    "    # 评估函数：按 domain 求平均 acc（并可保存 cm）\n",
    "    def eval_domains(loaders, tag, epoch=None):\n",
    "        if len(loaders) == 0:\n",
    "            return None, {}\n",
    "        accs = {}\n",
    "        cms = {}\n",
    "        for dom, ld in loaders.items():\n",
    "            acc, cm = evaluate_model(model, ld, DEVICE, num_classes)\n",
    "            accs[dom] = acc\n",
    "            cms[dom] = cm\n",
    "        mean_acc = float(np.mean(list(accs.values()))) if len(accs) else None\n",
    "        return mean_acc, (accs, cms)\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running += loss.item()\n",
    "\n",
    "        train_loss = running / max(1, len(train_loader))\n",
    "        train_loss_hist.append(train_loss)\n",
    "\n",
    "        # 验证：两套版本（单域）\n",
    "        model.eval()\n",
    "        val_xfr_mean, (val_xfr_accs, _) = eval_domains(val_xfr_loaders, \"val_xfr\")\n",
    "        val_raw_mean, (val_raw_accs, _) = eval_domains(val_raw_loaders, \"val_raw\")\n",
    "\n",
    "        val_xfr_mean_hist.append(val_xfr_mean if val_xfr_mean is not None else np.nan)\n",
    "        val_raw_mean_hist.append(val_raw_mean if val_raw_mean is not None else np.nan)\n",
    "\n",
    "        log = (f\"Epoch {epoch:03d}/{EPOCHS} | \"\n",
    "               f\"TrainLoss={train_loss:.4f} | \"\n",
    "               f\"ValXFR_mean={val_xfr_mean if val_xfr_mean is not None else -1:.2f}% | \"\n",
    "               f\"ValRAW_mean={val_raw_mean if val_raw_mean is not None else -1:.2f}%\")\n",
    "        print(log)\n",
    "        with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(log + \"\\n\")\n",
    "\n",
    "        # 早停：用 val_xfr_mean\n",
    "        score = val_xfr_mean if val_xfr_mean is not None else -1.0\n",
    "        if score > best_val + MIN_DELTA:\n",
    "            best_val = score\n",
    "            best_wts = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_cnt = 0\n",
    "\n",
    "            # 保存每次改进的域级结果\n",
    "            with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"[BEST] epoch={epoch}  ValXFR_mean={best_val:.2f}%\\n\")\n",
    "                # 打印各 domain（可选，太长可注释）\n",
    "                for dom, acc in sorted(val_xfr_accs.items()):\n",
    "                    f.write(f\"    ValXFR {dom}: {acc:.2f}%\\n\")\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= PATIENCE:\n",
    "                print(\"[INFO] Early stopping.\")\n",
    "                with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(\"[INFO] Early stopping.\\n\")\n",
    "                break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # 恢复最佳\n",
    "    if best_wts is not None:\n",
    "        model.load_state_dict(best_wts)\n",
    "\n",
    "    # 最终测试：两套版本（单域）\n",
    "    model.eval()\n",
    "    test_xfr_mean, (test_xfr_accs, test_xfr_cms) = (None, ({}, {}))\n",
    "    test_raw_mean, (test_raw_accs, test_raw_cms) = (None, ({}, {}))\n",
    "\n",
    "    if len(test_xfr_loaders) > 0:\n",
    "        test_xfr_mean, (test_xfr_accs, test_xfr_cms) = (lambda loaders: (\n",
    "            float(np.mean([evaluate_model(model, ld, DEVICE, num_classes)[0] for ld in loaders.values()])),\n",
    "            {dom: evaluate_model(model, ld, DEVICE, num_classes) for dom, ld in loaders.items()}\n",
    "        ))(test_xfr_loaders)\n",
    "\n",
    "        # 拆出来\n",
    "        txfr_accs = {dom: v[0] for dom, v in test_xfr_accs.items()}\n",
    "        txfr_cms = {dom: v[1] for dom, v in test_xfr_accs.items()}\n",
    "        test_xfr_accs, test_xfr_cms = txfr_accs, txfr_cms\n",
    "\n",
    "    if len(test_raw_loaders) > 0:\n",
    "        test_raw_mean, (test_raw_accs, test_raw_cms) = (lambda loaders: (\n",
    "            float(np.mean([evaluate_model(model, ld, DEVICE, num_classes)[0] for ld in loaders.values()])),\n",
    "            {dom: evaluate_model(model, ld, DEVICE, num_classes) for dom, ld in loaders.items()}\n",
    "        ))(test_raw_loaders)\n",
    "\n",
    "        traw_accs = {dom: v[0] for dom, v in test_raw_accs.items()}\n",
    "        traw_cms = {dom: v[1] for dom, v in test_raw_accs.items()}\n",
    "        test_raw_accs, test_raw_cms = traw_accs, traw_cms\n",
    "\n",
    "    # 写结果\n",
    "    with open(results_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n=== FINAL TEST RESULTS ===\\n\")\n",
    "        f.write(f\"TestXFR_mean: {test_xfr_mean if test_xfr_mean is not None else 'N/A'}\\n\")\n",
    "        if test_xfr_mean is not None:\n",
    "            for dom, acc in sorted(test_xfr_accs.items()):\n",
    "                f.write(f\"    TestXFR {dom}: {acc:.2f}%\\n\")\n",
    "        f.write(f\"TestRAW_mean: {test_raw_mean if test_raw_mean is not None else 'N/A'}\\n\")\n",
    "        if test_raw_mean is not None:\n",
    "            for dom, acc in sorted(test_raw_accs.items()):\n",
    "                f.write(f\"    TestRAW {dom}: {acc:.2f}%\\n\")\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), os.path.join(save_folder, \"best_model.pth\"))\n",
    "\n",
    "    # 画训练曲线\n",
    "    plot_curve(\n",
    "        x=list(range(1, len(train_loss_hist) + 1)),\n",
    "        y=train_loss_hist,\n",
    "        xlabel=\"Epoch\",\n",
    "        ylabel=\"Train Loss\",\n",
    "        title=f\"SNR={snr_db} Train Loss\",\n",
    "        save_path=os.path.join(save_folder, \"train_loss.png\")\n",
    "    )\n",
    "    plot_curve(\n",
    "        x=list(range(1, len(val_xfr_mean_hist) + 1)),\n",
    "        y=val_xfr_mean_hist,\n",
    "        xlabel=\"Epoch\",\n",
    "        ylabel=\"Val XFR Mean Acc (%)\",\n",
    "        title=f\"SNR={snr_db} Val XFR Mean Acc\",\n",
    "        save_path=os.path.join(save_folder, \"val_xfr_mean_acc.png\")\n",
    "    )\n",
    "    plot_curve(\n",
    "        x=list(range(1, len(val_raw_mean_hist) + 1)),\n",
    "        y=val_raw_mean_hist,\n",
    "        xlabel=\"Epoch\",\n",
    "        ylabel=\"Val RAW Mean Acc (%)\",\n",
    "        title=f\"SNR={snr_db} Val RAW Mean Acc\",\n",
    "        save_path=os.path.join(save_folder, \"val_raw_mean_acc.png\")\n",
    "    )\n",
    "\n",
    "    # 保存若干 domain 的混淆矩阵（避免太多文件）\n",
    "    # 选 acc 最低/最高的各 1 个做示例\n",
    "    def save_cm_examples(accs, cms, tag):\n",
    "        if not accs:\n",
    "            return\n",
    "        dom_sorted = sorted(accs.items(), key=lambda x: x[1])\n",
    "        worst_dom = dom_sorted[0][0]\n",
    "        best_dom = dom_sorted[-1][0]\n",
    "        plot_confusion(cms[worst_dom], f\"{tag} WORST {worst_dom}\", os.path.join(save_folder, f\"cm_{tag}_worst.png\"))\n",
    "        plot_confusion(cms[best_dom], f\"{tag} BEST {best_dom}\", os.path.join(save_folder, f\"cm_{tag}_best.png\"))\n",
    "\n",
    "    save_cm_examples(test_xfr_accs, test_xfr_cms, \"test_xfr\")\n",
    "    save_cm_examples(test_raw_accs, test_raw_cms, \"test_raw\")\n",
    "\n",
    "    # 返回测试均值（两套）\n",
    "    return (test_raw_mean if test_raw_mean is not None else np.nan,\n",
    "            test_xfr_mean if test_xfr_mean is not None else np.nan)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 主程序：缓存 clean -> 对每个 SNR 构造 noisy -> 构造数据集 -> 训练 -> 汇总曲线\n",
    "# =========================\n",
    "def main():\n",
    "    os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "    print(\"[INFO] DEVICE:\", DEVICE)\n",
    "\n",
    "    # 1) 载入干净多域数据（只做功率归一化，不加噪）\n",
    "    clean_data, label_to_idx, sample_len = load_clean_multidomain_dataset(DATA_ROOT)\n",
    "    num_classes = len(label_to_idx)\n",
    "\n",
    "    # 保存一个 meta\n",
    "    meta = {\n",
    "        \"data_root\": DATA_ROOT,\n",
    "        \"num_domains\": len(clean_data),\n",
    "        \"num_classes\": num_classes,\n",
    "        \"sample_len_raw\": sample_len,\n",
    "        \"block_size_xfr\": BLOCK_SIZE,\n",
    "        \"y_take\": Y_TAKE,\n",
    "        \"train_ratio\": TRAIN_RATIO,\n",
    "        \"val_ratio\": VAL_RATIO,\n",
    "        \"test_ratio\": TEST_RATIO,\n",
    "        \"apply_awgn\": APPLY_AWGN,\n",
    "        \"apply_extra_doppler\": APPLY_EXTRA_DOPPLER,\n",
    "        \"fs\": FS,\n",
    "        \"fc\": FC,\n",
    "        \"snr_list\": SNR_LIST,\n",
    "        \"seed\": SEED\n",
    "    }\n",
    "    with open(os.path.join(SAVE_ROOT, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    snr_test_raw_means = []\n",
    "    snr_test_xfr_means = []\n",
    "\n",
    "    timestamp_all = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    for snr_db in SNR_LIST:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"[RUN] SNR = {snr_db} dB\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # 2) 生成带噪版本（每个 SNR 单独生成）\n",
    "        noisy_data = make_noisy_dataset(\n",
    "            clean_data, label_to_idx, snr_db=snr_db,\n",
    "            seed_base=777,\n",
    "            apply_awgn=APPLY_AWGN,\n",
    "            apply_extra_doppler=APPLY_EXTRA_DOPPLER\n",
    "        )\n",
    "\n",
    "        # 3) 划分 train/val/test（每个 domain & label 按时序切）\n",
    "        noisy_train, noisy_val, noisy_test = split_by_domain_label_ordered(\n",
    "            noisy_data, TRAIN_RATIO, VAL_RATIO, TEST_RATIO\n",
    "        )\n",
    "\n",
    "        # 4) 构造训练 blocks（多域）+ val/test 单域 RAW & XFR\n",
    "        (X_train_blocks, y_train_blocks), (val_raw, val_xfr), (test_raw, test_xfr), domains = build_all_datasets_for_snr(\n",
    "            noisy_train, noisy_val, noisy_test,\n",
    "            label_to_idx=label_to_idx,\n",
    "            sample_len=sample_len,\n",
    "            block_size=BLOCK_SIZE,\n",
    "            y_take=Y_TAKE\n",
    "        )\n",
    "\n",
    "        print(f\"[INFO] Train blocks: {len(X_train_blocks)}\")\n",
    "        print(f\"[INFO] Val domains RAW={len(val_raw)} XFR={len(val_xfr)} | Test domains RAW={len(test_raw)} XFR={len(test_xfr)}\")\n",
    "\n",
    "        # 5) 训练与评估\n",
    "        run_name = f\"{timestamp_all}_LTV_SNR{snr_db}dB_bs{BLOCK_SIZE}_y{Y_TAKE}_cls{num_classes}\"\n",
    "        save_folder = os.path.join(SAVE_ROOT, run_name)\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "        test_raw_mean, test_xfr_mean = train_one_snr(\n",
    "            snr_db=snr_db,\n",
    "            X_train_blocks=X_train_blocks,\n",
    "            y_train_blocks=y_train_blocks,\n",
    "            val_raw=val_raw,\n",
    "            val_xfr=val_xfr,\n",
    "            test_raw=test_raw,\n",
    "            test_xfr=test_xfr,\n",
    "            num_classes=num_classes,\n",
    "            sample_len=sample_len,\n",
    "            block_size=BLOCK_SIZE,\n",
    "            save_folder=save_folder\n",
    "        )\n",
    "\n",
    "        snr_test_raw_means.append(test_raw_mean)\n",
    "        snr_test_xfr_means.append(test_xfr_mean)\n",
    "\n",
    "        print(f\"[DONE] SNR={snr_db} | TestRAW_mean={test_raw_mean:.2f}% | TestXFR_mean={test_xfr_mean:.2f}% | saved: {save_folder}\")\n",
    "\n",
    "        # 清理\n",
    "        del noisy_data, noisy_train, noisy_val, noisy_test\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # 6) 汇总曲线（两条：RAW vs XFR）\n",
    "    plot_curve(\n",
    "        x=SNR_LIST,\n",
    "        y=snr_test_raw_means,\n",
    "        xlabel=\"SNR (dB)\",\n",
    "        ylabel=\"Mean Test Acc (%)\",\n",
    "        title=\"LTE-V Mean Test Acc vs SNR (Single-domain RAW)\",\n",
    "        save_path=os.path.join(SAVE_ROOT, f\"{timestamp_all}_snr_vs_acc_raw.png\")\n",
    "    )\n",
    "    plot_curve(\n",
    "        x=SNR_LIST,\n",
    "        y=snr_test_xfr_means,\n",
    "        xlabel=\"SNR (dB)\",\n",
    "        ylabel=\"Mean Test Acc (%)\",\n",
    "        title=\"LTE-V Mean Test Acc vs SNR (Single-domain XFR)\",\n",
    "        save_path=os.path.join(SAVE_ROOT, f\"{timestamp_all}_snr_vs_acc_xfr.png\")\n",
    "    )\n",
    "\n",
    "    # 保存汇总\n",
    "    summary = []\n",
    "    for snr_db, a_raw, a_xfr in zip(SNR_LIST, snr_test_raw_means, snr_test_xfr_means):\n",
    "        summary.append({\"snr_db\": snr_db, \"test_raw_mean\": a_raw, \"test_xfr_mean\": a_xfr})\n",
    "    with open(os.path.join(SAVE_ROOT, f\"{timestamp_all}_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n=== FINAL SUMMARY ===\")\n",
    "    for snr_db, a_raw, a_xfr in zip(SNR_LIST, snr_test_raw_means, snr_test_xfr_means):\n",
    "        print(f\"SNR {snr_db:>3} dB | RAW_mean={a_raw:.2f}% | XFR_mean={a_xfr:.2f}%\")\n",
    "    print(\"[INFO] Plots & logs saved to:\", SAVE_ROOT)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
