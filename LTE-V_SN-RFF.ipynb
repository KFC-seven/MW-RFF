{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LTE-V (.mat via HDF5) - Spec + Siamese (FAST GPU PIPELINE)\n",
    "#\n",
    "# 目标设置（按你的要求）：\n",
    "# 1) Train online augmentation（每样本随机）：\n",
    "#    - Multipath: 指数 PDP 的 TDL，多径 RMS delay spread ~ Uniform[5,300] ns\n",
    "#    - Doppler: v ~ Uniform[0,120] km/h -> fd=(v/c)*fc\n",
    "#    - AWGN: SNR ~ Uniform[-40,20] dB\n",
    "# 2) Test：\n",
    "#    - Doppler 固定 120 km/h\n",
    "#    - AWGN SNR sweep: 20,15,...,-40 dB\n",
    "#    - 默认不额外加 multipath（如要测试也加：TEST_USE_MULTIPATH=True）\n",
    "# 3) 模型与损失：\n",
    "#    - 输入：log|STFT| 2D spectrogram（resize 到 64x64）\n",
    "#    - Siamese 两分支共享权重\n",
    "#    - Loss: L = NT-Xent(tau=0.05) + CE\n",
    "# 4) 训练策略：\n",
    "#    - Adam lr=3e-4\n",
    "#    - ReduceLROnPlateau(val_loss, patience=10, factor=0.5)\n",
    "#    - val loss 30 epoch 不降 -> early stop\n",
    "# 5) 数据划分：\n",
    "#    - 优先文件级 stratify split；不满足则回退样本级 stratify\n",
    "#\n",
    "# 加速点：\n",
    "# - Dataset 只输出 IQ，不在 __getitem__ 中 STFT\n",
    "# - 增强 + STFT 全部 GPU batch 计算\n",
    "# - AMP 混合精度（NT-Xent 强制 fp32，避免溢出）\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "# ----------------------------\n",
    "# 0) 全局配置\n",
    "# ----------------------------\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "USE_AMP = (DEVICE.type == \"cuda\")\n",
    "\n",
    "# LTE-V 数据根目录\n",
    "DATA_PATH = \"E:/rf_datasets_IQ/\"\n",
    "RECURSIVE_GLOB = True\n",
    "\n",
    "# RF 参数（与你 XFR LTE-V 脚本一致）\n",
    "FS = 5e6\n",
    "FC = 5.9e9\n",
    "\n",
    "# 训练参数\n",
    "BATCH_SIZE = 256          # 你可根据显存调整；NT-Xent 的 sim 矩阵是 (2B)^2，过大可能OOM\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 0.0\n",
    "MAX_EPOCHS = 200\n",
    "N_SPLITS = 5\n",
    "\n",
    "LR_PATIENCE = 10\n",
    "LR_FACTOR = 0.5\n",
    "ES_PATIENCE = 30\n",
    "\n",
    "# Siamese/对比学习\n",
    "TAU = 0.05\n",
    "LAMBDA_CL = 1.0\n",
    "LAMBDA_CE = 1.0\n",
    "\n",
    "# 训练增强范围\n",
    "AUG_USE_MULTIPATH = True\n",
    "AUG_USE_DOPPLER   = True\n",
    "AUG_USE_AWGN      = True\n",
    "\n",
    "RMS_DS_NS_RANGE = (5.0, 300.0)\n",
    "TRAIN_V_KMH_RANGE = (0.0, 120.0)\n",
    "TRAIN_SNR_DB_RANGE = (-40.0, 20.0)\n",
    "\n",
    "# 测试增强\n",
    "TEST_V_KMH_FIXED = 120.0\n",
    "TEST_USE_MULTIPATH = False\n",
    "TEST_RMS_DS_NS_RANGE = RMS_DS_NS_RANGE\n",
    "TEST_SNR_LIST = list(range(20, -45, -5))\n",
    "\n",
    "# Spectrogram\n",
    "SPEC_NFFT = 128\n",
    "SPEC_WIN  = 128\n",
    "SPEC_HOP  = 16\n",
    "SPEC_SIZE = 64\n",
    "\n",
    "# multipath taps\n",
    "MAX_TAPS = 16\n",
    "\n",
    "# 读取控制\n",
    "MAX_SAMPLES_PER_FILE = None  # 如 2000；None 表示全取\n",
    "\n",
    "# DataLoader workers（Windows 建议先用 0）\n",
    "NUM_WORKERS_TRAIN = 0\n",
    "NUM_WORKERS_EVAL  = 0\n",
    "\n",
    "SAVE_ROOT = \"./training_results\"\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "SCRIPT_NAME = \"LTEV_SpecSiamese_FASTGPU_SNRsweep_Doppler120\"\n",
    "\n",
    "RETURN_CM = False  # 如需 confusion matrix，可设 True（会更慢/更占内存）\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) 读取 LTE-V HDF5(.mat)\n",
    "# ----------------------------\n",
    "def decode_txid(txid_arr: np.ndarray) -> str:\n",
    "    txid_arr = txid_arr.flatten()\n",
    "    chars = []\n",
    "    for c in txid_arr:\n",
    "        if int(c) == 0:\n",
    "            continue\n",
    "        try:\n",
    "            chars.append(chr(int(c)))\n",
    "        except Exception:\n",
    "            pass\n",
    "    s = \"\".join(chars).strip()\n",
    "    return s if s else \"UNKNOWN\"\n",
    "\n",
    "def load_dmrs_complex_from_file(h5f: h5py.File) -> np.ndarray:\n",
    "    rfDataset = h5f[\"rfDataset\"]\n",
    "    dmrs_obj = rfDataset[\"dmrs\"]\n",
    "\n",
    "    if isinstance(dmrs_obj, h5py.Dataset) and dmrs_obj.dtype.fields is not None:\n",
    "        real = dmrs_obj[\"real\"][()]\n",
    "        imag = dmrs_obj[\"imag\"][()]\n",
    "    elif isinstance(dmrs_obj, h5py.Group):\n",
    "        real = dmrs_obj[\"real\"][()]\n",
    "        imag = dmrs_obj[\"imag\"][()]\n",
    "    else:\n",
    "        tmp = dmrs_obj[()]\n",
    "        if hasattr(tmp, \"dtype\") and tmp.dtype.fields is not None:\n",
    "            real = tmp[\"real\"]\n",
    "            imag = tmp[\"imag\"]\n",
    "        else:\n",
    "            raise RuntimeError(\"无法解析 dmrs：既不是 compound dataset 也不是 group(real/imag).\")\n",
    "\n",
    "    dmrs_complex = np.asarray(real + 1j * imag)\n",
    "\n",
    "    if dmrs_complex.ndim != 2:\n",
    "        raise RuntimeError(f\"dmrs_complex 维度异常: {dmrs_complex.shape}\")\n",
    "\n",
    "    # 常见：读出来是 (L,N) 则转置为 (N,L)\n",
    "    if dmrs_complex.shape[0] <= 2048 and dmrs_complex.shape[1] > dmrs_complex.shape[0]:\n",
    "        dmrs_complex = dmrs_complex.T\n",
    "\n",
    "    return dmrs_complex.astype(np.complex64)\n",
    "\n",
    "def load_ltev_dataset(data_path: str, recursive: bool = True, max_samples_per_file=None):\n",
    "    if recursive:\n",
    "        mat_files = glob.glob(os.path.join(data_path, \"**\", \"*.mat\"), recursive=True)\n",
    "    else:\n",
    "        mat_files = glob.glob(os.path.join(data_path, \"*.mat\"))\n",
    "\n",
    "    if len(mat_files) == 0:\n",
    "        raise RuntimeError(f\"未找到 .mat 文件：{data_path}\")\n",
    "\n",
    "    signals_by_file = []\n",
    "    label_str_by_file = []\n",
    "    file_paths = []\n",
    "\n",
    "    print(f\"[INFO] Found {len(mat_files)} .mat files\")\n",
    "\n",
    "    for fp in mat_files:\n",
    "        try:\n",
    "            with h5py.File(fp, \"r\") as f:\n",
    "                rfDataset = f[\"rfDataset\"]\n",
    "                txid_arr = np.asarray(rfDataset[\"txID\"][()])\n",
    "                tx_str = decode_txid(txid_arr)\n",
    "\n",
    "                dmrs_complex = load_dmrs_complex_from_file(f)\n",
    "                if max_samples_per_file is not None:\n",
    "                    dmrs_complex = dmrs_complex[: int(max_samples_per_file)]\n",
    "\n",
    "                if dmrs_complex.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                signals_by_file.append(dmrs_complex)  # (N,L) complex64\n",
    "                label_str_by_file.append(tx_str)\n",
    "                file_paths.append(fp)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skip file due to read/parse error: {fp} | {repr(e)}\")\n",
    "\n",
    "    if len(signals_by_file) == 0:\n",
    "        raise RuntimeError(\"所有文件都读取失败或为空，请检查数据结构。\")\n",
    "\n",
    "    # 统计\n",
    "    cnt = Counter(label_str_by_file)\n",
    "    print(\"[INFO] txID classes:\", len(cnt))\n",
    "    for k, v in sorted(cnt.items(), key=lambda x: (-x[1], x[0])):\n",
    "        print(f\"  {k}: {v} files\")\n",
    "\n",
    "    Ls = [arr.shape[1] for arr in signals_by_file]\n",
    "    mode_L = Counter(Ls).most_common(1)[0][0]\n",
    "    print(f\"[INFO] DMRS length stats: min={min(Ls)}, max={max(Ls)}, mode={mode_L}\")\n",
    "\n",
    "    # 为了 batch 化训练：强制所有文件的 L 一致（trim / zero-pad 到 mode_L）\n",
    "    for i in range(len(signals_by_file)):\n",
    "        x = signals_by_file[i]\n",
    "        if x.shape[1] == mode_L:\n",
    "            continue\n",
    "        if x.shape[1] > mode_L:\n",
    "            signals_by_file[i] = x[:, :mode_L].astype(np.complex64)\n",
    "        else:\n",
    "            pad = mode_L - x.shape[1]\n",
    "            signals_by_file[i] = np.pad(x, ((0,0),(0,pad)), mode=\"constant\").astype(np.complex64)\n",
    "\n",
    "    return signals_by_file, label_str_by_file, file_paths, mode_L\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) split：优先文件级 stratify，失败回退样本级\n",
    "# ----------------------------\n",
    "def build_label_mapping(label_str_by_file):\n",
    "    label_list = sorted(list(set(label_str_by_file)))\n",
    "    label_to_idx = {lab: i for i, lab in enumerate(label_list)}\n",
    "    return label_list, label_to_idx\n",
    "\n",
    "def build_all_sample_list(signals_by_file):\n",
    "    sample_list = []\n",
    "    for fi, arr in enumerate(signals_by_file):\n",
    "        n = arr.shape[0]\n",
    "        sample_list.extend([(fi, si) for si in range(n)])\n",
    "    return sample_list\n",
    "\n",
    "def split_train_test(signals_by_file, label_idx_by_file, test_size=0.25):\n",
    "    n_files = len(signals_by_file)\n",
    "    file_indices = np.arange(n_files)\n",
    "    y_files = np.asarray(label_idx_by_file, dtype=np.int64)\n",
    "\n",
    "    per_class_files = Counter(y_files.tolist())\n",
    "    can_file_split = all(v >= 2 for v in per_class_files.values())\n",
    "\n",
    "    if can_file_split:\n",
    "        try:\n",
    "            train_fi, test_fi = train_test_split(\n",
    "                file_indices, test_size=test_size, stratify=y_files, random_state=SEED\n",
    "            )\n",
    "            train_fi = np.array(train_fi, dtype=np.int64)\n",
    "            test_fi  = np.array(test_fi, dtype=np.int64)\n",
    "\n",
    "            train_samples, test_samples = [], []\n",
    "            for fi in train_fi:\n",
    "                n = signals_by_file[int(fi)].shape[0]\n",
    "                train_samples.extend([(int(fi), si) for si in range(n)])\n",
    "            for fi in test_fi:\n",
    "                n = signals_by_file[int(fi)].shape[0]\n",
    "                test_samples.extend([(int(fi), si) for si in range(n)])\n",
    "\n",
    "            print(\"[INFO] Split mode: FILE-LEVEL\")\n",
    "            print(f\"[INFO] Train files={len(train_fi)}, Test files={len(test_fi)}\")\n",
    "            return \"file\", train_fi, test_fi, train_samples, test_samples\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] File-level stratified split failed, fallback to sample-level. Reason: {repr(e)}\")\n",
    "\n",
    "    all_samples = build_all_sample_list(signals_by_file)\n",
    "    y_samples = np.array([label_idx_by_file[fi] for (fi, _) in all_samples], dtype=np.int64)\n",
    "    tr_i, te_i = train_test_split(\n",
    "        np.arange(len(all_samples)), test_size=test_size, stratify=y_samples, random_state=SEED\n",
    "    )\n",
    "    train_samples = [all_samples[i] for i in tr_i]\n",
    "    test_samples  = [all_samples[i] for i in te_i]\n",
    "\n",
    "    print(\"[INFO] Split mode: SAMPLE-LEVEL (fallback)\")\n",
    "    print(f\"[INFO] Train samples={len(train_samples)}, Test samples={len(test_samples)}\")\n",
    "    return \"sample\", None, None, train_samples, test_samples\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Dataset：只返回 IQ（用于 GPU batch 增强+STFT）\n",
    "# ----------------------------\n",
    "class LTEVSiameseIQDataset(Dataset):\n",
    "    \"\"\"\n",
    "    训练：返回 (iq1, iq2, label)\n",
    "    正样本构造：同一 label 下优先不同文件；否则同文件不同帧\n",
    "    \"\"\"\n",
    "    def __init__(self, signals_by_file, label_idx_by_file, sample_list):\n",
    "        self.signals_by_file = signals_by_file\n",
    "        self.label_idx_by_file = label_idx_by_file\n",
    "        self.sample_list = sample_list\n",
    "\n",
    "        self.label_to_files = defaultdict(list)\n",
    "        self.file_to_count = {}\n",
    "        present_files = sorted(list(set([fi for (fi, _) in sample_list])))\n",
    "        for fi in present_files:\n",
    "            lab = label_idx_by_file[fi]\n",
    "            self.label_to_files[lab].append(fi)\n",
    "            self.file_to_count[fi] = signals_by_file[fi].shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fi1, si1 = self.sample_list[idx]\n",
    "        lab = int(self.label_idx_by_file[fi1])\n",
    "\n",
    "        sig1 = self.signals_by_file[fi1][si1]  # complex (L,)\n",
    "\n",
    "        files = self.label_to_files[lab]\n",
    "        if len(files) >= 2:\n",
    "            fi2 = fi1\n",
    "            while fi2 == fi1:\n",
    "                fi2 = random.choice(files)\n",
    "        else:\n",
    "            fi2 = fi1\n",
    "\n",
    "        n2 = self.file_to_count[fi2]\n",
    "        if fi2 == fi1 and n2 >= 2:\n",
    "            si2 = si1\n",
    "            while si2 == si1:\n",
    "                si2 = random.randrange(n2)\n",
    "        else:\n",
    "            si2 = random.randrange(n2)\n",
    "\n",
    "        sig2 = self.signals_by_file[fi2][si2]\n",
    "\n",
    "        # 输出 IQ float32 (L,2)\n",
    "        iq1 = np.stack([sig1.real, sig1.imag], axis=-1).astype(np.float32)\n",
    "        iq2 = np.stack([sig2.real, sig2.imag], axis=-1).astype(np.float32)\n",
    "        return iq1, iq2, np.int64(lab)\n",
    "\n",
    "class LTEVSingleIQDataset(Dataset):\n",
    "    \"\"\"\n",
    "    验证/测试：返回 (iq, label)，增强与 STFT 在 GPU eval 函数里做\n",
    "    \"\"\"\n",
    "    def __init__(self, signals_by_file, label_idx_by_file, sample_list):\n",
    "        self.signals_by_file = signals_by_file\n",
    "        self.label_idx_by_file = label_idx_by_file\n",
    "        self.sample_list = sample_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fi, si = self.sample_list[idx]\n",
    "        lab = int(self.label_idx_by_file[fi])\n",
    "        sig = self.signals_by_file[fi][si]\n",
    "        iq = np.stack([sig.real, sig.imag], axis=-1).astype(np.float32)\n",
    "        return iq, np.int64(lab)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) GPU batch 增强：Multipath / Doppler / AWGN\n",
    "# ----------------------------\n",
    "def _to_complex(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    return iq_b[..., 0].to(torch.float32) + 1j * iq_b[..., 1].to(torch.float32)\n",
    "\n",
    "def _from_complex(sig: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.stack([sig.real, sig.imag], dim=-1)\n",
    "\n",
    "def batch_normalize_power(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    # power: (B,1) -> scale: (B,1,1)\n",
    "    power = (iq_b[..., 0] ** 2 + iq_b[..., 1] ** 2).mean(dim=1, keepdim=True) + 1e-12\n",
    "    scale = torch.rsqrt(power).unsqueeze(-1)\n",
    "    return iq_b * scale\n",
    "\n",
    "def batch_apply_doppler(iq_b: torch.Tensor, v_kmh: torch.Tensor) -> torch.Tensor:\n",
    "    B, L, _ = iq_b.shape\n",
    "    sig = _to_complex(iq_b)\n",
    "\n",
    "    c = 3e8\n",
    "    v = v_kmh / 3.6\n",
    "    fd = (v / c) * FC  # (B,)\n",
    "    n = torch.arange(L, device=iq_b.device, dtype=torch.float32).unsqueeze(0)  # (1,L)\n",
    "    phase = torch.exp(1j * 2.0 * np.pi * fd.unsqueeze(1).to(torch.float32) * n / FS)  # (B,L)\n",
    "    sig = sig * phase\n",
    "    return _from_complex(sig)\n",
    "\n",
    "def _grouped_conv1d_real(x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    # x: (B,1,L), w: (B,1,K) -> grouped conv -> (B,L+K-1)\n",
    "    B, _, L = x.shape\n",
    "    _, _, K = w.shape\n",
    "    x2 = x.permute(1, 0, 2).contiguous()      # (1,B,L)\n",
    "    y2 = F.conv1d(x2, w, padding=K - 1, groups=B)  # (1,B,L+K-1)\n",
    "    return y2.squeeze(0)  # (B, L+K-1)\n",
    "\n",
    "def batch_apply_multipath(iq_b: torch.Tensor, rms_ns: torch.Tensor) -> torch.Tensor:\n",
    "    B, L, _ = iq_b.shape\n",
    "    device = iq_b.device\n",
    "\n",
    "    rms_s = rms_ns * 1e-9\n",
    "    rms_samples = (rms_s * FS).clamp(min=1e-3)  # (B,)\n",
    "    k = torch.arange(MAX_TAPS, device=device, dtype=torch.float32).unsqueeze(0)  # (1,K)\n",
    "    p = torch.exp(-k / rms_samples.unsqueeze(1))  # (B,K)\n",
    "    p = p / (p.sum(dim=1, keepdim=True) + 1e-12)\n",
    "\n",
    "    hr = torch.randn(B, MAX_TAPS, device=device) * torch.sqrt(p / 2.0)\n",
    "    hi = torch.randn(B, MAX_TAPS, device=device) * torch.sqrt(p / 2.0)\n",
    "\n",
    "    hpow = (hr**2 + hi**2).sum(dim=1, keepdim=True) + 1e-12\n",
    "    norm = torch.rsqrt(hpow)\n",
    "    hr = hr * norm\n",
    "    hi = hi * norm\n",
    "\n",
    "    xr = iq_b[..., 0]\n",
    "    xi = iq_b[..., 1]\n",
    "\n",
    "    xr_ = xr.unsqueeze(1)\n",
    "    xi_ = xi.unsqueeze(1)\n",
    "    hr_ = hr.unsqueeze(1)\n",
    "    hi_ = hi.unsqueeze(1)\n",
    "\n",
    "    xr_hr = _grouped_conv1d_real(xr_, hr_)\n",
    "    xi_hi = _grouped_conv1d_real(xi_, hi_)\n",
    "    xr_hi = _grouped_conv1d_real(xr_, hi_)\n",
    "    xi_hr = _grouped_conv1d_real(xi_, hr_)\n",
    "\n",
    "    yr = xr_hr - xi_hi\n",
    "    yi = xr_hi + xi_hr\n",
    "\n",
    "    yr = yr[:, :L]\n",
    "    yi = yi[:, :L]\n",
    "    return torch.stack([yr, yi], dim=-1)\n",
    "\n",
    "def batch_add_awgn(iq_b: torch.Tensor, snr_db: torch.Tensor) -> torch.Tensor:\n",
    "    B, L, _ = iq_b.shape\n",
    "    sig = _to_complex(iq_b)\n",
    "    p = (sig.real**2 + sig.imag**2).mean(dim=1) + 1e-12\n",
    "    npow = p / (10.0 ** (snr_db / 10.0))\n",
    "    std = torch.sqrt(npow / 2.0).unsqueeze(1)\n",
    "    noise = std * (torch.randn(B, L, device=iq_b.device) + 1j * torch.randn(B, L, device=iq_b.device))\n",
    "    sig = sig + noise\n",
    "    return _from_complex(sig)\n",
    "\n",
    "def augment_train_batch(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    iq_b = batch_normalize_power(iq_b)\n",
    "    B = iq_b.shape[0]\n",
    "\n",
    "    if AUG_USE_MULTIPATH:\n",
    "        rms = (RMS_DS_NS_RANGE[1] - RMS_DS_NS_RANGE[0]) * torch.rand(B, device=iq_b.device) + RMS_DS_NS_RANGE[0]\n",
    "        iq_b = batch_apply_multipath(iq_b, rms)\n",
    "\n",
    "    if AUG_USE_DOPPLER:\n",
    "        v = (TRAIN_V_KMH_RANGE[1] - TRAIN_V_KMH_RANGE[0]) * torch.rand(B, device=iq_b.device) + TRAIN_V_KMH_RANGE[0]\n",
    "        iq_b = batch_apply_doppler(iq_b, v)\n",
    "\n",
    "    if AUG_USE_AWGN:\n",
    "        snr = (TRAIN_SNR_DB_RANGE[1] - TRAIN_SNR_DB_RANGE[0]) * torch.rand(B, device=iq_b.device) + TRAIN_SNR_DB_RANGE[0]\n",
    "        iq_b = batch_add_awgn(iq_b, snr)\n",
    "\n",
    "    return iq_b\n",
    "\n",
    "def augment_test_batch(iq_b: torch.Tensor, snr_db: float) -> torch.Tensor:\n",
    "    iq_b = batch_normalize_power(iq_b)\n",
    "    B = iq_b.shape[0]\n",
    "\n",
    "    if TEST_USE_MULTIPATH:\n",
    "        rms = (TEST_RMS_DS_NS_RANGE[1] - TEST_RMS_DS_NS_RANGE[0]) * torch.rand(B, device=iq_b.device) + TEST_RMS_DS_NS_RANGE[0]\n",
    "        iq_b = batch_apply_multipath(iq_b, rms)\n",
    "\n",
    "    v = torch.full((B,), float(TEST_V_KMH_FIXED), device=iq_b.device)\n",
    "    iq_b = batch_apply_doppler(iq_b, v)\n",
    "\n",
    "    snr = torch.full((B,), float(snr_db), device=iq_b.device)\n",
    "    iq_b = batch_add_awgn(iq_b, snr)\n",
    "    return iq_b\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5) GPU batch STFT -> logmag -> resize\n",
    "# ----------------------------\n",
    "_WINDOW_CACHE = {}\n",
    "\n",
    "def get_hann_window(device: torch.device):\n",
    "    key = (device.type, device.index, SPEC_WIN)\n",
    "    if key not in _WINDOW_CACHE:\n",
    "        _WINDOW_CACHE[key] = torch.hann_window(SPEC_WIN, periodic=True, device=device)\n",
    "    return _WINDOW_CACHE[key]\n",
    "\n",
    "def iq_to_logspec_batch(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    # iq_b: (B,L,2) float32\n",
    "    sig = _to_complex(iq_b).to(torch.complex64)  # (B,L)\n",
    "\n",
    "    win = get_hann_window(iq_b.device)\n",
    "    S = torch.stft(\n",
    "        sig,\n",
    "        n_fft=SPEC_NFFT,\n",
    "        hop_length=SPEC_HOP,\n",
    "        win_length=SPEC_WIN,\n",
    "        window=win,\n",
    "        center=True,\n",
    "        return_complex=True\n",
    "    )  # (B,F,T)\n",
    "\n",
    "    mag = torch.abs(S) + 1e-12\n",
    "    logmag = torch.log(mag)  # (B,F,T)\n",
    "\n",
    "    mu = logmag.mean(dim=(1,2), keepdim=True)\n",
    "    sd = logmag.std(dim=(1,2), keepdim=True) + 1e-6\n",
    "    logmag = (logmag - mu) / sd\n",
    "    logmag = torch.nan_to_num(logmag, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    x = logmag.unsqueeze(1)  # (B,1,F,T)\n",
    "    x = F.interpolate(x, size=(SPEC_SIZE, SPEC_SIZE), mode=\"bilinear\", align_corners=False)\n",
    "    return x\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 6) 模型\n",
    "# ----------------------------\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.down = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.down is not None:\n",
    "            identity = self.down(identity)\n",
    "        return self.relu(out + identity)\n",
    "\n",
    "class SpecFeatureNet(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.b1 = BasicBlock(32, 32, stride=1)\n",
    "        self.b2 = BasicBlock(32, 32, stride=1)\n",
    "        self.b3 = BasicBlock(32, 64, stride=1)\n",
    "        self.b4 = BasicBlock(64, 64, stride=1)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(64, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.cls = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.b1(x); x = self.b2(x); x = self.b3(x); x = self.b4(x)\n",
    "        x = self.gap(x).squeeze(-1).squeeze(-1)  # (B,64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        z = self.fc2(x)                          # (B,256)\n",
    "        logits = self.cls(z)                     # (B,K)\n",
    "        return z, logits\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        z1, p1 = self.forward_once(x1)\n",
    "        if x2 is None:\n",
    "            return z1, p1\n",
    "        z2, p2 = self.forward_once(x2)\n",
    "        return z1, p1, z2, p2\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 7) 损失与评估（NT-Xent：强制 fp32，避免 AMP 溢出）\n",
    "# ----------------------------\n",
    "def nt_xent_loss(z1: torch.Tensor, z2: torch.Tensor, tau: float = TAU) -> torch.Tensor:\n",
    "    # 关键：禁用 autocast，强制 float32，避免 fp16 下对角线填充值溢出\n",
    "    with torch.cuda.amp.autocast(enabled=False):\n",
    "        z1 = z1.float()\n",
    "        z2 = z2.float()\n",
    "\n",
    "        N = z1.size(0)\n",
    "        z = torch.cat([z1, z2], dim=0)          # (2N,D)\n",
    "        z = F.normalize(z, dim=1)\n",
    "\n",
    "        sim = (z @ z.T) / float(tau)            # float32\n",
    "        sim.fill_diagonal_(torch.finfo(sim.dtype).min)\n",
    "\n",
    "        pos = torch.arange(2 * N, device=z.device)\n",
    "        pos = (pos + N) % (2 * N)\n",
    "\n",
    "        log_prob = sim - torch.logsumexp(sim, dim=1, keepdim=True)\n",
    "        loss = -log_prob[torch.arange(2 * N, device=z.device), pos]\n",
    "        return loss.mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_single_iq(model: SpecFeatureNet, loader: DataLoader, num_classes: int, mode: str, snr_db: float = None):\n",
    "    model.eval()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    loss_sum, nb = 0.0, 0\n",
    "    all_y, all_p = [], []\n",
    "\n",
    "    for iq, y in loader:\n",
    "        iq = iq.to(DEVICE, non_blocking=True)\n",
    "        y  = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        if mode == \"test\":\n",
    "            iq = augment_test_batch(iq, snr_db=float(snr_db))\n",
    "\n",
    "        spec = iq_to_logspec_batch(iq)\n",
    "\n",
    "        _, logits = model(spec, None)\n",
    "        loss = ce(logits, y)\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        nb += 1\n",
    "\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "        if RETURN_CM:\n",
    "            all_y.append(y.detach().cpu().numpy())\n",
    "            all_p.append(pred.detach().cpu().numpy())\n",
    "\n",
    "    acc = 100.0 * correct / max(total, 1)\n",
    "    cm = None\n",
    "    if RETURN_CM and total > 0:\n",
    "        import numpy as _np\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        all_y = _np.concatenate(all_y) if all_y else _np.array([])\n",
    "        all_p = _np.concatenate(all_p) if all_p else _np.array([])\n",
    "        cm = confusion_matrix(all_y, all_p, labels=list(range(num_classes)))\n",
    "    return (loss_sum / max(nb, 1)), acc, cm\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 8) 主训练：KFold + Test SNR sweep\n",
    "# ----------------------------\n",
    "def train_kfold_ltev_spec_siamese_fast(data_path: str):\n",
    "    signals_by_file, label_str_by_file, file_paths, L = load_ltev_dataset(\n",
    "        data_path, recursive=RECURSIVE_GLOB, max_samples_per_file=MAX_SAMPLES_PER_FILE\n",
    "    )\n",
    "\n",
    "    label_list, label_to_idx = build_label_mapping(label_str_by_file)\n",
    "    label_idx_by_file = [label_to_idx[s] for s in label_str_by_file]\n",
    "    num_classes = len(label_list)\n",
    "\n",
    "    mode, train_files, test_files, train_samples, test_samples = split_train_test(\n",
    "        signals_by_file, label_idx_by_file, test_size=0.25\n",
    "    )\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    save_dir = f\"{timestamp}_{SCRIPT_NAME}\"\n",
    "    save_folder = os.path.join(SAVE_ROOT, save_dir)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    fd_test = (TEST_V_KMH_FIXED / 3.6) / 3e8 * FC\n",
    "\n",
    "    with open(os.path.join(save_folder, \"config.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"DEVICE={DEVICE}\\nAMP={USE_AMP}\\n\")\n",
    "        f.write(f\"DATA_PATH={data_path}\\nRECURSIVE_GLOB={RECURSIVE_GLOB}\\n\")\n",
    "        f.write(f\"MAX_SAMPLES_PER_FILE={MAX_SAMPLES_PER_FILE}\\n\")\n",
    "        f.write(f\"DMRS_LEN(L)={L}\\n\")\n",
    "        f.write(f\"num_classes={num_classes}\\nsplit_mode={mode}\\n\")\n",
    "        f.write(f\"BATCH_SIZE={BATCH_SIZE}, LR={LR}, WEIGHT_DECAY={WEIGHT_DECAY}\\n\")\n",
    "        f.write(f\"MAX_EPOCHS={MAX_EPOCHS}, N_SPLITS={N_SPLITS}\\n\")\n",
    "        f.write(f\"LR_PATIENCE={LR_PATIENCE}, LR_FACTOR={LR_FACTOR}, ES_PATIENCE={ES_PATIENCE}\\n\")\n",
    "        f.write(f\"TAU={TAU}, LAMBDA_CL={LAMBDA_CL}, LAMBDA_CE={LAMBDA_CE}\\n\")\n",
    "        f.write(f\"FS={FS}, FC={FC}\\n\")\n",
    "        f.write(f\"AUG_USE_MULTIPATH={AUG_USE_MULTIPATH}, RMS_DS_NS_RANGE={RMS_DS_NS_RANGE}, MAX_TAPS={MAX_TAPS}\\n\")\n",
    "        f.write(f\"AUG_USE_DOPPLER={AUG_USE_DOPPLER}, TRAIN_V_KMH_RANGE={TRAIN_V_KMH_RANGE}\\n\")\n",
    "        f.write(f\"AUG_USE_AWGN={AUG_USE_AWGN}, TRAIN_SNR_DB_RANGE={TRAIN_SNR_DB_RANGE}\\n\")\n",
    "        f.write(f\"TEST_V_KMH_FIXED={TEST_V_KMH_FIXED}, fd_test={fd_test}\\n\")\n",
    "        f.write(f\"TEST_USE_MULTIPATH={TEST_USE_MULTIPATH}, TEST_SNR_LIST={TEST_SNR_LIST}\\n\")\n",
    "        f.write(f\"SPEC_NFFT={SPEC_NFFT}, SPEC_WIN={SPEC_WIN}, SPEC_HOP={SPEC_HOP}, SPEC_SIZE={SPEC_SIZE}\\n\")\n",
    "        f.write(f\"workers(train/eval)={NUM_WORKERS_TRAIN}/{NUM_WORKERS_EVAL}\\n\")\n",
    "\n",
    "    print(f\"[INFO] DEVICE={DEVICE} | AMP={USE_AMP}\")\n",
    "    print(f\"[INFO] Classes={num_classes}, TrainSamples={len(train_samples)}, TestSamples={len(test_samples)}, L={L}\")\n",
    "    print(f\"[INFO] Train: multipath={AUG_USE_MULTIPATH}, doppler={AUG_USE_DOPPLER}, awgn={AUG_USE_AWGN}\")\n",
    "    print(f\"[INFO] Train SNR~U{TRAIN_SNR_DB_RANGE}, Train v~U{TRAIN_V_KMH_RANGE}\")\n",
    "    print(f\"[INFO] Test: v={TEST_V_KMH_FIXED} km/h (fd={fd_test:.2f} Hz), SNR sweep={TEST_SNR_LIST}, TEST_USE_MULTIPATH={TEST_USE_MULTIPATH}\")\n",
    "    print(f\"[INFO] SaveFolder: {save_folder}\")\n",
    "\n",
    "    snr_to_accs = {snr: [] for snr in TEST_SNR_LIST}\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "    if mode == \"file\":\n",
    "        train_files = np.array(train_files, dtype=np.int64)\n",
    "        kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "        fold_iter = kf.split(train_files)\n",
    "\n",
    "        for fold, (tr_fi_idx, va_fi_idx) in enumerate(fold_iter, 1):\n",
    "            tr_files = train_files[tr_fi_idx].tolist()\n",
    "            va_files = train_files[va_fi_idx].tolist()\n",
    "\n",
    "            tr_samples_fold = [(fi, si) for fi in tr_files for si in range(signals_by_file[fi].shape[0])]\n",
    "            va_samples_fold = [(fi, si) for fi in va_files for si in range(signals_by_file[fi].shape[0])]\n",
    "\n",
    "            _run_one_fold_fast(\n",
    "                fold, save_folder, num_classes,\n",
    "                signals_by_file, label_idx_by_file,\n",
    "                tr_samples_fold, va_samples_fold, test_samples,\n",
    "                snr_to_accs, scaler\n",
    "            )\n",
    "    else:\n",
    "        kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "        idx_all = np.arange(len(train_samples))\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(idx_all), 1):\n",
    "            tr_samples_fold = [train_samples[i] for i in tr_idx]\n",
    "            va_samples_fold = [train_samples[i] for i in va_idx]\n",
    "\n",
    "            _run_one_fold_fast(\n",
    "                fold, save_folder, num_classes,\n",
    "                signals_by_file, label_idx_by_file,\n",
    "                tr_samples_fold, va_samples_fold, test_samples,\n",
    "                snr_to_accs, scaler\n",
    "            )\n",
    "\n",
    "    print(\"\\n========== Overall Test SNR Sweep (mean±std over folds) ==========\")\n",
    "    rows = []\n",
    "    for snr in TEST_SNR_LIST:\n",
    "        arr = np.array(snr_to_accs[snr], dtype=np.float64)\n",
    "        mean = float(arr.mean()) if arr.size else 0.0\n",
    "        std  = float(arr.std()) if arr.size else 0.0\n",
    "        print(f\"SNR {snr:>3} dB | Acc {mean:.2f} ± {std:.2f}\")\n",
    "        rows.append([snr, mean, std] + snr_to_accs[snr])\n",
    "\n",
    "    csv_path = os.path.join(save_folder, \"test_snr_sweep.csv\")\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        header = [\"snr_db\", \"acc_mean\", \"acc_std\"] + [f\"fold{i}\" for i in range(1, N_SPLITS + 1)]\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"\\n[INFO] All saved in: {save_folder}\")\n",
    "    print(f\"[INFO] SNR sweep CSV: {csv_path}\")\n",
    "    return save_folder\n",
    "\n",
    "\n",
    "def _run_one_fold_fast(\n",
    "    fold: int,\n",
    "    save_folder: str,\n",
    "    num_classes: int,\n",
    "    signals_by_file,\n",
    "    label_idx_by_file,\n",
    "    tr_samples_fold,\n",
    "    va_samples_fold,\n",
    "    test_samples,\n",
    "    snr_to_accs: dict,\n",
    "    scaler: torch.cuda.amp.GradScaler\n",
    "):\n",
    "    print(f\"\\n========== Fold {fold}/{N_SPLITS} ==========\")\n",
    "\n",
    "    tr_ds = LTEVSiameseIQDataset(signals_by_file, label_idx_by_file, tr_samples_fold)\n",
    "    va_ds = LTEVSingleIQDataset(signals_by_file, label_idx_by_file, va_samples_fold)\n",
    "    te_ds = LTEVSingleIQDataset(signals_by_file, label_idx_by_file, test_samples)\n",
    "\n",
    "    tr_loader = DataLoader(\n",
    "        tr_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True,\n",
    "        num_workers=NUM_WORKERS_TRAIN, pin_memory=True\n",
    "    )\n",
    "    va_loader = DataLoader(\n",
    "        va_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS_EVAL, pin_memory=True\n",
    "    )\n",
    "    te_loader = DataLoader(\n",
    "        te_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS_EVAL, pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = SpecFeatureNet(num_classes=num_classes).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode=\"min\", factor=LR_FACTOR, patience=LR_PATIENCE\n",
    "    )\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "    es_count = 0\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        model.train()\n",
    "        loss_sum, nb = 0.0, 0\n",
    "\n",
    "        for iq1, iq2, y in tr_loader:\n",
    "            iq1 = iq1.to(DEVICE, non_blocking=True)\n",
    "            iq2 = iq2.to(DEVICE, non_blocking=True)\n",
    "            y   = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            # 2B 拼起来，一起增强 + 一起 STFT\n",
    "            iq_cat = torch.cat([iq1, iq2], dim=0)   # (2B,L,2)\n",
    "            iq_cat = augment_train_batch(iq_cat)    # (2B,L,2)\n",
    "            spec_cat = iq_to_logspec_batch(iq_cat)  # (2B,1,S,S)\n",
    "            spec1, spec2 = spec_cat.chunk(2, dim=0)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "                z1, p1, z2, p2 = model(spec1, spec2)\n",
    "                loss_cl = nt_xent_loss(z1, z2, tau=TAU)  # 内部强制 fp32\n",
    "                loss_ce = 0.5 * (ce(p1, y) + ce(p2, y))\n",
    "                loss = LAMBDA_CL * loss_cl + LAMBDA_CE * loss_ce\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "\n",
    "            loss_sum += float(loss.item())\n",
    "            nb += 1\n",
    "\n",
    "        train_loss = loss_sum / max(nb, 1)\n",
    "        val_loss, val_acc, _ = eval_single_iq(model, va_loader, num_classes, mode=\"val\")\n",
    "\n",
    "        prev_lr = opt.param_groups[0][\"lr\"]\n",
    "        scheduler.step(val_loss)\n",
    "        cur_lr = opt.param_groups[0][\"lr\"]\n",
    "        if cur_lr < prev_lr:\n",
    "            print(f\"[LR DROP] {prev_lr:.2e} -> {cur_lr:.2e} (val_loss={val_loss:.4f})\")\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | LR={cur_lr:.2e} | TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | ValAcc={val_acc:.2f}%\")\n",
    "\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            es_count = 0\n",
    "        else:\n",
    "            es_count += 1\n",
    "            if es_count >= ES_PATIENCE:\n",
    "                print(\"[INFO] Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_folder, f\"model_fold{fold}.pth\"))\n",
    "\n",
    "    # Test SNR sweep（复用同一个 te_loader，每个 snr 在 eval 内做增强）\n",
    "    model.eval()\n",
    "    fold_snr_acc = {}\n",
    "    for snr in TEST_SNR_LIST:\n",
    "        _, test_acc, _ = eval_single_iq(model, te_loader, num_classes, mode=\"test\", snr_db=float(snr))\n",
    "        snr_to_accs[snr].append(test_acc)\n",
    "        fold_snr_acc[snr] = test_acc\n",
    "\n",
    "    print(\"[FOLD TEST] Acc@SNR:\", {snr: f\"{fold_snr_acc[snr]:.2f}%\" for snr in TEST_SNR_LIST})\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 9) main\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_kfold_ltev_spec_siamese_fast(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7bdd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LTE-V XFR (PER-FILE sequential blocks) + Block-level split (no leakage)\n",
    "# + Spec Siamese (NT-Xent) + CE head + FAST GPU STFT + AMP\n",
    "#\n",
    "# XFR block construction YOU WANT:\n",
    "#   - For each FILE:\n",
    "#       take sequential group_size=m samples -> (m, L, 2)\n",
    "#       drop remainder\n",
    "#       XFR \"flip\"/transpose -> (L, m, 2)  (same as your reference)\n",
    "#\n",
    "# Split:\n",
    "#   - block-level train/val/test (no block leakage)\n",
    "#   - optional STRICT balanced test blocks per class\n",
    "#\n",
    "# Positive pair:\n",
    "#   - \"same_block\": two different XFR-samples (two rows) within SAME block\n",
    "#   - \"simclr\": same XFR-sample, two random augmented views\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Config\n",
    "# ----------------------------\n",
    "SEED = 42\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "USE_AMP = (DEVICE.type == \"cuda\")\n",
    "\n",
    "# ---- paths ----\n",
    "DATA_PATH = \"E:/rf_datasets/\"   # <-- change this\n",
    "RECURSIVE_GLOB = True\n",
    "\n",
    "# ---- XFR (block) ----\n",
    "GROUP_SIZE = 288     # m (你说公平起见 m=L=288)\n",
    "TEST_SIZE = 0.25\n",
    "STRICT_TEST_BALANCE = True  # 测试集每类 block 数严格一致\n",
    "\n",
    "# ---- training ----\n",
    "BATCH_SIZE = 256\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 0.0\n",
    "MAX_EPOCHS = 200\n",
    "N_SPLITS = 5\n",
    "\n",
    "LR_PATIENCE = 10\n",
    "LR_FACTOR = 0.5\n",
    "ES_PATIENCE = 30\n",
    "\n",
    "# Positive pair mode: \"same_block\" or \"simclr\"\n",
    "POS_PAIR_MODE = \"same_block\"\n",
    "\n",
    "# ---- Contrastive loss ----\n",
    "TAU = 0.05\n",
    "LAMBDA_CL = 1.0\n",
    "LAMBDA_CE = 1.0  # set 0.0 to disable CE\n",
    "\n",
    "# ---- augmentation ----\n",
    "AUG_USE_MULTIPATH = True\n",
    "AUG_USE_DOPPLER   = True\n",
    "AUG_USE_AWGN      = True\n",
    "\n",
    "FS = 5e6\n",
    "FC = 5.9e9\n",
    "RMS_DS_NS_RANGE = (5.0, 300.0)\n",
    "MAX_TAPS = 16\n",
    "\n",
    "TRAIN_V_KMH_RANGE = (120.0, 120.0)\n",
    "TRAIN_SNR_DB_RANGE = (-40.0, 20.0)\n",
    "\n",
    "TEST_V_KMH_FIXED = 120.0\n",
    "TEST_USE_MULTIPATH = False\n",
    "TEST_RMS_DS_NS_RANGE = RMS_DS_NS_RANGE\n",
    "TEST_SNR_LIST = list(range(20, -45, -5))\n",
    "\n",
    "# ---- spectrogram ----\n",
    "SPEC_NFFT = 128\n",
    "SPEC_WIN  = 128\n",
    "SPEC_HOP  = 16\n",
    "SPEC_SIZE = 64\n",
    "\n",
    "# ---- dataloader ----\n",
    "NUM_WORKERS_TRAIN = 0\n",
    "NUM_WORKERS_EVAL  = 0\n",
    "\n",
    "# ---- saving ----\n",
    "SAVE_ROOT = \"./training_results\"\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "SCRIPT_NAME = \"LTEV_XFR_PerFileSeqBlocks_SpecSiamese\"\n",
    "\n",
    "# ----------------------------\n",
    "# AMP compatibility (no device_type bug)\n",
    "# ----------------------------\n",
    "def amp_autocast(enabled: bool = True):\n",
    "    if not enabled:\n",
    "        return nullcontext()\n",
    "    if hasattr(torch, \"amp\") and hasattr(torch.amp, \"autocast\"):\n",
    "        try:\n",
    "            return torch.amp.autocast(device_type=DEVICE.type, enabled=True)\n",
    "        except TypeError:\n",
    "            return torch.amp.autocast(DEVICE.type, enabled=True)\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        return torch.cuda.amp.autocast(enabled=True)\n",
    "    return nullcontext()\n",
    "\n",
    "def make_grad_scaler(enabled: bool = True):\n",
    "    if (not enabled) or (DEVICE.type != \"cuda\"):\n",
    "        return None\n",
    "    if hasattr(torch, \"amp\") and hasattr(torch.amp, \"GradScaler\"):\n",
    "        for ctor in (\n",
    "            lambda: torch.amp.GradScaler(\"cuda\", enabled=True),\n",
    "            lambda: torch.amp.GradScaler(enabled=True),\n",
    "        ):\n",
    "            try:\n",
    "                return ctor()\n",
    "            except TypeError:\n",
    "                pass\n",
    "    return torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Logging helpers\n",
    "# ----------------------------\n",
    "class CSVLogger:\n",
    "    def __init__(self, path, header):\n",
    "        self.path = path\n",
    "        self.header = header\n",
    "        self._init_file()\n",
    "\n",
    "    def _init_file(self):\n",
    "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
    "        with open(self.path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.writer(f).writerow(self.header)\n",
    "\n",
    "    def log_row(self, row):\n",
    "        with open(self.path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.writer(f).writerow(row)\n",
    "\n",
    "def write_line(path, line):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line.rstrip() + \"\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) HDF5 reader\n",
    "# ----------------------------\n",
    "def decode_txid(txid_arr: np.ndarray) -> str:\n",
    "    txid_arr = txid_arr.flatten()\n",
    "    chars = []\n",
    "    for c in txid_arr:\n",
    "        if int(c) == 0:\n",
    "            continue\n",
    "        try:\n",
    "            chars.append(chr(int(c)))\n",
    "        except Exception:\n",
    "            pass\n",
    "    s = \"\".join(chars).strip()\n",
    "    return s if s else \"UNKNOWN\"\n",
    "\n",
    "def load_dmrs_complex_from_file(h5f: h5py.File) -> np.ndarray:\n",
    "    rfDataset = h5f[\"rfDataset\"]\n",
    "    dmrs_obj = rfDataset[\"dmrs\"]\n",
    "\n",
    "    if isinstance(dmrs_obj, h5py.Dataset) and dmrs_obj.dtype.fields is not None:\n",
    "        real = dmrs_obj[\"real\"][()]\n",
    "        imag = dmrs_obj[\"imag\"][()]\n",
    "    elif isinstance(dmrs_obj, h5py.Group):\n",
    "        real = dmrs_obj[\"real\"][()]\n",
    "        imag = dmrs_obj[\"imag\"][()]\n",
    "    else:\n",
    "        tmp = dmrs_obj[()]\n",
    "        if hasattr(tmp, \"dtype\") and tmp.dtype.fields is not None:\n",
    "            real = tmp[\"real\"]\n",
    "            imag = tmp[\"imag\"]\n",
    "        else:\n",
    "            raise RuntimeError(\"Cannot parse dmrs (not compound dataset nor group real/imag).\")\n",
    "\n",
    "    dmrs_complex = np.asarray(real + 1j * imag)\n",
    "    if dmrs_complex.ndim != 2:\n",
    "        raise RuntimeError(f\"dmrs_complex dim error: {dmrs_complex.shape}\")\n",
    "\n",
    "    # Some files store as (L,N)\n",
    "    if dmrs_complex.shape[0] <= 2048 and dmrs_complex.shape[1] > dmrs_complex.shape[0]:\n",
    "        dmrs_complex = dmrs_complex.T\n",
    "\n",
    "    return dmrs_complex.astype(np.complex64)\n",
    "\n",
    "def power_normalize_rows(x: np.ndarray, eps=1e-12) -> np.ndarray:\n",
    "    p = np.mean(np.abs(x)**2, axis=1, keepdims=True)\n",
    "    return x / (np.sqrt(p) + eps)\n",
    "\n",
    "def load_ltev_files_as_iq(data_path: str, recursive: bool = True, max_samples_per_file=None):\n",
    "    if recursive:\n",
    "        mat_files = glob.glob(os.path.join(data_path, \"**\", \"*.mat\"), recursive=True)\n",
    "    else:\n",
    "        mat_files = glob.glob(os.path.join(data_path, \"*.mat\"))\n",
    "\n",
    "    if len(mat_files) == 0:\n",
    "        raise RuntimeError(f\"No .mat files found: {data_path}\")\n",
    "\n",
    "    X_files = []\n",
    "    y_files = []\n",
    "    file_paths = []\n",
    "\n",
    "    print(f\"[INFO] Found {len(mat_files)} .mat files\")\n",
    "    for fp in mat_files:\n",
    "        try:\n",
    "            with h5py.File(fp, \"r\") as f:\n",
    "                rfDataset = f[\"rfDataset\"]\n",
    "                txid_arr = np.asarray(rfDataset[\"txID\"][()])\n",
    "                tx_str = decode_txid(txid_arr)\n",
    "\n",
    "                dmrs_complex = load_dmrs_complex_from_file(f)\n",
    "                if max_samples_per_file is not None:\n",
    "                    dmrs_complex = dmrs_complex[: int(max_samples_per_file)]\n",
    "\n",
    "                if dmrs_complex.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                dmrs_complex = power_normalize_rows(dmrs_complex)\n",
    "\n",
    "                iq = np.stack([dmrs_complex.real, dmrs_complex.imag], axis=-1).astype(np.float32)  # (N,L,2)\n",
    "                X_files.append(iq)\n",
    "                y_files.append(tx_str)\n",
    "                file_paths.append(fp)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skip file due to read/parse error: {fp} | {repr(e)}\")\n",
    "\n",
    "    if len(X_files) == 0:\n",
    "        raise RuntimeError(\"All files failed to read or are empty.\")\n",
    "\n",
    "    cnt = Counter(y_files)\n",
    "    print(\"[INFO] txID classes:\", len(cnt))\n",
    "    for k, v in sorted(cnt.items(), key=lambda x: (-x[1], x[0])):\n",
    "        print(f\"  {k}: {v} files\")\n",
    "\n",
    "    Ls = [arr.shape[1] for arr in X_files]\n",
    "    mode_L = Counter(Ls).most_common(1)[0][0]\n",
    "    print(f\"[INFO] DMRS length stats: min={min(Ls)}, max={max(Ls)}, mode={mode_L}\")\n",
    "\n",
    "    # unify length to mode_L\n",
    "    for i in range(len(X_files)):\n",
    "        x = X_files[i]  # (N,L,2)\n",
    "        L = x.shape[1]\n",
    "        if L == mode_L:\n",
    "            continue\n",
    "        if L > mode_L:\n",
    "            X_files[i] = x[:, :mode_L, :].astype(np.float32)\n",
    "        else:\n",
    "            pad = mode_L - L\n",
    "            X_files[i] = np.pad(x, ((0,0),(0,pad),(0,0)), mode=\"constant\").astype(np.float32)\n",
    "\n",
    "    return X_files, y_files, file_paths, mode_L\n",
    "\n",
    "# ----------------------------\n",
    "# 2) XFR blocks: PER-FILE sequential grouping (NO cross-file)\n",
    "# ----------------------------\n",
    "def build_xfr_blocks_per_file_sequential(X_files, y_files, group_size: int):\n",
    "    \"\"\"\n",
    "    For each file:\n",
    "      chunk = X_file[b*m:(b+1)*m] -> (m, L, 2)\n",
    "      XFR flip: transpose -> (L, m, 2)\n",
    "    Returns:\n",
    "      X_blocks: (num_blocks, L, m, 2)\n",
    "      y_blocks: (num_blocks,)\n",
    "      label_to_idx\n",
    "      block_meta: list[(file_index, block_in_file)]\n",
    "    \"\"\"\n",
    "    label_list = sorted(list(set(y_files)))\n",
    "    label_to_idx = {lab: i for i, lab in enumerate(label_list)}\n",
    "    y_idx_by_file = [label_to_idx[s] for s in y_files]\n",
    "\n",
    "    X_blocks_list = []\n",
    "    y_blocks_list = []\n",
    "    block_meta = []\n",
    "\n",
    "    for fi, Xf in enumerate(X_files):\n",
    "        N, L, _ = Xf.shape\n",
    "        nb = N // group_size\n",
    "        if nb <= 0:\n",
    "            continue\n",
    "        for bi in range(nb):\n",
    "            start = bi * group_size\n",
    "            end = start + group_size\n",
    "            chunk = Xf[start:end]  # (m, L, 2)\n",
    "            # XFR flip as your reference: (L, m, 2)\n",
    "            xfr_block = np.transpose(chunk, (1, 0, 2)).astype(np.float32)\n",
    "            X_blocks_list.append(xfr_block)\n",
    "            y_blocks_list.append(int(y_idx_by_file[fi]))\n",
    "            block_meta.append((fi, bi))\n",
    "\n",
    "    if len(X_blocks_list) == 0:\n",
    "        raise RuntimeError(\"No blocks generated. Check GROUP_SIZE and data.\")\n",
    "\n",
    "    X_blocks = np.stack(X_blocks_list, axis=0)  # (B, L, m, 2)\n",
    "    y_blocks = np.array(y_blocks_list, dtype=np.int64)\n",
    "\n",
    "    print(f\"[INFO] XFR blocks(per-file) generated: num_blocks={X_blocks.shape[0]}, \"\n",
    "          f\"L={X_blocks.shape[1]}, m={X_blocks.shape[2]}\")\n",
    "    # class distribution on blocks\n",
    "    print(\"[INFO] Block counts per class:\", dict(sorted(Counter(y_blocks.tolist()).items())))\n",
    "    return X_blocks, y_blocks, label_to_idx, block_meta\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Block split (optional strict balanced test)\n",
    "# ----------------------------\n",
    "def balanced_block_split(y_blocks: np.ndarray, test_size=0.25, seed=SEED, strict_balance=True):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    cls_to_idx = defaultdict(list)\n",
    "    for i, y in enumerate(y_blocks.tolist()):\n",
    "        cls_to_idx[int(y)].append(i)\n",
    "\n",
    "    classes = sorted(cls_to_idx.keys())\n",
    "    counts = {c: len(cls_to_idx[c]) for c in classes}\n",
    "    print(\"[INFO] blocks per class (raw):\", dict(sorted(counts.items())))\n",
    "\n",
    "    if not strict_balance:\n",
    "        idx = np.arange(len(y_blocks), dtype=np.int64)\n",
    "        tr, te = train_test_split(idx, test_size=test_size, stratify=y_blocks, random_state=seed)\n",
    "        return idx, tr, te\n",
    "\n",
    "    Bmin = min(counts.values())\n",
    "    if Bmin < 2:\n",
    "        raise RuntimeError(f\"Not enough blocks per class to split: Bmin={Bmin}\")\n",
    "\n",
    "    test_k = int(np.floor(Bmin * test_size))\n",
    "    test_k = max(1, min(Bmin - 1, test_k))\n",
    "\n",
    "    kept, trainval, test = [], [], []\n",
    "    for c in classes:\n",
    "        arr = np.array(cls_to_idx[c], dtype=np.int64)\n",
    "        rng.shuffle(arr)\n",
    "        arr = arr[:Bmin]           # trim each class to Bmin\n",
    "        kept.extend(arr.tolist())\n",
    "        test.extend(arr[:test_k].tolist())\n",
    "        trainval.extend(arr[test_k:].tolist())\n",
    "\n",
    "    kept = np.array(kept, dtype=np.int64)\n",
    "    trainval = np.array(trainval, dtype=np.int64)\n",
    "    test = np.array(test, dtype=np.int64)\n",
    "\n",
    "    print(f\"[INFO] STRICT balance: Bmin={Bmin}, test_k={test_k}\")\n",
    "    print(\"[INFO] blocks/class trainval:\", dict(sorted(Counter(y_blocks[trainval].tolist()).items())))\n",
    "    print(\"[INFO] blocks/class test   :\", dict(sorted(Counter(y_blocks[test].tolist()).items())))\n",
    "    return kept, trainval, test\n",
    "\n",
    "def check_block_overlap(train_idx, val_idx, test_idx):\n",
    "    s_tr = set(train_idx.tolist()) if hasattr(train_idx, \"tolist\") else set(train_idx)\n",
    "    s_va = set(val_idx.tolist()) if hasattr(val_idx, \"tolist\") else set(val_idx)\n",
    "    s_te = set(test_idx.tolist()) if hasattr(test_idx, \"tolist\") else set(test_idx)\n",
    "    if (s_tr & s_va) or (s_tr & s_te) or (s_va & s_te):\n",
    "        raise RuntimeError(\"[ERROR] Block overlap detected among train/val/test.\")\n",
    "    print(\"[INFO] Block overlap check passed.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Datasets (XFR blocks)\n",
    "# ----------------------------\n",
    "class XFRTrainPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    X_blocks: (num_blocks, L, m, 2)\n",
    "    One item corresponds to (block_id, t_idx), where t_idx in [0, L).\n",
    "    Each \"sample\" is X_blocks[block_id, t_idx] -> (m,2).\n",
    "    \"\"\"\n",
    "    def __init__(self, X_blocks, y_blocks, block_indices, pos_pair_mode=\"same_block\"):\n",
    "        self.X = X_blocks\n",
    "        self.y = y_blocks\n",
    "        self.block_indices = np.array(block_indices, dtype=np.int64)\n",
    "        self.L = int(X_blocks.shape[1])\n",
    "        self.pos_mode = pos_pair_mode\n",
    "\n",
    "        self.sample_list = []\n",
    "        for bi in self.block_indices.tolist():\n",
    "            for t in range(self.L):\n",
    "                self.sample_list.append((int(bi), int(t)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bi, t1 = self.sample_list[idx]\n",
    "        y = int(self.y[bi])\n",
    "\n",
    "        iq1 = self.X[bi, t1]  # (m,2)\n",
    "\n",
    "        if self.pos_mode == \"same_block\":\n",
    "            t2 = random.randrange(self.L)\n",
    "            while t2 == t1 and self.L > 1:\n",
    "                t2 = random.randrange(self.L)\n",
    "            iq2 = self.X[bi, t2]  # (m,2)\n",
    "        else:\n",
    "            iq2 = iq1.copy()\n",
    "\n",
    "        return iq1.astype(np.float32), iq2.astype(np.float32), np.int64(y)\n",
    "\n",
    "class XFRSingleDataset(Dataset):\n",
    "    def __init__(self, X_blocks, y_blocks, block_indices):\n",
    "        self.X = X_blocks\n",
    "        self.y = y_blocks\n",
    "        self.block_indices = np.array(block_indices, dtype=np.int64)\n",
    "        self.L = int(X_blocks.shape[1])\n",
    "\n",
    "        self.sample_list = []\n",
    "        for bi in self.block_indices.tolist():\n",
    "            for t in range(self.L):\n",
    "                self.sample_list.append((int(bi), int(t)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bi, t = self.sample_list[idx]\n",
    "        y = int(self.y[bi])\n",
    "        iq = self.X[bi, t]  # (m,2)\n",
    "        return iq.astype(np.float32), np.int64(y)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) GPU augmentations\n",
    "# ----------------------------\n",
    "def _to_complex(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    return iq_b[..., 0].to(torch.float32) + 1j * iq_b[..., 1].to(torch.float32)\n",
    "\n",
    "def _from_complex(sig: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.stack([sig.real, sig.imag], dim=-1)\n",
    "\n",
    "def batch_normalize_power(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    power = (iq_b[..., 0] ** 2 + iq_b[..., 1] ** 2).mean(dim=1, keepdim=True) + 1e-12\n",
    "    scale = torch.rsqrt(power).unsqueeze(-1)\n",
    "    return iq_b * scale\n",
    "\n",
    "def batch_apply_doppler(iq_b: torch.Tensor, v_kmh: torch.Tensor) -> torch.Tensor:\n",
    "    B, L, _ = iq_b.shape\n",
    "    sig = _to_complex(iq_b)\n",
    "    c = 3e8\n",
    "    v = v_kmh / 3.6\n",
    "    fd = (v / c) * FC\n",
    "    n = torch.arange(L, device=iq_b.device, dtype=torch.float32).unsqueeze(0)\n",
    "    phase = torch.exp(1j * 2.0 * np.pi * fd.unsqueeze(1).to(torch.float32) * n / FS)\n",
    "    sig = sig * phase\n",
    "    return _from_complex(sig)\n",
    "\n",
    "def _grouped_conv1d_real(x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    B, _, L = x.shape\n",
    "    x2 = x.permute(1, 0, 2).contiguous()\n",
    "    y2 = F.conv1d(x2, w, padding=w.shape[-1] - 1, groups=B)\n",
    "    return y2.squeeze(0)\n",
    "\n",
    "def batch_apply_multipath(iq_b: torch.Tensor, rms_ns: torch.Tensor, max_taps: int = MAX_TAPS) -> torch.Tensor:\n",
    "    B, L, _ = iq_b.shape\n",
    "    device = iq_b.device\n",
    "\n",
    "    rms_s = rms_ns * 1e-9\n",
    "    rms_samples = (rms_s * FS).clamp(min=1e-3)\n",
    "\n",
    "    k = torch.arange(max_taps, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "    p = torch.exp(-k / rms_samples.unsqueeze(1))\n",
    "    p = p / (p.sum(dim=1, keepdim=True) + 1e-12)\n",
    "\n",
    "    hr = torch.randn(B, max_taps, device=device) * torch.sqrt(p / 2.0)\n",
    "    hi = torch.randn(B, max_taps, device=device) * torch.sqrt(p / 2.0)\n",
    "\n",
    "    hpow = (hr**2 + hi**2).sum(dim=1, keepdim=True) + 1e-12\n",
    "    norm = torch.rsqrt(hpow)\n",
    "    hr = hr * norm\n",
    "    hi = hi * norm\n",
    "\n",
    "    xr = iq_b[..., 0]\n",
    "    xi = iq_b[..., 1]\n",
    "    xr_ = xr.unsqueeze(1)\n",
    "    xi_ = xi.unsqueeze(1)\n",
    "    hr_ = hr.unsqueeze(1)\n",
    "    hi_ = hi.unsqueeze(1)\n",
    "\n",
    "    xr_hr = _grouped_conv1d_real(xr_, hr_)\n",
    "    xi_hi = _grouped_conv1d_real(xi_, hi_)\n",
    "    xr_hi = _grouped_conv1d_real(xr_, hi_)\n",
    "    xi_hr = _grouped_conv1d_real(xi_, hr_)\n",
    "\n",
    "    yr = xr_hr - xi_hi\n",
    "    yi = xr_hi + xi_hr\n",
    "    yr = yr[:, :L]\n",
    "    yi = yi[:, :L]\n",
    "    return torch.stack([yr, yi], dim=-1)\n",
    "\n",
    "def batch_add_awgn(iq_b: torch.Tensor, snr_db: torch.Tensor) -> torch.Tensor:\n",
    "    B, L, _ = iq_b.shape\n",
    "    sig = _to_complex(iq_b)\n",
    "    p = (sig.real**2 + sig.imag**2).mean(dim=1) + 1e-12\n",
    "    npow = p / (10.0 ** (snr_db / 10.0))\n",
    "    std = torch.sqrt(npow / 2.0).unsqueeze(1)\n",
    "    noise = std * (torch.randn(B, L, device=iq_b.device) + 1j * torch.randn(B, L, device=iq_b.device))\n",
    "    sig = sig + noise\n",
    "    return _from_complex(sig)\n",
    "\n",
    "def augment_train_batch(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    iq_b = batch_normalize_power(iq_b)\n",
    "    B = iq_b.shape[0]\n",
    "\n",
    "    if AUG_USE_MULTIPATH:\n",
    "        rms = (RMS_DS_NS_RANGE[1] - RMS_DS_NS_RANGE[0]) * torch.rand(B, device=iq_b.device) + RMS_DS_NS_RANGE[0]\n",
    "        iq_b = batch_apply_multipath(iq_b, rms, max_taps=MAX_TAPS)\n",
    "\n",
    "    if AUG_USE_DOPPLER:\n",
    "        vmin, vmax = TRAIN_V_KMH_RANGE\n",
    "        if abs(vmax - vmin) < 1e-12:\n",
    "            v = torch.full((B,), float(vmin), device=iq_b.device)\n",
    "        else:\n",
    "            v = (vmax - vmin) * torch.rand(B, device=iq_b.device) + vmin\n",
    "        iq_b = batch_apply_doppler(iq_b, v)\n",
    "\n",
    "    if AUG_USE_AWGN:\n",
    "        smin, smax = TRAIN_SNR_DB_RANGE\n",
    "        snr = (smax - smin) * torch.rand(B, device=iq_b.device) + smin\n",
    "        iq_b = batch_add_awgn(iq_b, snr)\n",
    "\n",
    "    return iq_b\n",
    "\n",
    "def augment_test_batch(iq_b: torch.Tensor, snr_db: float) -> torch.Tensor:\n",
    "    iq_b = batch_normalize_power(iq_b)\n",
    "    B = iq_b.shape[0]\n",
    "\n",
    "    if TEST_USE_MULTIPATH:\n",
    "        rms = (TEST_RMS_DS_NS_RANGE[1] - TEST_RMS_DS_NS_RANGE[0]) * torch.rand(B, device=iq_b.device) + TEST_RMS_DS_NS_RANGE[0]\n",
    "        iq_b = batch_apply_multipath(iq_b, rms, max_taps=MAX_TAPS)\n",
    "\n",
    "    v = torch.full((B,), float(TEST_V_KMH_FIXED), device=iq_b.device)\n",
    "    iq_b = batch_apply_doppler(iq_b, v)\n",
    "\n",
    "    snr = torch.full((B,), float(snr_db), device=iq_b.device)\n",
    "    iq_b = batch_add_awgn(iq_b, snr)\n",
    "    return iq_b\n",
    "\n",
    "# ----------------------------\n",
    "# 6) GPU STFT\n",
    "# ----------------------------\n",
    "_WINDOW_CACHE = {}\n",
    "def get_hann_window(device: torch.device):\n",
    "    key = (device.type, device.index, SPEC_WIN)\n",
    "    if key not in _WINDOW_CACHE:\n",
    "        _WINDOW_CACHE[key] = torch.hann_window(SPEC_WIN, periodic=True, device=device)\n",
    "    return _WINDOW_CACHE[key]\n",
    "\n",
    "def iq_to_logspec_batch(iq_b: torch.Tensor) -> torch.Tensor:\n",
    "    sig = _to_complex(iq_b).to(torch.complex64)  # (B,L)\n",
    "    win = get_hann_window(iq_b.device)\n",
    "    S = torch.stft(\n",
    "        sig,\n",
    "        n_fft=SPEC_NFFT,\n",
    "        hop_length=SPEC_HOP,\n",
    "        win_length=SPEC_WIN,\n",
    "        window=win,\n",
    "        center=True,\n",
    "        return_complex=True\n",
    "    )  # (B,F,T)\n",
    "\n",
    "    mag = torch.abs(S) + 1e-12\n",
    "    logmag = torch.log(mag)\n",
    "\n",
    "    mu = logmag.mean(dim=(1, 2), keepdim=True)\n",
    "    sd = logmag.std(dim=(1, 2), keepdim=True) + 1e-6\n",
    "    logmag = (logmag - mu) / sd\n",
    "    try:\n",
    "        logmag = torch.nan_to_num(logmag, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    except Exception:\n",
    "        logmag[~torch.isfinite(logmag)] = 0.0\n",
    "\n",
    "    x = logmag.unsqueeze(1)  # (B,1,F,T)\n",
    "    x = F.interpolate(x, size=(SPEC_SIZE, SPEC_SIZE), mode=\"bilinear\", align_corners=False)\n",
    "    return x\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Model\n",
    "# ----------------------------\n",
    "class BasicBlock2D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.down = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.down is not None:\n",
    "            identity = self.down(identity)\n",
    "        return self.relu(out + identity)\n",
    "\n",
    "class SpecFeatureNet(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.b1 = BasicBlock2D(32, 32, stride=1)\n",
    "        self.b2 = BasicBlock2D(32, 32, stride=1)\n",
    "        self.b3 = BasicBlock2D(32, 64, stride=1)\n",
    "        self.b4 = BasicBlock2D(64, 64, stride=1)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(64, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.cls = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.b1(x); x = self.b2(x); x = self.b3(x); x = self.b4(x)\n",
    "        x = self.gap(x).squeeze(-1).squeeze(-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        z = self.fc2(x)\n",
    "        logits = self.cls(z)\n",
    "        return z, logits\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        z1, p1 = self.forward_once(x1)\n",
    "        if x2 is None:\n",
    "            return z1, p1\n",
    "        z2, p2 = self.forward_once(x2)\n",
    "        return z1, p1, z2, p2\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Loss + Eval\n",
    "# ----------------------------\n",
    "def nt_xent_loss(z1: torch.Tensor, z2: torch.Tensor, tau: float = TAU) -> torch.Tensor:\n",
    "    with amp_autocast(enabled=False):\n",
    "        z1 = z1.float()\n",
    "        z2 = z2.float()\n",
    "        N = z1.size(0)\n",
    "        z = torch.cat([z1, z2], dim=0)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        sim = (z @ z.T) / float(tau)\n",
    "        sim.fill_diagonal_(torch.finfo(sim.dtype).min)\n",
    "        pos = torch.arange(2 * N, device=z.device)\n",
    "        pos = (pos + N) % (2 * N)\n",
    "        log_prob = sim - torch.logsumexp(sim, dim=1, keepdim=True)\n",
    "        loss = -log_prob[torch.arange(2 * N, device=z.device), pos]\n",
    "        return loss.mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_single_iq(model: SpecFeatureNet, loader: DataLoader, num_classes: int, mode: str, snr_db: float = None):\n",
    "    model.eval()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    total, correct = 0, 0\n",
    "    loss_sum, nb = 0.0, 0\n",
    "\n",
    "    for iq, y in loader:\n",
    "        iq = iq.to(DEVICE, non_blocking=True)\n",
    "        y  = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        if mode == \"test\":\n",
    "            iq = augment_test_batch(iq, snr_db=float(snr_db))\n",
    "\n",
    "        spec = iq_to_logspec_batch(iq)\n",
    "        _, logits = model(spec, None)\n",
    "        loss = ce(logits, y)\n",
    "\n",
    "        loss_sum += float(loss.item())\n",
    "        nb += 1\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    acc = 100.0 * correct / max(total, 1)\n",
    "    return (loss_sum / max(nb, 1)), acc\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Train one fold\n",
    "# ----------------------------\n",
    "def run_one_fold(\n",
    "    fold: int,\n",
    "    save_folder: str,\n",
    "    results_txt: str,\n",
    "    num_classes: int,\n",
    "    X_blocks: np.ndarray,\n",
    "    y_blocks: np.ndarray,\n",
    "    tr_blocks: np.ndarray,\n",
    "    va_blocks: np.ndarray,\n",
    "    test_loader: DataLoader,\n",
    "    snr_to_accs: dict,\n",
    "    scaler\n",
    "):\n",
    "    write_line(results_txt, f\"\\n========== Fold {fold}/{N_SPLITS} ==========\")\n",
    "\n",
    "    tr_ds = XFRTrainPairDataset(X_blocks, y_blocks, tr_blocks, pos_pair_mode=POS_PAIR_MODE)\n",
    "    va_ds = XFRSingleDataset(X_blocks, y_blocks, va_blocks)\n",
    "\n",
    "    tr_loader = DataLoader(\n",
    "        tr_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True,\n",
    "        num_workers=NUM_WORKERS_TRAIN, pin_memory=(DEVICE.type == \"cuda\")\n",
    "    )\n",
    "    va_loader = DataLoader(\n",
    "        va_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS_EVAL, pin_memory=(DEVICE.type == \"cuda\")\n",
    "    )\n",
    "\n",
    "    model = SpecFeatureNet(num_classes=num_classes).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode=\"min\", factor=LR_FACTOR, patience=LR_PATIENCE\n",
    "    )\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "    es_count = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    fold_log_path = os.path.join(save_folder, f\"fold{fold}_trainlog.csv\")\n",
    "    fold_logger = CSVLogger(\n",
    "        fold_log_path,\n",
    "        header=[\"epoch\", \"lr\", \"train_loss\", \"val_loss\", \"val_acc\", \"best_val_loss\", \"es_count\", \"epoch_time_sec\"]\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        loss_sum, nb = 0.0, 0\n",
    "\n",
    "        for iq1_np, iq2_np, y_np in tr_loader:\n",
    "            iq1 = iq1_np.to(DEVICE, non_blocking=True)  # (B,m,2)\n",
    "            iq2 = iq2_np.to(DEVICE, non_blocking=True)\n",
    "            y   = y_np.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            if POS_PAIR_MODE == \"simclr\":\n",
    "                v1 = augment_train_batch(iq1)\n",
    "                v2 = augment_train_batch(iq1)\n",
    "            else:\n",
    "                cat = torch.cat([iq1, iq2], dim=0)\n",
    "                cat = augment_train_batch(cat)\n",
    "                v1, v2 = cat.chunk(2, dim=0)\n",
    "\n",
    "            spec1 = iq_to_logspec_batch(v1)\n",
    "            spec2 = iq_to_logspec_batch(v2)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            with amp_autocast(enabled=USE_AMP):\n",
    "                z1, p1, z2, p2 = model(spec1, spec2)\n",
    "                loss_cl = nt_xent_loss(z1, z2, tau=TAU)\n",
    "                if LAMBDA_CE > 0.0:\n",
    "                    loss_ce = 0.5 * (ce(p1, y) + ce(p2, y))\n",
    "                else:\n",
    "                    loss_ce = torch.zeros((), device=DEVICE)\n",
    "                loss = LAMBDA_CL * loss_cl + LAMBDA_CE * loss_ce\n",
    "\n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            loss_sum += float(loss.item())\n",
    "            nb += 1\n",
    "\n",
    "        train_loss = loss_sum / max(nb, 1)\n",
    "        val_loss, val_acc = eval_single_iq(model, va_loader, num_classes=num_classes, mode=\"val\")\n",
    "\n",
    "        prev_lr = opt.param_groups[0][\"lr\"]\n",
    "        scheduler.step(val_loss)\n",
    "        cur_lr = opt.param_groups[0][\"lr\"]\n",
    "\n",
    "        epoch_time = time.time() - t0\n",
    "        msg = (f\"Epoch {epoch:03d} | LR={cur_lr:.2e} | \"\n",
    "               f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | ValAcc={val_acc:.2f}% | \"\n",
    "               f\"BestValLoss={best_val_loss:.4f} | ES={es_count}/{ES_PATIENCE}\")\n",
    "        print(msg)\n",
    "        write_line(results_txt, msg)\n",
    "        fold_logger.log_row([epoch, cur_lr, train_loss, val_loss, val_acc, best_val_loss, es_count, epoch_time])\n",
    "\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            best_epoch = epoch\n",
    "            es_count = 0\n",
    "        else:\n",
    "            es_count += 1\n",
    "            if es_count >= ES_PATIENCE:\n",
    "                msg = \"[INFO] Early stopping triggered.\"\n",
    "                print(msg)\n",
    "                write_line(results_txt, msg)\n",
    "                break\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_folder, f\"model_fold{fold}.pth\"))\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        torch.save(model.state_dict(), os.path.join(save_folder, f\"best_model_fold{fold}.pth\"))\n",
    "\n",
    "    write_line(results_txt, f\"[FOLD {fold}] BestEpoch={best_epoch}, BestValLoss={best_val_loss:.6f}\")\n",
    "\n",
    "    # Test sweep\n",
    "    fold_test_csv = os.path.join(save_folder, f\"fold{fold}_test_snr.csv\")\n",
    "    fold_test_logger = CSVLogger(fold_test_csv, header=[\"snr_db\", \"test_loss\", \"test_acc\"])\n",
    "\n",
    "    fold_snr_acc = {}\n",
    "    for snr in TEST_SNR_LIST:\n",
    "        test_loss, test_acc = eval_single_iq(model, test_loader, num_classes=num_classes, mode=\"test\", snr_db=float(snr))\n",
    "        snr_to_accs[snr].append(test_acc)\n",
    "        fold_snr_acc[snr] = test_acc\n",
    "        fold_test_logger.log_row([snr, test_loss, test_acc])\n",
    "\n",
    "    msg = \"[FOLD TEST] \" + \", \".join([f\"{snr}:{fold_snr_acc[snr]:.2f}%\" for snr in TEST_SNR_LIST])\n",
    "    print(msg)\n",
    "    write_line(results_txt, msg)\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Main pipeline\n",
    "# ----------------------------\n",
    "def train_kfold_ltev_xfr_per_file_seq(data_path: str):\n",
    "    X_files, y_files, file_paths, L = load_ltev_files_as_iq(data_path, recursive=RECURSIVE_GLOB)\n",
    "\n",
    "    # Optional fairness: enforce m == L if you want exactly your setting\n",
    "    if GROUP_SIZE != L:\n",
    "        print(f\"[WARN] You set GROUP_SIZE(m)={GROUP_SIZE} but DMRS length L={L}. \"\n",
    "              f\"After XFR transpose, each sample length will be m={GROUP_SIZE}, not L. \"\n",
    "              f\"If you want m=L, set GROUP_SIZE={L}.\")\n",
    "\n",
    "    X_blocks, y_blocks, label_to_idx, block_meta = build_xfr_blocks_per_file_sequential(\n",
    "        X_files, y_files, group_size=GROUP_SIZE\n",
    "    )\n",
    "    num_classes = len(label_to_idx)\n",
    "\n",
    "    kept_idx, trainval_idx, test_idx = balanced_block_split(\n",
    "        y_blocks, test_size=TEST_SIZE, seed=SEED, strict_balance=STRICT_TEST_BALANCE\n",
    "    )\n",
    "\n",
    "    # If strict balance: keep only kept blocks (trim each class to Bmin)\n",
    "    if STRICT_TEST_BALANCE:\n",
    "        X_blocks = X_blocks[kept_idx]\n",
    "        y_blocks = y_blocks[kept_idx]\n",
    "        block_meta = [block_meta[i] for i in kept_idx.tolist()]\n",
    "\n",
    "        old_to_new = {int(old): i for i, old in enumerate(kept_idx.tolist())}\n",
    "        trainval_idx = np.array([old_to_new[int(i)] for i in trainval_idx.tolist()], dtype=np.int64)\n",
    "        test_idx = np.array([old_to_new[int(i)] for i in test_idx.tolist()], dtype=np.int64)\n",
    "\n",
    "    # Save folder\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    save_dir = f\"{timestamp}_{SCRIPT_NAME}_m{GROUP_SIZE}_TestBal{int(STRICT_TEST_BALANCE)}_{POS_PAIR_MODE}\"\n",
    "    save_folder = os.path.join(SAVE_ROOT, save_dir)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    results_txt = os.path.join(save_folder, \"results.txt\")\n",
    "\n",
    "    # Config dump\n",
    "    fd_test = (TEST_V_KMH_FIXED / 3.6) / 3e8 * FC\n",
    "    with open(os.path.join(save_folder, \"config.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"DEVICE={DEVICE}\\nAMP={USE_AMP}\\n\")\n",
    "        f.write(f\"DATA_PATH={data_path}\\nRECURSIVE_GLOB={RECURSIVE_GLOB}\\n\")\n",
    "        f.write(f\"DMRS_LEN(L)={L}\\n\")\n",
    "        f.write(f\"GROUP_SIZE(m)={GROUP_SIZE}\\n\")\n",
    "        f.write(f\"X_blocks shape={X_blocks.shape} (num_blocks, L, m, 2)\\n\")\n",
    "        f.write(f\"num_classes={num_classes}\\n\")\n",
    "        f.write(f\"BLOCK_SPLIT=True | TEST_SIZE={TEST_SIZE} | STRICT_TEST_BALANCE={STRICT_TEST_BALANCE}\\n\")\n",
    "        f.write(f\"POS_PAIR_MODE={POS_PAIR_MODE}\\n\")\n",
    "        f.write(f\"BATCH_SIZE={BATCH_SIZE}, LR={LR}, WEIGHT_DECAY={WEIGHT_DECAY}\\n\")\n",
    "        f.write(f\"MAX_EPOCHS={MAX_EPOCHS}, N_SPLITS={N_SPLITS}\\n\")\n",
    "        f.write(f\"LR_PATIENCE={LR_PATIENCE}, LR_FACTOR={LR_FACTOR}, ES_PATIENCE={ES_PATIENCE}\\n\")\n",
    "        f.write(f\"TAU={TAU}, LAMBDA_CL={LAMBDA_CL}, LAMBDA_CE={LAMBDA_CE}\\n\")\n",
    "        f.write(f\"FS={FS}, FC={FC}\\n\")\n",
    "        f.write(f\"AUG_USE_MULTIPATH={AUG_USE_MULTIPATH}, RMS_DS_NS_RANGE={RMS_DS_NS_RANGE}, MAX_TAPS={MAX_TAPS}\\n\")\n",
    "        f.write(f\"AUG_USE_DOPPLER={AUG_USE_DOPPLER}, TRAIN_V_KMH_RANGE={TRAIN_V_KMH_RANGE}\\n\")\n",
    "        f.write(f\"AUG_USE_AWGN={AUG_USE_AWGN}, TRAIN_SNR_DB_RANGE={TRAIN_SNR_DB_RANGE}\\n\")\n",
    "        f.write(f\"TEST_V_KMH_FIXED={TEST_V_KMH_FIXED}, fd_test={fd_test}\\n\")\n",
    "        f.write(f\"TEST_USE_MULTIPATH={TEST_USE_MULTIPATH}, TEST_SNR_LIST={TEST_SNR_LIST}\\n\")\n",
    "        f.write(f\"SPEC_NFFT={SPEC_NFFT}, SPEC_WIN={SPEC_WIN}, SPEC_HOP={SPEC_HOP}, SPEC_SIZE={SPEC_SIZE}\\n\")\n",
    "\n",
    "    print(f\"[INFO] DEVICE={DEVICE} | AMP={USE_AMP}\")\n",
    "    print(f\"[INFO] X_blocks shape={X_blocks.shape} (num_blocks, L, m, 2)\")\n",
    "    print(f\"[INFO] Classes={num_classes}, TrainValBlocks={len(trainval_idx)}, TestBlocks={len(test_idx)}\")\n",
    "    print(f\"[INFO] SaveFolder: {save_folder}\")\n",
    "\n",
    "    write_line(results_txt, f\"DEVICE={DEVICE}, AMP={USE_AMP}\")\n",
    "    write_line(results_txt, f\"X_blocks shape={X_blocks.shape} (num_blocks, L, m, 2)\")\n",
    "    write_line(results_txt, f\"TrainValBlocks={len(trainval_idx)}, TestBlocks={len(test_idx)}\")\n",
    "    write_line(results_txt, f\"POS_PAIR_MODE={POS_PAIR_MODE}\")\n",
    "    write_line(results_txt, \"-\" * 80)\n",
    "\n",
    "    # Fixed test loader\n",
    "    test_ds = XFRSingleDataset(X_blocks, y_blocks, test_idx)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS_EVAL, pin_memory=(DEVICE.type == \"cuda\")\n",
    "    )\n",
    "\n",
    "    # KFold on trainval blocks (stratified by block label)\n",
    "    y_trainval = y_blocks[trainval_idx]\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    check_block_overlap(trainval_idx, [], test_idx)\n",
    "\n",
    "    snr_to_accs = {snr: [] for snr in TEST_SNR_LIST}\n",
    "    scaler = make_grad_scaler(enabled=USE_AMP)\n",
    "\n",
    "    for fold, (tr_pos, va_pos) in enumerate(skf.split(trainval_idx, y_trainval), 1):\n",
    "        tr_blocks = trainval_idx[tr_pos]\n",
    "        va_blocks = trainval_idx[va_pos]\n",
    "        check_block_overlap(tr_blocks, va_blocks, test_idx)\n",
    "\n",
    "        c_tr = Counter(y_blocks[tr_blocks].tolist())\n",
    "        c_va = Counter(y_blocks[va_blocks].tolist())\n",
    "        print(f\"\\n========== Fold {fold}/{N_SPLITS} ==========\")\n",
    "        print(f\"[FOLD {fold}] train_blocks/class:\", dict(sorted(c_tr.items())))\n",
    "        print(f\"[FOLD {fold}] val_blocks/class  :\", dict(sorted(c_va.items())))\n",
    "\n",
    "        run_one_fold(\n",
    "            fold=fold,\n",
    "            save_folder=save_folder,\n",
    "            results_txt=results_txt,\n",
    "            num_classes=num_classes,\n",
    "            X_blocks=X_blocks,\n",
    "            y_blocks=y_blocks,\n",
    "            tr_blocks=tr_blocks,\n",
    "            va_blocks=va_blocks,\n",
    "            test_loader=test_loader,\n",
    "            snr_to_accs=snr_to_accs,\n",
    "            scaler=scaler\n",
    "        )\n",
    "\n",
    "    # Summarize sweep\n",
    "    rows = []\n",
    "    for snr in TEST_SNR_LIST:\n",
    "        arr = np.array(snr_to_accs[snr], dtype=np.float64)\n",
    "        mean = float(arr.mean()) if arr.size else 0.0\n",
    "        std  = float(arr.std()) if arr.size else 0.0\n",
    "        rows.append([snr, mean, std] + snr_to_accs[snr])\n",
    "\n",
    "    csv_path = os.path.join(save_folder, \"test_snr_sweep.csv\")\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        header = [\"snr_db\", \"acc_mean\", \"acc_std\"] + [f\"fold{i}\" for i in range(1, N_SPLITS + 1)]\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    write_line(results_txt, \"\\n========== Overall Test SNR Sweep (mean±std over folds) ==========\")\n",
    "    for snr in TEST_SNR_LIST:\n",
    "        arr = np.array(snr_to_accs[snr], dtype=np.float64)\n",
    "        mean = float(arr.mean()) if arr.size else 0.0\n",
    "        std  = float(arr.std()) if arr.size else 0.0\n",
    "        write_line(results_txt, f\"SNR {snr:>3} dB | Acc {mean:.2f} ± {std:.2f}\")\n",
    "\n",
    "    print(f\"\\n[INFO] All saved in: {save_folder}\")\n",
    "    print(f\"[INFO] SNR sweep CSV: {csv_path}\")\n",
    "    return save_folder\n",
    "\n",
    "# ----------------------------\n",
    "# 11) main\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_kfold_ltev_xfr_per_file_seq(DATA_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
