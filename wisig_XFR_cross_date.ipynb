{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae7865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# run_date_cross_experiments.py\n",
    "# å®Œæ•´ï¼šæŒ‰å•æ—¥è®­ç»ƒ/å•æ—¥æµ‹è¯•ï¼ˆ4å¤© -> 16 å®éªŒï¼‰è‡ªåŠ¨åŒ–è„šæœ¬\n",
    "# 2025-xx-xx\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import load\n",
    "from data_utilities import *   # ä½ åŸæ¥çš„å·¥å…·å‡½æ•°\n",
    "import gc\n",
    "import h5py\n",
    "\n",
    "# -------------------- ç”¨æˆ·å¯é…ç½®é¡¹ --------------------\n",
    "dataset_name = 'ManySig'\n",
    "dataset_path = '../ManySig.pkl/'   # è¯·æ ¹æ®æœ¬åœ°è·¯å¾„ä¿®æ”¹\n",
    "# è‹¥ compact pickle çš„åŠ è½½å‡½æ•°ä¸åŒï¼Œè¯·è°ƒæ•´load_compact_pkl_datasetçš„è°ƒç”¨\n",
    "\n",
    "# è®­ç»ƒ/æ•°æ®å‚æ•°\n",
    "EQUALIZED = 0\n",
    "MAX_SIG = None         # æ¯ä¸ª TX-RX-æ—¥æœŸæœ€å¤šä½¿ç”¨ä¿¡å·æ•°ï¼ˆNone è¡¨ç¤ºä¸æˆªæ–­ï¼‰\n",
    "BLOCK_SIZE = 240       # æ¯ä¸ª block çš„æ ·æœ¬æ•°ï¼ˆæ¨¡å‹æœŸæœ›é•¿åº¦ï¼‰\n",
    "Y = 5                  # æ¯æ¬¡ä»åŒä¸€ä¸ª RX æŠ½å–çš„ä¿¡å·æ•°\n",
    "N_SPLITS = 5\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 5\n",
    "MIN_DELTA = 0.1  # early stopping çš„æœ€å°ç™¾åˆ†æ¯”ç‚¹æ•°æå‡\n",
    "\n",
    "SAVE_ROOT = \"./training_results_date_cross\"\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"[INFO] Device:\", DEVICE)\n",
    "\n",
    "# -------------------- åŠ è½½ compact dataset --------------------\n",
    "print(\"[INFO] Loading compact dataset ...\")\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path, dataset_name)\n",
    "tx_list = compact_dataset['tx_list']\n",
    "rx_list = compact_dataset['rx_list']\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "\n",
    "print(\"[INFO] tx_list:\", tx_list)\n",
    "print(\"[INFO] rx_list:\", rx_list)\n",
    "print(\"[INFO] capture_date_list:\", capture_date_list)\n",
    "\n",
    "# é€‰æ‹©è¦åšå•æ—¥äº¤å‰çš„å‰ 4 å¤©ï¼ˆå¦‚æœä¸è¶³ 4 å¤©ï¼Œåˆ™å–å…¨éƒ¨ï¼‰\n",
    "num_days = len(capture_date_list)\n",
    "dates_to_use = capture_date_list[:num_days]\n",
    "print(f\"[INFO] Using {len(dates_to_use)} dates for cross experiments:\", dates_to_use)\n",
    "\n",
    "# -------------------- æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼ˆå•æ—¥è®­ç»ƒ / å•æ—¥æµ‹è¯•ï¼‰ --------------------\n",
    "def preprocess_dataset_cross_IQ_blocks_single_date_per_rx_cyclic(compact_dataset, tx_list, train_dates, test_dates,\n",
    "                                                                 max_sig=None, equalized=0, block_size=240, y=5):\n",
    "    \"\"\"\n",
    "    ä» compact_dataset ä¸­æŒ‰æŒ‡å®š train_dates ä¸ test_dates åˆ†åˆ«æå–æ ·æœ¬ã€‚\n",
    "    åŒ RX å‘¨æœŸæ€§æŠ½æ ·æ‹¼æ¥æˆ blocksï¼ˆblock_sizeï¼‰ï¼Œç„¶åè½¬ç½®å¾—åˆ°æ¯ä¸ªæ ·æœ¬é•¿åº¦ä¸º block_sizeï¼Œé€šé“ä¸º I/Qã€‚\n",
    "    è¿”å›ï¼šX_train (N_train, block_size, 2), y_train (N_train,), X_test, y_test\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    def extract_samples_for_dates(dates):\n",
    "        X = []\n",
    "        y_labels = []\n",
    "        # tx_list çš„é¡ºåºå†³å®šç±»æ ‡ç­¾\n",
    "        for tx_idx, tx in enumerate(tx_list):\n",
    "            tx_i = compact_dataset['tx_list'].index(tx)\n",
    "            # equalized æ‰¾ index\n",
    "            if equalized in compact_dataset['equalized_list']:\n",
    "                eq_i = compact_dataset['equalized_list'].index(equalized)\n",
    "            else:\n",
    "                eq_i = 0\n",
    "            for date in dates:\n",
    "                if date not in compact_dataset['capture_date_list']:\n",
    "                    # è·³è¿‡æ²¡æœ‰çš„æ—¥æœŸ\n",
    "                    continue\n",
    "                date_i = compact_dataset['capture_date_list'].index(date)\n",
    "\n",
    "                # æ”¶é›†æ¯ä¸ª RX çš„ä¿¡å·åˆ—è¡¨ï¼ˆå¹¶æ‰“ä¹±é¡ºåºï¼‰\n",
    "                rx_signals = []\n",
    "                for rx_i in range(len(compact_dataset['rx_list'])):\n",
    "                    # å¯èƒ½æ•°æ®ä¸ºç©ºæˆ–å½¢çŠ¶ä¸åŒï¼Œéœ€å®‰å…¨å–\n",
    "                    sig_data = compact_dataset['data'][tx_i][rx_i][date_i][eq_i]\n",
    "                    if max_sig is not None:\n",
    "                        sig_data = sig_data[:max_sig]\n",
    "                    # å°† array è½¬ä¸º listï¼Œä¾¿äº pop\n",
    "                    sig_list = list(sig_data.copy())\n",
    "                    # éšæœºåŒ–é¡ºåº\n",
    "                    np.random.shuffle(sig_list)\n",
    "                    rx_signals.append(sig_list)\n",
    "\n",
    "                num_rx = len(rx_signals)\n",
    "                rx_pointer = 0\n",
    "                accum_block = []\n",
    "\n",
    "                # å¾ªç¯ç›´åˆ°æ‰€æœ‰ rx çš„ä¿¡å·è€—å°½\n",
    "                while any(len(sig_list) > 0 for sig_list in rx_signals):\n",
    "                    rx_idx = rx_pointer % num_rx\n",
    "                    sig_list = rx_signals[rx_idx]\n",
    "                    if len(sig_list) > 0:\n",
    "                        take_n = min(y, len(sig_list))\n",
    "                        # é¡ºåºå–å‡º take_n ä¸ª\n",
    "                        sampled = [sig_list.pop(0) for _ in range(take_n)]\n",
    "                        accum_block.extend(sampled)\n",
    "                    rx_pointer += 1\n",
    "\n",
    "                    # å½“ç´¯ç§¯å¤Ÿä¸€ä¸ª block æ—¶ï¼Œåˆ‡åˆ†å¹¶æŠŠæ¯ä¸ªé‡‡æ ·ç‚¹å½“æˆä¸€ä¸ªæ ·æœ¬ï¼ˆtransposeï¼‰\n",
    "                    while len(accum_block) >= block_size:\n",
    "                        block_chunk = accum_block[:block_size]\n",
    "                        accum_block = accum_block[block_size:]\n",
    "                        block_array = np.array(block_chunk)  # (block_size, sample_len, 2) â€” æ³¨æ„ sample_len æ˜¯åŸå•ä¸ªä¿¡å·é•¿åº¦\n",
    "                        # è¿™é‡ŒåŸå§‹ä¿¡å·é•¿åº¦ï¼ˆä¾‹å¦‚ 256ï¼‰å¯èƒ½åœ¨ç¬¬ 1 ç»´ï¼Œå› æ­¤éœ€è¦è½¬ç½®ä¸º (sample_len, block_size, 2)\n",
    "                        # åŸå§‹ä¿¡å·é€šå¸¸æ˜¯ (L,2) å•ä¿¡å·ã€‚ block_array -> (block_size, L, 2)\n",
    "                        # æˆ‘ä»¬è¦æŠŠæ¯ä¸ªé‡‡æ ·ç‚¹ï¼ˆ0..L-1ï¼‰å½“æˆæ ·æœ¬ï¼šshape -> (L, block_size, 2) -> ç„¶åæŠŠæ¯ä¸ª j å½“ä½œä¸€ä¸ªæ ·æœ¬\n",
    "                        block_transposed = block_array.transpose(1, 0, 2)  # (L, block_size, 2)\n",
    "                        # å¯¹äºæ¯ä¸ªé‡‡æ ·ç‚¹ j, æ·»åŠ æ ·æœ¬é•¿åº¦ä¸º block_size çš„æ ·æœ¬ï¼Œé€šé“ I/Q\n",
    "                        for j in range(block_transposed.shape[0]):\n",
    "                            X.append(block_transposed[j])\n",
    "                            y_labels.append(tx_idx)\n",
    "                # ä¸¢å¼ƒå‰©ä½™çš„ accum_blockï¼ˆä¸è¶³ block_sizeï¼‰\n",
    "                accum_block = []\n",
    "\n",
    "        if len(X) == 0:\n",
    "            return np.zeros((0, block_size, 2), dtype=np.float32), np.zeros((0,), dtype=np.int64)\n",
    "        return np.array(X, dtype=np.float32), np.array(y_labels, dtype=np.int64)\n",
    "\n",
    "    X_train, y_train = extract_samples_for_dates(train_dates)\n",
    "    X_test, y_test = extract_samples_for_dates(test_dates)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# -------------------- æ¨¡å‹å®šä¹‰ï¼šRF1DResidualï¼ˆä¸ä½ ä¹‹å‰çš„ RF1DCNN ç±»ä¼¼ï¼‰ --------------------\n",
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.downsample = None\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class RF1DResNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.3, input_length=BLOCK_SIZE):\n",
    "        super().__init__()\n",
    "        # è¾“å…¥æ˜¯ (B, L, 2) -> è½¬ä¸º (B, 2, L)\n",
    "        self.layer1 = ResidualBlock1D(2, 32, kernel_size=7)\n",
    "        self.pool1 = nn.MaxPool1d(2)  # L -> L/2\n",
    "\n",
    "        self.layer2 = ResidualBlock1D(32, 64, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool1d(2)  # L/4\n",
    "\n",
    "        self.layer3 = ResidualBlock1D(64, 128, kernel_size=5)\n",
    "        self.pool3 = nn.MaxPool1d(2)  # L/8\n",
    "\n",
    "        self.layer4 = ResidualBlock1D(128, 256, kernel_size=3)\n",
    "        self.pool4 = nn.MaxPool1d(2)  # L/16\n",
    "\n",
    "        # è®¡ç®— flatten å¤§å°ï¼ˆå‘ä¸‹å–æ•´ï¼‰\n",
    "        L_after = input_length\n",
    "        for _ in range(4):\n",
    "            L_after = (L_after + 1) // 2  # approximate for pool1d/2\n",
    "        self.flatten_dim = 256 * L_after\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flatten_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, 2)\n",
    "        x = x.permute(0, 2, 1)  # -> (B, 2, L)\n",
    "        x = self.layer1(x); x = self.pool1(x)\n",
    "        x = self.layer2(x); x = self.pool2(x)\n",
    "        x = self.layer3(x); x = self.pool3(x)\n",
    "        x = self.layer4(x); x = self.pool4(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# -------------------- è®­ç»ƒä¸è¯„ä¼°è¾…åŠ©å‡½æ•° --------------------\n",
    "def compute_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm += (p.grad.data.norm(2).item()) ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            _, p = torch.max(out, 1)\n",
    "            correct += (p == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "            all_preds.extend(p.cpu().numpy())\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "    return acc, cm\n",
    "\n",
    "def plot_confusion_matrix_save(cm, classes, save_path, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_curves_save(train_losses, val_losses, train_acc, val_acc, save_prefix):\n",
    "    # loss curve\n",
    "    plt.figure(); plt.plot(train_losses, label='Train Loss'); plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(save_prefix + '_loss.png'); plt.close()\n",
    "    # acc curve\n",
    "    plt.figure(); plt.plot(train_acc, label='Train Acc'); plt.plot(val_acc, label='Val Acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(save_prefix + '_acc.png'); plt.close()\n",
    "\n",
    "# -------------------- K-Fold è®­ç»ƒå‡½æ•°ï¼ˆå¹¶ä¿å­˜æ¯ fold çš„æ‰€æœ‰ artefactsï¼‰ --------------------\n",
    "def train_kfold_pointcloud(X_train, y_train, X_test, y_test, num_classes,\n",
    "                           device=DEVICE, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                           lr=LR, weight_decay=WEIGHT_DECAY, n_splits=N_SPLITS,\n",
    "                           patience=PATIENCE, min_delta=MIN_DELTA, save_folder=None,\n",
    "                           script_name=\"date_cross\"):\n",
    "    \"\"\"\n",
    "    X_train: torch.tensor or numpy array (N, L, 2)\n",
    "    y_train: torch.tensor or numpy (N,)\n",
    "    X_test, y_test: same form\n",
    "    save_folder: where to save per-experiment outputs\n",
    "    \"\"\"\n",
    "    if save_folder is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        save_folder = os.path.join(SAVE_ROOT, f\"{timestamp}_{script_name}\")\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    results_file = os.path.join(save_folder, \"results.txt\")\n",
    "\n",
    "    # å°†è¾“å…¥è½¬æ¢ä¸º TensorDatasetï¼ˆå¦‚æœè¿˜ä¸æ˜¯ tensorï¼‰\n",
    "    if not torch.is_tensor(X_train):\n",
    "        X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    else:\n",
    "        X_train_t = X_train\n",
    "    if not torch.is_tensor(y_train):\n",
    "        y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "    else:\n",
    "        y_train_t = y_train\n",
    "    if not torch.is_tensor(X_test):\n",
    "        X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "    else:\n",
    "        X_test_t = X_test\n",
    "    if not torch.is_tensor(y_test):\n",
    "        y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "    else:\n",
    "        y_test_t = y_test\n",
    "\n",
    "    test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # å†™å…¥å®éªŒå‚æ•°å¤´\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(\"=== Experiment Parameters ===\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Device: {DEVICE}\\n\")\n",
    "        f.write(f\"Batch: {batch_size}, Epochs: {epochs}, LR: {lr}, WD: {weight_decay}\\n\")\n",
    "        f.write(f\"K-Fold: {n_splits}, Patience: {patience}\\n\")\n",
    "        f.write(\"============================\\n\\n\")\n",
    "\n",
    "    full_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    indices = np.arange(len(full_dataset))\n",
    "\n",
    "    val_scores, test_scores = [], []\n",
    "\n",
    "    # K-Fold loop\n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"\\n[Experiment] Fold {fold+1}/{n_splits}\")\n",
    "        tr_sub = Subset(full_dataset, tr_idx)\n",
    "        va_sub = Subset(full_dataset, va_idx)\n",
    "        tr_loader = DataLoader(tr_sub, batch_size=batch_size, shuffle=True)\n",
    "        va_loader = DataLoader(va_sub, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = RF1DResNet(num_classes=num_classes, input_length=BLOCK_SIZE).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        best_val = 0.0\n",
    "        best_wts = None\n",
    "        patience_cnt = 0\n",
    "\n",
    "        train_losses, val_losses = [], []\n",
    "        train_accs, val_accs = [], []\n",
    "        avg_grad_list = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss, correct, total = 0.0, 0, 0\n",
    "            total_grad, cnt_grad = 0.0, 0\n",
    "            for xb, yb in tr_loader:\n",
    "                xb = xb.to(device); yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                # grad norm\n",
    "                grad_norms = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]\n",
    "                if grad_norms:\n",
    "                    total_grad += np.mean(grad_norms); cnt_grad += 1\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                _, p = torch.max(out, 1)\n",
    "                correct += (p == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "\n",
    "            avg_grad = total_grad / max(cnt_grad, 1)\n",
    "            avg_grad_list.append(avg_grad)\n",
    "\n",
    "            train_loss = running_loss / max(1, len(tr_loader))\n",
    "            train_acc = 100.0 * correct / max(1, total)\n",
    "            train_losses.append(train_loss); train_accs.append(train_acc)\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            vloss, vcorrect, vtotal = 0.0, 0, 0\n",
    "            all_labels, all_preds = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in va_loader:\n",
    "                    xb = xb.to(device); yb = yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    loss = criterion(out, yb)\n",
    "                    vloss += loss.item()\n",
    "                    _, p = torch.max(out, 1)\n",
    "                    vcorrect += (p == yb).sum().item()\n",
    "                    vtotal += yb.size(0)\n",
    "                    all_labels.extend(yb.cpu().numpy()); all_preds.extend(p.cpu().numpy())\n",
    "\n",
    "            val_loss = vloss / max(1, len(va_loader))\n",
    "            val_acc = 100.0 * vcorrect / max(1, vtotal)\n",
    "            val_losses.append(val_loss); val_accs.append(val_acc)\n",
    "\n",
    "            # æ‰“å°å¹¶å†™å…¥ results_file\n",
    "            line = f\"Fold{fold+1} Epoch{epoch+1} | TrainAcc={train_acc:.2f}% | ValAcc={val_acc:.2f}% | TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | AvgGrad={avg_grad:.4f}\"\n",
    "            print(line)\n",
    "            with open(results_file, 'a') as f:\n",
    "                f.write(line + \"\\n\")\n",
    "\n",
    "            # Early stopping on validation accuracy with min_delta (percentage points)\n",
    "            if val_acc > best_val + min_delta:\n",
    "                best_val = val_acc\n",
    "                best_wts = model.state_dict()\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "                if patience_cnt >= patience:\n",
    "                    print(\"[INFO] Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # restore best weights for this fold\n",
    "        if best_wts is not None:\n",
    "            model.load_state_dict(best_wts)\n",
    "\n",
    "        # ä¿å­˜ train/val æ··æ·†çŸ©é˜µå’Œæ›²çº¿ & æ¨¡å‹\n",
    "        train_acc_fold, train_cm = evaluate_model(model, tr_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'train_cm_fold{fold+1}.npy'), train_cm)\n",
    "        plot_confusion_matrix_save(train_cm, classes=list(range(num_classes)),\n",
    "                                   save_path=os.path.join(save_folder, f'train_cm_fold{fold+1}.png'),\n",
    "                                   title=f'Train CM Fold {fold+1}')\n",
    "\n",
    "        val_acc_fold, val_cm = evaluate_model(model, va_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'val_cm_fold{fold+1}.npy'), val_cm)\n",
    "        plot_confusion_matrix_save(val_cm, classes=list(range(num_classes)),\n",
    "                                   save_path=os.path.join(save_folder, f'val_cm_fold{fold+1}_val.png'),\n",
    "                                   title=f'Val CM Fold {fold+1}')\n",
    "\n",
    "        test_acc_fold, test_cm = evaluate_model(model, test_loader, device, num_classes)\n",
    "        np.save(os.path.join(save_folder, f'test_cm_fold{fold+1}.npy'), test_cm)\n",
    "        plot_confusion_matrix_save(test_cm, classes=list(range(num_classes)),\n",
    "                                   save_path=os.path.join(save_folder, f'test_cm_fold{fold+1}.png'),\n",
    "                                   title=f'Test CM Fold {fold+1}')\n",
    "\n",
    "        # curve plots per fold\n",
    "        plot_curves_save(train_losses, val_losses, train_accs, val_accs, save_prefix=os.path.join(save_folder, f'fold{fold+1}'))\n",
    "\n",
    "        # save model\n",
    "        torch.save(model.state_dict(), os.path.join(save_folder, f'model_fold{fold+1}.pth'))\n",
    "\n",
    "        # append fold metrics\n",
    "        val_scores.append(val_acc); test_scores.append(test_acc_fold)\n",
    "\n",
    "        print(f\"ğŸ”¥ Fold {fold+1} - TEST Accuracy: {test_acc_fold:.4f}\")\n",
    "        \n",
    "        # å†™ fold summary\n",
    "        with open(results_file, 'a') as f:\n",
    "            f.write(f\"\\n=== Fold {fold+1} Summary ===\\n\")\n",
    "            f.write(f\"TrainAcc (last) = {train_acc:.2f}%\\n\")\n",
    "            f.write(f\"ValAcc (best) = {best_val:.2f}%\\n\")\n",
    "            f.write(f\"TestAcc = {test_acc_fold:.2f}%\\n\")\n",
    "            f.write(\"===========================\\n\\n\")\n",
    "\n",
    "        # æ¸…ç†æ˜¾å­˜\n",
    "        del model; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    # overall summary\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(\"\\n=== Overall Summary ===\\n\")\n",
    "        f.write(f\"Val Acc: {np.mean(val_scores):.2f} Â± {np.std(val_scores):.2f}\\n\")\n",
    "        f.write(f\"Test Acc: {np.mean(test_scores):.2f} Â± {np.std(test_scores):.2f}\\n\")\n",
    "        f.write(\"=======================\\n\")\n",
    "    print(\"\\n=== Overall Summary ===\")\n",
    "    print(f\"Val Acc: {np.mean(val_scores):.2f} Â± {np.std(val_scores):.2f}\")\n",
    "    print(f\"Test Acc: {np.mean(test_scores):.2f} Â± {np.std(test_scores):.2f}\")\n",
    "\n",
    "    return os.path.abspath(save_folder)\n",
    "\n",
    "# -------------------- ä¸»æ§åˆ¶æµç¨‹ï¼š16 æ¬¡å®éªŒå¾ªç¯ --------------------\n",
    "def main_all_date_experiments():\n",
    "    # take first num_days dates\n",
    "    dates = dates_to_use\n",
    "    all_experiment_folders = []\n",
    "    for train_date in dates:\n",
    "        for test_date in dates:\n",
    "            # æ„é€  folder åç§°åŒ…å«è®­ç»ƒ/æµ‹è¯•æ—¥æœŸ\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            folder_name = f\"{timestamp}_train{train_date}_test{test_date}\"\n",
    "            save_folder = os.path.join(SAVE_ROOT, folder_name)\n",
    "            os.makedirs(save_folder, exist_ok=True)\n",
    "            results_file = os.path.join(save_folder, \"results.txt\")\n",
    "\n",
    "            with open(results_file, \"a\") as f:\n",
    "                f.write(f\"Experiment Timestamp: {timestamp}\\n\")\n",
    "                f.write(f\"Train Date: {train_date}\\n\")\n",
    "                f.write(f\"Test Date: {test_date}\\n\")\n",
    "                f.write(\"========================\\n\")\n",
    "\n",
    "            print(f\"\\n\\n===== Experiment: train={train_date}  test={test_date}  => folder: {save_folder} =====\\n\")\n",
    "            # é¢„å¤„ç†ï¼šåˆ†åˆ«æŒ‰ train_date å’Œ test_date æå–æ•°æ®\n",
    "            X_train, y_train, X_test, y_test = preprocess_dataset_cross_IQ_blocks_single_date_per_rx_cyclic(\n",
    "                compact_dataset=compact_dataset,\n",
    "                tx_list=tx_list,\n",
    "                train_dates=[train_date],\n",
    "                test_dates=[test_date],\n",
    "                max_sig=MAX_SIG,\n",
    "                equalized=EQUALIZED,\n",
    "                block_size=BLOCK_SIZE,\n",
    "                y=Y\n",
    "            )\n",
    "\n",
    "            # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡\n",
    "            if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "                print(f\"[WARN] Empty train/test for train={train_date} test={test_date}, skipping.\")\n",
    "                with open(results_file, \"a\") as f:\n",
    "                    f.write(\"SKIPPED: empty train/test\\n\")\n",
    "                continue\n",
    "\n",
    "            # æŠŠæ•°æ®æ ‡å‡†åŒ–ä¸º per-sample zero-mean unit-stdï¼ˆæŒ‰ sample çš„ä¸¤ä¸ªé€šé“ä¸€èµ·ï¼‰\n",
    "            def per_sample_normalize(X):\n",
    "                Xn = X.astype(np.float32).copy()\n",
    "                N = Xn.shape[0]\n",
    "                for i in range(N):\n",
    "                    mu = Xn[i].mean(axis=0)   # (2,)\n",
    "                    sigma = Xn[i].std(axis=0)\n",
    "                    sigma[sigma < 1e-8] = 1.0\n",
    "                    Xn[i] = (Xn[i] - mu) / sigma\n",
    "                return Xn\n",
    "            X_train_n = per_sample_normalize(X_train)\n",
    "            X_test_n = per_sample_normalize(X_test)\n",
    "\n",
    "            # è½¬ä¸º torch tensors\n",
    "            X_train_t = torch.tensor(X_train_n, dtype=torch.float32)\n",
    "            y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "            X_test_t = torch.tensor(X_test_n, dtype=torch.float32)\n",
    "            y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "            num_classes = len(np.unique(y_train))\n",
    "            # å†™å…¥ result æ–‡ä»¶ç®€è¦ä¿¡æ¯\n",
    "            with open(results_file, \"a\") as f:\n",
    "                f.write(f\"Num train samples: {len(X_train_t)}\\n\")\n",
    "                f.write(f\"Num test samples: {len(X_test_t)}\\n\")\n",
    "                f.write(f\"Num classes (from train): {num_classes}\\n\")\n",
    "                f.write(\"========================\\n\")\n",
    "\n",
    "            # è®­ç»ƒå¹¶ä¿å­˜\n",
    "            exp_folder = train_kfold_pointcloud(X_train_t, y_train_t, X_test_t, y_test_t,\n",
    "                                               num_classes=num_classes, save_folder=save_folder,\n",
    "                                               script_name=f\"train{train_date}_test{test_date}\")\n",
    "            all_experiment_folders.append(exp_folder)\n",
    "\n",
    "    # summary of all experiments\n",
    "    print(\"\\nAll experiments done. Results folders:\")\n",
    "    for p in all_experiment_folders:\n",
    "        print(p)\n",
    "    return all_experiment_folders\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folders = main_all_date_experiments()\n",
    "    print(\"\\nFinished. Saved experiment folders:\")\n",
    "    for fd in folders:\n",
    "        print(fd)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
